digraph G {
"PaddlePaddle/PaddleSpeech" -> "wenet-e2e/wenet"
"PaddlePaddle/PaddleSpeech" -> "espnet/espnet"
"PaddlePaddle/PaddleSpeech" -> "speechbrain/speechbrain"
"PaddlePaddle/PaddleSpeech" -> "coqui-ai/TTS"
"PaddlePaddle/PaddleSpeech" -> "nl8590687/ASRT_SpeechRecognition"
"PaddlePaddle/PaddleSpeech" -> "jaywalnut310/vits"
"PaddlePaddle/PaddleSpeech" -> "TensorSpeech/TensorFlowTTS"
"PaddlePaddle/PaddleSpeech" -> "PaddlePaddle/PaddleNLP" ["e"=1]
"PaddlePaddle/PaddleSpeech" -> "NVIDIA/NeMo"
"PaddlePaddle/PaddleSpeech" -> "babysor/MockingBird" ["e"=1]
"PaddlePaddle/PaddleSpeech" -> "yeyupiaoling/PPASR" ["e"=1]
"PaddlePaddle/PaddleSpeech" -> "MoonInTheRiver/DiffSinger"
"PaddlePaddle/PaddleSpeech" -> "ming024/FastSpeech2"
"PaddlePaddle/PaddleSpeech" -> "PaddlePaddle/PaddleGAN" ["e"=1]
"PaddlePaddle/PaddleSpeech" -> "mozilla/TTS"
"kaldi-asr/kaldi" -> "espnet/espnet"
"kaldi-asr/kaldi" -> "mravanelli/pytorch-kaldi"
"kaldi-asr/kaldi" -> "mozilla/DeepSpeech"
"kaldi-asr/kaldi" -> "speechbrain/speechbrain"
"kaldi-asr/kaldi" -> "nl8590687/ASRT_SpeechRecognition"
"kaldi-asr/kaldi" -> "facebookresearch/wav2letter"
"kaldi-asr/kaldi" -> "pytorch/fairseq" ["e"=1]
"kaldi-asr/kaldi" -> "pykaldi/pykaldi"
"kaldi-asr/kaldi" -> "wenet-e2e/wenet"
"kaldi-asr/kaldi" -> "zzw922cn/awesome-speech-recognition-speech-synthesis-papers"
"kaldi-asr/kaldi" -> "syhw/wer_are_we"
"kaldi-asr/kaldi" -> "cmusphinx/pocketsphinx"
"kaldi-asr/kaldi" -> "jameslyons/python_speech_features"
"kaldi-asr/kaldi" -> "Uberi/speech_recognition"
"kaldi-asr/kaldi" -> "NVIDIA/NeMo"
"hgneng/ekho" -> "junzew/HanTTS"
"hgneng/ekho" -> "Jackiexiao/MTTS"
"hgneng/ekho" -> "begeekmyfriend/tacotron"
"hgneng/ekho" -> "KuangDD/zhrtvc"
"hgneng/ekho" -> "thuhcsi/Crystal"
"hgneng/ekho" -> "begeekmyfriend/Tacotron-2"
"hgneng/ekho" -> "espeak-ng/espeak-ng"
"hgneng/ekho" -> "ranchlai/mandarin-tts"
"hgneng/ekho" -> "JasonWei512/Tacotron-2-Chinese"
"hgneng/ekho" -> "mmorise/World"
"hgneng/ekho" -> "CSTR-Edinburgh/merlin"
"hgneng/ekho" -> "Kyubyong/tacotron"
"hgneng/ekho" -> "TensorSpeech/TensorFlowTTS"
"hgneng/ekho" -> "xcmyz/FastSpeech"
"hgneng/ekho" -> "keithito/tacotron"
"alphacep/vosk-api" -> "alphacep/vosk-server"
"alphacep/vosk-api" -> "kaldi-asr/kaldi"
"alphacep/vosk-api" -> "coqui-ai/STT"
"alphacep/vosk-api" -> "mozilla/DeepSpeech"
"alphacep/vosk-api" -> "speechbrain/speechbrain"
"alphacep/vosk-api" -> "espnet/espnet"
"alphacep/vosk-api" -> "alphacep/vosk-android-demo"
"alphacep/vosk-api" -> "snakers4/silero-models"
"alphacep/vosk-api" -> "coqui-ai/TTS"
"alphacep/vosk-api" -> "NVIDIA/NeMo"
"alphacep/vosk-api" -> "cmusphinx/pocketsphinx"
"alphacep/vosk-api" -> "alphacep/vosk"
"alphacep/vosk-api" -> "Uberi/speech_recognition"
"alphacep/vosk-api" -> "ideasman42/nerd-dictation"
"alphacep/vosk-api" -> "pyannote/pyannote-audio"
"tyiannak/pyAudioAnalysis" -> "librosa/librosa"
"tyiannak/pyAudioAnalysis" -> "jiaaro/pydub"
"tyiannak/pyAudioAnalysis" -> "jameslyons/python_speech_features"
"tyiannak/pyAudioAnalysis" -> "pyannote/pyannote-audio"
"tyiannak/pyAudioAnalysis" -> "worldveil/dejavu" ["e"=1]
"tyiannak/pyAudioAnalysis" -> "wiseman/py-webrtcvad"
"tyiannak/pyAudioAnalysis" -> "aubio/aubio" ["e"=1]
"tyiannak/pyAudioAnalysis" -> "speechbrain/speechbrain"
"tyiannak/pyAudioAnalysis" -> "MTG/essentia" ["e"=1]
"tyiannak/pyAudioAnalysis" -> "wq2012/awesome-diarization"
"tyiannak/pyAudioAnalysis" -> "kaldi-asr/kaldi"
"tyiannak/pyAudioAnalysis" -> "pytorch/audio"
"tyiannak/pyAudioAnalysis" -> "google/uis-rnn"
"tyiannak/pyAudioAnalysis" -> "espnet/espnet"
"tyiannak/pyAudioAnalysis" -> "tyiannak/paura"
"suno-ai/bark" -> "lucidrains/audiolm-pytorch"
"suno-ai/bark" -> "facebookresearch/encodec"
"suno-ai/bark" -> "NVIDIA/BigVGAN"
"suno-ai/bark" -> "microsoft/NeuralSpeech"
"suno-ai/bark" -> "lifeiteng/vall-e"
"suno-ai/bark" -> "lucidrains/naturalspeech2-pytorch"
"suno-ai/bark" -> "haoheliu/AudioLDM"
"suno-ai/bark" -> "microsoft/SpeechT5"
"suno-ai/bark" -> "NATSpeech/NATSpeech" ["e"=1]
"suno-ai/bark" -> "archinetai/audio-diffusion-pytorch"
"suno-ai/bark" -> "liusongxiang/Large-Audio-Models"
"suno-ai/bark" -> "archinetai/audio-ai-timeline"
"suno-ai/bark" -> "jik876/hifi-gan"
"suno-ai/bark" -> "ming024/FastSpeech2"
"suno-ai/bark" -> "huawei-noah/Speech-Backbones"
"lturing/tacotronv2_wavernn_chinese" -> "JasonWei512/Tacotron-2-Chinese"
"lturing/tacotronv2_wavernn_chinese" -> "KuangDD/zhrtvc"
"lturing/tacotronv2_wavernn_chinese" -> "ranchlai/mandarin-tts"
"lturing/tacotronv2_wavernn_chinese" -> "thuhcsi/Crystal"
"lturing/tacotronv2_wavernn_chinese" -> "KuangDD/zhvoice"
"lturing/tacotronv2_wavernn_chinese" -> "syang1993/gst-tacotron"
"lturing/tacotronv2_wavernn_chinese" -> "kakaobrain/g2pM"
"lturing/tacotronv2_wavernn_chinese" -> "Jackiexiao/MTTS"
"lturing/tacotronv2_wavernn_chinese" -> "ivanvovk/DurIAN"
"lturing/tacotronv2_wavernn_chinese" -> "speechio/chinese_text_normalization"
"lturing/tacotronv2_wavernn_chinese" -> "xcmyz/FastSpeech"
"lturing/tacotronv2_wavernn_chinese" -> "xcmyz/speech-synthesis-paper"
"lturing/tacotronv2_wavernn_chinese" -> "jackaduma/CycleGAN-VC2"
"lturing/tacotronv2_wavernn_chinese" -> "Jackiexiao/zhtts"
"lturing/tacotronv2_wavernn_chinese" -> "jik876/hifi-gan"
"MycroftAI/mimic2" -> "MycroftAI/mimic-recording-studio"
"MycroftAI/mimic2" -> "MycroftAI/mimic1"
"MycroftAI/mimic2" -> "keithito/tacotron"
"MycroftAI/mimic2" -> "MycroftAI/selene-backend" ["e"=1]
"MycroftAI/mimic2" -> "google/voice-builder"
"MycroftAI/mimic2" -> "MycroftAI/mimic" ["e"=1]
"MycroftAI/mimic2" -> "MycroftAI/mycroft-precise" ["e"=1]
"MycroftAI/mimic2" -> "Kyubyong/dc_tts"
"MycroftAI/mimic2" -> "Rayhane-mamah/Tacotron-2"
"MycroftAI/mimic2" -> "r9y9/deepvoice3_pytorch"
"MycroftAI/mimic2" -> "bootphon/phonemizer"
"MycroftAI/mimic2" -> "thorstenMueller/deep-learning-german-tts"
"MycroftAI/mimic2" -> "MycroftAI/adapt" ["e"=1]
"MycroftAI/mimic2" -> "mozilla/TTS"
"MycroftAI/mimic2" -> "NVIDIA/flowtron"
"coqui-ai/TTS" -> "mozilla/TTS"
"coqui-ai/TTS" -> "neonbjb/tortoise-tts" ["e"=1]
"coqui-ai/TTS" -> "coqui-ai/STT"
"coqui-ai/TTS" -> "espnet/espnet"
"coqui-ai/TTS" -> "speechbrain/speechbrain"
"coqui-ai/TTS" -> "CorentinJ/Real-Time-Voice-Cloning" ["e"=1]
"coqui-ai/TTS" -> "NVIDIA/NeMo"
"coqui-ai/TTS" -> "TensorSpeech/TensorFlowTTS"
"coqui-ai/TTS" -> "openai/whisper" ["e"=1]
"coqui-ai/TTS" -> "jaywalnut310/vits"
"coqui-ai/TTS" -> "PaddlePaddle/PaddleSpeech"
"coqui-ai/TTS" -> "NVIDIA/tacotron2"
"coqui-ai/TTS" -> "ggerganov/whisper.cpp" ["e"=1]
"coqui-ai/TTS" -> "ming024/FastSpeech2"
"coqui-ai/TTS" -> "jik876/hifi-gan"
"Baidu-AIP/speech-demo" -> "Baidu-AIP/speech-tts-cors"
"Baidu-AIP/speech-demo" -> "Baidu-AIP/java-sdk"
"Baidu-AIP/speech-demo" -> "Baidu-AIP/python-sdk"
"Baidu-AIP/speech-demo" -> "Baidu-AIP/speech-vad-demo"
"Baidu-AIP/speech-demo" -> "Baidu-AIP/nodejs-sdk"
"Baidu-AIP/speech-demo" -> "ZhengkunTian/OpenTransformer"
"Baidu-AIP/speech-demo" -> "Baidu-AIP/speech_realtime_api"
"Baidu-AIP/speech-demo" -> "Renovamen/Speech-and-Text"
"IceKyrin/sovits_guide" -> "IceKyrin/sovits_f0_infer"
"IceKyrin/sovits_guide" -> "AlexandaJerry/whisper-vits-japanese"
"IceKyrin/sovits_guide" -> "CjangCjengh/vits"
"IceKyrin/sovits_guide" -> "Francis-Komizu/Sovits"
"IceKyrin/sovits_guide" -> "innnky/emotional-vits"
"IceKyrin/sovits_guide" -> "NaruseMioShirakana/MoeSS"
"IceKyrin/sovits_guide" -> "openvpi/audio-slicer"
"IceKyrin/sovits_guide" -> "luoyily/MoeTTS"
"IceKyrin/sovits_guide" -> "zhaohui8969/VST_NetProcess-"
"IceKyrin/sovits_guide" -> "innnky/so-vits-svc"
"IceKyrin/sovits_guide" -> "audeering/w2v2-how-to"
"IceKyrin/sovits_guide" -> "Francis-Komizu/StellaVoiceChanger"
"IceKyrin/sovits_guide" -> "fishaudio/fish-diffusion"
"IceKyrin/sovits_guide" -> "bshall/hubert"
"IceKyrin/sovits_guide" -> "Aloento/CraftVits"
"JOETtheIV/VITS-Paimon" -> "w4123/GenshinVoice"
"JOETtheIV/VITS-Paimon" -> "AlexandaJerry/vits-mandarin-biaobei"
"JOETtheIV/VITS-Paimon" -> "w4123/vits"
"JOETtheIV/VITS-Paimon" -> "innnky/emotional-vits"
"JOETtheIV/VITS-Paimon" -> "jaywalnut310/vits"
"JOETtheIV/VITS-Paimon" -> "yxlllc/DDSP-SVC"
"JOETtheIV/VITS-Paimon" -> "luoyily/MoeTTS"
"JOETtheIV/VITS-Paimon" -> "TheKOG/Gal-Voice-Bot"
"JOETtheIV/VITS-Paimon" -> "CjangCjengh/vits"
"JOETtheIV/VITS-Paimon" -> "Plachtaa/VITS-fast-fine-tuning"
"MuBai-He/ChatWaifu-marai" -> "cjyaddone/ChatWaifuL2D"
"MuBai-He/ChatWaifu-marai" -> "cjyaddone/ChatWaifu"
"MuBai-He/ChatWaifu-marai" -> "RockChinQ/qcg-installer" ["e"=1]
"MuBai-He/ChatWaifu-marai" -> "dominoar/QChatPlugins"
"MuBai-He/ChatWaifu-marai" -> "dominoar/QchatPlugins"
"Stardust-minus/vits" -> "HuanLinMaster/Genshin-VitsWeb"
"innnky/so-vits-svc" -> "svc-develop-team/so-vits-svc"
"innnky/so-vits-svc" -> "jaywalnut310/vits"
"innnky/so-vits-svc" -> "prophesier/diff-svc"
"innnky/so-vits-svc" -> "CjangCjengh/MoeGoe"
"innnky/so-vits-svc" -> "openvpi/DiffSinger"
"innnky/so-vits-svc" -> "Plachtaa/VITS-fast-fine-tuning"
"innnky/so-vits-svc" -> "NaruseMioShirakana/MoeSS"
"innnky/so-vits-svc" -> "luoyily/MoeTTS"
"innnky/so-vits-svc" -> "Anjok07/ultimatevocalremovergui"
"innnky/so-vits-svc" -> "IceKyrin/sovits_guide"
"innnky/so-vits-svc" -> "MoonInTheRiver/DiffSinger"
"innnky/so-vits-svc" -> "innnky/emotional-vits"
"innnky/so-vits-svc" -> "yuyuyzl/EasyVtuber"
"innnky/so-vits-svc" -> "CjangCjengh/vits"
"innnky/so-vits-svc" -> "liujing04/Retrieval-based-Voice-Conversion-WebUI"
"jaywalnut310/vits" -> "CjangCjengh/MoeGoe"
"jaywalnut310/vits" -> "MoonInTheRiver/DiffSinger"
"jaywalnut310/vits" -> "jik876/hifi-gan"
"jaywalnut310/vits" -> "innnky/so-vits-svc"
"jaywalnut310/vits" -> "svc-develop-team/so-vits-svc"
"jaywalnut310/vits" -> "CjangCjengh/vits"
"jaywalnut310/vits" -> "luoyily/MoeTTS"
"jaywalnut310/vits" -> "Plachtaa/VITS-fast-fine-tuning"
"jaywalnut310/vits" -> "ming024/FastSpeech2"
"jaywalnut310/vits" -> "kan-bayashi/ParallelWaveGAN"
"jaywalnut310/vits" -> "innnky/emotional-vits"
"jaywalnut310/vits" -> "w4123/vits"
"jaywalnut310/vits" -> "jaywalnut310/glow-tts"
"jaywalnut310/vits" -> "NVIDIA/tacotron2"
"jaywalnut310/vits" -> "espnet/espnet"
"luoyily/MoeTTS" -> "CjangCjengh/MoeGoe"
"luoyily/MoeTTS" -> "CjangCjengh/tacotron2-japanese"
"luoyily/MoeTTS" -> "CjangCjengh/vits"
"luoyily/MoeTTS" -> "NaruseMioShirakana/MoeSS"
"luoyily/MoeTTS" -> "innnky/emotional-vits"
"luoyily/MoeTTS" -> "CjangCjengh/TTSModels"
"luoyily/MoeTTS" -> "jaywalnut310/vits"
"luoyily/MoeTTS" -> "CjangCjengh/MoeGoe_GUI"
"luoyily/MoeTTS" -> "Francis-Komizu/VITS"
"luoyily/MoeTTS" -> "TheKOG/Gal-Voice-Bot"
"luoyily/MoeTTS" -> "w4123/vits"
"luoyily/MoeTTS" -> "Francis-Komizu/Sovits"
"luoyily/MoeTTS" -> "IceKyrin/sovits_guide"
"luoyily/MoeTTS" -> "Paraworks/vits_with_chatgpt-gpt3"
"luoyily/MoeTTS" -> "innnky/so-vits-svc"
"w4123/vits" -> "w4123/GenshinVoice"
"w4123/vits" -> "jaywalnut310/vits"
"w4123/vits" -> "luoyily/MoeTTS"
"w4123/vits" -> "JOETtheIV/VITS-Paimon"
"w4123/vits" -> "Stardust-minus/vits"
"w4123/vits" -> "HuanLinMaster/Genshin-VitsWeb"
"w4123/vits" -> "TheKOG/Gal-Voice-Bot"
"w4123/vits" -> "CjangCjengh/vits"
"w4123/vits" -> "CjangCjengh/MoeGoe"
"w4123/vits" -> "ctrlcvs/xiaoyao-cvs-plugin" ["e"=1]
"w4123/vits" -> "Yiyuiii/nonebot-plugin-moegoe" ["e"=1]
"w4123/vits" -> "innnky/emotional-vits"
"w4123/vits" -> "jerryuhoo/VTuberTalk"
"w4123/vits" -> "IceKyrin/sovits_guide"
"w4123/vits" -> "wac81/vits_chinese"
"deezer/spleeter" -> "facebookresearch/demucs"
"deezer/spleeter" -> "CorentinJ/Real-Time-Voice-Cloning" ["e"=1]
"deezer/spleeter" -> "iperov/DeepFaceLab" ["e"=1]
"deezer/spleeter" -> "openai/jukebox"
"deezer/spleeter" -> "Anjok07/ultimatevocalremovergui"
"deezer/spleeter" -> "boy1dr/SpleeterGui"
"deezer/spleeter" -> "openai/whisper" ["e"=1]
"deezer/spleeter" -> "mozilla/DeepSpeech"
"deezer/spleeter" -> "tensorflow/magenta" ["e"=1]
"deezer/spleeter" -> "jantic/DeOldify" ["e"=1]
"deezer/spleeter" -> "ExistentialAudio/BlackHole" ["e"=1]
"deezer/spleeter" -> "librosa/librosa"
"deezer/spleeter" -> "deepfakes/faceswap" ["e"=1]
"deezer/spleeter" -> "ytdl-org/youtube-dl" ["e"=1]
"deezer/spleeter" -> "yt-dlp/yt-dlp" ["e"=1]
"DemisEom/SpecAugment" -> "zcaceres/spec_augment"
"DemisEom/SpecAugment" -> "facebookresearch/WavAugment"
"DemisEom/SpecAugment" -> "asteroid-team/torch-audiomentations"
"DemisEom/SpecAugment" -> "iver56/audiomentations"
"DemisEom/SpecAugment" -> "hirofumi0810/neural_sp"
"DemisEom/SpecAugment" -> "s3prl/s3prl"
"DemisEom/SpecAugment" -> "KimJeongSun/SpecAugment_numpy_scipy"
"DemisEom/SpecAugment" -> "kaituoxu/Speech-Transformer"
"DemisEom/SpecAugment" -> "kaituoxu/Listen-Attend-Spell"
"DemisEom/SpecAugment" -> "aliutkus/speechmetrics" ["e"=1]
"DemisEom/SpecAugment" -> "google-research/leaf-audio"
"DemisEom/SpecAugment" -> "mravanelli/SincNet"
"DemisEom/SpecAugment" -> "qiuqiangkong/torchlibrosa"
"DemisEom/SpecAugment" -> "facebookresearch/denoiser" ["e"=1]
"DemisEom/SpecAugment" -> "qiuqiangkong/audioset_tagging_cnn"
"Kaljurand/dictate.js" -> "jcsilva/docker-kaldi-gstreamer-server"
"Kaljurand/dictate.js" -> "alumae/gst-kaldi-nnet2-online"
"Kaljurand/dictate.js" -> "alumae/kaldi-gstreamer-server"
"Kaljurand/dictate.js" -> "Kaljurand/K6nele"
"Kaljurand/dictate.js" -> "dialogflow/asr-server"
"Rayhane-mamah/Tacotron-2" -> "keithito/tacotron"
"Rayhane-mamah/Tacotron-2" -> "r9y9/wavenet_vocoder"
"Rayhane-mamah/Tacotron-2" -> "NVIDIA/tacotron2"
"Rayhane-mamah/Tacotron-2" -> "fatchord/WaveRNN"
"Rayhane-mamah/Tacotron-2" -> "NVIDIA/waveglow"
"Rayhane-mamah/Tacotron-2" -> "mozilla/LPCNet"
"Rayhane-mamah/Tacotron-2" -> "Kyubyong/tacotron"
"Rayhane-mamah/Tacotron-2" -> "r9y9/deepvoice3_pytorch"
"Rayhane-mamah/Tacotron-2" -> "kan-bayashi/ParallelWaveGAN"
"Rayhane-mamah/Tacotron-2" -> "syang1993/gst-tacotron"
"Rayhane-mamah/Tacotron-2" -> "xcmyz/FastSpeech"
"Rayhane-mamah/Tacotron-2" -> "espnet/espnet"
"Rayhane-mamah/Tacotron-2" -> "jik876/hifi-gan"
"Rayhane-mamah/Tacotron-2" -> "CSTR-Edinburgh/merlin"
"Rayhane-mamah/Tacotron-2" -> "descriptinc/melgan-neurips"
"RicherMans/GPV" -> "RicherMans/Datadriven-GPVAD"
"alumae/kaldi-gstreamer-server" -> "jcsilva/docker-kaldi-gstreamer-server"
"alumae/kaldi-gstreamer-server" -> "alumae/gst-kaldi-nnet2-online"
"alumae/kaldi-gstreamer-server" -> "Kaljurand/dictate.js"
"alumae/kaldi-gstreamer-server" -> "YoavRamon/awesome-kaldi"
"alumae/kaldi-gstreamer-server" -> "alumae/kaldi-offline-transcriber"
"alumae/kaldi-gstreamer-server" -> "pykaldi/pykaldi"
"alumae/kaldi-gstreamer-server" -> "srvk/eesen"
"alumae/kaldi-gstreamer-server" -> "XiaoMi/kaldi-onnx"
"alumae/kaldi-gstreamer-server" -> "Kaljurand/K6nele"
"alumae/kaldi-gstreamer-server" -> "dialogflow/asr-server"
"alumae/kaldi-gstreamer-server" -> "mravanelli/pytorch-kaldi"
"alumae/kaldi-gstreamer-server" -> "gooofy/py-kaldi-asr"
"alumae/kaldi-gstreamer-server" -> "cmusphinx/g2p-seq2seq"
"alumae/kaldi-gstreamer-server" -> "gooofy/zamia-speech"
"alumae/kaldi-gstreamer-server" -> "k2-fsa/k2"
"cvqluu/Factorized-TDNN" -> "cvqluu/TDNN"
"cvqluu/Factorized-TDNN" -> "manojpamk/pytorch_xvectors"
"cvqluu/Factorized-TDNN" -> "yuyq96/D-TDNN"
"cvqluu/Factorized-TDNN" -> "lawlict/ECAPA-TDNN"
"cvqluu/Factorized-TDNN" -> "idiap/pkwrap"
"google-research/leaf-audio" -> "denfed/leaf-audio-pytorch"
"google-research/leaf-audio" -> "SarthakYadav/leaf-pytorch"
"google-research/leaf-audio" -> "KinWaiCheuk/nnAudio"
"google-research/leaf-audio" -> "aliutkus/speechmetrics" ["e"=1]
"google-research/leaf-audio" -> "csteinmetz1/auraloss"
"google-research/leaf-audio" -> "YuanGongND/ast"
"google-research/leaf-audio" -> "asteroid-team/torch-audiomentations"
"google-research/leaf-audio" -> "pranaymanocha/PerceptualAudio" ["e"=1]
"google-research/leaf-audio" -> "iver56/audiomentations"
"google-research/leaf-audio" -> "facebookresearch/CPC_audio"
"google-research/leaf-audio" -> "adefossez/julius"
"google-research/leaf-audio" -> "FrancoisGrondin/BIRD" ["e"=1]
"google-research/leaf-audio" -> "facebookresearch/WavAugment"
"google-research/leaf-audio" -> "RetroCirce/Zero_Shot_Audio_Source_Separation"
"google-research/leaf-audio" -> "Spijkervet/torchaudio-augmentations" ["e"=1]
"ibab/tensorflow-wavenet" -> "tomlepaine/fast-wavenet"
"ibab/tensorflow-wavenet" -> "buriburisuri/speech-to-text-wavenet"
"ibab/tensorflow-wavenet" -> "basveeling/wavenet"
"ibab/tensorflow-wavenet" -> "r9y9/wavenet_vocoder"
"ibab/tensorflow-wavenet" -> "Kyubyong/tacotron"
"ibab/tensorflow-wavenet" -> "keithito/tacotron"
"ibab/tensorflow-wavenet" -> "tensorflow/magenta" ["e"=1]
"ibab/tensorflow-wavenet" -> "Rayhane-mamah/Tacotron-2"
"ibab/tensorflow-wavenet" -> "CSTR-Edinburgh/merlin"
"ibab/tensorflow-wavenet" -> "andabi/deep-voice-conversion"
"ibab/tensorflow-wavenet" -> "pannous/tensorflow-speech-recognition"
"ibab/tensorflow-wavenet" -> "NVIDIA/waveglow"
"ibab/tensorflow-wavenet" -> "NVIDIA/tacotron2"
"ibab/tensorflow-wavenet" -> "openai/pixel-cnn" ["e"=1]
"ibab/tensorflow-wavenet" -> "carpedm20/DCGAN-tensorflow" ["e"=1]
"iiscleap/NeuralPlda" -> "cvqluu/dropclass_speaker"
"jefflai108/pytorch-kaldi-neural-speaker-embeddings" -> "funcwj/ge2e-speaker-verification"
"jefflai108/pytorch-kaldi-neural-speaker-embeddings" -> "cvqluu/dropclass_speaker"
"jefflai108/pytorch-kaldi-neural-speaker-embeddings" -> "nii-yamagishilab/multi-speaker-tacotron"
"jefflai108/pytorch-kaldi-neural-speaker-embeddings" -> "mycrazycracy/tf-kaldi-speaker"
"qiuqiangkong/audioset_tagging_cnn" -> "qiuqiangkong/torchlibrosa"
"qiuqiangkong/audioset_tagging_cnn" -> "YuanGongND/ast"
"qiuqiangkong/audioset_tagging_cnn" -> "qiuqiangkong/audioset_classification" ["e"=1]
"qiuqiangkong/audioset_tagging_cnn" -> "kkoutini/PaSST"
"qiuqiangkong/audioset_tagging_cnn" -> "qiuqiangkong/panns_inference"
"qiuqiangkong/audioset_tagging_cnn" -> "iver56/audiomentations"
"qiuqiangkong/audioset_tagging_cnn" -> "asteroid-team/torch-audiomentations"
"qiuqiangkong/audioset_tagging_cnn" -> "lRomul/argus-freesound" ["e"=1]
"qiuqiangkong/audioset_tagging_cnn" -> "yinkalario/General-Purpose-Sound-Recognition-Demo"
"qiuqiangkong/audioset_tagging_cnn" -> "karolpiczak/ESC-50"
"qiuqiangkong/audioset_tagging_cnn" -> "qiuqiangkong/panns_transfer_to_gtzan"
"qiuqiangkong/audioset_tagging_cnn" -> "RetroCirce/HTS-Audio-Transformer"
"qiuqiangkong/audioset_tagging_cnn" -> "MaigoAkisame/cmu-thesis" ["e"=1]
"qiuqiangkong/audioset_tagging_cnn" -> "KinWaiCheuk/nnAudio"
"qiuqiangkong/audioset_tagging_cnn" -> "YuanGongND/ssast"
"sigsep/open-unmix-pytorch" -> "f90/Wave-U-Net" ["e"=1]
"sigsep/open-unmix-pytorch" -> "sigsep/sigsep-mus-db"
"sigsep/open-unmix-pytorch" -> "marl/crepe" ["e"=1]
"sigsep/open-unmix-pytorch" -> "KinWaiCheuk/nnAudio"
"sigsep/open-unmix-pytorch" -> "sigsep/sigsep-mus-eval"
"sigsep/open-unmix-pytorch" -> "sigsep/norbert"
"sigsep/open-unmix-pytorch" -> "facebookresearch/demucs"
"sigsep/open-unmix-pytorch" -> "jordipons/musicnn" ["e"=1]
"sigsep/open-unmix-pytorch" -> "francesclluis/source-separation-wavenet" ["e"=1]
"sigsep/open-unmix-pytorch" -> "csteinmetz1/auraloss"
"sigsep/open-unmix-pytorch" -> "mir-dataset-loaders/mirdata" ["e"=1]
"sigsep/open-unmix-pytorch" -> "mpariente/asteroid" ["e"=1]
"sigsep/open-unmix-pytorch" -> "pfnet-research/meta-tasnet"
"sigsep/open-unmix-pytorch" -> "asteroid-team/asteroid" ["e"=1]
"sigsep/open-unmix-pytorch" -> "magenta/ddsp" ["e"=1]
"zzw922cn/awesome-speech-recognition-speech-synthesis-papers" -> "espnet/espnet"
"zzw922cn/awesome-speech-recognition-speech-synthesis-papers" -> "zzw922cn/Automatic_Speech_Recognition"
"zzw922cn/awesome-speech-recognition-speech-synthesis-papers" -> "mravanelli/pytorch-kaldi"
"zzw922cn/awesome-speech-recognition-speech-synthesis-papers" -> "xcmyz/speech-synthesis-paper"
"zzw922cn/awesome-speech-recognition-speech-synthesis-papers" -> "syhw/wer_are_we"
"zzw922cn/awesome-speech-recognition-speech-synthesis-papers" -> "ming024/FastSpeech2"
"zzw922cn/awesome-speech-recognition-speech-synthesis-papers" -> "speechbrain/speechbrain"
"zzw922cn/awesome-speech-recognition-speech-synthesis-papers" -> "kan-bayashi/ParallelWaveGAN"
"zzw922cn/awesome-speech-recognition-speech-synthesis-papers" -> "wq2012/awesome-diarization"
"zzw922cn/awesome-speech-recognition-speech-synthesis-papers" -> "jik876/hifi-gan"
"zzw922cn/awesome-speech-recognition-speech-synthesis-papers" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"zzw922cn/awesome-speech-recognition-speech-synthesis-papers" -> "jameslyons/python_speech_features"
"zzw922cn/awesome-speech-recognition-speech-synthesis-papers" -> "philipperemy/deep-speaker"
"zzw922cn/awesome-speech-recognition-speech-synthesis-papers" -> "SeanNaren/deepspeech.pytorch"
"zzw922cn/awesome-speech-recognition-speech-synthesis-papers" -> "srvk/eesen"
"KuangDD/zhvoice" -> "KuangDD/zhrtvc"
"KuangDD/zhvoice" -> "KuangDD/aukit"
"KuangDD/zhvoice" -> "lturing/tacotronv2_wavernn_chinese"
"KuangDD/zhvoice" -> "KuangDD/phkit"
"KuangDD/zhvoice" -> "open-speech/speech-aligner"
"KuangDD/zhvoice" -> "kakaobrain/g2pM"
"KuangDD/zhvoice" -> "mindslab-ai/cotatron"
"KuangDD/zhvoice" -> "Helsinki-NLP/prosody"
"KuangDD/zhvoice" -> "thuhcsi/Crystal"
"KuangDD/zhvoice" -> "Tomiinek/Multilingual_Text_to_Speech"
"KuangDD/zhvoice" -> "xcmyz/speech-synthesis-paper"
"KuangDD/zhvoice" -> "xcmyz/FastSpeech"
"KuangDD/zhvoice" -> "ivanvovk/DurIAN"
"KuangDD/zhvoice" -> "speechio/chinese_text_normalization"
"KuangDD/zhvoice" -> "speech-io/chinese_text_normalization"
"JingShing/NovelAI-installation-tutorial" -> "JingShing/NovelAI-4chan-lowvram-ver"
"JingShing/NovelAI-installation-tutorial" -> "JingShing/AI-Drawing-Spell-Generator"
"JingShing/NovelAI-installation-tutorial" -> "JingShing/novelai-colab-ver"
"JingShing/NovelAI-installation-tutorial" -> "JingShing/AI-image-tag-extractor"
"archinetai/audio-ai-timeline" -> "archinetai/audio-diffusion-pytorch"
"archinetai/audio-ai-timeline" -> "haoheliu/AudioLDM"
"archinetai/audio-ai-timeline" -> "lucidrains/audiolm-pytorch"
"archinetai/audio-ai-timeline" -> "lucidrains/musiclm-pytorch"
"archinetai/audio-ai-timeline" -> "LAION-AI/CLAP"
"archinetai/audio-ai-timeline" -> "facebookresearch/encodec"
"archinetai/audio-ai-timeline" -> "NVIDIA/BigVGAN"
"archinetai/audio-ai-timeline" -> "acids-ircam/RAVE"
"archinetai/audio-ai-timeline" -> "teticio/audio-diffusion"
"archinetai/audio-ai-timeline" -> "samim23/polymath"
"archinetai/audio-ai-timeline" -> "csteinmetz1/auraloss"
"archinetai/audio-ai-timeline" -> "csteinmetz1/ai-audio-startups"
"archinetai/audio-ai-timeline" -> "liusongxiang/Large-Audio-Models"
"archinetai/audio-ai-timeline" -> "marcoppasini/musika"
"archinetai/audio-ai-timeline" -> "Harmonai-org/sample-generator"
"riffusion/riffusion-app" -> "riffusion/riffusion"
"riffusion/riffusion-app" -> "riffusion/riffusion-inference"
"riffusion/riffusion-app" -> "haoheliu/AudioLDM"
"riffusion/riffusion-app" -> "enlyth/sd-webui-riffusion"
"riffusion/riffusion-app" -> "Harmonai-org/sample-generator"
"riffusion/riffusion-app" -> "MubertAI/Mubert-Text-to-Music"
"riffusion/riffusion-app" -> "archinetai/audio-diffusion-pytorch"
"riffusion/riffusion-app" -> "lucidrains/musiclm-pytorch"
"riffusion/riffusion-app" -> "Kahsolt/stable-diffusion-webui-prompt-travel" ["e"=1]
"riffusion/riffusion-app" -> "nateraw/stable-diffusion-videos" ["e"=1]
"riffusion/riffusion-app" -> "salu133445/musegan" ["e"=1]
"riffusion/riffusion-app" -> "zero01101/openOutpaint-webUI-extension" ["e"=1]
"riffusion/riffusion-app" -> "lucidrains/audiolm-pytorch"
"riffusion/riffusion-app" -> "sharonzhou/long_stable_diffusion" ["e"=1]
"riffusion/riffusion-app" -> "teticio/audio-diffusion"
"JingShing/ImageAI-colab-ver" -> "JingShing/Encryptor-Decryptor"
"JingShing/ImageAI-colab-ver" -> "JingShing/Sorry-NovelAI"
"JingShing/ImageAI-colab-ver" -> "JingShing/ImgaeAI-installation-tutorial"
"JingShing/NovelAI-4chan-lowvram-ver" -> "JingShing/AI-Drawing-Spell-Generator"
"JingShing/NovelAI-4chan-lowvram-ver" -> "JingShing/NovelAI-installation-tutorial"
"yuyuyzl/EasyVtuber" -> "GunwooHan/EasyVtuber"
"yuyuyzl/EasyVtuber" -> "innnky/so-vits-svc"
"yuyuyzl/EasyVtuber" -> "pkhungurn/talking-head-anime-2-demo"
"yuyuyzl/EasyVtuber" -> "CjangCjengh/MoeGoe"
"yuyuyzl/EasyVtuber" -> "openvpi/DiffSinger"
"yuyuyzl/EasyVtuber" -> "luoyily/MoeTTS"
"yuyuyzl/EasyVtuber" -> "xfgryujk/blivechat" ["e"=1]
"yuyuyzl/EasyVtuber" -> "jaywalnut310/vits"
"yuyuyzl/EasyVtuber" -> "itorr/sakana" ["e"=1]
"yuyuyzl/EasyVtuber" -> "pkhungurn/talking-head-anime-demo"
"yuyuyzl/EasyVtuber" -> "xianfei/SysMocap" ["e"=1]
"yuyuyzl/EasyVtuber" -> "Tsuk1ko/bilibili-live-chat" ["e"=1]
"yuyuyzl/EasyVtuber" -> "CjangCjengh/vits"
"yuyuyzl/EasyVtuber" -> "IceKyrin/sovits_guide"
"yuyuyzl/EasyVtuber" -> "DoodleBears/blivechat" ["e"=1]
"jameslyons/python_speech_features" -> "mravanelli/pytorch-kaldi"
"jameslyons/python_speech_features" -> "wiseman/py-webrtcvad"
"jameslyons/python_speech_features" -> "zzw922cn/awesome-speech-recognition-speech-synthesis-papers"
"jameslyons/python_speech_features" -> "espnet/espnet"
"jameslyons/python_speech_features" -> "pykaldi/pykaldi"
"jameslyons/python_speech_features" -> "SeanNaren/deepspeech.pytorch"
"jameslyons/python_speech_features" -> "librosa/librosa"
"jameslyons/python_speech_features" -> "tyiannak/pyAudioAnalysis"
"jameslyons/python_speech_features" -> "pannous/tensorflow-speech-recognition"
"jameslyons/python_speech_features" -> "zzw922cn/Automatic_Speech_Recognition"
"jameslyons/python_speech_features" -> "mravanelli/SincNet"
"jameslyons/python_speech_features" -> "google/uis-rnn"
"jameslyons/python_speech_features" -> "srvk/eesen"
"jameslyons/python_speech_features" -> "kaldi-asr/kaldi"
"jameslyons/python_speech_features" -> "jtkim-kaist/VAD"
"wiseman/py-webrtcvad" -> "jtkim-kaist/VAD"
"wiseman/py-webrtcvad" -> "pyannote/pyannote-audio"
"wiseman/py-webrtcvad" -> "snakers4/silero-vad"
"wiseman/py-webrtcvad" -> "marsbroshok/VAD-python"
"wiseman/py-webrtcvad" -> "wq2012/awesome-diarization"
"wiseman/py-webrtcvad" -> "mravanelli/pytorch-kaldi"
"wiseman/py-webrtcvad" -> "google/uis-rnn"
"wiseman/py-webrtcvad" -> "pykaldi/pykaldi"
"wiseman/py-webrtcvad" -> "HarryVolek/PyTorch_Speaker_Verification"
"wiseman/py-webrtcvad" -> "microsoft/DNS-Challenge" ["e"=1]
"wiseman/py-webrtcvad" -> "espnet/espnet"
"wiseman/py-webrtcvad" -> "hcmlab/vadnet"
"wiseman/py-webrtcvad" -> "aliutkus/speechmetrics" ["e"=1]
"wiseman/py-webrtcvad" -> "speechbrain/speechbrain"
"wiseman/py-webrtcvad" -> "jameslyons/python_speech_features"
"Anjok07/ultimatevocalremovergui" -> "svc-develop-team/so-vits-svc"
"Anjok07/ultimatevocalremovergui" -> "innnky/so-vits-svc"
"Anjok07/ultimatevocalremovergui" -> "facebookresearch/demucs"
"Anjok07/ultimatevocalremovergui" -> "tsurumeso/vocal-remover"
"Anjok07/ultimatevocalremovergui" -> "openvpi/DiffSinger"
"Anjok07/ultimatevocalremovergui" -> "atelier-anchor/smiley-sans" ["e"=1]
"Anjok07/ultimatevocalremovergui" -> "deezer/spleeter"
"Anjok07/ultimatevocalremovergui" -> "jaywalnut310/vits"
"Anjok07/ultimatevocalremovergui" -> "MoonInTheRiver/DiffSinger"
"Anjok07/ultimatevocalremovergui" -> "babysor/MockingBird" ["e"=1]
"Anjok07/ultimatevocalremovergui" -> "the1812/Bilibili-Evolved" ["e"=1]
"Anjok07/ultimatevocalremovergui" -> "boy1dr/SpleeterGui"
"Anjok07/ultimatevocalremovergui" -> "prophesier/diff-svc"
"Anjok07/ultimatevocalremovergui" -> "xinntao/Real-ESRGAN" ["e"=1]
"Anjok07/ultimatevocalremovergui" -> "AaronFeng753/Waifu2x-Extension-GUI" ["e"=1]
"jpush/jpush-api-java-client" -> "qiniu/java-sdk"
"jpush/jpush-api-java-client" -> "aliyun/aliyun-oss-java-sdk" ["e"=1]
"jpush/jpush-api-java-client" -> "good-life/PushTalk" ["e"=1]
"jpush/jpush-api-java-client" -> "jpush/jmessage-api-java-client"
"jpush/jpush-api-java-client" -> "jpush/jpush-phonegap-plugin" ["e"=1]
"jpush/jpush-api-java-client" -> "easemob/emchat-server-examples"
"jpush/jpush-api-java-client" -> "Baidu-AIP/java-sdk"
"jpush/jpush-api-java-client" -> "PingPlusPlus/pingpp-java" ["e"=1]
"tsurumeso/vocal-remover" -> "Anjok07/ultimatevocalremovergui"
"tsurumeso/vocal-remover" -> "sigsep/open-unmix-pytorch"
"tsurumeso/vocal-remover" -> "asteroid-team/asteroid" ["e"=1]
"tsurumeso/vocal-remover" -> "kuielab/mdx-net-submission"
"tsurumeso/vocal-remover" -> "fishaudio/fish-diffusion"
"tsurumeso/vocal-remover" -> "facebookresearch/demucs"
"tsurumeso/vocal-remover" -> "JeffreyCA/spleeter-web"
"tsurumeso/vocal-remover" -> "kuielab/mdx-net"
"tsurumeso/vocal-remover" -> "haoheliu/voicefixer" ["e"=1]
"tsurumeso/vocal-remover" -> "MasayaKawamura/MB-iSTFT-VITS"
"tsurumeso/vocal-remover" -> "jik876/hifi-gan"
"tsurumeso/vocal-remover" -> "mindslab-ai/nuwave"
"tsurumeso/vocal-remover" -> "lucidrains/musiclm-pytorch"
"tsurumeso/vocal-remover" -> "mindslab-ai/voicefilter" ["e"=1]
"tsurumeso/vocal-remover" -> "Rongjiehuang/ProDiff"
"Kyubyong/dc_tts" -> "Kyubyong/deepvoice3"
"Kyubyong/dc_tts" -> "Kyubyong/tacotron"
"Kyubyong/dc_tts" -> "Rayhane-mamah/Tacotron-2"
"Kyubyong/dc_tts" -> "keithito/tacotron"
"Kyubyong/dc_tts" -> "r9y9/wavenet_vocoder"
"Kyubyong/dc_tts" -> "r9y9/deepvoice3_pytorch"
"Kyubyong/dc_tts" -> "fatchord/WaveRNN"
"Kyubyong/dc_tts" -> "syang1993/gst-tacotron"
"Kyubyong/dc_tts" -> "NVIDIA/tacotron2"
"Kyubyong/dc_tts" -> "mozilla/LPCNet"
"Kyubyong/dc_tts" -> "soobinseo/Transformer-TTS"
"Kyubyong/dc_tts" -> "NVIDIA/waveglow"
"Kyubyong/dc_tts" -> "CSTR-Edinburgh/merlin"
"Kyubyong/dc_tts" -> "xcmyz/FastSpeech"
"Kyubyong/dc_tts" -> "Kyubyong/speaker_adapted_tts"
"Kyubyong/deepvoice3" -> "Kyubyong/tacotron"
"Kyubyong/deepvoice3" -> "Kyubyong/dc_tts"
"Kyubyong/deepvoice3" -> "israelg99/deepvoice"
"Kyubyong/deepvoice3" -> "r9y9/deepvoice3_pytorch"
"Kyubyong/deepvoice3" -> "riverphoenix/tacotron2"
"Kyubyong/deepvoice3" -> "ksw0306/ClariNet"
"Kyubyong/deepvoice3" -> "syang1993/gst-tacotron"
"Kyubyong/deepvoice3" -> "keithito/tacotron"
"Kyubyong/deepvoice3" -> "r9y9/wavenet_vocoder"
"Kyubyong/deepvoice3" -> "Rayhane-mamah/Tacotron-2"
"Kyubyong/deepvoice3" -> "azraelkuan/parallel_wavenet_vocoder"
"Kyubyong/deepvoice3" -> "sotelo/parrot"
"Kyubyong/deepvoice3" -> "facebookresearch/loop"
"Kyubyong/deepvoice3" -> "tiberiu44/TTS-Cube"
"Kyubyong/deepvoice3" -> "begeekmyfriend/tacotron"
"leimao/Voice-Converter-CycleGAN" -> "jackaduma/CycleGAN-VC3"
"leimao/Voice-Converter-CycleGAN" -> "jackaduma/CycleGAN-VC2"
"leimao/Voice-Converter-CycleGAN" -> "ebadawy/voice_conversion"
"leimao/Voice_Converter_CycleGAN" -> "liusongxiang/StarGAN-Voice-Conversion"
"leimao/Voice_Converter_CycleGAN" -> "hujinsen/StarGAN-Voice-Conversion"
"leimao/Voice_Converter_CycleGAN" -> "r9y9/gantts"
"leimao/Voice_Converter_CycleGAN" -> "jjery2243542/voice_conversion"
"leimao/Voice_Converter_CycleGAN" -> "auspicious3000/autovc"
"leimao/Voice_Converter_CycleGAN" -> "jjery2243542/adaptive_voice_conversion"
"leimao/Voice_Converter_CycleGAN" -> "mazzzystar/randomCNN-voice-transfer"
"leimao/Voice_Converter_CycleGAN" -> "k2kobayashi/sprocket"
"leimao/Voice_Converter_CycleGAN" -> "pritishyuvraj/Voice-Conversion-GAN"
"leimao/Voice_Converter_CycleGAN" -> "JeremyCCHsu/vae-npvc"
"leimao/Voice_Converter_CycleGAN" -> "hujinsen/pytorch-StarGAN-VC"
"leimao/Voice_Converter_CycleGAN" -> "TaiChunYen/Pytorch-CycleGAN-VC2"
"leimao/Voice_Converter_CycleGAN" -> "bshall/ZeroSpeech"
"leimao/Voice_Converter_CycleGAN" -> "SamuelBroughton/StarGAN-Voice-Conversion-2"
"leimao/Voice_Converter_CycleGAN" -> "jxzhanggg/nonparaSeq2seqVC_code"
"r9y9/deepvoice3_pytorch" -> "r9y9/wavenet_vocoder"
"r9y9/deepvoice3_pytorch" -> "Rayhane-mamah/Tacotron-2"
"r9y9/deepvoice3_pytorch" -> "keithito/tacotron"
"r9y9/deepvoice3_pytorch" -> "NVIDIA/waveglow"
"r9y9/deepvoice3_pytorch" -> "Kyubyong/deepvoice3"
"r9y9/deepvoice3_pytorch" -> "fatchord/WaveRNN"
"r9y9/deepvoice3_pytorch" -> "SforAiDl/Neural-Voice-Cloning-With-Few-Samples"
"r9y9/deepvoice3_pytorch" -> "NVIDIA/tacotron2"
"r9y9/deepvoice3_pytorch" -> "kan-bayashi/ParallelWaveGAN"
"r9y9/deepvoice3_pytorch" -> "mozilla/LPCNet"
"r9y9/deepvoice3_pytorch" -> "xcmyz/FastSpeech"
"r9y9/deepvoice3_pytorch" -> "Kyubyong/tacotron"
"r9y9/deepvoice3_pytorch" -> "andabi/deep-voice-conversion"
"r9y9/deepvoice3_pytorch" -> "r9y9/gantts"
"r9y9/deepvoice3_pytorch" -> "CSTR-Edinburgh/merlin"
"Turing-Project/AntiFraudChatBot" -> "Shawn-Inspur/Yuan-1.0"
"Turing-Project/AntiFraudChatBot" -> "bigbrother666sh/shezhangbujianle"
"Turing-Project/AntiFraudChatBot" -> "jaywalnut310/vits"
"Turing-Project/AntiFraudChatBot" -> "openvpi/DiffSinger"
"Turing-Project/AntiFraudChatBot" -> "innnky/so-vits-svc"
"Turing-Project/AntiFraudChatBot" -> "wechaty/wechaty" ["e"=1]
"Turing-Project/AntiFraudChatBot" -> "nonebot/nonebot2" ["e"=1]
"Turing-Project/AntiFraudChatBot" -> "EssayKillerBrain/EssayKiller_V2" ["e"=1]
"Turing-Project/AntiFraudChatBot" -> "wechaty/python-wechaty" ["e"=1]
"Turing-Project/AntiFraudChatBot" -> "THUDM/ChatGLM-6B" ["e"=1]
"Turing-Project/AntiFraudChatBot" -> "yuyuyzl/EasyVtuber"
"Turing-Project/AntiFraudChatBot" -> "svc-develop-team/so-vits-svc"
"Turing-Project/AntiFraudChatBot" -> "CjangCjengh/MoeGoe"
"Turing-Project/AntiFraudChatBot" -> "MoonInTheRiver/DiffSinger"
"Turing-Project/AntiFraudChatBot" -> "fuergaosi233/wechat-chatgpt" ["e"=1]
"bynil/sov2ex" -> "sciooga/v2ex-plus"
"bynil/sov2ex" -> "Kenshin/sov2ex"
"Uberi/speech_recognition" -> "mozilla/DeepSpeech"
"Uberi/speech_recognition" -> "pannous/tensorflow-speech-recognition"
"Uberi/speech_recognition" -> "kaldi-asr/kaldi"
"Uberi/speech_recognition" -> "cmusphinx/pocketsphinx"
"Uberi/speech_recognition" -> "zzw922cn/Automatic_Speech_Recognition"
"Uberi/speech_recognition" -> "buriburisuri/speech-to-text-wavenet"
"Uberi/speech_recognition" -> "nl8590687/ASRT_SpeechRecognition"
"Uberi/speech_recognition" -> "jiaaro/pydub"
"Uberi/speech_recognition" -> "tyiannak/pyAudioAnalysis"
"Uberi/speech_recognition" -> "espnet/espnet"
"Uberi/speech_recognition" -> "alphacep/vosk-api"
"Uberi/speech_recognition" -> "librosa/librosa"
"Uberi/speech_recognition" -> "facebookresearch/wav2letter"
"Uberi/speech_recognition" -> "pndurette/gTTS"
"Uberi/speech_recognition" -> "bambocher/pocketsphinx-python"
"google/live-transcribe-speech-engine" -> "android/animation" ["e"=1]
"google/live-transcribe-speech-engine" -> "GoogleCloudPlatform/android-docs-samples" ["e"=1]
"google/live-transcribe-speech-engine" -> "biemster/asr"
"google/live-transcribe-speech-engine" -> "googleapis/java-speech"
"google/live-transcribe-speech-engine" -> "libai3/masr"
"google/live-transcribe-speech-engine" -> "MidCamp/live-captioning"
"google/live-transcribe-speech-engine" -> "ctripcorp/C-OCR" ["e"=1]
"google/live-transcribe-speech-engine" -> "mozilla/androidspeech"
"google/live-transcribe-speech-engine" -> "TensorSpeech/TensorFlowASR"
"google/live-transcribe-speech-engine" -> "google/oboe" ["e"=1]
"google/live-transcribe-speech-engine" -> "wiseman/py-webrtcvad"
"google/live-transcribe-speech-engine" -> "Kaljurand/K6nele"
"google/live-transcribe-speech-engine" -> "facebookresearch/wav2letter"
"google/live-transcribe-speech-engine" -> "noahchalifour/rnnt-speech-recognition"
"google/live-transcribe-speech-engine" -> "jcsilva/docker-kaldi-gstreamer-server"
"hollowstrawberry/kohya-colab" -> "liasece/sd-webui-train-tools"
"StarStringStudio/so-vits-svc" -> "StarStringStudio/diff-svc"
"lmnt-com/wavegrad" -> "ivanvovk/WaveGrad"
"lmnt-com/wavegrad" -> "lmnt-com/diffwave"
"lmnt-com/wavegrad" -> "janvainer/speedyspeech"
"lmnt-com/wavegrad" -> "rishikksh20/VocGAN"
"lmnt-com/wavegrad" -> "yanggeng1995/FB-MelGAN"
"lmnt-com/wavegrad" -> "tianrengao/SqueezeWave"
"lmnt-com/wavegrad" -> "k2kobayashi/crank"
"lmnt-com/wavegrad" -> "nii-yamagishilab/multi-speaker-tacotron"
"microsoft/muzic" -> "salu133445/musegan" ["e"=1]
"microsoft/muzic" -> "Natooz/MidiTok" ["e"=1]
"microsoft/muzic" -> "lucidrains/audiolm-pytorch"
"microsoft/muzic" -> "microsoft/NeuralSpeech"
"microsoft/muzic" -> "lucidrains/musiclm-pytorch"
"microsoft/muzic" -> "magenta/magenta" ["e"=1]
"microsoft/muzic" -> "archinetai/audio-diffusion-pytorch"
"microsoft/muzic" -> "haoheliu/AudioLDM"
"microsoft/muzic" -> "jik876/hifi-gan"
"microsoft/muzic" -> "bytedance/music_source_separation"
"microsoft/muzic" -> "salu133445/muspy" ["e"=1]
"microsoft/muzic" -> "music-x-lab/POP909-Dataset" ["e"=1]
"microsoft/muzic" -> "archinetai/audio-ai-timeline"
"microsoft/muzic" -> "MoonInTheRiver/DiffSinger"
"microsoft/muzic" -> "SJTMusicTeam/Muskits"
"pkhungurn/talking-head-anime-demo" -> "pkhungurn/talking-head-anime-2-demo"
"pkhungurn/talking-head-anime-demo" -> "pkhungurn/talking-head-anime-3-demo"
"pkhungurn/talking-head-anime-demo" -> "pkhungurn/talking-head-anime"
"pkhungurn/talking-head-anime-demo" -> "yuyuyzl/EasyVtuber"
"pkhungurn/talking-head-anime-demo" -> "yiranran/Audio-driven-TalkingFace-HeadPose" ["e"=1]
"pkhungurn/talking-head-anime-demo" -> "TianxingWu/OpenVHead" ["e"=1]
"pkhungurn/talking-head-anime-demo" -> "pkhungurn/talking-head-anime-2"
"pkhungurn/talking-head-anime-demo" -> "nagadomi/lbpcascade_animeface" ["e"=1]
"pkhungurn/talking-head-anime-demo" -> "mchong6/GANsNRoses" ["e"=1]
"pkhungurn/talking-head-anime-demo" -> "luoyily/MoeTTS"
"pkhungurn/talking-head-anime-demo" -> "albertpumarola/GANimation" ["e"=1]
"pkhungurn/talking-head-anime-demo" -> "YadiraF/DECA" ["e"=1]
"pkhungurn/talking-head-anime-demo" -> "kwea123/VTuber_Unity" ["e"=1]
"pkhungurn/talking-head-anime-demo" -> "peterljq/OpenMMD" ["e"=1]
"pkhungurn/talking-head-anime-demo" -> "RimoChan/Vtuber_Tutorial" ["e"=1]
"pytorch/audio" -> "mravanelli/pytorch-kaldi"
"pytorch/audio" -> "KinWaiCheuk/nnAudio"
"pytorch/audio" -> "speechbrain/speechbrain"
"pytorch/audio" -> "iver56/audiomentations"
"pytorch/audio" -> "espnet/espnet"
"pytorch/audio" -> "asteroid-team/torch-audiomentations"
"pytorch/audio" -> "facebookresearch/WavAugment"
"pytorch/audio" -> "lhotse-speech/lhotse"
"pytorch/audio" -> "s3prl/s3prl"
"pytorch/audio" -> "mravanelli/SincNet"
"pytorch/audio" -> "aliutkus/speechmetrics" ["e"=1]
"pytorch/audio" -> "SeanNaren/deepspeech.pytorch"
"pytorch/audio" -> "pyannote/pyannote-audio"
"pytorch/audio" -> "LCAV/pyroomacoustics" ["e"=1]
"pytorch/audio" -> "kan-bayashi/ParallelWaveGAN"
"weirdseed/Vits-Android-ncnn" -> "weirdseed/vits-ncnn-convert-tool"
"weirdseed/Vits-Android-ncnn" -> "Voine/ChatWaifu_Mobile"
"weirdseed/Vits-Android-ncnn" -> "EdVince/diffusers-ncnn" ["e"=1]
"weirdseed/Vits-Android-ncnn" -> "FeiGeChuanShu/CodeFormer-ncnn" ["e"=1]
"Baidu-AIP/java-sdk" -> "Baidu-AIP/speech-demo"
"Baidu-AIP/java-sdk" -> "Baidu-AIP/python-sdk"
"Baidu-AIP/java-sdk" -> "dueros/bot-sdk-java" ["e"=1]
"Baidu-AIP/java-sdk" -> "jpush/jpush-api-java-client"
"Baidu-AIP/java-sdk" -> "aliyun/aliyun-oss-java-sdk" ["e"=1]
"Baidu-AIP/java-sdk" -> "Baidu-AIP/nodejs-sdk"
"Baidu-AIP/java-sdk" -> "qiniu/java-sdk"
"Baidu-AIP/java-sdk" -> "penggle/kaptcha" ["e"=1]
"Baidu-AIP/java-sdk" -> "happyfish100/fastdfs-client-java" ["e"=1]
"nobody132/masr" -> "xxbb1234021/speech_recognition"
"nobody132/masr" -> "yeyupiaoling/MASR" ["e"=1]
"nobody132/masr" -> "yeyupiaoling/PaddlePaddle-DeepSpeech" ["e"=1]
"nobody132/masr" -> "binzhouchn/masr" ["e"=1]
"nobody132/masr" -> "nl8590687/ASRT_SpeechRecognition"
"nobody132/masr" -> "Z-yq/TensorflowASR"
"nobody132/masr" -> "yeyupiaoling/PPASR" ["e"=1]
"nobody132/masr" -> "zw76859420/ASR_Theory"
"nobody132/masr" -> "audier/DeepSpeechRecognition"
"nobody132/masr" -> "xdcesc/my_ch_speech_recognition"
"nobody132/masr" -> "sailist/ASRFrame"
"nobody132/masr" -> "PaddlePaddle/DeepSpeech"
"nobody132/masr" -> "kaituoxu/Speech-Transformer"
"nobody132/masr" -> "KuangDD/zhvoice"
"nobody132/masr" -> "SeanNaren/deepspeech.pytorch"
"andabi/deep-voice-conversion" -> "buriburisuri/speech-to-text-wavenet"
"andabi/deep-voice-conversion" -> "r9y9/deepvoice3_pytorch"
"andabi/deep-voice-conversion" -> "r9y9/wavenet_vocoder"
"andabi/deep-voice-conversion" -> "liusongxiang/StarGAN-Voice-Conversion"
"andabi/deep-voice-conversion" -> "k2kobayashi/sprocket"
"andabi/deep-voice-conversion" -> "MrNothing/AI-Blocks"
"andabi/deep-voice-conversion" -> "keithito/tacotron"
"andabi/deep-voice-conversion" -> "Rayhane-mamah/Tacotron-2"
"andabi/deep-voice-conversion" -> "Kyubyong/tacotron"
"andabi/deep-voice-conversion" -> "ibab/tensorflow-wavenet"
"andabi/deep-voice-conversion" -> "NVIDIA/waveglow"
"andabi/deep-voice-conversion" -> "oarriaga/face_classification"
"andabi/deep-voice-conversion" -> "auspicious3000/autovc"
"andabi/deep-voice-conversion" -> "NVIDIA/tacotron2"
"andabi/deep-voice-conversion" -> "fatchord/WaveRNN"
"astorfi/speechpy" -> "astorfi/TensorFlow-World-Resources" ["e"=1]
"astorfi/speechpy" -> "astorfi/3D-convolutional-speaker-recognition"
"astorfi/speechpy" -> "jameslyons/python_speech_features"
"astorfi/speechpy" -> "JRMeyer/open-speech-corpora"
"astorfi/speechpy" -> "r9y9/pysptk"
"astorfi/speechpy" -> "philipperemy/deep-speaker"
"astorfi/speechpy" -> "ppwwyyxx/speaker-recognition"
"astorfi/speechpy" -> "freewym/espresso"
"astorfi/speechpy" -> "mravanelli/SincNet"
"astorfi/speechpy" -> "pykaldi/pykaldi"
"astorfi/speechpy" -> "qqueing/DeepSpeaker-pytorch"
"astorfi/speechpy" -> "a-nagrani/VGGVox"
"astorfi/speechpy" -> "mindorii/kws" ["e"=1]
"astorfi/speechpy" -> "awni/speech"
"astorfi/speechpy" -> "wiseman/py-webrtcvad"
"nl8590687/ASRT_SpeechRecognition" -> "audier/DeepSpeechRecognition"
"nl8590687/ASRT_SpeechRecognition" -> "kaldi-asr/kaldi"
"nl8590687/ASRT_SpeechRecognition" -> "xxbb1234021/speech_recognition"
"nl8590687/ASRT_SpeechRecognition" -> "nobody132/masr"
"nl8590687/ASRT_SpeechRecognition" -> "espnet/espnet"
"nl8590687/ASRT_SpeechRecognition" -> "libai3/masr"
"nl8590687/ASRT_SpeechRecognition" -> "PaddlePaddle/DeepSpeech"
"nl8590687/ASRT_SpeechRecognition" -> "mravanelli/pytorch-kaldi"
"nl8590687/ASRT_SpeechRecognition" -> "wenet-e2e/wenet"
"nl8590687/ASRT_SpeechRecognition" -> "zzw922cn/awesome-speech-recognition-speech-synthesis-papers"
"nl8590687/ASRT_SpeechRecognition" -> "facebookresearch/wav2letter"
"nl8590687/ASRT_SpeechRecognition" -> "PaddlePaddle/PaddleSpeech"
"nl8590687/ASRT_SpeechRecognition" -> "speechbrain/speechbrain"
"nl8590687/ASRT_SpeechRecognition" -> "brightmart/nlp_chinese_corpus" ["e"=1]
"nl8590687/ASRT_SpeechRecognition" -> "shibing624/pycorrector" ["e"=1]
"jiaaro/pydub" -> "librosa/librosa"
"jiaaro/pydub" -> "tyiannak/pyAudioAnalysis"
"jiaaro/pydub" -> "worldveil/dejavu" ["e"=1]
"jiaaro/pydub" -> "Zulko/moviepy" ["e"=1]
"jiaaro/pydub" -> "Uberi/speech_recognition"
"jiaaro/pydub" -> "espnet/espnet"
"jiaaro/pydub" -> "kkroening/ffmpeg-python" ["e"=1]
"jiaaro/pydub" -> "wiseman/py-webrtcvad"
"jiaaro/pydub" -> "jameslyons/python_speech_features"
"jiaaro/pydub" -> "kaldi-asr/kaldi"
"jiaaro/pydub" -> "speechbrain/speechbrain"
"jiaaro/pydub" -> "pyannote/pyannote-audio"
"jiaaro/pydub" -> "pytorch/audio"
"jiaaro/pydub" -> "aubio/aubio" ["e"=1]
"jiaaro/pydub" -> "NVIDIA/NeMo"
"xinjli/allosaurus" -> "dmort27/epitran"
"xinjli/allosaurus" -> "xinjli/transphone"
"xinjli/allosaurus" -> "dmort27/allovera"
"xinjli/allosaurus" -> "bootphon/phonemizer"
"xinjli/allosaurus" -> "dmort27/panphon"
"xinjli/allosaurus" -> "festvox/datasets-CMU_Wilderness"
"xinjli/allosaurus" -> "lumaku/ctc-segmentation"
"xinjli/allosaurus" -> "xinjli/ucla-phonetic-corpus"
"xinjli/allosaurus" -> "andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning"
"xinjli/allosaurus" -> "phoible/dev"
"xinjli/allosaurus" -> "yistLin/FragmentVC"
"xinjli/allosaurus" -> "lingjzhu/charsiu"
"xinjli/allosaurus" -> "auspicious3000/SpeechSplit"
"xinjli/allosaurus" -> "Shahabks/myprosody" ["e"=1]
"NVIDIA/tacotron2" -> "NVIDIA/waveglow"
"NVIDIA/tacotron2" -> "Rayhane-mamah/Tacotron-2"
"NVIDIA/tacotron2" -> "keithito/tacotron"
"NVIDIA/tacotron2" -> "r9y9/wavenet_vocoder"
"NVIDIA/tacotron2" -> "fatchord/WaveRNN"
"NVIDIA/tacotron2" -> "espnet/espnet"
"NVIDIA/tacotron2" -> "jik876/hifi-gan"
"NVIDIA/tacotron2" -> "mozilla/TTS"
"NVIDIA/tacotron2" -> "kan-bayashi/ParallelWaveGAN"
"NVIDIA/tacotron2" -> "r9y9/deepvoice3_pytorch"
"NVIDIA/tacotron2" -> "ming024/FastSpeech2"
"NVIDIA/tacotron2" -> "xcmyz/FastSpeech"
"NVIDIA/tacotron2" -> "NVIDIA/mellotron"
"NVIDIA/tacotron2" -> "jaywalnut310/vits"
"NVIDIA/tacotron2" -> "Kyubyong/tacotron"
"NVIDIA/waveglow" -> "NVIDIA/tacotron2"
"NVIDIA/waveglow" -> "r9y9/wavenet_vocoder"
"NVIDIA/waveglow" -> "fatchord/WaveRNN"
"NVIDIA/waveglow" -> "kan-bayashi/ParallelWaveGAN"
"NVIDIA/waveglow" -> "Rayhane-mamah/Tacotron-2"
"NVIDIA/waveglow" -> "mozilla/LPCNet"
"NVIDIA/waveglow" -> "descriptinc/melgan-neurips"
"NVIDIA/waveglow" -> "NVIDIA/mellotron"
"NVIDIA/waveglow" -> "xcmyz/FastSpeech"
"NVIDIA/waveglow" -> "jik876/hifi-gan"
"NVIDIA/waveglow" -> "ksw0306/FloWaveNet"
"NVIDIA/waveglow" -> "seungwonpark/melgan"
"NVIDIA/waveglow" -> "NVIDIA/nv-wavenet"
"NVIDIA/waveglow" -> "keithito/tacotron"
"NVIDIA/waveglow" -> "r9y9/deepvoice3_pytorch"
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "Tomiinek/Multilingual_Text_to_Speech"
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "keonlee9420/Parallel-Tacotron2"
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "rishikksh20/FastSpeech2"
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "xcmyz/speech-synthesis-paper"
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "nii-yamagishilab/multi-speaker-tacotron"
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "keonlee9420/Expressive-FastSpeech2"
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "jxzhanggg/nonparaSeq2seqVC_code"
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "k2kobayashi/crank"
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "NVIDIA/mellotron"
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "keonlee9420/DiffGAN-TTS"
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "ivanvovk/DurIAN"
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "Wendison/VQMIVC"
"mozilla/DeepSpeech" -> "kaldi-asr/kaldi"
"mozilla/DeepSpeech" -> "mozilla/TTS"
"mozilla/DeepSpeech" -> "facebookresearch/wav2letter"
"mozilla/DeepSpeech" -> "espnet/espnet"
"mozilla/DeepSpeech" -> "Uberi/speech_recognition"
"mozilla/DeepSpeech" -> "buriburisuri/speech-to-text-wavenet"
"mozilla/DeepSpeech" -> "explosion/spaCy" ["e"=1]
"mozilla/DeepSpeech" -> "pannous/tensorflow-speech-recognition"
"mozilla/DeepSpeech" -> "nl8590687/ASRT_SpeechRecognition"
"mozilla/DeepSpeech" -> "facebookresearch/fastText" ["e"=1]
"mozilla/DeepSpeech" -> "pytorch/fairseq" ["e"=1]
"mozilla/DeepSpeech" -> "SeanNaren/deepspeech.pytorch"
"mozilla/DeepSpeech" -> "tensorflow/models" ["e"=1]
"mozilla/DeepSpeech" -> "oxford-cs-deepnlp-2017/lectures" ["e"=1]
"mozilla/DeepSpeech" -> "CorentinJ/Real-Time-Voice-Cloning" ["e"=1]
"openai/jukebox" -> "magenta/magenta" ["e"=1]
"openai/jukebox" -> "salu133445/musegan" ["e"=1]
"openai/jukebox" -> "magenta/ddsp" ["e"=1]
"openai/jukebox" -> "microsoft/muzic"
"openai/jukebox" -> "deezer/spleeter"
"openai/jukebox" -> "lucidrains/musiclm-pytorch"
"openai/jukebox" -> "NVIDIA/tacotron2"
"openai/jukebox" -> "facebookresearch/demucs"
"openai/jukebox" -> "openai/gpt-2" ["e"=1]
"openai/jukebox" -> "openai/gpt-3" ["e"=1]
"openai/jukebox" -> "lucidrains/DALLE-pytorch" ["e"=1]
"openai/jukebox" -> "openai/DALL-E" ["e"=1]
"openai/jukebox" -> "openai/CLIP" ["e"=1]
"openai/jukebox" -> "ybayle/awesome-deep-learning-music" ["e"=1]
"openai/jukebox" -> "librosa/librosa"
"openvpi/vocoders" -> "openvpi/DiffSinger"
"openvpi/vocoders" -> "yxlllc/DDSP-SVC"
"openvpi/vocoders" -> "xunmengshe/OpenUtau"
"openvpi/vocoders" -> "openvpi/diff-svc"
"openvpi/vocoders" -> "innnky/diff-svc"
"openvpi/vocoders" -> "fishaudio/fish-diffusion"
"openvpi/vocoders" -> "flutydeer/audio-slicer"
"PaddlePaddle/Parakeet" -> "kan-bayashi/ParallelWaveGAN"
"PaddlePaddle/Parakeet" -> "xcmyz/FastSpeech"
"PaddlePaddle/Parakeet" -> "jaywalnut310/glow-tts"
"PaddlePaddle/Parakeet" -> "ivanvovk/WaveGrad"
"PaddlePaddle/Parakeet" -> "ming024/FastSpeech2"
"PaddlePaddle/Parakeet" -> "soobinseo/Transformer-TTS"
"PaddlePaddle/Parakeet" -> "mozilla/LPCNet"
"PaddlePaddle/Parakeet" -> "jik876/hifi-gan"
"PaddlePaddle/Parakeet" -> "tianrengao/SqueezeWave"
"PaddlePaddle/Parakeet" -> "seungwonpark/melgan"
"PaddlePaddle/Parakeet" -> "L0SG/WaveFlow"
"PaddlePaddle/Parakeet" -> "descriptinc/melgan-neurips"
"PaddlePaddle/Parakeet" -> "janvainer/speedyspeech"
"PaddlePaddle/Parakeet" -> "xcmyz/speech-synthesis-paper"
"PaddlePaddle/Parakeet" -> "Kyubyong/g2p"
"Jungjee/RawNet" -> "clovaai/aasist" ["e"=1]
"Jungjee/RawNet" -> "clovaai/voxceleb_trainer"
"Jungjee/RawNet" -> "Snowdar/asv-subtools"
"Jungjee/RawNet" -> "sasv-challenge/SASVC2022_Baseline" ["e"=1]
"Jungjee/RawNet" -> "joonson/voxceleb_unsupervised"
"Jungjee/RawNet" -> "asvspoof-challenge/2021" ["e"=1]
"Jungjee/RawNet" -> "mravanelli/SincNet"
"Jungjee/RawNet" -> "manojpamk/pytorch_xvectors"
"Jungjee/RawNet" -> "WeidiXie/VGG-Speaker-Recognition"
"Jungjee/RawNet" -> "VITA-Group/AutoSpeech"
"Jungjee/RawNet" -> "iiscleap/NeuralPlda"
"Jungjee/RawNet" -> "qqueing/DeepSpeaker-pytorch"
"Jungjee/RawNet" -> "seongmin-kye/meta-SR"
"Jungjee/RawNet" -> "cvqluu/Factorized-TDNN"
"Jungjee/RawNet" -> "Janghyun1230/Speaker_Verification"
"Plachtaa/VITS-fast-fine-tuning" -> "SayaSS/vits-finetuning"
"Plachtaa/VITS-fast-fine-tuning" -> "innnky/emotional-vits"
"Plachtaa/VITS-fast-fine-tuning" -> "jaywalnut310/vits"
"Plachtaa/VITS-fast-fine-tuning" -> "CjangCjengh/MoeGoe"
"Plachtaa/VITS-fast-fine-tuning" -> "CjangCjengh/vits"
"Plachtaa/VITS-fast-fine-tuning" -> "PlayVoice/vits_chinese"
"Plachtaa/VITS-fast-fine-tuning" -> "svc-develop-team/so-vits-svc"
"Plachtaa/VITS-fast-fine-tuning" -> "innnky/so-vits-svc"
"Plachtaa/VITS-fast-fine-tuning" -> "liujing04/Retrieval-based-Voice-Conversion-WebUI"
"Plachtaa/VITS-fast-fine-tuning" -> "openvpi/DiffSinger"
"Plachtaa/VITS-fast-fine-tuning" -> "NaruseMioShirakana/MoeSS"
"Plachtaa/VITS-fast-fine-tuning" -> "CjangCjengh/TTSModels"
"Plachtaa/VITS-fast-fine-tuning" -> "luoyily/MoeTTS"
"Plachtaa/VITS-fast-fine-tuning" -> "yxlllc/DDSP-SVC"
"Plachtaa/VITS-fast-fine-tuning" -> "CjangCjengh/MoeGoe_GUI"
"Snowdar/asv-subtools" -> "clovaai/voxceleb_trainer"
"Snowdar/asv-subtools" -> "wenet-e2e/wespeaker"
"Snowdar/asv-subtools" -> "manojpamk/pytorch_xvectors"
"Snowdar/asv-subtools" -> "Jungjee/RawNet"
"Snowdar/asv-subtools" -> "wq2012/awesome-diarization"
"Snowdar/asv-subtools" -> "BUTSpeechFIT/VBx"
"Snowdar/asv-subtools" -> "lhotse-speech/lhotse"
"Snowdar/asv-subtools" -> "k2-fsa/k2"
"Snowdar/asv-subtools" -> "qqueing/DeepSpeaker-pytorch"
"Snowdar/asv-subtools" -> "TaoRuijie/ECAPA-TDNN"
"Snowdar/asv-subtools" -> "hitachi-speech/EEND"
"Snowdar/asv-subtools" -> "vesis84/kaldi-io-for-python"
"Snowdar/asv-subtools" -> "zyzisyz/mfa_conformer"
"Snowdar/asv-subtools" -> "lawlict/ECAPA-TDNN"
"Snowdar/asv-subtools" -> "WeidiXie/VGG-Speaker-Recognition"
"asteroid-team/torch-audiomentations" -> "iver56/audiomentations"
"asteroid-team/torch-audiomentations" -> "facebookresearch/WavAugment"
"asteroid-team/torch-audiomentations" -> "KinWaiCheuk/nnAudio"
"asteroid-team/torch-audiomentations" -> "qiuqiangkong/torchlibrosa"
"asteroid-team/torch-audiomentations" -> "Spijkervet/torchaudio-augmentations" ["e"=1]
"asteroid-team/torch-audiomentations" -> "facebookresearch/AudioMAE"
"asteroid-team/torch-audiomentations" -> "csteinmetz1/auraloss"
"asteroid-team/torch-audiomentations" -> "asteroid-team/asteroid" ["e"=1]
"asteroid-team/torch-audiomentations" -> "aliutkus/speechmetrics" ["e"=1]
"asteroid-team/torch-audiomentations" -> "DemisEom/SpecAugment"
"asteroid-team/torch-audiomentations" -> "YuanGongND/ast"
"asteroid-team/torch-audiomentations" -> "qiuqiangkong/audioset_tagging_cnn"
"asteroid-team/torch-audiomentations" -> "pranaymanocha/PerceptualAudio" ["e"=1]
"asteroid-team/torch-audiomentations" -> "juanmc2005/StreamingSpeakerDiarization"
"asteroid-team/torch-audiomentations" -> "lucidrains/audiolm-pytorch"
"clovaai/voxceleb_trainer" -> "Snowdar/asv-subtools"
"clovaai/voxceleb_trainer" -> "TaoRuijie/ECAPA-TDNN"
"clovaai/voxceleb_trainer" -> "Jungjee/RawNet"
"clovaai/voxceleb_trainer" -> "manojpamk/pytorch_xvectors"
"clovaai/voxceleb_trainer" -> "wq2012/awesome-diarization"
"clovaai/voxceleb_trainer" -> "mravanelli/SincNet"
"clovaai/voxceleb_trainer" -> "HarryVolek/PyTorch_Speaker_Verification"
"clovaai/voxceleb_trainer" -> "qqueing/DeepSpeaker-pytorch"
"clovaai/voxceleb_trainer" -> "joonson/voxceleb_unsupervised"
"clovaai/voxceleb_trainer" -> "wenet-e2e/wespeaker"
"clovaai/voxceleb_trainer" -> "WeidiXie/VGG-Speaker-Recognition"
"clovaai/voxceleb_trainer" -> "a-nagrani/VGGVox"
"clovaai/voxceleb_trainer" -> "BUTSpeechFIT/VBx"
"clovaai/voxceleb_trainer" -> "s3prl/s3prl"
"clovaai/voxceleb_trainer" -> "facebookresearch/WavAugment"
"resemble-ai/Resemblyzer" -> "wq2012/awesome-diarization"
"resemble-ai/Resemblyzer" -> "kan-bayashi/ParallelWaveGAN"
"resemble-ai/Resemblyzer" -> "auspicious3000/autovc"
"resemble-ai/Resemblyzer" -> "jik876/hifi-gan"
"resemble-ai/Resemblyzer" -> "fatchord/WaveRNN"
"resemble-ai/Resemblyzer" -> "ming024/FastSpeech2"
"resemble-ai/Resemblyzer" -> "descriptinc/melgan-neurips"
"resemble-ai/Resemblyzer" -> "espnet/espnet"
"resemble-ai/Resemblyzer" -> "Rayhane-mamah/Tacotron-2"
"resemble-ai/Resemblyzer" -> "r9y9/wavenet_vocoder"
"resemble-ai/Resemblyzer" -> "HarryVolek/PyTorch_Speaker_Verification"
"resemble-ai/Resemblyzer" -> "NVIDIA/mellotron"
"resemble-ai/Resemblyzer" -> "pyannote/pyannote-audio"
"resemble-ai/Resemblyzer" -> "auspicious3000/SpeechSplit"
"resemble-ai/Resemblyzer" -> "bootphon/phonemizer"
"speechbrain/speechbrain" -> "espnet/espnet"
"speechbrain/speechbrain" -> "pyannote/pyannote-audio"
"speechbrain/speechbrain" -> "s3prl/s3prl"
"speechbrain/speechbrain" -> "mravanelli/pytorch-kaldi"
"speechbrain/speechbrain" -> "NVIDIA/NeMo"
"speechbrain/speechbrain" -> "wenet-e2e/wenet"
"speechbrain/speechbrain" -> "asteroid-team/asteroid" ["e"=1]
"speechbrain/speechbrain" -> "kaldi-asr/kaldi"
"speechbrain/speechbrain" -> "k2-fsa/k2"
"speechbrain/speechbrain" -> "wq2012/awesome-diarization"
"speechbrain/speechbrain" -> "clovaai/voxceleb_trainer"
"speechbrain/speechbrain" -> "kan-bayashi/ParallelWaveGAN"
"speechbrain/speechbrain" -> "facebookresearch/denoiser" ["e"=1]
"speechbrain/speechbrain" -> "zzw922cn/awesome-speech-recognition-speech-synthesis-papers"
"speechbrain/speechbrain" -> "pytorch/fairseq" ["e"=1]
"bytedance/music_source_separation" -> "sigsep/open-unmix-pytorch"
"bytedance/music_source_separation" -> "moscow-technologies/blockchain-voting_2021" ["e"=1]
"bytedance/music_source_separation" -> "haoheliu/Subband-Music-Separation"
"bytedance/music_source_separation" -> "haoheliu/2021-ISMIR-MSS-Challenge-CWS-PResUNet"
"bytedance/music_source_separation" -> "microsoft/muzic"
"bytedance/music_source_separation" -> "microsoft/NeuralSpeech"
"bytedance/music_source_separation" -> "haoheliu/voicefixer" ["e"=1]
"bytedance/music_source_separation" -> "NATSpeech/NATSpeech" ["e"=1]
"bytedance/music_source_separation" -> "asteroid-team/asteroid" ["e"=1]
"bytedance/music_source_separation" -> "magenta/mt3" ["e"=1]
"bytedance/music_source_separation" -> "csteinmetz1/auraloss"
"bytedance/music_source_separation" -> "qiuqiangkong/torchlibrosa"
"bytedance/music_source_separation" -> "microsoft/DNS-Challenge" ["e"=1]
"bytedance/music_source_separation" -> "lmnt-com/diffwave"
"bytedance/music_source_separation" -> "jik876/hifi-gan"
"CjangCjengh/MoeGoe" -> "CjangCjengh/MoeGoe_GUI"
"CjangCjengh/MoeGoe" -> "CjangCjengh/TTSModels"
"CjangCjengh/MoeGoe" -> "luoyily/MoeTTS"
"CjangCjengh/MoeGoe" -> "CjangCjengh/vits"
"CjangCjengh/MoeGoe" -> "jaywalnut310/vits"
"CjangCjengh/MoeGoe" -> "Plachtaa/VITS-fast-fine-tuning"
"CjangCjengh/MoeGoe" -> "innnky/so-vits-svc"
"CjangCjengh/MoeGoe" -> "innnky/emotional-vits"
"CjangCjengh/MoeGoe" -> "svc-develop-team/so-vits-svc"
"CjangCjengh/MoeGoe" -> "cjyaddone/ChatWaifu"
"CjangCjengh/MoeGoe" -> "NaruseMioShirakana/MoeSS"
"CjangCjengh/MoeGoe" -> "yuyuyzl/EasyVtuber"
"CjangCjengh/MoeGoe" -> "CjangCjengh/tacotron2-japanese"
"CjangCjengh/MoeGoe" -> "openvpi/DiffSinger"
"CjangCjengh/MoeGoe" -> "TheKOG/Gal-Voice-Bot"
"TheKOG/Gal-Voice-Bot" -> "luoyily/MoeTTS"
"TheKOG/Gal-Voice-Bot" -> "CjangCjengh/MoeGoe_GUI"
"TheKOG/Gal-Voice-Bot" -> "CjangCjengh/TTSModels"
"TheKOG/Gal-Voice-Bot" -> "CjangCjengh/MoeGoe"
"TheKOG/Gal-Voice-Bot" -> "CjangCjengh/tacotron2-japanese"
"TheKOG/Gal-Voice-Bot" -> "w4123/vits"
"TheKOG/Gal-Voice-Bot" -> "innnky/vits-japanese"
"TheKOG/Gal-Voice-Bot" -> "Kyomotoi/ATRI" ["e"=1]
"TheKOG/Gal-Voice-Bot" -> "audeering/w2v2-how-to"
"TheKOG/Gal-Voice-Bot" -> "JOETtheIV/VITS-Paimon"
"nateshmbhat/pyttsx3" -> "pndurette/gTTS"
"nateshmbhat/pyttsx3" -> "Uberi/speech_recognition"
"nateshmbhat/pyttsx3" -> "Renovamen/Speech-and-Text"
"nateshmbhat/pyttsx3" -> "espeak-ng/espeak-ng"
"nateshmbhat/pyttsx3" -> "mozilla/TTS"
"nateshmbhat/pyttsx3" -> "RapidWareTech/pyttsx"
"nateshmbhat/pyttsx3" -> "TaylorSMarks/playsound"
"nateshmbhat/pyttsx3" -> "kivy/plyer" ["e"=1]
"nateshmbhat/pyttsx3" -> "coqui-ai/TTS"
"nateshmbhat/pyttsx3" -> "Kyubyong/tacotron"
"nateshmbhat/pyttsx3" -> "jiaaro/pydub"
"nateshmbhat/pyttsx3" -> "as-ideas/TransformerTTS"
"nateshmbhat/pyttsx3" -> "TensorSpeech/TensorFlowTTS"
"nateshmbhat/pyttsx3" -> "ggeop/Python-ai-assistant" ["e"=1]
"nateshmbhat/pyttsx3" -> "asweigart/pyautogui" ["e"=1]
"pndurette/gTTS" -> "nateshmbhat/pyttsx3"
"pndurette/gTTS" -> "Uberi/speech_recognition"
"pndurette/gTTS" -> "ssut/py-googletrans" ["e"=1]
"pndurette/gTTS" -> "readbeyond/aeneas"
"pndurette/gTTS" -> "mozilla/TTS"
"pndurette/gTTS" -> "RapidWareTech/pyttsx"
"pndurette/gTTS" -> "marytts/marytts"
"pndurette/gTTS" -> "desbma/GoogleSpeech"
"pndurette/gTTS" -> "jiaaro/pydub"
"pndurette/gTTS" -> "hungtruong/Google-Translate-TTS"
"pndurette/gTTS" -> "TaylorSMarks/playsound"
"pndurette/gTTS" -> "espeak-ng/espeak-ng"
"pndurette/gTTS" -> "buriburisuri/speech-to-text-wavenet"
"pndurette/gTTS" -> "r9y9/deepvoice3_pytorch"
"pndurette/gTTS" -> "wit-ai/pywit" ["e"=1]
"oarriaga/face_classification" -> "davidsandberg/facenet" ["e"=1]
"oarriaga/face_classification" -> "buriburisuri/speech-to-text-wavenet"
"oarriaga/face_classification" -> "andabi/deep-voice-conversion"
"oarriaga/face_classification" -> "omar178/Emotion-recognition" ["e"=1]
"oarriaga/face_classification" -> "junyanz/iGAN" ["e"=1]
"oarriaga/face_classification" -> "AKSHAYUBHAT/DeepVideoAnalytics"
"oarriaga/face_classification" -> "WuJie1010/Facial-Expression-Recognition.Pytorch" ["e"=1]
"oarriaga/face_classification" -> "MrNothing/AI-Blocks"
"oarriaga/face_classification" -> "yu4u/age-gender-estimation" ["e"=1]
"oarriaga/face_classification" -> "PAIR-code/facets" ["e"=1]
"oarriaga/face_classification" -> "dpressel/rude-carnie" ["e"=1]
"oarriaga/face_classification" -> "lengstrom/fast-style-transfer" ["e"=1]
"oarriaga/face_classification" -> "DmitryUlyanov/deep-image-prior" ["e"=1]
"oarriaga/face_classification" -> "isseu/emotion-recognition-neural-networks" ["e"=1]
"oarriaga/face_classification" -> "yunjey/StarGAN" ["e"=1]
"cmusphinx/sphinx4" -> "cmusphinx/pocketsphinx"
"cmusphinx/sphinx4" -> "cmusphinx/sphinxbase"
"cmusphinx/sphinx4" -> "cmusphinx/pocketsphinx-android-demo"
"cmusphinx/sphinx4" -> "cmusphinx/pocketsphinx-android"
"cmusphinx/sphinx4" -> "cmusphinx/sphinxtrain"
"cmusphinx/sphinx4" -> "cmusphinx/pocketsphinx-python"
"cmusphinx/sphinx4" -> "marytts/marytts"
"cmusphinx/sphinx4" -> "cmusphinx/cmudict"
"cmusphinx/sphinx4" -> "alumae/kaldi-gstreamer-server"
"cmusphinx/sphinx4" -> "lkuza2/java-speech-api"
"cmusphinx/sphinx4" -> "kaldi-asr/kaldi"
"cmusphinx/sphinx4" -> "cmusphinx/g2p-seq2seq"
"cmusphinx/sphinx4" -> "pannous/tensorflow-speech-recognition"
"cmusphinx/sphinx4" -> "syl22-00/pocketsphinx.js" ["e"=1]
"cmusphinx/sphinx4" -> "julius-speech/julius"
"lkuza2/java-speech-api" -> "cmusphinx/sphinx4"
"adefossez/mdx21_demucs" -> "kuielab/mdx-net-submission"
"facebookresearch/demucs" -> "deezer/spleeter"
"facebookresearch/demucs" -> "sigsep/open-unmix-pytorch"
"facebookresearch/demucs" -> "speechbrain/speechbrain"
"facebookresearch/demucs" -> "espnet/espnet"
"facebookresearch/demucs" -> "facebookresearch/denoiser" ["e"=1]
"facebookresearch/demucs" -> "magenta/ddsp" ["e"=1]
"facebookresearch/demucs" -> "asteroid-team/asteroid" ["e"=1]
"facebookresearch/demucs" -> "Anjok07/ultimatevocalremovergui"
"facebookresearch/demucs" -> "librosa/librosa"
"facebookresearch/demucs" -> "spotify/basic-pitch"
"facebookresearch/demucs" -> "f90/Wave-U-Net" ["e"=1]
"facebookresearch/demucs" -> "microsoft/muzic"
"facebookresearch/demucs" -> "stemrollerapp/stemroller"
"facebookresearch/demucs" -> "bytedance/music_source_separation"
"facebookresearch/demucs" -> "marl/crepe" ["e"=1]
"jimbozhang/speechocean762" -> "JazminVidal/gop-dnn-epadb"
"34j/so-vits-svc-fork" -> "w-okada/voice-changer"
"34j/so-vits-svc-fork" -> "svc-develop-team/so-vits-svc"
"34j/so-vits-svc-fork" -> "liujing04/Retrieval-based-Voice-Conversion-WebUI"
"34j/so-vits-svc-fork" -> "Plachtaa/VITS-fast-fine-tuning"
"34j/so-vits-svc-fork" -> "yxlllc/DDSP-SVC"
"34j/so-vits-svc-fork" -> "prophesier/diff-svc"
"34j/so-vits-svc-fork" -> "PlayVoice/so-vits-svc-5.0"
"34j/so-vits-svc-fork" -> "fishaudio/fish-diffusion"
"34j/so-vits-svc-fork" -> "innnky/so-vits-svc"
"34j/so-vits-svc-fork" -> "Winfredy/SadTalker" ["e"=1]
"34j/so-vits-svc-fork" -> "enhuiz/vall-e"
"34j/so-vits-svc-fork" -> "NaruseMioShirakana/MoeSS"
"34j/so-vits-svc-fork" -> "jaywalnut310/vits"
"34j/so-vits-svc-fork" -> "openvpi/audio-slicer"
"34j/so-vits-svc-fork" -> "neonbjb/tortoise-tts" ["e"=1]
"NVIDIA/NeMo" -> "speechbrain/speechbrain"
"NVIDIA/NeMo" -> "espnet/espnet"
"NVIDIA/NeMo" -> "pyannote/pyannote-audio"
"NVIDIA/NeMo" -> "NVIDIA/DeepLearningExamples" ["e"=1]
"NVIDIA/NeMo" -> "NVIDIA/Megatron-LM" ["e"=1]
"NVIDIA/NeMo" -> "pytorch/fairseq" ["e"=1]
"NVIDIA/NeMo" -> "coqui-ai/TTS"
"NVIDIA/NeMo" -> "mravanelli/pytorch-kaldi"
"NVIDIA/NeMo" -> "microsoft/unilm" ["e"=1]
"NVIDIA/NeMo" -> "s3prl/s3prl"
"NVIDIA/NeMo" -> "NVIDIA/tacotron2"
"NVIDIA/NeMo" -> "wenet-e2e/wenet"
"NVIDIA/NeMo" -> "kaldi-asr/kaldi"
"NVIDIA/NeMo" -> "jik876/hifi-gan"
"NVIDIA/NeMo" -> "snakers4/silero-vad"
"philipperemy/timit" -> "Faur/TIMIT"
"espeak-ng/espeak-ng" -> "bootphon/phonemizer"
"espeak-ng/espeak-ng" -> "ming024/FastSpeech2"
"espeak-ng/espeak-ng" -> "TensorSpeech/TensorFlowTTS"
"espeak-ng/espeak-ng" -> "rhdunn/espeak"
"espeak-ng/espeak-ng" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"espeak-ng/espeak-ng" -> "marytts/marytts"
"espeak-ng/espeak-ng" -> "numediart/MBROLA"
"espeak-ng/espeak-ng" -> "kan-bayashi/ParallelWaveGAN"
"espeak-ng/espeak-ng" -> "jik876/hifi-gan"
"espeak-ng/espeak-ng" -> "festvox/flite"
"espeak-ng/espeak-ng" -> "dmort27/epitran"
"espeak-ng/espeak-ng" -> "jaywalnut310/glow-tts"
"espeak-ng/espeak-ng" -> "mozilla/TTS"
"espeak-ng/espeak-ng" -> "espnet/espnet"
"espeak-ng/espeak-ng" -> "coqui-ai/TTS"
"espnet/espnet" -> "speechbrain/speechbrain"
"espnet/espnet" -> "mravanelli/pytorch-kaldi"
"espnet/espnet" -> "kaldi-asr/kaldi"
"espnet/espnet" -> "kan-bayashi/ParallelWaveGAN"
"espnet/espnet" -> "wenet-e2e/wenet"
"espnet/espnet" -> "zzw922cn/awesome-speech-recognition-speech-synthesis-papers"
"espnet/espnet" -> "NVIDIA/NeMo"
"espnet/espnet" -> "s3prl/s3prl"
"espnet/espnet" -> "NVIDIA/tacotron2"
"espnet/espnet" -> "pytorch/fairseq" ["e"=1]
"espnet/espnet" -> "r9y9/wavenet_vocoder"
"espnet/espnet" -> "pyannote/pyannote-audio"
"espnet/espnet" -> "ming024/FastSpeech2"
"espnet/espnet" -> "facebookresearch/wav2letter"
"espnet/espnet" -> "jik876/hifi-gan"
"facebookresearch/flashlight" -> "facebookresearch/wav2letter"
"facebookresearch/flashlight" -> "arrayfire/arrayfire" ["e"=1]
"facebookresearch/flashlight" -> "arrayfire/arrayfire-ml"
"facebookresearch/flashlight" -> "facebookresearch/gtn"
"facebookresearch/flashlight" -> "facebookresearch/libri-light"
"facebookresearch/flashlight" -> "facebookresearch/nevergrad" ["e"=1]
"facebookresearch/flashlight" -> "mravanelli/pytorch-kaldi"
"facebookresearch/flashlight" -> "NVIDIA/OpenSeq2Seq"
"facebookresearch/flashlight" -> "Kyubyong/g2p"
"facebookresearch/flashlight" -> "YiwenShaoStephen/pychain"
"facebookresearch/flashlight" -> "mravanelli/SincNet"
"facebookresearch/flashlight" -> "kpu/kenlm"
"facebookresearch/flashlight" -> "hirofumi0810/neural_sp"
"facebookresearch/flashlight" -> "open-speech/speech-aligner"
"facebookresearch/flashlight" -> "prabhuomkar/pytorch-cpp" ["e"=1]
"facebookresearch/wav2letter" -> "mravanelli/pytorch-kaldi"
"facebookresearch/wav2letter" -> "espnet/espnet"
"facebookresearch/wav2letter" -> "kaldi-asr/kaldi"
"facebookresearch/wav2letter" -> "syhw/wer_are_we"
"facebookresearch/wav2letter" -> "facebookresearch/flashlight"
"facebookresearch/wav2letter" -> "SeanNaren/deepspeech.pytorch"
"facebookresearch/wav2letter" -> "mozilla/DeepSpeech"
"facebookresearch/wav2letter" -> "PaddlePaddle/DeepSpeech"
"facebookresearch/wav2letter" -> "buriburisuri/speech-to-text-wavenet"
"facebookresearch/wav2letter" -> "zzw922cn/awesome-speech-recognition-speech-synthesis-papers"
"facebookresearch/wav2letter" -> "zzw922cn/Automatic_Speech_Recognition"
"facebookresearch/wav2letter" -> "NVIDIA/OpenSeq2Seq"
"facebookresearch/wav2letter" -> "srvk/eesen"
"facebookresearch/wav2letter" -> "kpu/kenlm"
"facebookresearch/wav2letter" -> "nl8590687/ASRT_SpeechRecognition"
"kpu/kenlm" -> "shibing624/pycorrector" ["e"=1]
"kpu/kenlm" -> "parlance/ctcdecode"
"kpu/kenlm" -> "moses-smt/mosesdecoder" ["e"=1]
"kpu/kenlm" -> "rsennrich/subword-nmt" ["e"=1]
"kpu/kenlm" -> "google/sentencepiece" ["e"=1]
"kpu/kenlm" -> "PaddlePaddle/DeepSpeech"
"kpu/kenlm" -> "srvk/eesen"
"kpu/kenlm" -> "SeanNaren/deepspeech.pytorch"
"kpu/kenlm" -> "espnet/espnet"
"kpu/kenlm" -> "pykaldi/pykaldi"
"kpu/kenlm" -> "syhw/wer_are_we"
"kpu/kenlm" -> "mravanelli/pytorch-kaldi"
"kpu/kenlm" -> "wiseman/py-webrtcvad"
"kpu/kenlm" -> "clab/fast_align" ["e"=1]
"kpu/kenlm" -> "facebookresearch/wav2letter"
"librosa/librosa" -> "tyiannak/pyAudioAnalysis"
"librosa/librosa" -> "MTG/essentia" ["e"=1]
"librosa/librosa" -> "jiaaro/pydub"
"librosa/librosa" -> "jameslyons/python_speech_features"
"librosa/librosa" -> "aubio/aubio" ["e"=1]
"librosa/librosa" -> "CPJKU/madmom" ["e"=1]
"librosa/librosa" -> "pytorch/audio"
"librosa/librosa" -> "ybayle/awesome-deep-learning-music" ["e"=1]
"librosa/librosa" -> "faroit/awesome-python-scientific-audio" ["e"=1]
"librosa/librosa" -> "cuthbertLab/music21" ["e"=1]
"librosa/librosa" -> "keunwoochoi/kapre"
"librosa/librosa" -> "craffel/mir_eval" ["e"=1]
"librosa/librosa" -> "ibab/tensorflow-wavenet"
"librosa/librosa" -> "kaldi-asr/kaldi"
"librosa/librosa" -> "magenta/ddsp" ["e"=1]
"CSTR-Edinburgh/merlin" -> "mmorise/World"
"CSTR-Edinburgh/merlin" -> "mozilla/LPCNet"
"CSTR-Edinburgh/merlin" -> "r9y9/wavenet_vocoder"
"CSTR-Edinburgh/merlin" -> "Kyubyong/tacotron"
"CSTR-Edinburgh/merlin" -> "Jackiexiao/MTTS"
"CSTR-Edinburgh/merlin" -> "r9y9/nnmnkwii"
"CSTR-Edinburgh/merlin" -> "Rayhane-mamah/Tacotron-2"
"CSTR-Edinburgh/merlin" -> "sotelo/parrot"
"CSTR-Edinburgh/merlin" -> "keithito/tacotron"
"CSTR-Edinburgh/merlin" -> "fatchord/WaveRNN"
"CSTR-Edinburgh/merlin" -> "JeremyCCHsu/Python-Wrapper-for-World-Vocoder"
"CSTR-Edinburgh/merlin" -> "xcmyz/FastSpeech"
"CSTR-Edinburgh/merlin" -> "kan-bayashi/ParallelWaveGAN"
"CSTR-Edinburgh/merlin" -> "r9y9/gantts"
"CSTR-Edinburgh/merlin" -> "r9y9/deepvoice3_pytorch"
"JasonWei512/Tacotron-2-Chinese" -> "foamliu/Tacotron2-Mandarin"
"JasonWei512/Tacotron-2-Chinese" -> "lturing/tacotronv2_wavernn_chinese"
"JasonWei512/Tacotron-2-Chinese" -> "JasonWei512/wavenet_vocoder"
"JasonWei512/Tacotron-2-Chinese" -> "begeekmyfriend/tacotron"
"JasonWei512/Tacotron-2-Chinese" -> "Jackiexiao/MTTS"
"JasonWei512/Tacotron-2-Chinese" -> "Joee1995/tacotron2-mandarin-griffin-lim"
"JasonWei512/Tacotron-2-Chinese" -> "KuangDD/zhrtvc"
"JasonWei512/Tacotron-2-Chinese" -> "KuangDD/zhvoice"
"JasonWei512/Tacotron-2-Chinese" -> "MachineLP/TensorFlowTTS_chinese"
"JasonWei512/Tacotron-2-Chinese" -> "ranchlai/mandarin-tts"
"JasonWei512/Tacotron-2-Chinese" -> "bbepoch/CuteChineseTTS"
"JasonWei512/Tacotron-2-Chinese" -> "begeekmyfriend/Tacotron-2"
"JasonWei512/Tacotron-2-Chinese" -> "aidreamwin/TTS-Clone-Chinese"
"JasonWei512/Tacotron-2-Chinese" -> "iwater/Real-Time-Voice-Cloning-Chinese"
"JasonWei512/Tacotron-2-Chinese" -> "xcmyz/FastSpeech"
"Joee1995/tacotron2-mandarin-griffin-lim" -> "foamliu/Tacotron2-Mandarin"
"Joee1995/tacotron2-mandarin-griffin-lim" -> "ArwenFeng/tacotron_mandarin"
"Joee1995/tacotron2-mandarin-griffin-lim" -> "wqt2019/tacotron-2_melgan"
"Joee1995/tacotron2-mandarin-griffin-lim" -> "wqt2019/tacotron-2_wavernn"
"Joee1995/tacotron2-mandarin-griffin-lim" -> "awesome-archive/tacotron_cn"
"Kyubyong/tacotron" -> "keithito/tacotron"
"Kyubyong/tacotron" -> "Rayhane-mamah/Tacotron-2"
"Kyubyong/tacotron" -> "barronalex/Tacotron"
"Kyubyong/tacotron" -> "Kyubyong/deepvoice3"
"Kyubyong/tacotron" -> "Kyubyong/dc_tts"
"Kyubyong/tacotron" -> "r9y9/wavenet_vocoder"
"Kyubyong/tacotron" -> "CSTR-Edinburgh/merlin"
"Kyubyong/tacotron" -> "ibab/tensorflow-wavenet"
"Kyubyong/tacotron" -> "r9y9/deepvoice3_pytorch"
"Kyubyong/tacotron" -> "tomlepaine/fast-wavenet"
"Kyubyong/tacotron" -> "fatchord/WaveRNN"
"Kyubyong/tacotron" -> "NVIDIA/tacotron2"
"Kyubyong/tacotron" -> "buriburisuri/speech-to-text-wavenet"
"Kyubyong/tacotron" -> "andabi/deep-voice-conversion"
"Kyubyong/tacotron" -> "NVIDIA/waveglow"
"begeekmyfriend/tacotron" -> "begeekmyfriend/Tacotron-2"
"begeekmyfriend/tacotron" -> "Jackiexiao/MTTS"
"begeekmyfriend/tacotron" -> "Joee1995/tacotron2-mandarin-griffin-lim"
"begeekmyfriend/tacotron" -> "syang1993/gst-tacotron"
"begeekmyfriend/tacotron" -> "JasonWei512/Tacotron-2-Chinese"
"begeekmyfriend/tacotron" -> "keithito/tacotron"
"begeekmyfriend/tacotron" -> "Rayhane-mamah/Tacotron-2"
"begeekmyfriend/tacotron" -> "zuoxiang95/tacotron-1"
"begeekmyfriend/tacotron" -> "xcmyz/FastSpeech"
"begeekmyfriend/tacotron" -> "Kyubyong/deepvoice3"
"begeekmyfriend/tacotron" -> "Kyubyong/tacotron"
"begeekmyfriend/tacotron" -> "mozilla/LPCNet"
"begeekmyfriend/tacotron" -> "rishikksh20/vae_tacotron2"
"mozilla/TTS" -> "coqui-ai/TTS"
"mozilla/TTS" -> "NVIDIA/tacotron2"
"mozilla/TTS" -> "fatchord/WaveRNN"
"mozilla/TTS" -> "TensorSpeech/TensorFlowTTS"
"mozilla/TTS" -> "keithito/tacotron"
"mozilla/TTS" -> "espnet/espnet"
"mozilla/TTS" -> "Rayhane-mamah/Tacotron-2"
"mozilla/TTS" -> "mozilla/DeepSpeech"
"mozilla/TTS" -> "r9y9/wavenet_vocoder"
"mozilla/TTS" -> "kan-bayashi/ParallelWaveGAN"
"mozilla/TTS" -> "NVIDIA/waveglow"
"mozilla/TTS" -> "r9y9/deepvoice3_pytorch"
"mozilla/TTS" -> "zzw922cn/awesome-speech-recognition-speech-synthesis-papers"
"mozilla/TTS" -> "ming024/FastSpeech2"
"mozilla/TTS" -> "NVIDIA/NeMo"
"mozilla/voice-web" -> "mozilla/DeepSpeech"
"mozilla/voice-web" -> "facebookresearch/wav2letter"
"mozilla/voice-web" -> "mozilla/TTS"
"mozilla/voice-web" -> "alumae/kaldi-gstreamer-server"
"mozilla/voice-web" -> "keithito/tacotron"
"mozilla/voice-web" -> "syhw/wer_are_we"
"mozilla/voice-web" -> "kaldi-asr/kaldi"
"mozilla/voice-web" -> "MycroftAI/mycroft-core" ["e"=1]
"mozilla/voice-web" -> "SeanNaren/deepspeech.pytorch"
"mozilla/voice-web" -> "cmusphinx/pocketsphinx"
"mozilla/voice-web" -> "JRMeyer/open-speech-corpora"
"mozilla/voice-web" -> "mravanelli/pytorch-kaldi"
"mozilla/voice-web" -> "srvk/eesen"
"mozilla/voice-web" -> "freewym/espresso"
"mozilla/voice-web" -> "google/uis-rnn"
"spotify/pedalboard" -> "spotify/basic-pitch"
"spotify/pedalboard" -> "magenta/ddsp" ["e"=1]
"spotify/pedalboard" -> "DBraun/DawDreamer" ["e"=1]
"spotify/pedalboard" -> "geemion/Khepri" ["e"=1]
"spotify/pedalboard" -> "olilarkin/awesome-musicdsp" ["e"=1]
"spotify/pedalboard" -> "csteinmetz1/ai-audio-startups"
"spotify/pedalboard" -> "juce-framework/JUCE" ["e"=1]
"spotify/pedalboard" -> "facebookresearch/demucs"
"spotify/pedalboard" -> "iver56/audiomentations"
"spotify/pedalboard" -> "speechbrain/speechbrain"
"spotify/pedalboard" -> "microsoft/muzic"
"spotify/pedalboard" -> "librosa/librosa"
"spotify/pedalboard" -> "archinetai/audio-diffusion-pytorch"
"spotify/pedalboard" -> "archinetai/audio-ai-timeline"
"spotify/pedalboard" -> "soul-lang/SOUL" ["e"=1]
"Alexander-H-Liu/End-to-end-ASR-Pytorch" -> "kaituoxu/Speech-Transformer"
"Alexander-H-Liu/End-to-end-ASR-Pytorch" -> "hirofumi0810/neural_sp"
"Alexander-H-Liu/End-to-end-ASR-Pytorch" -> "mravanelli/pytorch-kaldi"
"Alexander-H-Liu/End-to-end-ASR-Pytorch" -> "kaituoxu/Listen-Attend-Spell"
"Alexander-H-Liu/End-to-end-ASR-Pytorch" -> "espnet/espnet"
"Alexander-H-Liu/End-to-end-ASR-Pytorch" -> "SeanNaren/deepspeech.pytorch"
"Alexander-H-Liu/End-to-end-ASR-Pytorch" -> "awni/speech"
"Alexander-H-Liu/End-to-end-ASR-Pytorch" -> "gentaiscool/end2end-asr-pytorch"
"Alexander-H-Liu/End-to-end-ASR-Pytorch" -> "freewym/espresso"
"Alexander-H-Liu/End-to-end-ASR-Pytorch" -> "s3prl/s3prl"
"Alexander-H-Liu/End-to-end-ASR-Pytorch" -> "k2-fsa/k2"
"Alexander-H-Liu/End-to-end-ASR-Pytorch" -> "speechbrain/speechbrain"
"Alexander-H-Liu/End-to-end-ASR-Pytorch" -> "srvk/eesen"
"Alexander-H-Liu/End-to-end-ASR-Pytorch" -> "zcaceres/spec_augment"
"Alexander-H-Liu/End-to-end-ASR-Pytorch" -> "pykaldi/pykaldi"
"brentspell/hifi-gan-bwe" -> "NVIDIA/radtts"
"brentspell/hifi-gan-bwe" -> "rishikksh20/iSTFTNet-pytorch"
"brentspell/hifi-gan-bwe" -> "Labmem-Zhouyx/CDFSE_FastSpeech2"
"jik876/hifi-gan" -> "kan-bayashi/ParallelWaveGAN"
"jik876/hifi-gan" -> "ming024/FastSpeech2"
"jik876/hifi-gan" -> "jaywalnut310/glow-tts"
"jik876/hifi-gan" -> "descriptinc/melgan-neurips"
"jik876/hifi-gan" -> "xcmyz/FastSpeech"
"jik876/hifi-gan" -> "xcmyz/speech-synthesis-paper"
"jik876/hifi-gan" -> "NVIDIA/BigVGAN"
"jik876/hifi-gan" -> "lmnt-com/diffwave"
"jik876/hifi-gan" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"jik876/hifi-gan" -> "auspicious3000/autovc"
"jik876/hifi-gan" -> "mozilla/LPCNet"
"jik876/hifi-gan" -> "soobinseo/Transformer-TTS"
"jik876/hifi-gan" -> "Tomiinek/Multilingual_Text_to_Speech"
"jik876/hifi-gan" -> "NVIDIA/mellotron"
"jik876/hifi-gan" -> "tts-tutorial/survey"
"openspeech-team/openspeech" -> "sooftware/conformer"
"openspeech-team/openspeech" -> "sooftware/kospeech" ["e"=1]
"openspeech-team/openspeech" -> "burchim/EfficientConformer"
"openspeech-team/openspeech" -> "kssteven418/Squeezeformer"
"openspeech-team/openspeech" -> "gentaiscool/end2end-asr-pytorch"
"openspeech-team/openspeech" -> "hirofumi0810/neural_sp"
"openspeech-team/openspeech" -> "SpeechColab/GigaSpeech"
"openspeech-team/openspeech" -> "lhotse-speech/lhotse"
"openspeech-team/openspeech" -> "upskyy/Squeezeformer"
"openspeech-team/openspeech" -> "TensorSpeech/TensorFlowASR"
"openspeech-team/openspeech" -> "facebookresearch/WavAugment"
"openspeech-team/openspeech" -> "danpovey/fast_rnnt"
"openspeech-team/openspeech" -> "tencent-ailab/pika"
"openspeech-team/openspeech" -> "k2-fsa/icefall"
"openspeech-team/openspeech" -> "sooftware/Speech-Recognition-Tutorial" ["e"=1]
"sooftware/conformer" -> "openspeech-team/openspeech"
"sooftware/conformer" -> "lucidrains/conformer"
"sooftware/conformer" -> "YuanGongND/ast"
"sooftware/conformer" -> "burchim/EfficientConformer"
"sooftware/conformer" -> "s3prl/s3prl"
"sooftware/conformer" -> "microsoft/DNS-Challenge" ["e"=1]
"sooftware/conformer" -> "sooftware/KoSpeech" ["e"=1]
"sooftware/conformer" -> "iver56/audiomentations"
"sooftware/conformer" -> "TensorSpeech/TensorFlowASR"
"sooftware/conformer" -> "sooftware/openspeech" ["e"=1]
"sooftware/conformer" -> "huyanxin/DeepComplexCRN" ["e"=1]
"sooftware/conformer" -> "aliutkus/speechmetrics" ["e"=1]
"sooftware/conformer" -> "Wenzhe-Liu/awesome-speech-enhancement" ["e"=1]
"sooftware/conformer" -> "kaituoxu/Speech-Transformer"
"sooftware/conformer" -> "SpeechColab/GigaSpeech"
"xdcesc/my_ch_speech_recognition" -> "ky1994/SpeechRecognition"
"lucidrains/musiclm-pytorch" -> "lucidrains/audiolm-pytorch"
"lucidrains/musiclm-pytorch" -> "archinetai/audio-diffusion-pytorch"
"lucidrains/musiclm-pytorch" -> "haoheliu/AudioLDM"
"lucidrains/musiclm-pytorch" -> "archinetai/audio-ai-timeline"
"lucidrains/musiclm-pytorch" -> "riffusion/riffusion"
"lucidrains/musiclm-pytorch" -> "LAION-AI/CLAP"
"lucidrains/musiclm-pytorch" -> "facebookresearch/encodec"
"lucidrains/musiclm-pytorch" -> "microsoft/muzic"
"lucidrains/musiclm-pytorch" -> "enhuiz/vall-e"
"lucidrains/musiclm-pytorch" -> "zhvng/open-musiclm"
"lucidrains/musiclm-pytorch" -> "LAION-AI/audio-dataset"
"lucidrains/musiclm-pytorch" -> "teticio/audio-diffusion"
"lucidrains/musiclm-pytorch" -> "MubertAI/Mubert-Text-to-Music"
"lucidrains/musiclm-pytorch" -> "salu133445/musegan" ["e"=1]
"lucidrains/musiclm-pytorch" -> "NVIDIA/BigVGAN"
"keithito/tacotron" -> "Rayhane-mamah/Tacotron-2"
"keithito/tacotron" -> "Kyubyong/tacotron"
"keithito/tacotron" -> "r9y9/wavenet_vocoder"
"keithito/tacotron" -> "NVIDIA/tacotron2"
"keithito/tacotron" -> "fatchord/WaveRNN"
"keithito/tacotron" -> "r9y9/deepvoice3_pytorch"
"keithito/tacotron" -> "NVIDIA/waveglow"
"keithito/tacotron" -> "mozilla/TTS"
"keithito/tacotron" -> "CSTR-Edinburgh/merlin"
"keithito/tacotron" -> "begeekmyfriend/tacotron"
"keithito/tacotron" -> "kan-bayashi/ParallelWaveGAN"
"keithito/tacotron" -> "syang1993/gst-tacotron"
"keithito/tacotron" -> "ibab/tensorflow-wavenet"
"keithito/tacotron" -> "xcmyz/FastSpeech"
"keithito/tacotron" -> "espnet/espnet"
"jongwoojeff/DiscreteMathematics" -> "hongshin/DiscreteMath"
"QosmoInc/neutone_sdk" -> "acids-ircam/nn_tilde"
"QosmoInc/neutone_sdk" -> "jatinchowdhury18/RTNeural" ["e"=1]
"QosmoInc/neutone_sdk" -> "acids-ircam/RAVE"
"QosmoInc/neutone_sdk" -> "fcaspe/ddx7"
"QosmoInc/neutone_sdk" -> "caillonantoine/RAVE"
"QosmoInc/neutone_sdk" -> "ben-hayes/neural-waveshaping-synthesis"
"QosmoInc/neutone_sdk" -> "Fyfe93/RAVE-audition"
"QosmoInc/neutone_sdk" -> "moiseshorta/MelSpecVAE"
"QosmoInc/neutone_sdk" -> "naotokui/RhythmVAE_M4L"
"QosmoInc/neutone_sdk" -> "magenta/ddsp-vst"
"QosmoInc/neutone_sdk" -> "adobe-research/DeepAFx-ST"
"QosmoInc/neutone_sdk" -> "ninon-io/Impact-Synth-Hardware"
"adefossez/julius" -> "csteinmetz1/auraloss"
"adefossez/julius" -> "maxrmorrison/torchcrepe"
"adefossez/julius" -> "torchsynth/torchsynth"
"adefossez/julius" -> "KinWaiCheuk/nnAudio"
"adefossez/julius" -> "ivanvovk/WaveGrad"
"adefossez/julius" -> "pranaymanocha/PerceptualAudio" ["e"=1]
"adefossez/julius" -> "google-research/leaf-audio"
"adefossez/julius" -> "asteroid-team/torch-audiomentations"
"adefossez/julius" -> "facebookresearch/speech-resynthesis"
"adefossez/julius" -> "DolbyLaboratories/neural-upsampling-artifacts-audio"
"adefossez/julius" -> "yoyololicon/diffwave-sr"
"adefossez/julius" -> "ben-hayes/neural-waveshaping-synthesis"
"adefossez/julius" -> "facebookresearch/WavAugment"
"csteinmetz1/ai-audio-startups" -> "csteinmetz1/auraloss"
"csteinmetz1/ai-audio-startups" -> "asteroid-team/torch-audiomentations"
"csteinmetz1/ai-audio-startups" -> "archinetai/audio-ai-timeline"
"csteinmetz1/ai-audio-startups" -> "mir-dataset-loaders/mirdata" ["e"=1]
"csteinmetz1/ai-audio-startups" -> "minzwon/sota-music-tagging-models" ["e"=1]
"csteinmetz1/ai-audio-startups" -> "adobe-research/DeepAFx-ST"
"csteinmetz1/ai-audio-startups" -> "archinetai/audio-diffusion-pytorch"
"csteinmetz1/ai-audio-startups" -> "KinWaiCheuk/nnAudio"
"csteinmetz1/ai-audio-startups" -> "Spijkervet/CLMR" ["e"=1]
"csteinmetz1/ai-audio-startups" -> "iver56/audiomentations"
"csteinmetz1/ai-audio-startups" -> "facebookresearch/WavAugment"
"csteinmetz1/ai-audio-startups" -> "pranaymanocha/PerceptualAudio" ["e"=1]
"csteinmetz1/ai-audio-startups" -> "QosmoInc/neutone_sdk"
"csteinmetz1/ai-audio-startups" -> "magenta/ddsp" ["e"=1]
"csteinmetz1/ai-audio-startups" -> "haoheliu/AudioLDM"
"csteinmetz1/auraloss" -> "adefossez/julius"
"csteinmetz1/auraloss" -> "ben-hayes/neural-waveshaping-synthesis"
"csteinmetz1/auraloss" -> "torchsynth/torchsynth"
"csteinmetz1/auraloss" -> "KinWaiCheuk/nnAudio"
"csteinmetz1/auraloss" -> "acids-ircam/ddsp_pytorch"
"csteinmetz1/auraloss" -> "facebookresearch/WavAugment"
"csteinmetz1/auraloss" -> "pranaymanocha/PerceptualAudio" ["e"=1]
"csteinmetz1/auraloss" -> "asteroid-team/torch-audiomentations"
"csteinmetz1/auraloss" -> "csteinmetz1/micro-tcn"
"csteinmetz1/auraloss" -> "maxrmorrison/torchcrepe"
"csteinmetz1/auraloss" -> "NVIDIA/BigVGAN"
"csteinmetz1/auraloss" -> "aliutkus/speechmetrics" ["e"=1]
"csteinmetz1/auraloss" -> "magenta/midi-ddsp"
"csteinmetz1/auraloss" -> "YatingMusic/ddsp-singing-vocoders"
"csteinmetz1/auraloss" -> "lmnt-com/diffwave"
"Francis-Komizu/Sovits" -> "bshall/soft-vc"
"Francis-Komizu/Sovits" -> "Francis-Komizu/VITS"
"Francis-Komizu/Sovits" -> "PlayVoice/VI-SVC"
"Francis-Komizu/Sovits" -> "OlaWod/FreeVC"
"Francis-Komizu/Sovits" -> "bshall/hubert"
"Francis-Komizu/Sovits" -> "IceKyrin/sovits_guide"
"Francis-Komizu/Sovits" -> "MasayaKawamura/MB-iSTFT-VITS"
"Francis-Komizu/Sovits" -> "AlexandaJerry/whisper-vits-japanese"
"Francis-Komizu/Sovits" -> "anonymous-pits/pits"
"Francis-Komizu/Sovits" -> "luoyily/MoeTTS"
"Francis-Komizu/Sovits" -> "polvanrijn/VoiceMe"
"Francis-Komizu/Sovits" -> "innnky/emotional-vits"
"Francis-Komizu/Sovits" -> "Rongjiehuang/GenerSpeech"
"Francis-Komizu/Sovits" -> "heatz123/naturalspeech"
"Francis-Komizu/Sovits" -> "PlayVoice/vits_chinese"
"BuglyDevTeam/Bugly-Android-Demo" -> "Meituan-Dianping/walle" ["e"=1]
"BuglyDevTeam/Bugly-Android-Demo" -> "umeng/MultiFunctionAndroidDemo"
"BuglyDevTeam/Bugly-Android-Demo" -> "Jay-Goo/ProtectedApkResignerForWalle" ["e"=1]
"BuglyDevTeam/Bugly-Android-Demo" -> "aliyun/aliyun-oss-android-sdk"
"BuglyDevTeam/Bugly-Android-Demo" -> "aliyun/alicloud-android-demo"
"microsoft/SpeechT5" -> "keonlee9420/DailyTalk"
"microsoft/SpeechT5" -> "microsoft/NeuralSpeech"
"microsoft/SpeechT5" -> "NVIDIA/BigVGAN"
"microsoft/SpeechT5" -> "lucidrains/audiolm-pytorch"
"microsoft/SpeechT5" -> "yangdongchao/Text-to-sound-Synthesis"
"microsoft/SpeechT5" -> "Edresson/YourTTS"
"microsoft/SpeechT5" -> "hhguo/MSMC-TTS"
"microsoft/SpeechT5" -> "anonymous-pits/pits"
"microsoft/SpeechT5" -> "huawei-noah/Speech-Backbones"
"microsoft/SpeechT5" -> "microsoft/UniSpeech"
"microsoft/SpeechT5" -> "keonlee9420/Comprehensive-Transformer-TTS"
"microsoft/SpeechT5" -> "wenet-e2e/wetts"
"microsoft/SpeechT5" -> "Rongjiehuang/ProDiff"
"microsoft/SpeechT5" -> "NVIDIA/radtts"
"microsoft/SpeechT5" -> "Rongjiehuang/FastDiff"
"VOICEVOX/voicevox" -> "VOICEVOX/voicevox_engine"
"VOICEVOX/voicevox" -> "VOICEVOX/voicevox_core"
"VOICEVOX/voicevox" -> "isletennos/MMVC_Trainer"
"VOICEVOX/voicevox" -> "yuru7/HackGen" ["e"=1]
"VOICEVOX/voicevox" -> "ePi5131/patch.aul" ["e"=1]
"VOICEVOX/voicevox" -> "jiro4989/ojosama" ["e"=1]
"VOICEVOX/voicevox" -> "suzune25254649/bakusoku_aviutl_plugin" ["e"=1]
"VOICEVOX/voicevox" -> "yuru7/udev-gothic" ["e"=1]
"VOICEVOX/voicevox" -> "misskey-dev/misskey" ["e"=1]
"VOICEVOX/voicevox" -> "cocoa-mhlw/cocoa" ["e"=1]
"VOICEVOX/voicevox" -> "googlefonts/morisawa-biz-ud-gothic" ["e"=1]
"VOICEVOX/voicevox" -> "lilxyzw/lilToon" ["e"=1]
"VOICEVOX/voicevox" -> "hecomi/uLipSync" ["e"=1]
"VOICEVOX/voicevox" -> "vrm-c/UniVRM" ["e"=1]
"VOICEVOX/voicevox" -> "google/mozc-devices" ["e"=1]
"wenet-e2e/wenet" -> "espnet/espnet"
"wenet-e2e/wenet" -> "k2-fsa/k2"
"wenet-e2e/wenet" -> "speechbrain/speechbrain"
"wenet-e2e/wenet" -> "s3prl/s3prl"
"wenet-e2e/wenet" -> "k2-fsa/icefall"
"wenet-e2e/wenet" -> "TencentGameMate/chinese_speech_pretrain"
"wenet-e2e/wenet" -> "mravanelli/pytorch-kaldi"
"wenet-e2e/wenet" -> "microsoft/DNS-Challenge" ["e"=1]
"wenet-e2e/wenet" -> "ming024/FastSpeech2"
"wenet-e2e/wenet" -> "microsoft/NeuralSpeech"
"wenet-e2e/wenet" -> "athena-team/athena" ["e"=1]
"wenet-e2e/wenet" -> "Snowdar/asv-subtools"
"wenet-e2e/wenet" -> "PaddlePaddle/PaddleSpeech"
"wenet-e2e/wenet" -> "jik876/hifi-gan"
"wenet-e2e/wenet" -> "SpeechColab/GigaSpeech"
"cmusphinx/pocketsphinx" -> "cmusphinx/sphinxbase"
"cmusphinx/pocketsphinx" -> "cmusphinx/sphinx4"
"cmusphinx/pocketsphinx" -> "cmusphinx/pocketsphinx-android-demo"
"cmusphinx/pocketsphinx" -> "Kitt-AI/snowboy" ["e"=1]
"cmusphinx/pocketsphinx" -> "kaldi-asr/kaldi"
"cmusphinx/pocketsphinx" -> "cmusphinx/pocketsphinx-python"
"cmusphinx/pocketsphinx" -> "julius-speech/julius"
"cmusphinx/pocketsphinx" -> "Uberi/speech_recognition"
"cmusphinx/pocketsphinx" -> "jasperproject/jasper-client" ["e"=1]
"cmusphinx/pocketsphinx" -> "cmusphinx/pocketsphinx-android"
"cmusphinx/pocketsphinx" -> "cmusphinx/sphinxtrain"
"cmusphinx/pocketsphinx" -> "alphacep/vosk-api"
"cmusphinx/pocketsphinx" -> "syl22-00/pocketsphinx.js" ["e"=1]
"cmusphinx/pocketsphinx" -> "bambocher/pocketsphinx-python"
"cmusphinx/pocketsphinx" -> "MycroftAI/mycroft-precise" ["e"=1]
"UFAL-DSG/alex" -> "UFAL-DSG/pykaldi"
"UFAL-DSG/alex" -> "UFAL-DSG/alex-asr"
"UFAL-DSG/alex" -> "cuayahuitl/SimpleDS"
"UFAL-DSG/alex" -> "alumae/gst-kaldi-nnet2-online"
"NaruseMioShirakana/MoeSS" -> "luoyily/MoeTTS"
"NaruseMioShirakana/MoeSS" -> "fishaudio/fish-diffusion"
"NaruseMioShirakana/MoeSS" -> "innnky/emotional-vits"
"NaruseMioShirakana/MoeSS" -> "zhaohui8969/VST_NetProcess-"
"NaruseMioShirakana/MoeSS" -> "IceKyrin/sovits_guide"
"NaruseMioShirakana/MoeSS" -> "CjangCjengh/vits"
"NaruseMioShirakana/MoeSS" -> "Paraworks/vits_with_chatgpt-gpt3"
"NaruseMioShirakana/MoeSS" -> "liujing04/Retrieval-based-Voice-Conversion-WebUI"
"NaruseMioShirakana/MoeSS" -> "innnky/so-vits-svc"
"NaruseMioShirakana/MoeSS" -> "weirdseed/Vits-Android-ncnn"
"NaruseMioShirakana/MoeSS" -> "w4123/GenshinVoice"
"NaruseMioShirakana/MoeSS" -> "Plachtaa/VITS-fast-fine-tuning"
"NaruseMioShirakana/MoeSS" -> "SayaSS/vits-finetuning"
"NaruseMioShirakana/MoeSS" -> "Francis-Komizu/Sovits"
"NaruseMioShirakana/MoeSS" -> "CjangCjengh/MoeGoe"
"AKSHAYUBHAT/DeepVideoAnalytics" -> "oarriaga/face_classification"
"AKSHAYUBHAT/DeepVideoAnalytics" -> "MrNothing/AI-Blocks"
"AKSHAYUBHAT/DeepVideoAnalytics" -> "junyanz/iGAN" ["e"=1]
"AKSHAYUBHAT/DeepVideoAnalytics" -> "bgshih/crnn" ["e"=1]
"AKSHAYUBHAT/DeepVideoAnalytics" -> "andabi/deep-voice-conversion"
"AKSHAYUBHAT/DeepVideoAnalytics" -> "buriburisuri/speech-to-text-wavenet"
"AKSHAYUBHAT/DeepVideoAnalytics" -> "uber/horovod" ["e"=1]
"AKSHAYUBHAT/DeepVideoAnalytics" -> "eragonruan/text-detection-ctpn" ["e"=1]
"AKSHAYUBHAT/DeepVideoAnalytics" -> "OpenNMT/OpenNMT" ["e"=1]
"AKSHAYUBHAT/DeepVideoAnalytics" -> "bear63/sceneReco" ["e"=1]
"AKSHAYUBHAT/DeepVideoAnalytics" -> "yunjey/StarGAN" ["e"=1]
"AKSHAYUBHAT/DeepVideoAnalytics" -> "argman/EAST" ["e"=1]
"AKSHAYUBHAT/DeepVideoAnalytics" -> "NVIDIA/pix2pixHD" ["e"=1]
"AKSHAYUBHAT/DeepVideoAnalytics" -> "tianzhi0549/CTPN" ["e"=1]
"AKSHAYUBHAT/DeepVideoAnalytics" -> "DmitryUlyanov/deep-image-prior" ["e"=1]
"Zuntan03/CharFramework" -> "Zuntan03/LatentCoupleHelper" ["e"=1]
"Zuntan03/CharFramework" -> "Zuntan03/LoraBlockWeightPlotHelper"
"Zuntan03/CharFramework" -> "liasece/sd-webui-train-tools"
"kslz/SoundLabel" -> "dominoar/QChatPlugins"
"SamuelBroughton/StarGAN-Voice-Conversion-2" -> "hujinsen/pytorch-StarGAN-VC"
"SamuelBroughton/StarGAN-Voice-Conversion-2" -> "bigpon/vcc20_baseline_cyclevae"
"SamuelBroughton/StarGAN-Voice-Conversion-2" -> "liusongxiang/StarGAN-Voice-Conversion"
"SamuelBroughton/StarGAN-Voice-Conversion-2" -> "TaiChunYen/Pytorch-CycleGAN-VC2"
"SamuelBroughton/StarGAN-Voice-Conversion-2" -> "SamuelBroughton/StarGAN-Voice-Conversion"
"SamuelBroughton/StarGAN-Voice-Conversion-2" -> "Oscarshu0719/pytorch-StarGAN-VC2"
"SamuelBroughton/StarGAN-Voice-Conversion-2" -> "MingjieChen/LowResourceVC"
"SamuelBroughton/StarGAN-Voice-Conversion-2" -> "k2kobayashi/crank"
"SamuelBroughton/StarGAN-Voice-Conversion-2" -> "bshall/ZeroSpeech"
"SamuelBroughton/StarGAN-Voice-Conversion-2" -> "Wendison/VQMIVC"
"SamuelBroughton/StarGAN-Voice-Conversion-2" -> "dipjyoti92/StarGAN-Voice-Conversion-2"
"hujinsen/pytorch-StarGAN-VC" -> "liusongxiang/StarGAN-Voice-Conversion"
"hujinsen/pytorch-StarGAN-VC" -> "SamuelBroughton/StarGAN-Voice-Conversion-2"
"hujinsen/pytorch-StarGAN-VC" -> "hujinsen/StarGAN-Voice-Conversion"
"hujinsen/pytorch-StarGAN-VC" -> "jjery2243542/adaptive_voice_conversion"
"hujinsen/pytorch-StarGAN-VC" -> "auspicious3000/autovc"
"hujinsen/pytorch-StarGAN-VC" -> "jjery2243542/voice_conversion"
"hujinsen/pytorch-StarGAN-VC" -> "leimao/Voice_Converter_CycleGAN"
"hujinsen/pytorch-StarGAN-VC" -> "TaiChunYen/Pytorch-CycleGAN-VC2"
"hujinsen/pytorch-StarGAN-VC" -> "Oscarshu0719/pytorch-StarGAN-VC2"
"hujinsen/pytorch-StarGAN-VC" -> "bigpon/vcc20_baseline_cyclevae"
"hujinsen/pytorch-StarGAN-VC" -> "jackaduma/CycleGAN-VC2"
"hujinsen/pytorch-StarGAN-VC" -> "joansj/blow"
"hujinsen/pytorch-StarGAN-VC" -> "k2kobayashi/sprocket"
"hujinsen/pytorch-StarGAN-VC" -> "ericwudayi/SkipVQVC"
"hujinsen/pytorch-StarGAN-VC" -> "marcoppasini/MelGAN-VC"
"0x454447415244/HandwritingRecognitionSystem" -> "sushant097/Handwritten-Line-Text-Recognition-using-Deep-Learning-with-Tensorflow"
"0x454447415244/HandwritingRecognitionSystem" -> "Breta01/handwriting-ocr"
"0x454447415244/HandwritingRecognitionSystem" -> "githubharald/SimpleHTR"
"0x454447415244/HandwritingRecognitionSystem" -> "awslabs/handwritten-text-recognition-for-apache-mxnet"
"0x454447415244/HandwritingRecognitionSystem" -> "arthurflor23/handwritten-text-recognition"
"0x454447415244/HandwritingRecognitionSystem" -> "cwig/start_follow_read"
"0x454447415244/HandwritingRecognitionSystem" -> "githubharald/WordDetector"
"0x454447415244/HandwritingRecognitionSystem" -> "frereit/TensorflowHandwritingRecognition"
"0x454447415244/HandwritingRecognitionSystem" -> "lquirosd/P2PaLA" ["e"=1]
"0x454447415244/HandwritingRecognitionSystem" -> "lamhoangtung/LineHTR"
"0x454447415244/HandwritingRecognitionSystem" -> "dhlab-epfl/dhSegment" ["e"=1]
"0x454447415244/HandwritingRecognitionSystem" -> "mittagessen/kraken" ["e"=1]
"0x454447415244/HandwritingRecognitionSystem" -> "githubharald/CTCDecoder"
"0x454447415244/HandwritingRecognitionSystem" -> "githubharald/CTCWordBeamSearch"
"0x454447415244/HandwritingRecognitionSystem" -> "sjvasquez/handwriting-synthesis" ["e"=1]
"google/speaker-id" -> "wq2012/SpectralCluster"
"google/speaker-id" -> "nryant/dscore"
"google/speaker-id" -> "hitachi-speech/EEND"
"google/speaker-id" -> "taylorlu/Speaker-Diarization"
"google/speaker-id" -> "yistLin/dvector"
"google/speaker-id" -> "BUTSpeechFIT/VBx"
"google/speaker-id" -> "VITA-Group/AutoSpeech"
"Voine/ChatWaifu_Mobile" -> "weirdseed/Vits-Android-ncnn"
"Voine/ChatWaifu_Mobile" -> "cjyaddone/ChatWaifu"
"Voine/ChatWaifu_Mobile" -> "weirdseed/vits-ncnn-convert-tool"
"Voine/ChatWaifu_Mobile" -> "Arkueid/Live2DMascot"
"Voine/ChatWaifu_Mobile" -> "EdVince/diffusers-ncnn" ["e"=1]
"Voine/ChatWaifu_Mobile" -> "MuBai-He/ChatWaifu-marai"
"Voine/ChatWaifu_Mobile" -> "cjyaddone/ChatWaifuL2D"
"Voine/ChatWaifu_Mobile" -> "Paraworks/vits_with_chatgpt-gpt3"
"Voine/ChatWaifu_Mobile" -> "gstory0404/chatgpt_app"
"Voine/ChatWaifu_Mobile" -> "FeiGeChuanShu/CodeFormer-ncnn" ["e"=1]
"BenAAndrew/Voice-Cloning-App" -> "deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning"
"BenAAndrew/Voice-Cloning-App" -> "Edresson/YourTTS"
"BenAAndrew/Voice-Cloning-App" -> "SforAiDl/Neural-Voice-Cloning-With-Few-Samples"
"BenAAndrew/Voice-Cloning-App" -> "MachineEditor/MachineVideoEditor" ["e"=1]
"BenAAndrew/Voice-Cloning-App" -> "Tomiinek/Multilingual_Text_to_Speech"
"BenAAndrew/Voice-Cloning-App" -> "neonbjb/tortoise-tts" ["e"=1]
"BenAAndrew/Voice-Cloning-App" -> "coqui-ai/TTS"
"BenAAndrew/Voice-Cloning-App" -> "SortAnon/ControllableTalkNet"
"BenAAndrew/Voice-Cloning-App" -> "resemble-ai/Resemblyzer"
"BenAAndrew/Voice-Cloning-App" -> "neuralchen/SimSwap" ["e"=1]
"BenAAndrew/Voice-Cloning-App" -> "dunky11/voicesmith"
"BenAAndrew/Voice-Cloning-App" -> "vlomme/Multi-Tacotron-Voice-Cloning"
"BenAAndrew/Voice-Cloning-App" -> "CMsmartvoice/One-Shot-Voice-Cloning"
"BenAAndrew/Voice-Cloning-App" -> "rotemtzaban/STIT" ["e"=1]
"BenAAndrew/Voice-Cloning-App" -> "enhuiz/vall-e"
"dominoar/QChatPlugins" -> "kslz/SoundLabel"
"PaddlePaddle/DeepSpeech" -> "SeanNaren/deepspeech.pytorch"
"PaddlePaddle/DeepSpeech" -> "audier/DeepSpeechRecognition"
"PaddlePaddle/DeepSpeech" -> "libai3/masr"
"PaddlePaddle/DeepSpeech" -> "mravanelli/pytorch-kaldi"
"PaddlePaddle/DeepSpeech" -> "nl8590687/ASRT_SpeechRecognition"
"PaddlePaddle/DeepSpeech" -> "kpu/kenlm"
"PaddlePaddle/DeepSpeech" -> "facebookresearch/wav2letter"
"PaddlePaddle/DeepSpeech" -> "zzw922cn/Automatic_Speech_Recognition"
"PaddlePaddle/DeepSpeech" -> "srvk/eesen"
"PaddlePaddle/DeepSpeech" -> "PaddlePaddle/Parakeet"
"PaddlePaddle/DeepSpeech" -> "syhw/wer_are_we"
"PaddlePaddle/DeepSpeech" -> "espnet/espnet"
"PaddlePaddle/DeepSpeech" -> "kaituoxu/Speech-Transformer"
"PaddlePaddle/DeepSpeech" -> "zzw922cn/awesome-speech-recognition-speech-synthesis-papers"
"PaddlePaddle/DeepSpeech" -> "athena-team/athena" ["e"=1]
"astorfi/3D-convolutional-speaker-recognition" -> "philipperemy/deep-speaker"
"astorfi/3D-convolutional-speaker-recognition" -> "qqueing/DeepSpeaker-pytorch"
"astorfi/3D-convolutional-speaker-recognition" -> "ppwwyyxx/speaker-recognition"
"astorfi/3D-convolutional-speaker-recognition" -> "HarryVolek/PyTorch_Speaker_Verification"
"astorfi/3D-convolutional-speaker-recognition" -> "crouchred/speaker-recognition-py3"
"astorfi/3D-convolutional-speaker-recognition" -> "a-nagrani/VGGVox"
"astorfi/3D-convolutional-speaker-recognition" -> "Janghyun1230/Speaker_Verification"
"astorfi/3D-convolutional-speaker-recognition" -> "astorfi/3D-convolutional-speaker-recognition-pytorch"
"astorfi/3D-convolutional-speaker-recognition" -> "Walleclipse/Deep_Speaker-speaker_recognition_system"
"astorfi/3D-convolutional-speaker-recognition" -> "mravanelli/SincNet"
"astorfi/3D-convolutional-speaker-recognition" -> "WeidiXie/VGG-Speaker-Recognition"
"astorfi/3D-convolutional-speaker-recognition" -> "rajathkmp/speaker-verification"
"astorfi/3D-convolutional-speaker-recognition" -> "astorfi/speechpy"
"astorfi/3D-convolutional-speaker-recognition" -> "google/uis-rnn"
"astorfi/3D-convolutional-speaker-recognition" -> "andabi/voice-vector"
"google/sparrowhawk" -> "google/TextNormalizationCoveringGrammars"
"google/sparrowhawk" -> "rwsproat/text-normalization-data"
"google/sparrowhawk" -> "thuhcsi/Crystal"
"google/sparrowhawk" -> "googlei18n/language-resources"
"google/sparrowhawk" -> "danijel3/SparrowhawkTest"
"ppwwyyxx/speaker-recognition" -> "crouchred/speaker-recognition-py3"
"ppwwyyxx/speaker-recognition" -> "astorfi/3D-convolutional-speaker-recognition"
"ppwwyyxx/speaker-recognition" -> "philipperemy/deep-speaker"
"ppwwyyxx/speaker-recognition" -> "qqueing/DeepSpeaker-pytorch"
"ppwwyyxx/speaker-recognition" -> "Atul-Anand-Jha/Speaker-Identification-Python"
"ppwwyyxx/speaker-recognition" -> "ALIZE-Speaker-Recognition/LIA_RAL"
"ppwwyyxx/speaker-recognition" -> "a-nagrani/VGGVox"
"ppwwyyxx/speaker-recognition" -> "pannous/tensorflow-speech-recognition"
"ppwwyyxx/speaker-recognition" -> "jameslyons/python_speech_features"
"ppwwyyxx/speaker-recognition" -> "orchidas/Speaker-Recognition"
"ppwwyyxx/speaker-recognition" -> "amaurycrickx/recognito"
"ppwwyyxx/speaker-recognition" -> "Microsoft/Cognitive-SpeakerRecognition-Python"
"ppwwyyxx/speaker-recognition" -> "andabi/voice-vector"
"ppwwyyxx/speaker-recognition" -> "GauravWaghmare/Speaker-Identification"
"ppwwyyxx/speaker-recognition" -> "WeidiXie/VGG-Speaker-Recognition"
"Jakobovski/free-spoken-digit-dataset" -> "YoavRamon/awesome-kaldi"
"Jakobovski/free-spoken-digit-dataset" -> "soerenab/AudioMNIST"
"Jakobovski/free-spoken-digit-dataset" -> "pykaldi/pykaldi"
"Jakobovski/free-spoken-digit-dataset" -> "mravanelli/pytorch-kaldi"
"Jakobovski/free-spoken-digit-dataset" -> "karoldvl/ESC-50" ["e"=1]
"Jakobovski/free-spoken-digit-dataset" -> "jim-schwoebel/voice_datasets" ["e"=1]
"Jakobovski/free-spoken-digit-dataset" -> "philipperemy/timit"
"Jakobovski/free-spoken-digit-dataset" -> "castorini/honk" ["e"=1]
"Jakobovski/free-spoken-digit-dataset" -> "DemisEom/SpecAugment"
"Jakobovski/free-spoken-digit-dataset" -> "jtkim-kaist/VAD"
"Jakobovski/free-spoken-digit-dataset" -> "mindorii/kws" ["e"=1]
"Jakobovski/free-spoken-digit-dataset" -> "lhotse-speech/lhotse"
"Jakobovski/free-spoken-digit-dataset" -> "vesis84/kaldi-io-for-python"
"Jakobovski/free-spoken-digit-dataset" -> "festvox/datasets-CMU_Wilderness"
"Jakobovski/free-spoken-digit-dataset" -> "drethage/speech-denoising-wavenet" ["e"=1]
"chrisdonahue/wavegan" -> "descriptinc/melgan-neurips"
"chrisdonahue/wavegan" -> "kan-bayashi/ParallelWaveGAN"
"chrisdonahue/wavegan" -> "NVIDIA/waveglow"
"chrisdonahue/wavegan" -> "r9y9/wavenet_vocoder"
"chrisdonahue/wavegan" -> "mozilla/LPCNet"
"chrisdonahue/wavegan" -> "NVIDIA/nv-wavenet"
"chrisdonahue/wavegan" -> "mostafaelaraby/wavegan-pytorch"
"chrisdonahue/wavegan" -> "fatchord/WaveRNN"
"chrisdonahue/wavegan" -> "magenta/ddsp" ["e"=1]
"chrisdonahue/wavegan" -> "seungwonpark/melgan"
"chrisdonahue/wavegan" -> "f90/Wave-U-Net" ["e"=1]
"chrisdonahue/wavegan" -> "soobinseo/Transformer-TTS"
"chrisdonahue/wavegan" -> "santi-pdp/segan" ["e"=1]
"chrisdonahue/wavegan" -> "ybayle/awesome-deep-learning-music" ["e"=1]
"chrisdonahue/wavegan" -> "marl/crepe" ["e"=1]
"HGU-DLLAB/Korean-FastSpeech2-Pytorch" -> "rishikksh20/VocGAN"
"HGU-DLLAB/Korean-FastSpeech2-Pytorch" -> "Kyubyong/g2pK" ["e"=1]
"HGU-DLLAB/Korean-FastSpeech2-Pytorch" -> "emotiontts/emotiontts_open_db"
"HGU-DLLAB/Korean-FastSpeech2-Pytorch" -> "keonlee9420/Expressive-FastSpeech2"
"HGU-DLLAB/Korean-FastSpeech2-Pytorch" -> "WICWIU/WICWIU"
"mmorise/ita-corpus" -> "mmorise/rohan4600"
"mmorise/ita-corpus" -> "isletennos/MMVC_Client"
"SeanNaren/deepspeech.pytorch" -> "mravanelli/pytorch-kaldi"
"SeanNaren/deepspeech.pytorch" -> "PaddlePaddle/DeepSpeech"
"SeanNaren/deepspeech.pytorch" -> "syhw/wer_are_we"
"SeanNaren/deepspeech.pytorch" -> "awni/speech"
"SeanNaren/deepspeech.pytorch" -> "espnet/espnet"
"SeanNaren/deepspeech.pytorch" -> "Alexander-H-Liu/End-to-end-ASR-Pytorch"
"SeanNaren/deepspeech.pytorch" -> "kaituoxu/Speech-Transformer"
"SeanNaren/deepspeech.pytorch" -> "zzw922cn/Automatic_Speech_Recognition"
"SeanNaren/deepspeech.pytorch" -> "facebookresearch/wav2letter"
"SeanNaren/deepspeech.pytorch" -> "srvk/eesen"
"SeanNaren/deepspeech.pytorch" -> "SeanNaren/warp-ctc" ["e"=1]
"SeanNaren/deepspeech.pytorch" -> "parlance/ctcdecode"
"SeanNaren/deepspeech.pytorch" -> "audier/DeepSpeechRecognition"
"SeanNaren/deepspeech.pytorch" -> "zzw922cn/awesome-speech-recognition-speech-synthesis-papers"
"SeanNaren/deepspeech.pytorch" -> "jameslyons/python_speech_features"
"svc-develop-team/so-vits-svc" -> "innnky/so-vits-svc"
"svc-develop-team/so-vits-svc" -> "jaywalnut310/vits"
"svc-develop-team/so-vits-svc" -> "34j/so-vits-svc-fork"
"svc-develop-team/so-vits-svc" -> "Plachtaa/VITS-fast-fine-tuning"
"svc-develop-team/so-vits-svc" -> "openvpi/DiffSinger"
"svc-develop-team/so-vits-svc" -> "Anjok07/ultimatevocalremovergui"
"svc-develop-team/so-vits-svc" -> "prophesier/diff-svc"
"svc-develop-team/so-vits-svc" -> "w-okada/voice-changer"
"svc-develop-team/so-vits-svc" -> "CjangCjengh/MoeGoe"
"svc-develop-team/so-vits-svc" -> "MoonInTheRiver/DiffSinger"
"svc-develop-team/so-vits-svc" -> "liujing04/Retrieval-based-Voice-Conversion-WebUI"
"svc-develop-team/so-vits-svc" -> "THUDM/ChatGLM-6B" ["e"=1]
"svc-develop-team/so-vits-svc" -> "Mikubill/sd-webui-controlnet" ["e"=1]
"svc-develop-team/so-vits-svc" -> "innnky/emotional-vits"
"svc-develop-team/so-vits-svc" -> "babysor/MockingBird" ["e"=1]
"aliyun/alicloud-android-demo" -> "aliyun/aliyun-oss-android-sdk"
"aliyun/alicloud-android-demo" -> "BuglyDevTeam/Bugly-Android-Demo"
"aliyun/alicloud-android-demo" -> "alibaba/alpha" ["e"=1]
"aliyun/alicloud-android-demo" -> "alibaba/Tangram-Android" ["e"=1]
"aliyun/alicloud-android-demo" -> "Qihoo360/ArgusAPM" ["e"=1]
"aliyun/alicloud-android-demo" -> "HujiangTechnology/gradle_plugin_android_aspectjx" ["e"=1]
"xxbb1234021/speech_recognition" -> "nobody132/masr"
"xxbb1234021/speech_recognition" -> "audier/DeepSpeechRecognition"
"xxbb1234021/speech_recognition" -> "libai3/masr"
"xxbb1234021/speech_recognition" -> "nl8590687/ASRT_SpeechRecognition"
"xxbb1234021/speech_recognition" -> "zw76859420/ASR_Theory"
"xxbb1234021/speech_recognition" -> "sailist/ASRFrame"
"xxbb1234021/speech_recognition" -> "zw76859420/ASR_Syllable"
"xxbb1234021/speech_recognition" -> "Pelhans/ZASR_tensorflow"
"xxbb1234021/speech_recognition" -> "Z-yq/TensorflowASR"
"xxbb1234021/speech_recognition" -> "tramphero/kaldi"
"xxbb1234021/speech_recognition" -> "daixiang789/tensorflow-examples"
"xxbb1234021/speech_recognition" -> "xdcesc/my_ch_speech_recognition"
"xxbb1234021/speech_recognition" -> "EliasCai/speech_recognition_ctc"
"xxbb1234021/speech_recognition" -> "hirofumi0810/tensorflow_end2end_speech_recognition"
"xxbb1234021/speech_recognition" -> "kaituoxu/Speech-Transformer"
"RHVoice/RHVoice" -> "Stypox/dicio-android" ["e"=1]
"RHVoice/RHVoice" -> "espeak-ng/espeak-ng"
"RHVoice/RHVoice" -> "snakers4/silero-models"
"RHVoice/RHVoice" -> "sodapng/voice-over-translation" ["e"=1]
"RHVoice/RHVoice" -> "alphacep/vosk-server"
"RHVoice/RHVoice" -> "jcsteh/osara" ["e"=1]
"RHVoice/RHVoice" -> "sharkboyto/nao"
"RHVoice/RHVoice" -> "qw123wh/new-clock-fdroid" ["e"=1]
"RHVoice/RHVoice" -> "SergeyShk/Speech-to-Text-Russian" ["e"=1]
"Rongjiehuang/FastDiff" -> "Rongjiehuang/ProDiff"
"Rongjiehuang/FastDiff" -> "tencent-ailab/bddm"
"Rongjiehuang/FastDiff" -> "YatingMusic/ddsp-singing-vocoders"
"Rongjiehuang/FastDiff" -> "Rongjiehuang/GenerSpeech"
"Rongjiehuang/FastDiff" -> "NVIDIA/BigVGAN"
"Rongjiehuang/FastDiff" -> "Rongjiehuang/TranSpeech"
"Rongjiehuang/FastDiff" -> "yerfor/SyntaSpeech"
"Rongjiehuang/FastDiff" -> "NVIDIA/radtts"
"Rongjiehuang/ProDiff" -> "Rongjiehuang/FastDiff"
"Rongjiehuang/ProDiff" -> "Rongjiehuang/GenerSpeech"
"Rongjiehuang/ProDiff" -> "Rongjiehuang/TranSpeech"
"Rongjiehuang/ProDiff" -> "YatingMusic/ddsp-singing-vocoders"
"Rongjiehuang/ProDiff" -> "WelkinYang/Learn2Sing2.0"
"Rongjiehuang/ProDiff" -> "huawei-noah/Speech-Backbones"
"rolczynski/Automatic-Speech-Recognition" -> "30stomercury/Automatic-Speech-Recognition"
"Hiroshiba/realtime-yukarin" -> "Hiroshiba/yukarin"
"Hiroshiba/realtime-yukarin" -> "Hiroshiba/become-yukarin"
"Hiroshiba/realtime-yukarin" -> "k2kobayashi/sprocket"
"Hiroshiba/realtime-yukarin" -> "pstuvwx/Deep_VoiceChanger"
"Hiroshiba/realtime-yukarin" -> "k2kobayashi/crank"
"Hiroshiba/realtime-yukarin" -> "auspicious3000/autovc"
"Hiroshiba/realtime-yukarin" -> "jjery2243542/voice_conversion"
"Hiroshiba/realtime-yukarin" -> "liusongxiang/ppg-vc"
"mravanelli/SincNet" -> "mravanelli/pytorch-kaldi"
"mravanelli/SincNet" -> "clovaai/voxceleb_trainer"
"mravanelli/SincNet" -> "HarryVolek/PyTorch_Speaker_Verification"
"mravanelli/SincNet" -> "Jungjee/RawNet"
"mravanelli/SincNet" -> "philipperemy/deep-speaker"
"mravanelli/SincNet" -> "WeidiXie/VGG-Speaker-Recognition"
"mravanelli/SincNet" -> "santi-pdp/pase"
"mravanelli/SincNet" -> "wq2012/awesome-diarization"
"mravanelli/SincNet" -> "vesis84/kaldi-io-for-python"
"mravanelli/SincNet" -> "manojpamk/pytorch_xvectors"
"mravanelli/SincNet" -> "Snowdar/asv-subtools"
"mravanelli/SincNet" -> "google/uis-rnn"
"mravanelli/SincNet" -> "pykaldi/pykaldi"
"mravanelli/SincNet" -> "pyannote/pyannote-audio"
"mravanelli/SincNet" -> "a-nagrani/VGGVox"
"JingShing/novelai-colab-ver" -> "JingShing/NovelAI-installation-tutorial"
"JingShing/novelai-colab-ver" -> "JingShing/AI-Drawing-Spell-Generator"
"JingShing/novelai-colab-ver" -> "JingShing/NovelAI-4chan-lowvram-ver"
"JingShing/novelai-colab-ver" -> "koishijs/novelai-bot" ["e"=1]
"JingShing/novelai-colab-ver" -> "JingShing/ImageAI-colab-ver"
"JingShing/novelai-colab-ver" -> "DominikDoom/a1111-sd-webui-tagcomplete" ["e"=1]
"JingShing/novelai-colab-ver" -> "luoyily/MoeTTS"
"JingShing/novelai-colab-ver" -> "sudoskys/StableDiffusionBook" ["e"=1]
"JingShing/novelai-colab-ver" -> "CjangCjengh/MoeGoe_GUI"
"JingShing/novelai-colab-ver" -> "CjangCjengh/TTSModels"
"JingShing/novelai-colab-ver" -> "wfjsw/danbooru-diffusion-prompt-builder" ["e"=1]
"JingShing/novelai-colab-ver" -> "zcyzcy88/TagTable" ["e"=1]
"JingShing/novelai-colab-ver" -> "acheong08/NovelAI-Colab"
"JingShing/novelai-colab-ver" -> "CjangCjengh/vits"
"JingShing/novelai-colab-ver" -> "KichangKim/DeepDanbooru" ["e"=1]
"dialogflow/asr-server" -> "alumae/gst-kaldi-nnet2-online"
"dialogflow/asr-server" -> "jimbozhang/kaldi-gop"
"dialogflow/asr-server" -> "alphacep/kaldi-websocket-python"
"dialogflow/asr-server" -> "jcsilva/docker-kaldi-gstreamer-server"
"dialogflow/asr-server" -> "jpuigcerver/kaldi-decoders"
"dialogflow/asr-server" -> "daanzu/kaldi-active-grammar" ["e"=1]
"dialogflow/asr-server" -> "Vernacular-ai/kaldi-serve" ["e"=1]
"SociallyIneptWeeb/LanguageLeapAI" -> "0Xiaohei0/VoiceToJapanese"
"SociallyIneptWeeb/LanguageLeapAI" -> "Koischizo/AI-Vtuber"
"SociallyIneptWeeb/LanguageLeapAI" -> "ardha27/AI-Waifu-Vtuber"
"SociallyIneptWeeb/LanguageLeapAI" -> "VOICEVOX/voicevox"
"SociallyIneptWeeb/LanguageLeapAI" -> "adi-panda/Kuebiko"
"carpedm20/multi-speaker-tacotron-tensorflow" -> "GSByeon/multi-speaker-tacotron-tensorflow"
"carpedm20/multi-speaker-tacotron-tensorflow" -> "hccho2/Tacotron2-Wavenet-Korean-TTS"
"carpedm20/multi-speaker-tacotron-tensorflow" -> "hccho2/Tacotron-Wavenet-Vocoder-Korean"
"carpedm20/multi-speaker-tacotron-tensorflow" -> "hccho2/Tacotron-Wavenet-Vocoder"
"carpedm20/multi-speaker-tacotron-tensorflow" -> "keithito/tacotron"
"carpedm20/multi-speaker-tacotron-tensorflow" -> "r9y9/deepvoice3_pytorch"
"carpedm20/multi-speaker-tacotron-tensorflow" -> "Kyubyong/tacotron"
"carpedm20/multi-speaker-tacotron-tensorflow" -> "kakao/khaiii" ["e"=1]
"carpedm20/multi-speaker-tacotron-tensorflow" -> "sokcuri/multi-speaker-tacotron-tensorflow"
"carpedm20/multi-speaker-tacotron-tensorflow" -> "insurgent92/CS231N_17_KOR_SUB" ["e"=1]
"carpedm20/multi-speaker-tacotron-tensorflow" -> "devsisters/multi-speaker-tacotron-tensorflow" ["e"=1]
"carpedm20/multi-speaker-tacotron-tensorflow" -> "lifefeel/SpeechSynthesis"
"carpedm20/multi-speaker-tacotron-tensorflow" -> "Rayhane-mamah/Tacotron-2"
"carpedm20/multi-speaker-tacotron-tensorflow" -> "syang1993/gst-tacotron"
"carpedm20/multi-speaker-tacotron-tensorflow" -> "NVIDIA/mellotron"
"pkhungurn/talking-head-anime-2-demo" -> "pkhungurn/talking-head-anime-demo"
"pkhungurn/talking-head-anime-2-demo" -> "pkhungurn/talking-head-anime-3-demo"
"pkhungurn/talking-head-anime-2-demo" -> "yuyuyzl/EasyVtuber"
"pkhungurn/talking-head-anime-2-demo" -> "pkhungurn/talking-head-anime-2"
"pkhungurn/talking-head-anime-2-demo" -> "hysts/anime-face-detector" ["e"=1]
"pkhungurn/talking-head-anime-2-demo" -> "yoyo-nb/Thin-Plate-Spline-Motion-Model" ["e"=1]
"pkhungurn/talking-head-anime-2-demo" -> "YadiraF/DECA" ["e"=1]
"pkhungurn/talking-head-anime-2-demo" -> "SerialLain3170/AwesomeAnimeResearch" ["e"=1]
"pkhungurn/talking-head-anime-2-demo" -> "yzhou359/MakeItTalk" ["e"=1]
"pkhungurn/talking-head-anime-2-demo" -> "Hangz-nju-cuhk/Talking-Face_PC-AVS" ["e"=1]
"pkhungurn/talking-head-anime-2-demo" -> "mchong6/GANsNRoses" ["e"=1]
"pkhungurn/talking-head-anime-2-demo" -> "TianxingWu/OpenVHead" ["e"=1]
"pkhungurn/talking-head-anime-2-demo" -> "YuanxunLu/LiveSpeechPortraits" ["e"=1]
"pkhungurn/talking-head-anime-2-demo" -> "transpchan/Live3D-v2" ["e"=1]
"pkhungurn/talking-head-anime-2-demo" -> "LynnHo/EigenGAN-Tensorflow" ["e"=1]
"snakers4/silero-models" -> "snakers4/silero-vad"
"snakers4/silero-models" -> "snakers4/open_stt" ["e"=1]
"snakers4/silero-models" -> "coqui-ai/TTS"
"snakers4/silero-models" -> "NVIDIA/NeMo"
"snakers4/silero-models" -> "speechbrain/speechbrain"
"snakers4/silero-models" -> "alphacep/vosk-api"
"snakers4/silero-models" -> "pyannote/pyannote-audio"
"snakers4/silero-models" -> "yandex/YaLM-100B" ["e"=1]
"snakers4/silero-models" -> "sberbank-ai/ru-gpts" ["e"=1]
"snakers4/silero-models" -> "espnet/espnet"
"snakers4/silero-models" -> "neonbjb/tortoise-tts" ["e"=1]
"snakers4/silero-models" -> "janvarev/Irene-Voice-Assistant"
"snakers4/silero-models" -> "coqui-ai/STT"
"snakers4/silero-models" -> "facebookresearch/denoiser" ["e"=1]
"snakers4/silero-models" -> "mozilla/TTS"
"openvpi/DiffSinger" -> "MoonInTheRiver/DiffSinger"
"openvpi/DiffSinger" -> "innnky/so-vits-svc"
"openvpi/DiffSinger" -> "openvpi/vocoders"
"openvpi/DiffSinger" -> "prophesier/diff-svc"
"openvpi/DiffSinger" -> "jaywalnut310/vits"
"openvpi/DiffSinger" -> "svc-develop-team/so-vits-svc"
"openvpi/DiffSinger" -> "Plachtaa/VITS-fast-fine-tuning"
"openvpi/DiffSinger" -> "NaruseMioShirakana/MoeSS"
"openvpi/DiffSinger" -> "luoyily/MoeTTS"
"openvpi/DiffSinger" -> "yuyuyzl/EasyVtuber"
"openvpi/DiffSinger" -> "CjangCjengh/MoeGoe"
"openvpi/DiffSinger" -> "innnky/emotional-vits"
"openvpi/DiffSinger" -> "xunmengshe/OpenUtau"
"openvpi/DiffSinger" -> "liujing04/Retrieval-based-Voice-Conversion-WebUI"
"openvpi/DiffSinger" -> "fishaudio/fish-diffusion"
"easemob/web-im" -> "easemob/emchat-server-examples"
"easemob/web-im" -> "easemob/webim"
"G-Wang/WaveRNN-Pytorch" -> "geneing/WaveRNN-Pytorch"
"G-Wang/WaveRNN-Pytorch" -> "h-meru/Tacotron-WaveRNN"
"G-Wang/WaveRNN-Pytorch" -> "fatchord/FFTNet"
"G-Wang/WaveRNN-Pytorch" -> "mkotha/WaveRNN"
"Hiroshiba/become-yukarin" -> "Hiroshiba/realtime-yukarin"
"Hiroshiba/become-yukarin" -> "Hiroshiba/yukarin"
"Hiroshiba/become-yukarin" -> "k2kobayashi/sprocket"
"Hiroshiba/become-yukarin" -> "isletennos/MMVC_Trainer"
"Hiroshiba/become-yukarin" -> "mmorise/World"
"Hiroshiba/become-yukarin" -> "dwango/UniVRM" ["e"=1]
"Hiroshiba/become-yukarin" -> "Hiroshiba/voicevox"
"Hiroshiba/become-yukarin" -> "Hiroshiba/voicevox_engine"
"Hiroshiba/become-yukarin" -> "r9y9/nnmnkwii"
"Hiroshiba/become-yukarin" -> "pstuvwx/Deep_VoiceChanger"
"Hiroshiba/become-yukarin" -> "sh-akira/VirtualMotionCapture" ["e"=1]
"Hiroshiba/become-yukarin" -> "omegasisters/homepage" ["e"=1]
"Hiroshiba/become-yukarin" -> "ksasao/Gochiusearch" ["e"=1]
"Hiroshiba/become-yukarin" -> "mmorise/kiritan_singing" ["e"=1]
"Hiroshiba/become-yukarin" -> "XVI/AniLipSync" ["e"=1]
"Jackiexiao/MTTS" -> "thuhcsi/Crystal"
"Jackiexiao/MTTS" -> "kakaobrain/g2pM"
"Jackiexiao/MTTS" -> "begeekmyfriend/tacotron"
"Jackiexiao/MTTS" -> "mozilla/LPCNet"
"Jackiexiao/MTTS" -> "begeekmyfriend/Tacotron-2"
"Jackiexiao/MTTS" -> "BoragoCode/AttentionBasedProsodyPrediction"
"Jackiexiao/MTTS" -> "syang1993/gst-tacotron"
"Jackiexiao/MTTS" -> "Helsinki-NLP/prosody"
"Jackiexiao/MTTS" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"Jackiexiao/MTTS" -> "open-speech/speech-aligner"
"Jackiexiao/MTTS" -> "xcmyz/FastSpeech"
"Jackiexiao/MTTS" -> "speechio/chinese_text_normalization"
"Jackiexiao/MTTS" -> "ksw0306/ClariNet"
"Jackiexiao/MTTS" -> "CSTR-Edinburgh/merlin"
"Jackiexiao/MTTS" -> "ivanvovk/DurIAN"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "mmorise/World"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "r9y9/pysptk"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "k2kobayashi/sprocket"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "kan-bayashi/ParallelWaveGAN"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "mozilla/LPCNet"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "kan-bayashi/PytorchWaveNetVocoder"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "r9y9/nnmnkwii"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "jik876/hifi-gan"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "xcmyz/FastSpeech"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "tuanad121/Python-WORLD"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "r9y9/gantts"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "descriptinc/melgan-neurips"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "tts-tutorial/survey"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "xcmyz/speech-synthesis-paper"
"KuangDD/aukit" -> "KuangDD/phkit"
"KuangDD/aukit" -> "KuangDD/zhvoice"
"KuangDD/zhrtvc" -> "KuangDD/zhvoice"
"KuangDD/zhrtvc" -> "lturing/tacotronv2_wavernn_chinese"
"KuangDD/zhrtvc" -> "jackaduma/CycleGAN-VC2"
"KuangDD/zhrtvc" -> "aidreamwin/TTS-Clone-Chinese"
"KuangDD/zhrtvc" -> "JasonWei512/Tacotron-2-Chinese"
"KuangDD/zhrtvc" -> "KuangDD/aukit"
"KuangDD/zhrtvc" -> "athena-team/athena" ["e"=1]
"KuangDD/zhrtvc" -> "kan-bayashi/ParallelWaveGAN"
"KuangDD/zhrtvc" -> "TensorSpeech/TensorFlowTTS"
"KuangDD/zhrtvc" -> "ming024/FastSpeech2"
"KuangDD/zhrtvc" -> "xcmyz/FastSpeech"
"KuangDD/zhrtvc" -> "liusongxiang/StarGAN-Voice-Conversion"
"KuangDD/zhrtvc" -> "fatchord/WaveRNN"
"KuangDD/zhrtvc" -> "KuangDD/phkit"
"KuangDD/zhrtvc" -> "xcmyz/speech-synthesis-paper"
"MoonInTheRiver/DiffSinger" -> "openvpi/DiffSinger"
"MoonInTheRiver/DiffSinger" -> "jaywalnut310/vits"
"MoonInTheRiver/DiffSinger" -> "prophesier/diff-svc"
"MoonInTheRiver/DiffSinger" -> "jik876/hifi-gan"
"MoonInTheRiver/DiffSinger" -> "MoonInTheRiver/NeuralSVB"
"MoonInTheRiver/DiffSinger" -> "innnky/so-vits-svc"
"MoonInTheRiver/DiffSinger" -> "svc-develop-team/so-vits-svc"
"MoonInTheRiver/DiffSinger" -> "NATSpeech/NATSpeech" ["e"=1]
"MoonInTheRiver/DiffSinger" -> "kan-bayashi/ParallelWaveGAN"
"MoonInTheRiver/DiffSinger" -> "ming024/FastSpeech2"
"MoonInTheRiver/DiffSinger" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"MoonInTheRiver/DiffSinger" -> "microsoft/muzic"
"MoonInTheRiver/DiffSinger" -> "espnet/espnet"
"MoonInTheRiver/DiffSinger" -> "lucidrains/audiolm-pytorch"
"MoonInTheRiver/DiffSinger" -> "microsoft/NeuralSpeech"
"TensorSpeech/TensorFlowTTS" -> "ming024/FastSpeech2"
"TensorSpeech/TensorFlowTTS" -> "mozilla/TTS"
"TensorSpeech/TensorFlowTTS" -> "kan-bayashi/ParallelWaveGAN"
"TensorSpeech/TensorFlowTTS" -> "fatchord/WaveRNN"
"TensorSpeech/TensorFlowTTS" -> "jik876/hifi-gan"
"TensorSpeech/TensorFlowTTS" -> "espnet/espnet"
"TensorSpeech/TensorFlowTTS" -> "TensorSpeech/TensorFlowASR"
"TensorSpeech/TensorFlowTTS" -> "Rayhane-mamah/Tacotron-2"
"TensorSpeech/TensorFlowTTS" -> "NVIDIA/tacotron2"
"TensorSpeech/TensorFlowTTS" -> "as-ideas/TransformerTTS"
"TensorSpeech/TensorFlowTTS" -> "coqui-ai/TTS"
"TensorSpeech/TensorFlowTTS" -> "xcmyz/FastSpeech"
"TensorSpeech/TensorFlowTTS" -> "keithito/tacotron"
"TensorSpeech/TensorFlowTTS" -> "KuangDD/zhrtvc"
"TensorSpeech/TensorFlowTTS" -> "lturing/tacotronv2_wavernn_chinese"
"begeekmyfriend/Tacotron-2" -> "begeekmyfriend/tacotron"
"begeekmyfriend/Tacotron-2" -> "begeekmyfriend/tacotron2"
"begeekmyfriend/Tacotron-2" -> "Jackiexiao/MTTS"
"begeekmyfriend/Tacotron-2" -> "azraelkuan/parallel_wavenet_vocoder"
"begeekmyfriend/Tacotron-2" -> "h-meru/Tacotron-WaveRNN"
"begeekmyfriend/Tacotron-2" -> "tiberiu44/TTS-Cube"
"buriburisuri/speech-to-text-wavenet" -> "ibab/tensorflow-wavenet"
"buriburisuri/speech-to-text-wavenet" -> "pannous/tensorflow-speech-recognition"
"buriburisuri/speech-to-text-wavenet" -> "tomlepaine/fast-wavenet"
"buriburisuri/speech-to-text-wavenet" -> "zzw922cn/Automatic_Speech_Recognition"
"buriburisuri/speech-to-text-wavenet" -> "andabi/deep-voice-conversion"
"buriburisuri/speech-to-text-wavenet" -> "Kyubyong/tacotron"
"buriburisuri/speech-to-text-wavenet" -> "facebookresearch/wav2letter"
"buriburisuri/speech-to-text-wavenet" -> "OpenNMT/OpenNMT" ["e"=1]
"buriburisuri/speech-to-text-wavenet" -> "junyanz/iGAN" ["e"=1]
"buriburisuri/speech-to-text-wavenet" -> "oarriaga/face_classification"
"buriburisuri/speech-to-text-wavenet" -> "MrNothing/AI-Blocks"
"buriburisuri/speech-to-text-wavenet" -> "mozilla/DeepSpeech"
"buriburisuri/speech-to-text-wavenet" -> "SeanNaren/deepspeech.pytorch"
"buriburisuri/speech-to-text-wavenet" -> "baidu-research/warp-ctc" ["e"=1]
"buriburisuri/speech-to-text-wavenet" -> "zzw922cn/awesome-speech-recognition-speech-synthesis-papers"
"fatchord/WaveRNN" -> "r9y9/wavenet_vocoder"
"fatchord/WaveRNN" -> "Rayhane-mamah/Tacotron-2"
"fatchord/WaveRNN" -> "mozilla/LPCNet"
"fatchord/WaveRNN" -> "kan-bayashi/ParallelWaveGAN"
"fatchord/WaveRNN" -> "NVIDIA/waveglow"
"fatchord/WaveRNN" -> "keithito/tacotron"
"fatchord/WaveRNN" -> "xcmyz/FastSpeech"
"fatchord/WaveRNN" -> "NVIDIA/tacotron2"
"fatchord/WaveRNN" -> "jik876/hifi-gan"
"fatchord/WaveRNN" -> "descriptinc/melgan-neurips"
"fatchord/WaveRNN" -> "NVIDIA/mellotron"
"fatchord/WaveRNN" -> "mozilla/TTS"
"fatchord/WaveRNN" -> "r9y9/deepvoice3_pytorch"
"fatchord/WaveRNN" -> "seungwonpark/melgan"
"fatchord/WaveRNN" -> "espnet/espnet"
"google/voice-builder" -> "MycroftAI/mimic2"
"google/voice-builder" -> "nii-yamagishilab/multi-speaker-tacotron"
"google/voice-builder" -> "google/language-resources"
"google/voice-builder" -> "google/sparrowhawk"
"google/voice-builder" -> "sotelo/parrot"
"google/voice-builder" -> "festvox/festival"
"google/voice-builder" -> "tiberiu44/TTS-Cube"
"h-meru/Tacotron-WaveRNN" -> "G-Wang/WaveRNN-Pytorch"
"h-meru/Tacotron-WaveRNN" -> "m-toman/tacorn"
"kaituoxu/Speech-Transformer" -> "kaituoxu/Listen-Attend-Spell"
"kaituoxu/Speech-Transformer" -> "ZhengkunTian/OpenTransformer"
"kaituoxu/Speech-Transformer" -> "gentaiscool/end2end-asr-pytorch"
"kaituoxu/Speech-Transformer" -> "hirofumi0810/neural_sp"
"kaituoxu/Speech-Transformer" -> "Alexander-H-Liu/End-to-end-ASR-Pytorch"
"kaituoxu/Speech-Transformer" -> "cywang97/StreamingTransformer"
"kaituoxu/Speech-Transformer" -> "mobvoi/wenet"
"kaituoxu/Speech-Transformer" -> "HawkAaron/warp-transducer"
"kaituoxu/Speech-Transformer" -> "mravanelli/pytorch-kaldi"
"kaituoxu/Speech-Transformer" -> "ZhengkunTian/rnn-transducer"
"kaituoxu/Speech-Transformer" -> "pykaldi/pykaldi"
"kaituoxu/Speech-Transformer" -> "audier/DeepSpeechRecognition"
"kaituoxu/Speech-Transformer" -> "xingchensong/speech-recognition-papers"
"kaituoxu/Speech-Transformer" -> "zcaceres/spec_augment"
"kaituoxu/Speech-Transformer" -> "athena-team/athena" ["e"=1]
"kan-bayashi/ParallelWaveGAN" -> "descriptinc/melgan-neurips"
"kan-bayashi/ParallelWaveGAN" -> "jik876/hifi-gan"
"kan-bayashi/ParallelWaveGAN" -> "seungwonpark/melgan"
"kan-bayashi/ParallelWaveGAN" -> "xcmyz/FastSpeech"
"kan-bayashi/ParallelWaveGAN" -> "ming024/FastSpeech2"
"kan-bayashi/ParallelWaveGAN" -> "mozilla/LPCNet"
"kan-bayashi/ParallelWaveGAN" -> "jaywalnut310/glow-tts"
"kan-bayashi/ParallelWaveGAN" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"kan-bayashi/ParallelWaveGAN" -> "auspicious3000/autovc"
"kan-bayashi/ParallelWaveGAN" -> "fatchord/WaveRNN"
"kan-bayashi/ParallelWaveGAN" -> "NVIDIA/mellotron"
"kan-bayashi/ParallelWaveGAN" -> "r9y9/wavenet_vocoder"
"kan-bayashi/ParallelWaveGAN" -> "NVIDIA/waveglow"
"kan-bayashi/ParallelWaveGAN" -> "xcmyz/speech-synthesis-paper"
"kan-bayashi/ParallelWaveGAN" -> "espnet/espnet"
"liusongxiang/StarGAN-Voice-Conversion" -> "hujinsen/StarGAN-Voice-Conversion"
"liusongxiang/StarGAN-Voice-Conversion" -> "hujinsen/pytorch-StarGAN-VC"
"liusongxiang/StarGAN-Voice-Conversion" -> "auspicious3000/autovc"
"liusongxiang/StarGAN-Voice-Conversion" -> "leimao/Voice_Converter_CycleGAN"
"liusongxiang/StarGAN-Voice-Conversion" -> "jjery2243542/adaptive_voice_conversion"
"liusongxiang/StarGAN-Voice-Conversion" -> "k2kobayashi/sprocket"
"liusongxiang/StarGAN-Voice-Conversion" -> "SamuelBroughton/StarGAN-Voice-Conversion-2"
"liusongxiang/StarGAN-Voice-Conversion" -> "jjery2243542/voice_conversion"
"liusongxiang/StarGAN-Voice-Conversion" -> "jxzhanggg/nonparaSeq2seqVC_code"
"liusongxiang/StarGAN-Voice-Conversion" -> "joansj/blow"
"liusongxiang/StarGAN-Voice-Conversion" -> "mazzzystar/randomCNN-voice-transfer"
"liusongxiang/StarGAN-Voice-Conversion" -> "jackaduma/CycleGAN-VC2"
"liusongxiang/StarGAN-Voice-Conversion" -> "auspicious3000/SpeechSplit"
"liusongxiang/StarGAN-Voice-Conversion" -> "kan-bayashi/ParallelWaveGAN"
"liusongxiang/StarGAN-Voice-Conversion" -> "r9y9/gantts"
"marytts/marytts" -> "CSTR-Edinburgh/merlin"
"marytts/marytts" -> "espeak-ng/espeak-ng"
"marytts/marytts" -> "mozilla/TTS"
"marytts/marytts" -> "synesthesiam/opentts" ["e"=1]
"marytts/marytts" -> "cmusphinx/sphinx4"
"marytts/marytts" -> "rhdunn/espeak"
"marytts/marytts" -> "keithito/tacotron"
"marytts/marytts" -> "Rayhane-mamah/Tacotron-2"
"marytts/marytts" -> "Kyubyong/tacotron"
"marytts/marytts" -> "r9y9/wavenet_vocoder"
"marytts/marytts" -> "MycroftAI/mimic2"
"marytts/marytts" -> "cmusphinx/pocketsphinx"
"marytts/marytts" -> "MycroftAI/mimic" ["e"=1]
"marytts/marytts" -> "googlecreativelab/aiexperiments-giorgio-cam" ["e"=1]
"marytts/marytts" -> "readbeyond/aeneas"
"mazzzystar/randomCNN-voice-transfer" -> "liusongxiang/StarGAN-Voice-Conversion"
"mazzzystar/randomCNN-voice-transfer" -> "auspicious3000/autovc"
"mazzzystar/randomCNN-voice-transfer" -> "marcoppasini/MelGAN-VC"
"mazzzystar/randomCNN-voice-transfer" -> "leimao/Voice_Converter_CycleGAN"
"mazzzystar/randomCNN-voice-transfer" -> "hujinsen/StarGAN-Voice-Conversion"
"mazzzystar/randomCNN-voice-transfer" -> "jjery2243542/adaptive_voice_conversion"
"mazzzystar/randomCNN-voice-transfer" -> "JeremyCCHsu/vae-npvc"
"mazzzystar/randomCNN-voice-transfer" -> "k2kobayashi/sprocket"
"mazzzystar/randomCNN-voice-transfer" -> "jjery2243542/voice_conversion"
"mazzzystar/randomCNN-voice-transfer" -> "auspicious3000/SpeechSplit"
"mazzzystar/randomCNN-voice-transfer" -> "hujinsen/pytorch-StarGAN-VC"
"mazzzystar/randomCNN-voice-transfer" -> "jackaduma/CycleGAN-VC2"
"mazzzystar/randomCNN-voice-transfer" -> "pritishyuvraj/Voice-Conversion-GAN"
"mazzzystar/randomCNN-voice-transfer" -> "alishdipani/Neural-Style-Transfer-Audio"
"mazzzystar/randomCNN-voice-transfer" -> "Kyubyong/cross_vc"
"mozilla/LPCNet" -> "kan-bayashi/ParallelWaveGAN"
"mozilla/LPCNet" -> "xcmyz/FastSpeech"
"mozilla/LPCNet" -> "descriptinc/melgan-neurips"
"mozilla/LPCNet" -> "fatchord/WaveRNN"
"mozilla/LPCNet" -> "seungwonpark/melgan"
"mozilla/LPCNet" -> "syang1993/gst-tacotron"
"mozilla/LPCNet" -> "mmorise/World"
"mozilla/LPCNet" -> "NVIDIA/waveglow"
"mozilla/LPCNet" -> "tianrengao/SqueezeWave"
"mozilla/LPCNet" -> "NVIDIA/mellotron"
"mozilla/LPCNet" -> "ksw0306/FloWaveNet"
"mozilla/LPCNet" -> "jik876/hifi-gan"
"mozilla/LPCNet" -> "alokprasad/LPCTron"
"mozilla/LPCNet" -> "Rayhane-mamah/Tacotron-2"
"mozilla/LPCNet" -> "Jackiexiao/MTTS"
"r9y9/gantts" -> "k2kobayashi/sprocket"
"r9y9/gantts" -> "leimao/Voice_Converter_CycleGAN"
"r9y9/gantts" -> "r9y9/nnmnkwii"
"r9y9/gantts" -> "liusongxiang/StarGAN-Voice-Conversion"
"r9y9/gantts" -> "JeremyCCHsu/Python-Wrapper-for-World-Vocoder"
"r9y9/gantts" -> "mozilla/LPCNet"
"r9y9/gantts" -> "yanggeng1995/GAN-TTS"
"r9y9/gantts" -> "r9y9/pysptk"
"r9y9/gantts" -> "NVIDIA/mellotron"
"r9y9/gantts" -> "jxzhanggg/nonparaSeq2seqVC_code"
"r9y9/gantts" -> "jjery2243542/voice_conversion"
"r9y9/gantts" -> "r9y9/wavenet_vocoder"
"r9y9/gantts" -> "ksw0306/FloWaveNet"
"r9y9/gantts" -> "CSTR-Edinburgh/merlin"
"r9y9/gantts" -> "andabi/parallel-wavenet-vocoder"
"syang1993/gst-tacotron" -> "KinglittleQ/GST-Tacotron"
"syang1993/gst-tacotron" -> "rishikksh20/vae_tacotron2"
"syang1993/gst-tacotron" -> "Kyubyong/expressive_tacotron"
"syang1993/gst-tacotron" -> "xcmyz/FastSpeech"
"syang1993/gst-tacotron" -> "mozilla/LPCNet"
"syang1993/gst-tacotron" -> "NVIDIA/mellotron"
"syang1993/gst-tacotron" -> "kan-bayashi/PytorchWaveNetVocoder"
"syang1993/gst-tacotron" -> "Jackiexiao/MTTS"
"syang1993/gst-tacotron" -> "kan-bayashi/ParallelWaveGAN"
"syang1993/gst-tacotron" -> "mkotha/WaveRNN"
"syang1993/gst-tacotron" -> "jaywalnut310/glow-tts"
"syang1993/gst-tacotron" -> "nii-yamagishilab/multi-speaker-tacotron"
"syang1993/gst-tacotron" -> "npuichigo/waveglow"
"syang1993/gst-tacotron" -> "seungwonpark/melgan"
"syang1993/gst-tacotron" -> "ksw0306/FloWaveNet"
"tiberiu44/TTS-Cube" -> "bfs18/nsynth_wavenet"
"tiberiu44/TTS-Cube" -> "npuichigo/waveglow"
"tiberiu44/TTS-Cube" -> "mkotha/WaveRNN"
"tiberiu44/TTS-Cube" -> "h-meru/Tacotron-WaveRNN"
"tiberiu44/TTS-Cube" -> "geneing/WaveRNN-Pytorch"
"tiberiu44/TTS-Cube" -> "G-Wang/WaveRNN-Pytorch"
"tiberiu44/TTS-Cube" -> "mozilla/FFTNet"
"tiberiu44/TTS-Cube" -> "ksw0306/ClariNet"
"tiberiu44/TTS-Cube" -> "azraelkuan/parallel_wavenet_vocoder"
"tiberiu44/TTS-Cube" -> "syang1993/FFTNet"
"w4123/GenshinVoice" -> "w4123/vits"
"w4123/GenshinVoice" -> "fishaudio/fish-diffusion"
"w4123/GenshinVoice" -> "innnky/emotional-vits"
"w4123/GenshinVoice" -> "JOETtheIV/VITS-Paimon"
"w4123/GenshinVoice" -> "Stardust-minus/vits"
"w4123/GenshinVoice" -> "PlayVoice/vits_chinese"
"w4123/GenshinVoice" -> "NaruseMioShirakana/MoeSS"
"w4123/GenshinVoice" -> "innnky/diff-svc"
"w4123/GenshinVoice" -> "IceKyrin/sovits_guide"
"w4123/GenshinVoice" -> "luoyily/MoeTTS"
"w4123/GenshinVoice" -> "CjangCjengh/vits"
"w4123/GenshinVoice" -> "yxlllc/DDSP-SVC"
"w4123/GenshinVoice" -> "Plachtaa/VITS-fast-fine-tuning"
"w4123/GenshinVoice" -> "SayaSS/vits-finetuning"
"w4123/GenshinVoice" -> "openvpi/audio-slicer"
"xcmyz/FastSpeech" -> "ming024/FastSpeech2"
"xcmyz/FastSpeech" -> "kan-bayashi/ParallelWaveGAN"
"xcmyz/FastSpeech" -> "soobinseo/Transformer-TTS"
"xcmyz/FastSpeech" -> "mozilla/LPCNet"
"xcmyz/FastSpeech" -> "xcmyz/speech-synthesis-paper"
"xcmyz/FastSpeech" -> "NVIDIA/mellotron"
"xcmyz/FastSpeech" -> "seungwonpark/melgan"
"xcmyz/FastSpeech" -> "jaywalnut310/glow-tts"
"xcmyz/FastSpeech" -> "syang1993/gst-tacotron"
"xcmyz/FastSpeech" -> "jik876/hifi-gan"
"xcmyz/FastSpeech" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"xcmyz/FastSpeech" -> "KinglittleQ/GST-Tacotron"
"xcmyz/FastSpeech" -> "descriptinc/melgan-neurips"
"xcmyz/FastSpeech" -> "Kyubyong/g2p"
"xcmyz/FastSpeech" -> "fatchord/WaveRNN"
"audier/DeepSpeechRecognition" -> "nl8590687/ASRT_SpeechRecognition"
"audier/DeepSpeechRecognition" -> "libai3/masr"
"audier/DeepSpeechRecognition" -> "PaddlePaddle/DeepSpeech"
"audier/DeepSpeechRecognition" -> "kaituoxu/Speech-Transformer"
"audier/DeepSpeechRecognition" -> "xxbb1234021/speech_recognition"
"audier/DeepSpeechRecognition" -> "zw76859420/ASR_Theory"
"audier/DeepSpeechRecognition" -> "mravanelli/pytorch-kaldi"
"audier/DeepSpeechRecognition" -> "SeanNaren/deepspeech.pytorch"
"audier/DeepSpeechRecognition" -> "alibaba/Alibaba-MIT-Speech"
"audier/DeepSpeechRecognition" -> "sailist/ASRFrame"
"audier/DeepSpeechRecognition" -> "zzw922cn/awesome-speech-recognition-speech-synthesis-papers"
"audier/DeepSpeechRecognition" -> "zw76859420/ASR_Syllable"
"audier/DeepSpeechRecognition" -> "athena-team/athena" ["e"=1]
"audier/DeepSpeechRecognition" -> "zzw922cn/Automatic_Speech_Recognition"
"audier/DeepSpeechRecognition" -> "mobvoi/wenet"
"NVIDIA/nv-wavenet" -> "mozilla/LPCNet"
"NVIDIA/nv-wavenet" -> "r9y9/wavenet_vocoder"
"NVIDIA/nv-wavenet" -> "NVIDIA/waveglow"
"NVIDIA/nv-wavenet" -> "ksw0306/FloWaveNet"
"NVIDIA/nv-wavenet" -> "kan-bayashi/PytorchWaveNetVocoder"
"NVIDIA/nv-wavenet" -> "tomlepaine/fast-wavenet"
"NVIDIA/nv-wavenet" -> "fatchord/WaveRNN"
"NVIDIA/nv-wavenet" -> "descriptinc/melgan-neurips"
"NVIDIA/nv-wavenet" -> "soobinseo/Transformer-TTS"
"NVIDIA/nv-wavenet" -> "NVIDIA/mellotron"
"NVIDIA/nv-wavenet" -> "Rayhane-mamah/Tacotron-2"
"NVIDIA/nv-wavenet" -> "tiberiu44/TTS-Cube"
"NVIDIA/nv-wavenet" -> "mkotha/WaveRNN"
"NVIDIA/nv-wavenet" -> "NVIDIA/tacotron2"
"NVIDIA/nv-wavenet" -> "azraelkuan/parallel_wavenet_vocoder"
"samim23/polymath" -> "archinetai/audio-ai-timeline"
"samim23/polymath" -> "haoheliu/AudioLDM"
"samim23/polymath" -> "archinetai/audio-diffusion-pytorch"
"samim23/polymath" -> "Harmonai-org/sample-generator"
"samim23/polymath" -> "acids-ircam/RAVE"
"samim23/polymath" -> "lucidrains/musiclm-pytorch"
"samim23/polymath" -> "teticio/audio-diffusion"
"samim23/polymath" -> "spotify/basic-pitch"
"samim23/polymath" -> "lucidrains/audiolm-pytorch"
"samim23/polymath" -> "marcoppasini/musika"
"samim23/polymath" -> "hmartiro/riffusion-app"
"samim23/polymath" -> "riffusion/riffusion"
"samim23/polymath" -> "caillonantoine/RAVE"
"samim23/polymath" -> "QosmoInc/neutone_sdk"
"samim23/polymath" -> "LAION-AI/CLAP"
"Hiroshiba/voicevox_engine" -> "Hiroshiba/voicevox"
"Hiroshiba/voicevox_engine" -> "Hiroshiba/voicevox_core"
"Hiroshiba/voicevox_engine" -> "mmorise/ita-corpus"
"Sharad24/Neural-Voice-Cloning-with-Few-Samples" -> "SforAiDl/Neural-Voice-Cloning-With-Few-Samples"
"Sharad24/Neural-Voice-Cloning-with-Few-Samples" -> "IEEE-NITK/Neural-Voice-Cloning"
"enhuiz/vall-e" -> "lifeiteng/vall-e"
"enhuiz/vall-e" -> "lucidrains/audiolm-pytorch"
"enhuiz/vall-e" -> "archinetai/audio-diffusion-pytorch"
"enhuiz/vall-e" -> "haoheliu/AudioLDM"
"enhuiz/vall-e" -> "facebookresearch/encodec"
"enhuiz/vall-e" -> "jik876/hifi-gan"
"enhuiz/vall-e" -> "lucidrains/musiclm-pytorch"
"enhuiz/vall-e" -> "m-bain/whisperX" ["e"=1]
"enhuiz/vall-e" -> "neonbjb/tortoise-tts" ["e"=1]
"enhuiz/vall-e" -> "NVIDIA/BigVGAN"
"enhuiz/vall-e" -> "microsoft/NeuralSpeech"
"enhuiz/vall-e" -> "jaywalnut310/vits"
"enhuiz/vall-e" -> "Edresson/YourTTS"
"enhuiz/vall-e" -> "suno-ai/bark"
"enhuiz/vall-e" -> "coqui-ai/TTS"
"KR-HappyFace/KoDALLE" -> "hihellohowareyou/RESREF_Chatbot_data_for_Korean"
"amsehili/auditok" -> "marsbroshok/VAD-python"
"amsehili/auditok" -> "ina-foss/inaSpeechSegmenter"
"amsehili/auditok" -> "wiseman/py-webrtcvad"
"amsehili/auditok" -> "hcmlab/vadnet"
"amsehili/auditok" -> "snakers4/silero-vad"
"amsehili/auditok" -> "jtkim-kaist/VAD"
"amsehili/auditok" -> "pyannote/pyannote-audio"
"amsehili/auditok" -> "eesungkim/Voice_Activity_Detector"
"amsehili/auditok" -> "wq2012/awesome-diarization"
"amsehili/auditok" -> "yistLin/dvector"
"amsehili/auditok" -> "filippogiruzzi/voice_activity_detection"
"amsehili/auditok" -> "lumaku/ctc-segmentation"
"amsehili/auditok" -> "facebookresearch/denoiser" ["e"=1]
"amsehili/auditok" -> "vesis84/kaldi-io-for-python"
"amsehili/auditok" -> "philipperemy/deep-speaker"
"r9y9/wavenet_vocoder" -> "Rayhane-mamah/Tacotron-2"
"r9y9/wavenet_vocoder" -> "fatchord/WaveRNN"
"r9y9/wavenet_vocoder" -> "NVIDIA/waveglow"
"r9y9/wavenet_vocoder" -> "keithito/tacotron"
"r9y9/wavenet_vocoder" -> "kan-bayashi/ParallelWaveGAN"
"r9y9/wavenet_vocoder" -> "r9y9/deepvoice3_pytorch"
"r9y9/wavenet_vocoder" -> "NVIDIA/tacotron2"
"r9y9/wavenet_vocoder" -> "descriptinc/melgan-neurips"
"r9y9/wavenet_vocoder" -> "mozilla/LPCNet"
"r9y9/wavenet_vocoder" -> "NVIDIA/nv-wavenet"
"r9y9/wavenet_vocoder" -> "tomlepaine/fast-wavenet"
"r9y9/wavenet_vocoder" -> "ibab/tensorflow-wavenet"
"r9y9/wavenet_vocoder" -> "espnet/espnet"
"r9y9/wavenet_vocoder" -> "kan-bayashi/PytorchWaveNetVocoder"
"r9y9/wavenet_vocoder" -> "CSTR-Edinburgh/merlin"
"GauravWaghmare/Speaker-Identification" -> "rajathkmp/speaker-verification"
"GauravWaghmare/Speaker-Identification" -> "linhdvu14/vggvox-speaker-identification"
"GauravWaghmare/Speaker-Identification" -> "swshon/voxceleb-ivector"
"a-nagrani/VGGVox" -> "WeidiXie/VGG-Speaker-Recognition"
"a-nagrani/VGGVox" -> "linhdvu14/vggvox-speaker-identification"
"a-nagrani/VGGVox" -> "qqueing/DeepSpeaker-pytorch"
"a-nagrani/VGGVox" -> "philipperemy/deep-speaker"
"a-nagrani/VGGVox" -> "Walleclipse/Deep_Speaker-speaker_recognition_system"
"a-nagrani/VGGVox" -> "clovaai/voxceleb_trainer"
"a-nagrani/VGGVox" -> "HarryVolek/PyTorch_Speaker_Verification"
"a-nagrani/VGGVox" -> "hsn-zeinali/x-vector-kaldi-tf"
"a-nagrani/VGGVox" -> "manojpamk/pytorch_xvectors"
"a-nagrani/VGGVox" -> "mravanelli/SincNet"
"a-nagrani/VGGVox" -> "Jungjee/RawNet"
"a-nagrani/VGGVox" -> "hbredin/TristouNet"
"a-nagrani/VGGVox" -> "rajathkmp/speaker-verification"
"a-nagrani/VGGVox" -> "Janghyun1230/Speaker_Verification"
"a-nagrani/VGGVox" -> "astorfi/3D-convolutional-speaker-recognition"
"alibaba/Alibaba-MIT-Speech" -> "tramphero/kaldi"
"alibaba/Alibaba-MIT-Speech" -> "lingochamp/kaldi-ctc"
"alibaba/Alibaba-MIT-Speech" -> "mobvoi/wenet"
"alibaba/Alibaba-MIT-Speech" -> "audier/DeepSpeechRecognition"
"alibaba/Alibaba-MIT-Speech" -> "mindorii/kws" ["e"=1]
"alibaba/Alibaba-MIT-Speech" -> "srvk/eesen"
"alibaba/Alibaba-MIT-Speech" -> "kaituoxu/Speech-Transformer"
"alibaba/Alibaba-MIT-Speech" -> "mravanelli/pytorch-kaldi"
"alibaba/Alibaba-MIT-Speech" -> "PaddlePaddle/DeepSpeech"
"alibaba/Alibaba-MIT-Speech" -> "HawkAaron/warp-transducer"
"alibaba/Alibaba-MIT-Speech" -> "aishell-foundation/DaCiDian"
"alibaba/Alibaba-MIT-Speech" -> "SeanNaren/deepspeech.pytorch"
"alibaba/Alibaba-MIT-Speech" -> "k2-fsa/k2"
"alibaba/Alibaba-MIT-Speech" -> "syhw/wer_are_we"
"alibaba/Alibaba-MIT-Speech" -> "XiaoMi/kaldi-onnx"
"andabi/voice-vector" -> "philipperemy/deep-speaker"
"andabi/voice-vector" -> "qqueing/DeepSpeaker-pytorch"
"andabi/voice-vector" -> "Walleclipse/Deep_Speaker-speaker_recognition_system"
"andabi/voice-vector" -> "linhdvu14/vggvox-speaker-identification"
"andabi/voice-vector" -> "rajathkmp/speaker-verification"
"andabi/voice-vector" -> "a-nagrani/VGGVox"
"andabi/voice-vector" -> "pyannote/pyannote-db-voxceleb"
"hbredin/TristouNet" -> "qqueing/DeepSpeaker-pytorch"
"hbredin/TristouNet" -> "RicherMans/PLDA"
"idiap/kaldi-ivector" -> "tiagofrepereira2012/ivector_example"
"idiap/kaldi-ivector" -> "swshon/voxceleb-ivector"
"philipperemy/deep-speaker" -> "qqueing/DeepSpeaker-pytorch"
"philipperemy/deep-speaker" -> "Walleclipse/Deep_Speaker-speaker_recognition_system"
"philipperemy/deep-speaker" -> "HarryVolek/PyTorch_Speaker_Verification"
"philipperemy/deep-speaker" -> "WeidiXie/VGG-Speaker-Recognition"
"philipperemy/deep-speaker" -> "astorfi/3D-convolutional-speaker-recognition"
"philipperemy/deep-speaker" -> "Janghyun1230/Speaker_Verification"
"philipperemy/deep-speaker" -> "mravanelli/SincNet"
"philipperemy/deep-speaker" -> "wq2012/awesome-diarization"
"philipperemy/deep-speaker" -> "a-nagrani/VGGVox"
"philipperemy/deep-speaker" -> "andabi/voice-vector"
"philipperemy/deep-speaker" -> "rajathkmp/speaker-verification"
"philipperemy/deep-speaker" -> "manojpamk/pytorch_xvectors"
"philipperemy/deep-speaker" -> "pyannote/pyannote-audio"
"philipperemy/deep-speaker" -> "google/uis-rnn"
"philipperemy/deep-speaker" -> "Snowdar/asv-subtools"
"pyannote/pyannote-audio" -> "wq2012/awesome-diarization"
"pyannote/pyannote-audio" -> "google/uis-rnn"
"pyannote/pyannote-audio" -> "speechbrain/speechbrain"
"pyannote/pyannote-audio" -> "snakers4/silero-vad"
"pyannote/pyannote-audio" -> "wiseman/py-webrtcvad"
"pyannote/pyannote-audio" -> "taylorlu/Speaker-Diarization"
"pyannote/pyannote-audio" -> "espnet/espnet"
"pyannote/pyannote-audio" -> "philipperemy/deep-speaker"
"pyannote/pyannote-audio" -> "wq2012/SpectralCluster"
"pyannote/pyannote-audio" -> "asteroid-team/asteroid" ["e"=1]
"pyannote/pyannote-audio" -> "hitachi-speech/EEND"
"pyannote/pyannote-audio" -> "mravanelli/SincNet"
"pyannote/pyannote-audio" -> "clovaai/voxceleb_trainer"
"pyannote/pyannote-audio" -> "NVIDIA/NeMo"
"pyannote/pyannote-audio" -> "m-bain/whisperX" ["e"=1]
"qqueing/DeepSpeaker-pytorch" -> "philipperemy/deep-speaker"
"qqueing/DeepSpeaker-pytorch" -> "Walleclipse/Deep_Speaker-speaker_recognition_system"
"qqueing/DeepSpeaker-pytorch" -> "HarryVolek/PyTorch_Speaker_Verification"
"qqueing/DeepSpeaker-pytorch" -> "a-nagrani/VGGVox"
"qqueing/DeepSpeaker-pytorch" -> "manojpamk/pytorch_xvectors"
"qqueing/DeepSpeaker-pytorch" -> "rajathkmp/speaker-verification"
"qqueing/DeepSpeaker-pytorch" -> "astorfi/3D-convolutional-speaker-recognition-pytorch"
"qqueing/DeepSpeaker-pytorch" -> "WeidiXie/VGG-Speaker-Recognition"
"qqueing/DeepSpeaker-pytorch" -> "jymsuper/SpeakerRecognition_tutorial"
"qqueing/DeepSpeaker-pytorch" -> "Snowdar/asv-subtools"
"qqueing/DeepSpeaker-pytorch" -> "astorfi/3D-convolutional-speaker-recognition"
"qqueing/DeepSpeaker-pytorch" -> "clovaai/voxceleb_trainer"
"qqueing/DeepSpeaker-pytorch" -> "hbredin/TristouNet"
"qqueing/DeepSpeaker-pytorch" -> "wangleiai/dVectorSpeakerRecognition"
"qqueing/DeepSpeaker-pytorch" -> "Janghyun1230/Speaker_Verification"
"rajathkmp/speaker-verification" -> "wangleiai/dVectorSpeakerRecognition"
"rajathkmp/speaker-verification" -> "qqueing/DeepSpeaker-pytorch"
"rajathkmp/speaker-verification" -> "RicherMans/PLDA"
"rajathkmp/speaker-verification" -> "GauravWaghmare/Speaker-Identification"
"syhw/wer_are_we" -> "mravanelli/pytorch-kaldi"
"syhw/wer_are_we" -> "SeanNaren/deepspeech.pytorch"
"syhw/wer_are_we" -> "srvk/eesen"
"syhw/wer_are_we" -> "espnet/espnet"
"syhw/wer_are_we" -> "zzw922cn/awesome-speech-recognition-speech-synthesis-papers"
"syhw/wer_are_we" -> "facebookresearch/wav2letter"
"syhw/wer_are_we" -> "pykaldi/pykaldi"
"syhw/wer_are_we" -> "k2-fsa/k2"
"syhw/wer_are_we" -> "awni/speech"
"syhw/wer_are_we" -> "hirofumi0810/neural_sp"
"syhw/wer_are_we" -> "kaituoxu/Speech-Transformer"
"syhw/wer_are_we" -> "freewym/espresso"
"syhw/wer_are_we" -> "SpeechColab/GigaSpeech"
"syhw/wer_are_we" -> "wiseman/py-webrtcvad"
"syhw/wer_are_we" -> "mobvoi/wenet"
"coqui-ai/open-speech-corpora" -> "coqui-ai/TTS-papers"
"coqui-ai/open-speech-corpora" -> "lumaku/ctc-segmentation"
"coqui-ai/open-speech-corpora" -> "tts-tutorial/survey"
"coqui-ai/open-speech-corpora" -> "xcmyz/speech-synthesis-paper"
"coqui-ai/open-speech-corpora" -> "jaywalnut310/glow-tts"
"coqui-ai/open-speech-corpora" -> "asteroid-team/torch-audiomentations"
"coqui-ai/open-speech-corpora" -> "keonlee9420/StyleSpeech"
"coqui-ai/open-speech-corpora" -> "as-ideas/DeepPhonemizer"
"coqui-ai/open-speech-corpora" -> "keonlee9420/Parallel-Tacotron2"
"coqui-ai/open-speech-corpora" -> "liusongxiang/Large-Audio-Models"
"coqui-ai/open-speech-corpora" -> "nii-yamagishilab/multi-speaker-tacotron"
"coqui-ai/open-speech-corpora" -> "Edresson/YourTTS"
"coqui-ai/open-speech-corpora" -> "keonlee9420/Comprehensive-Transformer-TTS"
"coqui-ai/open-speech-corpora" -> "Kyubyong/css10"
"coqui-ai/open-speech-corpora" -> "facebookresearch/voxpopuli"
"facebookresearch/encodec" -> "lucidrains/audiolm-pytorch"
"facebookresearch/encodec" -> "archinetai/audio-diffusion-pytorch"
"facebookresearch/encodec" -> "archinetai/audio-ai-timeline"
"facebookresearch/encodec" -> "haoheliu/AudioLDM"
"facebookresearch/encodec" -> "NVIDIA/BigVGAN"
"facebookresearch/encodec" -> "jik876/hifi-gan"
"facebookresearch/encodec" -> "wesbz/SoundStream"
"facebookresearch/encodec" -> "suno-ai/bark"
"facebookresearch/encodec" -> "lucidrains/vector-quantize-pytorch"
"facebookresearch/encodec" -> "lucidrains/musiclm-pytorch"
"facebookresearch/encodec" -> "enhuiz/vall-e"
"facebookresearch/encodec" -> "LAION-AI/CLAP"
"facebookresearch/encodec" -> "s3prl/s3prl"
"facebookresearch/encodec" -> "kan-bayashi/ParallelWaveGAN"
"facebookresearch/encodec" -> "asteroid-team/torch-audiomentations"
"NVIDIA/BigVGAN" -> "NVIDIA/radtts"
"NVIDIA/BigVGAN" -> "huawei-noah/Speech-Backbones"
"NVIDIA/BigVGAN" -> "mindslab-ai/phaseaug"
"NVIDIA/BigVGAN" -> "Rongjiehuang/GenerSpeech"
"NVIDIA/BigVGAN" -> "Rongjiehuang/FastDiff"
"NVIDIA/BigVGAN" -> "sh-lee-prml/BigVGAN"
"NVIDIA/BigVGAN" -> "keonlee9420/StyleSpeech"
"NVIDIA/BigVGAN" -> "yl4579/StyleTTS"
"NVIDIA/BigVGAN" -> "jaywalnut310/glow-tts"
"NVIDIA/BigVGAN" -> "LEEYOONHYUNG/BVAE-TTS"
"NVIDIA/BigVGAN" -> "keonlee9420/Parallel-Tacotron2"
"NVIDIA/BigVGAN" -> "anonymous-pits/pits"
"NVIDIA/BigVGAN" -> "ivanvovk/WaveGrad"
"NVIDIA/BigVGAN" -> "jik876/hifi-gan"
"NVIDIA/BigVGAN" -> "yerfor/SyntaSpeech"
"ivanvovk/WaveGrad" -> "lmnt-com/wavegrad"
"ivanvovk/WaveGrad" -> "rishikksh20/VocGAN"
"ivanvovk/WaveGrad" -> "lmnt-com/diffwave"
"ivanvovk/WaveGrad" -> "keonlee9420/Parallel-Tacotron2"
"ivanvovk/WaveGrad" -> "LEEYOONHYUNG/BVAE-TTS"
"ivanvovk/WaveGrad" -> "jaywalnut310/glow-tts"
"ivanvovk/WaveGrad" -> "thuhcsi/VAENAR-TTS"
"ivanvovk/WaveGrad" -> "NVIDIA/BigVGAN"
"ivanvovk/WaveGrad" -> "k2kobayashi/crank"
"ivanvovk/WaveGrad" -> "janvainer/speedyspeech"
"ivanvovk/WaveGrad" -> "tianrengao/SqueezeWave"
"ivanvovk/WaveGrad" -> "maxrmorrison/torchcrepe"
"ivanvovk/WaveGrad" -> "yanggeng1995/GAN-TTS"
"ivanvovk/WaveGrad" -> "bshall/UniversalVocoding"
"ivanvovk/WaveGrad" -> "mindslab-ai/wavegrad2"
"keonlee9420/DailyTalk" -> "yl4579/StyleTTS-VC"
"keonlee9420/DailyTalk" -> "Labmem-Zhouyx/CDFSE_FastSpeech2"
"keunwoochoi/kapre" -> "keunwoochoi/music-auto_tagging-keras" ["e"=1]
"keunwoochoi/kapre" -> "seth814/Audio-Classification" ["e"=1]
"keunwoochoi/kapre" -> "KinWaiCheuk/nnAudio"
"keunwoochoi/kapre" -> "keunwoochoi/dl4mir" ["e"=1]
"keunwoochoi/kapre" -> "marl/openl3" ["e"=1]
"keunwoochoi/kapre" -> "keunwoochoi/transfer_learning_music" ["e"=1]
"keunwoochoi/kapre" -> "faroit/awesome-python-scientific-audio" ["e"=1]
"keunwoochoi/kapre" -> "marl/crepe" ["e"=1]
"keunwoochoi/kapre" -> "pytorch/audio"
"keunwoochoi/kapre" -> "google-research/leaf-audio"
"keunwoochoi/kapre" -> "iver56/audiomentations"
"keunwoochoi/kapre" -> "keunwoochoi/torchaudio-contrib" ["e"=1]
"keunwoochoi/kapre" -> "zcaceres/spec_augment"
"keunwoochoi/kapre" -> "craffel/mir_eval" ["e"=1]
"keunwoochoi/kapre" -> "mir-dataset-loaders/mirdata" ["e"=1]
"lucidrains/audiolm-pytorch" -> "lucidrains/musiclm-pytorch"
"lucidrains/audiolm-pytorch" -> "archinetai/audio-diffusion-pytorch"
"lucidrains/audiolm-pytorch" -> "haoheliu/AudioLDM"
"lucidrains/audiolm-pytorch" -> "facebookresearch/encodec"
"lucidrains/audiolm-pytorch" -> "NVIDIA/BigVGAN"
"lucidrains/audiolm-pytorch" -> "LAION-AI/CLAP"
"lucidrains/audiolm-pytorch" -> "archinetai/audio-ai-timeline"
"lucidrains/audiolm-pytorch" -> "microsoft/NeuralSpeech"
"lucidrains/audiolm-pytorch" -> "enhuiz/vall-e"
"lucidrains/audiolm-pytorch" -> "lucidrains/vector-quantize-pytorch"
"lucidrains/audiolm-pytorch" -> "facebookresearch/AudioMAE"
"lucidrains/audiolm-pytorch" -> "lifeiteng/vall-e"
"lucidrains/audiolm-pytorch" -> "kan-bayashi/ParallelWaveGAN"
"lucidrains/audiolm-pytorch" -> "LAION-AI/audio-dataset"
"lucidrains/audiolm-pytorch" -> "jik876/hifi-gan"
"riffusion/riffusion" -> "riffusion/riffusion-app"
"riffusion/riffusion" -> "lucidrains/musiclm-pytorch"
"riffusion/riffusion" -> "haoheliu/AudioLDM"
"riffusion/riffusion" -> "archinetai/audio-diffusion-pytorch"
"riffusion/riffusion" -> "lucidrains/audiolm-pytorch"
"riffusion/riffusion" -> "teticio/audio-diffusion"
"riffusion/riffusion" -> "enlyth/sd-webui-riffusion"
"riffusion/riffusion" -> "Harmonai-org/sample-generator"
"riffusion/riffusion" -> "MubertAI/Mubert-Text-to-Music"
"riffusion/riffusion" -> "LAION-AI/CLAP"
"riffusion/riffusion" -> "microsoft/muzic"
"riffusion/riffusion" -> "Picsart-AI-Research/Text2Video-Zero" ["e"=1]
"riffusion/riffusion" -> "marcoppasini/musika"
"riffusion/riffusion" -> "salu133445/musegan" ["e"=1]
"riffusion/riffusion" -> "archinetai/audio-ai-timeline"
"speechbrain/speechbrain.github.io" -> "YiwenShaoStephen/pychain"
"speechbrain/speechbrain.github.io" -> "nttcslab-sp/kaldiio"
"speechbrain/speechbrain.github.io" -> "vesis84/kaldi-io-for-python"
"speechbrain/speechbrain.github.io" -> "fgnt/nara_wpe" ["e"=1]
"speechbrain/speechbrain.github.io" -> "santi-pdp/pase"
"speechbrain/speechbrain.github.io" -> "facebookresearch/WavAugment"
"speechbrain/speechbrain.github.io" -> "VITA-Group/AutoSpeech"
"speechbrain/speechbrain.github.io" -> "manojpamk/pytorch_xvectors"
"speechbrain/speechbrain.github.io" -> "funcwj/setk" ["e"=1]
"speechbrain/speechbrain.github.io" -> "FrancoisGrondin/BIRD" ["e"=1]
"speechbrain/speechbrain.github.io" -> "lochenchou/MOSNet" ["e"=1]
"speechbrain/speechbrain.github.io" -> "XiaoMi/kaldi-onnx"
"speechbrain/speechbrain.github.io" -> "k2-fsa/k2"
"karolpiczak/ESC-50" -> "YuanGongND/ast"
"karolpiczak/ESC-50" -> "qiuqiangkong/audioset_tagging_cnn"
"karolpiczak/ESC-50" -> "ksanjeevan/crnn-audio-classification" ["e"=1]
"karolpiczak/ESC-50" -> "iver56/audiomentations"
"karolpiczak/ESC-50" -> "audioset/ontology" ["e"=1]
"karolpiczak/ESC-50" -> "RetroCirce/HTS-Audio-Transformer"
"karolpiczak/ESC-50" -> "karolpiczak/paper-2015-esc-dataset"
"karolpiczak/ESC-50" -> "microsoft/DNS-Challenge" ["e"=1]
"karolpiczak/ESC-50" -> "asteroid-team/torch-audiomentations"
"karolpiczak/ESC-50" -> "YuanGongND/ssast"
"karolpiczak/ESC-50" -> "mtobeiyf/audio-classification" ["e"=1]
"karolpiczak/ESC-50" -> "seth814/Audio-Classification" ["e"=1]
"karolpiczak/ESC-50" -> "aliutkus/speechmetrics" ["e"=1]
"karolpiczak/ESC-50" -> "kkoutini/PaSST"
"karolpiczak/ESC-50" -> "qiuqiangkong/audioset_classification" ["e"=1]
"VOICEVOX/voicevox_engine" -> "VOICEVOX/voicevox_core"
"VOICEVOX/voicevox_engine" -> "VOICEVOX/voicevox"
"VOICEVOX/voicevox_engine" -> "isletennos/MMVC_Trainer"
"VOICEVOX/voicevox_engine" -> "jiro4989/ojosama" ["e"=1]
"VOICEVOX/voicevox_engine" -> "isletennos/MMVC_Client"
"VOICEVOX/voicevox_engine" -> "uezo/ChatdollKit"
"VOICEVOX/voicevox_engine" -> "lilxyzw/lilToon" ["e"=1]
"VOICEVOX/voicevox_engine" -> "ardha27/AI-Waifu-Vtuber"
"VOICEVOX/voicevox_engine" -> "Hiroshiba/realtime-yukarin"
"VOICEVOX/voicevox_engine" -> "ksasao/TTSController"
"VOICEVOX/voicevox_engine" -> "ePi5131/patch.aul" ["e"=1]
"isletennos/MMVC_Client" -> "isletennos/MMVC_Trainer"
"isletennos/MMVC_Client" -> "mmorise/ita-corpus"
"isletennos/MMVC_Client" -> "ddPn08/rvc-webui"
"ksasao/TTSController" -> "nanokina/ebyroid"
"ksasao/TTSController" -> "mikoto2000/VoiceroidController2"
"nanokina/ebyroid" -> "Ebycow/hanako"
"k2-fsa/k2" -> "k2-fsa/icefall"
"k2-fsa/k2" -> "lhotse-speech/lhotse"
"k2-fsa/k2" -> "k2-fsa/snowfall"
"k2-fsa/k2" -> "k2-fsa/sherpa"
"k2-fsa/k2" -> "mobvoi/wenet"
"k2-fsa/k2" -> "tencent-ailab/pika"
"k2-fsa/k2" -> "thu-spmi/CAT"
"k2-fsa/k2" -> "SpeechColab/GigaSpeech"
"k2-fsa/k2" -> "pykaldi/pykaldi"
"k2-fsa/k2" -> "hirofumi0810/neural_sp"
"k2-fsa/k2" -> "srvk/eesen"
"k2-fsa/k2" -> "wenet-e2e/wenet"
"k2-fsa/k2" -> "Snowdar/asv-subtools"
"k2-fsa/k2" -> "vesis84/kaldi-io-for-python"
"k2-fsa/k2" -> "athena-team/athena" ["e"=1]
"bambocher/pocketsphinx-python" -> "cmusphinx/pocketsphinx-python"
"bambocher/pocketsphinx-python" -> "cmusphinx/pocketsphinx"
"bambocher/pocketsphinx-python" -> "Uberi/speech_recognition"
"bambocher/pocketsphinx-python" -> "cmusphinx/sphinxbase"
"KinWaiCheuk/nnAudio" -> "asteroid-team/torch-audiomentations"
"KinWaiCheuk/nnAudio" -> "csteinmetz1/auraloss"
"KinWaiCheuk/nnAudio" -> "qiuqiangkong/torchlibrosa"
"KinWaiCheuk/nnAudio" -> "facebookresearch/WavAugment"
"KinWaiCheuk/nnAudio" -> "aliutkus/speechmetrics" ["e"=1]
"KinWaiCheuk/nnAudio" -> "google-research/leaf-audio"
"KinWaiCheuk/nnAudio" -> "marl/crepe" ["e"=1]
"KinWaiCheuk/nnAudio" -> "iver56/audiomentations"
"KinWaiCheuk/nnAudio" -> "acids-ircam/ddsp_pytorch"
"KinWaiCheuk/nnAudio" -> "mir-dataset-loaders/mirdata" ["e"=1]
"KinWaiCheuk/nnAudio" -> "adefossez/julius"
"KinWaiCheuk/nnAudio" -> "DavidDiazGuerra/gpuRIR" ["e"=1]
"KinWaiCheuk/nnAudio" -> "keunwoochoi/kapre"
"KinWaiCheuk/nnAudio" -> "torchsynth/torchsynth"
"KinWaiCheuk/nnAudio" -> "pytorch/audio"
"JingShing/AI-Drawing-Spell-Generator" -> "JingShing/AI-image-tag-extractor"
"JingShing/AI-Drawing-Spell-Generator" -> "JingShing/NovelAI-4chan-lowvram-ver"
"JingShing/AI-image-tag-extractor" -> "JingShing/AI-Drawing-Spell-Generator"
"lowerquality/gentle" -> "pettarin/forced-alignment-tools"
"lowerquality/gentle" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"lowerquality/gentle" -> "readbeyond/aeneas"
"lowerquality/gentle" -> "prosodylab/Prosodylab-Aligner"
"lowerquality/gentle" -> "mozilla/DSAlign"
"lowerquality/gentle" -> "Kyubyong/g2p"
"lowerquality/gentle" -> "YannickJadoul/Parselmouth"
"lowerquality/gentle" -> "xcmyz/FastSpeech"
"lowerquality/gentle" -> "AdolfVonKleist/Phonetisaurus"
"lowerquality/gentle" -> "facebookresearch/WavAugment"
"lowerquality/gentle" -> "syhw/wer_are_we"
"lowerquality/gentle" -> "pykaldi/pykaldi"
"lowerquality/gentle" -> "bootphon/phonemizer"
"lowerquality/gentle" -> "CSTR-Edinburgh/merlin"
"lowerquality/gentle" -> "kan-bayashi/ParallelWaveGAN"
"magenta/ddsp-vst" -> "magenta/midi-ddsp"
"magenta/ddsp-vst" -> "acids-ircam/ddsp_pytorch"
"magenta/ddsp-vst" -> "sudara/awesome-juce" ["e"=1]
"magenta/ddsp-vst" -> "QosmoInc/neutone_sdk"
"magenta/ddsp-vst" -> "jatinchowdhury18/RTNeural" ["e"=1]
"magenta/ddsp-vst" -> "magenta/ddsp" ["e"=1]
"magenta/ddsp-vst" -> "ben-hayes/neural-waveshaping-synthesis"
"magenta/ddsp-vst" -> "Tracktion/pluginval" ["e"=1]
"magenta/ddsp-vst" -> "Chowdhury-DSP/BYOD" ["e"=1]
"magenta/ddsp-vst" -> "adobe-research/DeepAFx-ST"
"magenta/ddsp-vst" -> "DBraun/DawDreamer" ["e"=1]
"magenta/ddsp-vst" -> "caillonantoine/RAVE"
"magenta/ddsp-vst" -> "Signalsmith-Audio/signalsmith-stretch" ["e"=1]
"magenta/ddsp-vst" -> "acids-ircam/rave_vst"
"magenta/ddsp-vst" -> "YatingMusic/ddsp-singing-vocoders"
"yxlllc/DDSP-SVC" -> "fishaudio/fish-diffusion"
"yxlllc/DDSP-SVC" -> "w-okada/voice-changer"
"yxlllc/DDSP-SVC" -> "liujing04/Retrieval-based-Voice-Conversion-WebUI"
"yxlllc/DDSP-SVC" -> "zhaohui8969/VST_NetProcess-"
"yxlllc/DDSP-SVC" -> "openvpi/audio-slicer"
"yxlllc/DDSP-SVC" -> "SayaSS/vits-finetuning"
"yxlllc/DDSP-SVC" -> "openvpi/vocoders"
"yxlllc/DDSP-SVC" -> "PlayVoice/VI-SVC"
"yxlllc/DDSP-SVC" -> "openvpi/diff-svc"
"yxlllc/DDSP-SVC" -> "PlayVoice/lora-svc"
"yxlllc/DDSP-SVC" -> "innnky/diff-svc"
"yxlllc/DDSP-SVC" -> "log1stics/voice-generator-webui"
"yxlllc/DDSP-SVC" -> "Francis-Komizu/Sovits"
"yxlllc/DDSP-SVC" -> "svc-develop-team/diff-svc"
"hongshin/OperatingSystem" -> "hongshin/interesting-articles-in-software-engineering"
"hongshin/OperatingSystem" -> "hongshin/LearningC"
"prophesier/diff-svc" -> "innnky/so-vits-svc"
"prophesier/diff-svc" -> "openvpi/DiffSinger"
"prophesier/diff-svc" -> "MoonInTheRiver/DiffSinger"
"prophesier/diff-svc" -> "svc-develop-team/so-vits-svc"
"prophesier/diff-svc" -> "jaywalnut310/vits"
"prophesier/diff-svc" -> "luoyily/MoeTTS"
"prophesier/diff-svc" -> "fishaudio/fish-diffusion"
"prophesier/diff-svc" -> "liujing04/Retrieval-based-Voice-Conversion-WebUI"
"prophesier/diff-svc" -> "CjangCjengh/MoeGoe"
"prophesier/diff-svc" -> "CjangCjengh/vits"
"prophesier/diff-svc" -> "NaruseMioShirakana/MoeSS"
"prophesier/diff-svc" -> "innnky/emotional-vits"
"prophesier/diff-svc" -> "Plachtaa/VITS-fast-fine-tuning"
"prophesier/diff-svc" -> "34j/so-vits-svc-fork"
"prophesier/diff-svc" -> "w-okada/voice-changer"
"Emotional-Text-to-Speech/dl-for-emo-tts" -> "keonlee9420/Expressive-FastSpeech2"
"Emotional-Text-to-Speech/dl-for-emo-tts" -> "KinglittleQ/GST-Tacotron"
"Emotional-Text-to-Speech/dl-for-emo-tts" -> "numediart/EmoV-DB"
"Emotional-Text-to-Speech/dl-for-emo-tts" -> "HLTSingapore/Emotional-Speech-Data"
"Emotional-Text-to-Speech/dl-for-emo-tts" -> "yangdongchao/Text-to-sound-Synthesis"
"Emotional-Text-to-Speech/dl-for-emo-tts" -> "keonlee9420/Comprehensive-Transformer-TTS"
"Emotional-Text-to-Speech/dl-for-emo-tts" -> "Rongjiehuang/ProDiff"
"Emotional-Text-to-Speech/dl-for-emo-tts" -> "hash2430/pitchtron"
"Emotional-Text-to-Speech/dl-for-emo-tts" -> "xcmyz/speech-synthesis-paper"
"Emotional-Text-to-Speech/dl-for-emo-tts" -> "mindslab-ai/cotatron"
"Emotional-Text-to-Speech/dl-for-emo-tts" -> "Emotional-Text-to-Speech/hmm-for-emo-tts"
"Emotional-Text-to-Speech/dl-for-emo-tts" -> "LEEYOONHYUNG/BVAE-TTS"
"Emotional-Text-to-Speech/dl-for-emo-tts" -> "jaywalnut310/glow-tts"
"Emotional-Text-to-Speech/dl-for-emo-tts" -> "keonlee9420/STYLER"
"Emotional-Text-to-Speech/dl-for-emo-tts" -> "hhguo/MSMC-TTS"
"TencentGameMate/chinese_speech_pretrain" -> "speechio/chinese_text_normalization"
"TencentGameMate/chinese_speech_pretrain" -> "microsoft/NeuralSpeech"
"TencentGameMate/chinese_speech_pretrain" -> "ddlBoJack/Speech-Resources"
"TencentGameMate/chinese_speech_pretrain" -> "cnlinxi/book-text-to-speech"
"TencentGameMate/chinese_speech_pretrain" -> "speechio/BigCiDian"
"TencentGameMate/chinese_speech_pretrain" -> "k2-fsa/icefall"
"TencentGameMate/chinese_speech_pretrain" -> "kakaobrain/g2pM"
"TencentGameMate/chinese_speech_pretrain" -> "s3prl/s3prl"
"TencentGameMate/chinese_speech_pretrain" -> "NATSpeech/NATSpeech" ["e"=1]
"TencentGameMate/chinese_speech_pretrain" -> "tts-tutorial/survey"
"TencentGameMate/chinese_speech_pretrain" -> "chenkui164/FastASR"
"TencentGameMate/chinese_speech_pretrain" -> "wenet-e2e/wenet"
"TencentGameMate/chinese_speech_pretrain" -> "k2-fsa/k2"
"TencentGameMate/chinese_speech_pretrain" -> "Rongjiehuang/FastDiff"
"TencentGameMate/chinese_speech_pretrain" -> "Moon0316/T2A"
"WelkinYang/GradTTS" -> "keonlee9420/DiffGAN-TTS"
"WelkinYang/GradTTS" -> "huawei-noah/Speech-Backbones"
"WelkinYang/GradTTS" -> "KevinMIN95/StyleSpeech"
"archinetai/audio-diffusion-pytorch" -> "lucidrains/audiolm-pytorch"
"archinetai/audio-diffusion-pytorch" -> "haoheliu/AudioLDM"
"archinetai/audio-diffusion-pytorch" -> "archinetai/audio-ai-timeline"
"archinetai/audio-diffusion-pytorch" -> "teticio/audio-diffusion"
"archinetai/audio-diffusion-pytorch" -> "NVIDIA/BigVGAN"
"archinetai/audio-diffusion-pytorch" -> "LAION-AI/CLAP"
"archinetai/audio-diffusion-pytorch" -> "yangdongchao/Text-to-sound-Synthesis"
"archinetai/audio-diffusion-pytorch" -> "lucidrains/musiclm-pytorch"
"archinetai/audio-diffusion-pytorch" -> "LAION-AI/audio-dataset"
"archinetai/audio-diffusion-pytorch" -> "Harmonai-org/sample-generator"
"archinetai/audio-diffusion-pytorch" -> "marcoppasini/musika"
"archinetai/audio-diffusion-pytorch" -> "facebookresearch/encodec"
"archinetai/audio-diffusion-pytorch" -> "lmnt-com/diffwave"
"archinetai/audio-diffusion-pytorch" -> "acids-ircam/RAVE"
"archinetai/audio-diffusion-pytorch" -> "csteinmetz1/auraloss"
"as-ideas/TransformerTTS" -> "soobinseo/Transformer-TTS"
"as-ideas/TransformerTTS" -> "as-ideas/ForwardTacotron"
"as-ideas/TransformerTTS" -> "ming024/FastSpeech2"
"as-ideas/TransformerTTS" -> "kan-bayashi/ParallelWaveGAN"
"as-ideas/TransformerTTS" -> "Tomiinek/Multilingual_Text_to_Speech"
"as-ideas/TransformerTTS" -> "jik876/hifi-gan"
"as-ideas/TransformerTTS" -> "xcmyz/speech-synthesis-paper"
"as-ideas/TransformerTTS" -> "NVIDIA/mellotron"
"as-ideas/TransformerTTS" -> "xcmyz/FastSpeech"
"as-ideas/TransformerTTS" -> "seungwonpark/melgan"
"as-ideas/TransformerTTS" -> "NVIDIA/flowtron"
"as-ideas/TransformerTTS" -> "jaywalnut310/glow-tts"
"as-ideas/TransformerTTS" -> "TensorSpeech/TensorFlowTTS"
"as-ideas/TransformerTTS" -> "fatchord/WaveRNN"
"as-ideas/TransformerTTS" -> "janvainer/speedyspeech"
"facebookresearch/textlesslib" -> "facebookresearch/speech-resynthesis"
"facebookresearch/textlesslib" -> "nii-yamagishilab/multi-speaker-tacotron"
"facebookresearch/textlesslib" -> "keonlee9420/Comprehensive-Transformer-TTS"
"facebookresearch/textlesslib" -> "facebookresearch/vocoder-benchmark"
"facebookresearch/textlesslib" -> "liusongxiang/Large-Audio-Models"
"facebookresearch/textlesslib" -> "tts-tutorial/survey"
"facebookresearch/textlesslib" -> "facebookresearch/WavAugment"
"facebookresearch/textlesslib" -> "lucidrains/natural-speech-pytorch"
"facebookresearch/textlesslib" -> "KevinMIN95/StyleSpeech"
"facebookresearch/textlesslib" -> "kahne/SpeechTransProgress" ["e"=1]
"facebookresearch/textlesslib" -> "s3prl/s3prl"
"facebookresearch/textlesslib" -> "NVIDIA/radtts"
"facebookresearch/textlesslib" -> "LAION-AI/audio-dataset"
"facebookresearch/textlesslib" -> "microsoft/UniSpeech"
"facebookresearch/textlesslib" -> "xcmyz/FastVocoder"
"haoheliu/AudioLDM" -> "archinetai/audio-diffusion-pytorch"
"haoheliu/AudioLDM" -> "lucidrains/audiolm-pytorch"
"haoheliu/AudioLDM" -> "archinetai/audio-ai-timeline"
"haoheliu/AudioLDM" -> "LAION-AI/CLAP"
"haoheliu/AudioLDM" -> "lucidrains/musiclm-pytorch"
"haoheliu/AudioLDM" -> "NVIDIA/BigVGAN"
"haoheliu/AudioLDM" -> "facebookresearch/encodec"
"haoheliu/AudioLDM" -> "jik876/hifi-gan"
"haoheliu/AudioLDM" -> "teticio/audio-diffusion"
"haoheliu/AudioLDM" -> "yangdongchao/Text-to-sound-Synthesis"
"haoheliu/AudioLDM" -> "LAION-AI/audio-dataset"
"haoheliu/AudioLDM" -> "enhuiz/vall-e"
"haoheliu/AudioLDM" -> "liusongxiang/Large-Audio-Models"
"haoheliu/AudioLDM" -> "microsoft/NeuralSpeech"
"haoheliu/AudioLDM" -> "riffusion/riffusion"
"jinhan/tacotron2-vae" -> "rishikksh20/vae_tacotron2"
"jinhan/tacotron2-vae" -> "yanggeng1995/vae_tacotron"
"jinhan/tacotron2-vae" -> "jxzhanggg/nonparaSeq2seqVC_code"
"jinhan/tacotron2-vae" -> "ide8/tacotron2"
"jinhan/tacotron2-vae" -> "mindslab-ai/cotatron"
"jinhan/tacotron2-vae" -> "KinglittleQ/GST-Tacotron"
"jinhan/tacotron2-vae" -> "nii-yamagishilab/multi-speaker-tacotron"
"jjery2243542/adaptive_voice_conversion" -> "jjery2243542/voice_conversion"
"jjery2243542/adaptive_voice_conversion" -> "auspicious3000/autovc"
"jjery2243542/adaptive_voice_conversion" -> "liusongxiang/StarGAN-Voice-Conversion"
"jjery2243542/adaptive_voice_conversion" -> "jxzhanggg/nonparaSeq2seqVC_code"
"jjery2243542/adaptive_voice_conversion" -> "auspicious3000/SpeechSplit"
"jjery2243542/adaptive_voice_conversion" -> "bshall/ZeroSpeech"
"jjery2243542/adaptive_voice_conversion" -> "KimythAnly/AGAIN-VC"
"jjery2243542/adaptive_voice_conversion" -> "kan-bayashi/ParallelWaveGAN"
"jjery2243542/adaptive_voice_conversion" -> "k2kobayashi/sprocket"
"jjery2243542/adaptive_voice_conversion" -> "hujinsen/StarGAN-Voice-Conversion"
"jjery2243542/adaptive_voice_conversion" -> "yistLin/FragmentVC"
"jjery2243542/adaptive_voice_conversion" -> "liusongxiang/ppg-vc"
"jjery2243542/adaptive_voice_conversion" -> "Wendison/VQMIVC"
"jjery2243542/adaptive_voice_conversion" -> "descriptinc/melgan-neurips"
"jjery2243542/adaptive_voice_conversion" -> "andi611/ZeroSpeech-TTS-without-T"
"keonlee9420/DiffGAN-TTS" -> "WelkinYang/GradTTS"
"keonlee9420/DiffGAN-TTS" -> "huawei-noah/Speech-Backbones"
"keonlee9420/DiffGAN-TTS" -> "keonlee9420/Comprehensive-Transformer-TTS"
"keonlee9420/DiffGAN-TTS" -> "keonlee9420/DiffSinger"
"keonlee9420/DiffGAN-TTS" -> "KevinMIN95/StyleSpeech"
"keonlee9420/DiffGAN-TTS" -> "NVIDIA/radtts"
"keonlee9420/DiffGAN-TTS" -> "thuhcsi/VAENAR-TTS"
"keonlee9420/DiffGAN-TTS" -> "Rongjiehuang/ProDiff"
"keonlee9420/DiffGAN-TTS" -> "keonlee9420/StyleSpeech"
"keonlee9420/DiffGAN-TTS" -> "keonlee9420/WaveGrad2"
"keonlee9420/DiffGAN-TTS" -> "keonlee9420/Parallel-Tacotron2"
"keonlee9420/DiffGAN-TTS" -> "Rongjiehuang/FastDiff"
"keonlee9420/Expressive-FastSpeech2" -> "keonlee9420/STYLER"
"keonlee9420/Expressive-FastSpeech2" -> "keonlee9420/StyleSpeech"
"keonlee9420/Expressive-FastSpeech2" -> "keonlee9420/PortaSpeech"
"keonlee9420/Expressive-FastSpeech2" -> "keonlee9420/Comprehensive-E2E-TTS"
"keonlee9420/Expressive-FastSpeech2" -> "Emotional-Text-to-Speech/dl-for-emo-tts"
"keonlee9420/Expressive-FastSpeech2" -> "keonlee9420/Cross-Speaker-Emotion-Transfer"
"keonlee9420/Expressive-FastSpeech2" -> "keonlee9420/DailyTalk"
"keonlee9420/PortaSpeech" -> "keonlee9420/StyleSpeech"
"keonlee9420/PortaSpeech" -> "keonlee9420/STYLER"
"keonlee9420/PortaSpeech" -> "keonlee9420/Comprehensive-Transformer-TTS"
"keonlee9420/PortaSpeech" -> "KevinMIN95/StyleSpeech"
"keonlee9420/PortaSpeech" -> "keonlee9420/Expressive-FastSpeech2"
"keonlee9420/PortaSpeech" -> "choiHkk/CVAEJETS"
"keonlee9420/PortaSpeech" -> "keonlee9420/Comprehensive-E2E-TTS"
"keonlee9420/PortaSpeech" -> "thuhcsi/VAENAR-TTS"
"keonlee9420/STYLER" -> "keonlee9420/StyleSpeech"
"keonlee9420/STYLER" -> "keonlee9420/Expressive-FastSpeech2"
"keonlee9420/STYLER" -> "thuhcsi/VAENAR-TTS"
"keonlee9420/STYLER" -> "keonlee9420/PortaSpeech"
"lifeiteng/vall-e" -> "enhuiz/vall-e"
"lifeiteng/vall-e" -> "lucidrains/audiolm-pytorch"
"lifeiteng/vall-e" -> "MasayaKawamura/MB-iSTFT-VITS"
"lifeiteng/vall-e" -> "yl4579/StyleTTS"
"lifeiteng/vall-e" -> "NVIDIA/BigVGAN"
"lifeiteng/vall-e" -> "anonymous-pits/pits"
"lifeiteng/vall-e" -> "Rongjiehuang/GenerSpeech"
"lifeiteng/vall-e" -> "microsoft/SpeechT5"
"lifeiteng/vall-e" -> "Edresson/YourTTS"
"lifeiteng/vall-e" -> "suno-ai/bark"
"lifeiteng/vall-e" -> "keonlee9420/Expressive-FastSpeech2"
"lifeiteng/vall-e" -> "microsoft/NeuralSpeech"
"lifeiteng/vall-e" -> "PlayVoice/vits_chinese"
"lifeiteng/vall-e" -> "heatz123/naturalspeech"
"lifeiteng/vall-e" -> "yangdongchao/Text-to-sound-Synthesis"
"lucidrains/vector-quantize-pytorch" -> "wesbz/SoundStream"
"lucidrains/vector-quantize-pytorch" -> "lucidrains/audiolm-pytorch"
"lucidrains/vector-quantize-pytorch" -> "microsoft/VQ-Diffusion" ["e"=1]
"lucidrains/vector-quantize-pytorch" -> "NVIDIA/BigVGAN"
"lucidrains/vector-quantize-pytorch" -> "facebookresearch/encodec"
"lucidrains/vector-quantize-pytorch" -> "archinetai/audio-diffusion-pytorch"
"lucidrains/vector-quantize-pytorch" -> "lucidrains/x-clip" ["e"=1]
"lucidrains/vector-quantize-pytorch" -> "facebookresearch/AudioMAE"
"lucidrains/vector-quantize-pytorch" -> "Rongjiehuang/GenerSpeech"
"lucidrains/vector-quantize-pytorch" -> "MishaLaskin/vqvae" ["e"=1]
"lucidrains/vector-quantize-pytorch" -> "lmnt-com/diffwave"
"lucidrains/vector-quantize-pytorch" -> "s3prl/s3prl"
"lucidrains/vector-quantize-pytorch" -> "lucidrains/x-transformers" ["e"=1]
"lucidrains/vector-quantize-pytorch" -> "facebookresearch/textlesslib"
"lucidrains/vector-quantize-pytorch" -> "karpathy/deep-vector-quantization" ["e"=1]
"ming024/FastSpeech2" -> "xcmyz/FastSpeech"
"ming024/FastSpeech2" -> "jik876/hifi-gan"
"ming024/FastSpeech2" -> "kan-bayashi/ParallelWaveGAN"
"ming024/FastSpeech2" -> "jaywalnut310/glow-tts"
"ming024/FastSpeech2" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"ming024/FastSpeech2" -> "xcmyz/speech-synthesis-paper"
"ming024/FastSpeech2" -> "NVIDIA/mellotron"
"ming024/FastSpeech2" -> "microsoft/NeuralSpeech"
"ming024/FastSpeech2" -> "tts-tutorial/survey"
"ming024/FastSpeech2" -> "Tomiinek/Multilingual_Text_to_Speech"
"ming024/FastSpeech2" -> "Kyubyong/g2p"
"ming024/FastSpeech2" -> "descriptinc/melgan-neurips"
"ming024/FastSpeech2" -> "seungwonpark/melgan"
"ming024/FastSpeech2" -> "soobinseo/Transformer-TTS"
"ming024/FastSpeech2" -> "TensorSpeech/TensorFlowTTS"
"mmorise/World" -> "JeremyCCHsu/Python-Wrapper-for-World-Vocoder"
"mmorise/World" -> "CSTR-Edinburgh/merlin"
"mmorise/World" -> "mozilla/LPCNet"
"mmorise/World" -> "kan-bayashi/ParallelWaveGAN"
"mmorise/World" -> "k2kobayashi/sprocket"
"mmorise/World" -> "r9y9/pysptk"
"mmorise/World" -> "fatchord/WaveRNN"
"mmorise/World" -> "jik876/hifi-gan"
"mmorise/World" -> "r9y9/nnmnkwii"
"mmorise/World" -> "google/REAPER"
"mmorise/World" -> "marl/crepe" ["e"=1]
"mmorise/World" -> "descriptinc/melgan-neurips"
"mmorise/World" -> "r9y9/wavenet_vocoder"
"mmorise/World" -> "NVIDIA/mellotron"
"mmorise/World" -> "NVIDIA/waveglow"
"ranchlai/mandarin-tts" -> "lturing/tacotronv2_wavernn_chinese"
"ranchlai/mandarin-tts" -> "ming024/FastSpeech2"
"ranchlai/mandarin-tts" -> "kuangdd/ttskit"
"ranchlai/mandarin-tts" -> "Jackiexiao/zhtts"
"ranchlai/mandarin-tts" -> "JasonWei512/Tacotron-2-Chinese"
"ranchlai/mandarin-tts" -> "keonlee9420/Expressive-FastSpeech2"
"ranchlai/mandarin-tts" -> "kakaobrain/g2pM"
"ranchlai/mandarin-tts" -> "thuhcsi/Crystal"
"ranchlai/mandarin-tts" -> "speechio/chinese_text_normalization"
"ranchlai/mandarin-tts" -> "Jackiexiao/MTTS"
"ranchlai/mandarin-tts" -> "cnlinxi/book-text-to-speech"
"ranchlai/mandarin-tts" -> "liusongxiang/ppg-vc"
"ranchlai/mandarin-tts" -> "aidreamwin/TTS-Clone-Chinese"
"ranchlai/mandarin-tts" -> "atomicoo/FCH-TTS"
"ranchlai/mandarin-tts" -> "thuhcsi/VAENAR-TTS"
"rishikksh20/FastSpeech2" -> "rishikksh20/VocGAN"
"rishikksh20/FastSpeech2" -> "rishikksh20/TFGAN"
"rishikksh20/FastSpeech2" -> "ga642381/FastSpeech2"
"rishikksh20/FastSpeech2" -> "WelkinYang/GradTTS"
"rishikksh20/FastSpeech2" -> "deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning"
"rishikksh20/FastSpeech2" -> "WelkinYang/Learn2Sing2.0"
"rishikksh20/FastSpeech2" -> "ming024/FastSpeech2"
"wesbz/SoundStream" -> "lucidrains/vector-quantize-pytorch"
"wesbz/SoundStream" -> "sp-uhh/sgmse" ["e"=1]
"wesbz/SoundStream" -> "jzi040941/PercepNet" ["e"=1]
"zhvng/open-musiclm" -> "seungheondoh/music-text-representation"
"k2kobayashi/sprocket" -> "liusongxiang/StarGAN-Voice-Conversion"
"k2kobayashi/sprocket" -> "r9y9/gantts"
"k2kobayashi/sprocket" -> "JeremyCCHsu/Python-Wrapper-for-World-Vocoder"
"k2kobayashi/sprocket" -> "jjery2243542/adaptive_voice_conversion"
"k2kobayashi/sprocket" -> "jxzhanggg/nonparaSeq2seqVC_code"
"k2kobayashi/sprocket" -> "kan-bayashi/PytorchWaveNetVocoder"
"k2kobayashi/sprocket" -> "mmorise/World"
"k2kobayashi/sprocket" -> "auspicious3000/autovc"
"k2kobayashi/sprocket" -> "jjery2243542/voice_conversion"
"k2kobayashi/sprocket" -> "JeremyCCHsu/vae-npvc"
"k2kobayashi/sprocket" -> "leimao/Voice_Converter_CycleGAN"
"k2kobayashi/sprocket" -> "r9y9/pysptk"
"k2kobayashi/sprocket" -> "k2kobayashi/crank"
"k2kobayashi/sprocket" -> "kan-bayashi/ParallelWaveGAN"
"k2kobayashi/sprocket" -> "hujinsen/StarGAN-Voice-Conversion"
"qiniu/qshell" -> "qiniu/QSunSync"
"qiniu/qshell" -> "qiniu/js-sdk"
"qiniu/qshell" -> "qiniu/php-sdk" ["e"=1]
"qiniu/qshell" -> "qiniu/python-sdk" ["e"=1]
"qiniu/qshell" -> "qiniu/kodo-browser"
"qiniu/qshell" -> "zgldh/qiniu-laravel-storage" ["e"=1]
"qiniu/qshell" -> "widuu/qiniu_ueditor_1.4.3"
"qiniu/qshell" -> "fooleap/disqus-php-api" ["e"=1]
"qiniu/qshell" -> "gyk001/hexo-qiniu-sync" ["e"=1]
"qiniu/qshell" -> "qiniu/logkit" ["e"=1]
"qiniu/qshell" -> "Suxiaogang/WeiboPicBed"
"qiniu/qshell" -> "oott123/bpcs_uploader" ["e"=1]
"qiniu/qshell" -> "mysql-inception/inception" ["e"=1]
"qiniu/qshell" -> "qiniu/nodejs-sdk"
"qiniu/qshell" -> "qiniu/qlang" ["e"=1]
"boy1dr/SpleeterGui" -> "lazydevyo/SpleetGUI"
"boy1dr/SpleeterGui" -> "thooore/SpleeterGUI"
"boy1dr/SpleeterGui" -> "deezer/spleeter"
"boy1dr/SpleeterGui" -> "Anjok07/ultimatevocalremovergui"
"boy1dr/SpleeterGui" -> "diracdeltas/spleeter4max" ["e"=1]
"boy1dr/SpleeterGui" -> "wxbool/video-srt-windows" ["e"=1]
"boy1dr/SpleeterGui" -> "LuckyHookin/edge-TTS-record" ["e"=1]
"boy1dr/SpleeterGui" -> "Justin62628/Squirrel-RIFE" ["e"=1]
"boy1dr/SpleeterGui" -> "azuwis/pianotrans" ["e"=1]
"boy1dr/SpleeterGui" -> "TianZerL/Anime4KCPP" ["e"=1]
"boy1dr/SpleeterGui" -> "xifangczy/cat-catch" ["e"=1]
"boy1dr/SpleeterGui" -> "JeffreyCA/spleeter-web"
"boy1dr/SpleeterGui" -> "X-Lucifer/AI-Lossless-Zoomer" ["e"=1]
"boy1dr/SpleeterGui" -> "vooidzero/B23Downloader" ["e"=1]
"boy1dr/SpleeterGui" -> "dream7180/foobox-cn" ["e"=1]
"liujing04/Retrieval-based-Voice-Conversion-WebUI" -> "w-okada/voice-changer"
"liujing04/Retrieval-based-Voice-Conversion-WebUI" -> "ddPn08/rvc-webui"
"liujing04/Retrieval-based-Voice-Conversion-WebUI" -> "yxlllc/DDSP-SVC"
"liujing04/Retrieval-based-Voice-Conversion-WebUI" -> "fishaudio/fish-diffusion"
"liujing04/Retrieval-based-Voice-Conversion-WebUI" -> "isletennos/MMVC_Trainer"
"liujing04/Retrieval-based-Voice-Conversion-WebUI" -> "NaruseMioShirakana/MoeSS"
"liujing04/Retrieval-based-Voice-Conversion-WebUI" -> "34j/so-vits-svc-fork"
"liujing04/Retrieval-based-Voice-Conversion-WebUI" -> "zhaohui8969/VST_NetProcess-"
"liujing04/Retrieval-based-Voice-Conversion-WebUI" -> "Plachtaa/VITS-fast-fine-tuning"
"liujing04/Retrieval-based-Voice-Conversion-WebUI" -> "PlayVoice/lora-svc"
"liujing04/Retrieval-based-Voice-Conversion-WebUI" -> "svc-develop-team/so-vits-svc"
"liujing04/Retrieval-based-Voice-Conversion-WebUI" -> "yantaisa11/Retrieval-based-Voice-Conversion-WebUI-JP-localization"
"liujing04/Retrieval-based-Voice-Conversion-WebUI" -> "prophesier/diff-svc"
"liujing04/Retrieval-based-Voice-Conversion-WebUI" -> "innnky/emotional-vits"
"liujing04/Retrieval-based-Voice-Conversion-WebUI" -> "PlayVoice/so-vits-svc-5.0"
"TensorSpeech/TensorFlowASR" -> "Z-yq/TensorflowASR"
"TensorSpeech/TensorFlowASR" -> "noahchalifour/rnnt-speech-recognition"
"TensorSpeech/TensorFlowASR" -> "tencent-ailab/pika"
"TensorSpeech/TensorFlowASR" -> "hirofumi0810/neural_sp"
"TensorSpeech/TensorFlowASR" -> "TensorSpeech/TensorFlowTTS"
"TensorSpeech/TensorFlowASR" -> "mobvoi/wenet"
"TensorSpeech/TensorFlowASR" -> "k2-fsa/k2"
"TensorSpeech/TensorFlowASR" -> "cywang97/StreamingTransformer"
"TensorSpeech/TensorFlowASR" -> "athena-team/athena" ["e"=1]
"TensorSpeech/TensorFlowASR" -> "SpeechColab/GigaSpeech"
"TensorSpeech/TensorFlowASR" -> "openspeech-team/openspeech"
"TensorSpeech/TensorFlowASR" -> "kaituoxu/Speech-Transformer"
"TensorSpeech/TensorFlowASR" -> "ZhengkunTian/OpenTransformer"
"TensorSpeech/TensorFlowASR" -> "HawkAaron/warp-transducer"
"TensorSpeech/TensorFlowASR" -> "lucidrains/conformer"
"iceychris/LibreASR" -> "cywang97/StreamingTransformer"
"iceychris/LibreASR" -> "theblackcat102/Online-Speech-Recognition"
"iceychris/LibreASR" -> "mozilla/DSAlign"
"iceychris/LibreASR" -> "TensorSpeech/TensorFlowASR"
"iceychris/LibreASR" -> "noahchalifour/rnnt-speech-recognition"
"iceychris/LibreASR" -> "tencent-ailab/pika"
"iceychris/LibreASR" -> "xingchensong/speech-recognition-papers"
"iceychris/LibreASR" -> "hirofumi0810/neural_sp"
"iceychris/LibreASR" -> "freewym/espresso"
"iceychris/LibreASR" -> "ZhengkunTian/OpenTransformer"
"iceychris/LibreASR" -> "andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning"
"iceychris/LibreASR" -> "lumaku/ctc-segmentation"
"iceychris/LibreASR" -> "midas-research/audino"
"spotify/basic-pitch" -> "spotify/pedalboard"
"spotify/basic-pitch" -> "magenta/mt3" ["e"=1]
"spotify/basic-pitch" -> "Music-and-Culture-Technology-Lab/omnizart" ["e"=1]
"spotify/basic-pitch" -> "spotify/basic-pitch-ts"
"spotify/basic-pitch" -> "magenta/ddsp" ["e"=1]
"spotify/basic-pitch" -> "archinetai/audio-diffusion-pytorch"
"spotify/basic-pitch" -> "magenta/midi-ddsp"
"spotify/basic-pitch" -> "magenta/ddsp-vst"
"spotify/basic-pitch" -> "csteinmetz1/ai-audio-startups"
"spotify/basic-pitch" -> "facebookresearch/demucs"
"spotify/basic-pitch" -> "Natooz/MidiTok" ["e"=1]
"spotify/basic-pitch" -> "marl/crepe" ["e"=1]
"spotify/basic-pitch" -> "samim23/polymath"
"spotify/basic-pitch" -> "marcoppasini/musika"
"spotify/basic-pitch" -> "lucidrains/musiclm-pytorch"
"hongshin/DiscreteMath" -> "hongshin/OperatingSystem"
"hongshin/DiscreteMath" -> "idebtor/nowic"
"hongshin/DiscreteMath" -> "hongshin/interesting-articles-in-software-engineering"
"hongshin/DiscreteMath" -> "hongshin/LearningC"
"hongshin/DiscreteMath" -> "jongwoojeff/DiscreteMathematics"
"hongshin/DiscreteMath" -> "idebtor/JoyAI"
"alphacep/vosk-android-demo" -> "ccoreilly/LocalSTT"
"alphacep/vosk-android-demo" -> "alphacep/vosk-android-service"
"alphacep/vosk-android-demo" -> "alphacep/vosk-api"
"alphacep/vosk-android-demo" -> "alphacep/vosk-server"
"alphacep/vosk-android-demo" -> "just-ai/aimybox-android-sdk"
"alphacep/vosk-android-demo" -> "alphacep/vosk"
"CjangCjengh/TTSModels" -> "CjangCjengh/MoeGoe_GUI"
"CjangCjengh/TTSModels" -> "CjangCjengh/MoeGoe"
"CjangCjengh/TTSModels" -> "CjangCjengh/vits"
"CjangCjengh/TTSModels" -> "innnky/emotional-vits"
"CjangCjengh/TTSModels" -> "luoyily/MoeTTS"
"CjangCjengh/TTSModels" -> "cjyaddone/ChatWaifu"
"CjangCjengh/TTSModels" -> "audeering/w2v2-how-to"
"CjangCjengh/TTSModels" -> "cjyaddone/ChatWaifuL2D"
"CjangCjengh/TTSModels" -> "AlexandaJerry/whisper-vits-japanese"
"CjangCjengh/TTSModels" -> "Paraworks/vits_with_chatgpt-gpt3"
"CjangCjengh/TTSModels" -> "Plachtaa/VITS-fast-fine-tuning"
"CjangCjengh/TTSModels" -> "fumiama/MoeGoe-Android"
"CjangCjengh/TTSModels" -> "TheKOG/Gal-Voice-Bot"
"CjangCjengh/TTSModels" -> "SayaSS/vits-finetuning"
"CjangCjengh/TTSModels" -> "Francis-Komizu/VITS"
"CjangCjengh/tacotron2-japanese" -> "luoyily/MoeTTS"
"CjangCjengh/tacotron2-japanese" -> "innnky/vits-japanese"
"CjangCjengh/tacotron2-japanese" -> "CjangCjengh/vits"
"CjangCjengh/tacotron2-japanese" -> "CjangCjengh/MoeGoe"
"CjangCjengh/tacotron2-japanese" -> "TheKOG/Gal-Voice-Bot"
"CjangCjengh/tacotron2-japanese" -> "CjangCjengh/TTSModels"
"CjangCjengh/tacotron2-japanese" -> "audeering/w2v2-how-to"
"CjangCjengh/tacotron2-japanese" -> "CjangCjengh/krkrFgiEditor"
"CjangCjengh/tacotron2-japanese" -> "Francis-Komizu/VITS"
"robin1001/xdecoder" -> "jpuigcerver/kaldi-decoders"
"Janghyun1230/Speaker_Verification" -> "HarryVolek/PyTorch_Speaker_Verification"
"Janghyun1230/Speaker_Verification" -> "philipperemy/deep-speaker"
"Janghyun1230/Speaker_Verification" -> "funcwj/ge2e-speaker-verification"
"Janghyun1230/Speaker_Verification" -> "qqueing/DeepSpeaker-pytorch"
"Janghyun1230/Speaker_Verification" -> "WeidiXie/VGG-Speaker-Recognition"
"Janghyun1230/Speaker_Verification" -> "jymsuper/SpeakerRecognition_tutorial"
"Janghyun1230/Speaker_Verification" -> "Walleclipse/Deep_Speaker-speaker_recognition_system"
"Janghyun1230/Speaker_Verification" -> "Jungjee/RawNet"
"Janghyun1230/Speaker_Verification" -> "taylorlu/Speaker-Diarization"
"Janghyun1230/Speaker_Verification" -> "wq2012/SpectralCluster"
"Janghyun1230/Speaker_Verification" -> "google/uis-rnn"
"Janghyun1230/Speaker_Verification" -> "rajathkmp/speaker-verification"
"Janghyun1230/Speaker_Verification" -> "wq2012/awesome-diarization"
"Janghyun1230/Speaker_Verification" -> "Suhee05/Text-Independent-Speaker-Verification"
"Janghyun1230/Speaker_Verification" -> "mravanelli/SincNet"
"filippogiruzzi/voice_activity_detection" -> "nicklashansen/voice-activity-detection"
"filippogiruzzi/voice_activity_detection" -> "hcmlab/vadnet"
"filippogiruzzi/voice_activity_detection" -> "jtkim-kaist/VAD"
"filippogiruzzi/voice_activity_detection" -> "voithru/voice-activity-detection"
"filippogiruzzi/voice_activity_detection" -> "marsbroshok/VAD-python"
"filippogiruzzi/voice_activity_detection" -> "wangshub/python-vad"
"filippogiruzzi/voice_activity_detection" -> "RicherMans/GPV"
"filippogiruzzi/voice_activity_detection" -> "snakers4/silero-vad"
"filippogiruzzi/voice_activity_detection" -> "jymsuper/VAD_tutorial"
"filippogiruzzi/voice_activity_detection" -> "zhenghuatan/rVAD"
"filippogiruzzi/voice_activity_detection" -> "wiseman/py-webrtcvad"
"qiniu/java-sdk" -> "qiniu/js-sdk"
"qiniu/java-sdk" -> "jpush/jpush-api-java-client"
"qiniu/java-sdk" -> "aliyun/aliyun-oss-java-sdk" ["e"=1]
"qiniu/java-sdk" -> "qiniu/android-sdk"
"vincentherrmann/pytorch-wavenet" -> "r9y9/wavenet_vocoder"
"vincentherrmann/pytorch-wavenet" -> "tomlepaine/fast-wavenet"
"vincentherrmann/pytorch-wavenet" -> "NVIDIA/nv-wavenet"
"vincentherrmann/pytorch-wavenet" -> "kan-bayashi/PytorchWaveNetVocoder"
"vincentherrmann/pytorch-wavenet" -> "dhpollack/fast-wavenet.pytorch"
"vincentherrmann/pytorch-wavenet" -> "basveeling/wavenet"
"vincentherrmann/pytorch-wavenet" -> "golbin/WaveNet"
"vincentherrmann/pytorch-wavenet" -> "descriptinc/melgan-neurips"
"vincentherrmann/pytorch-wavenet" -> "fatchord/WaveRNN"
"vincentherrmann/pytorch-wavenet" -> "kan-bayashi/ParallelWaveGAN"
"vincentherrmann/pytorch-wavenet" -> "ibab/tensorflow-wavenet"
"vincentherrmann/pytorch-wavenet" -> "chrisdonahue/wavegan"
"vincentherrmann/pytorch-wavenet" -> "NVIDIA/waveglow"
"vincentherrmann/pytorch-wavenet" -> "Dankrushen/Wavenet-PyTorch"
"vincentherrmann/pytorch-wavenet" -> "asteroid-team/torch-audiomentations"
"uezo/ChatdollKit" -> "ttizze/BabyDORA"
"uezo/ChatdollKit" -> "hecomi/uLipSync" ["e"=1]
"uezo/ChatdollKit" -> "w-okada/voice-changer"
"uezo/ChatdollKit" -> "unity3d-jp/AnimeToolbox" ["e"=1]
"uezo/ChatdollKit" -> "fishslot/video_loopback_for_webui" ["e"=1]
"uezo/ChatdollKit" -> "TREE-Ind/Blender-GPT"
"uezo/ChatdollKit" -> "AUTOMATIC1111/stable-diffusion-webui-rembg" ["e"=1]
"uezo/ChatdollKit" -> "pkhungurn/talking-head-anime-3-demo"
"uezo/ChatdollKit" -> "isletennos/MMVC_Trainer"
"uezo/ChatdollKit" -> "Zuntan03/CharFramework"
"uezo/ChatdollKit" -> "VOICEVOX/voicevox_engine"
"uezo/ChatdollKit" -> "mattyamonaca/layerdivider" ["e"=1]
"uezo/ChatdollKit" -> "vrchat-community/osc" ["e"=1]
"uezo/ChatdollKit" -> "uezo/gpt3-contextual"
"uezo/ChatdollKit" -> "tori29umai0123/VRM_AI"
"Tomiinek/Multilingual_Text_to_Speech" -> "nii-yamagishilab/multi-speaker-tacotron"
"Tomiinek/Multilingual_Text_to_Speech" -> "jaywalnut310/glow-tts"
"Tomiinek/Multilingual_Text_to_Speech" -> "deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning"
"Tomiinek/Multilingual_Text_to_Speech" -> "jik876/hifi-gan"
"Tomiinek/Multilingual_Text_to_Speech" -> "xcmyz/speech-synthesis-paper"
"Tomiinek/Multilingual_Text_to_Speech" -> "xcmyz/FastSpeech"
"Tomiinek/Multilingual_Text_to_Speech" -> "kan-bayashi/ParallelWaveGAN"
"Tomiinek/Multilingual_Text_to_Speech" -> "Kyubyong/css10"
"Tomiinek/Multilingual_Text_to_Speech" -> "ming024/FastSpeech2"
"Tomiinek/Multilingual_Text_to_Speech" -> "NVIDIA/mellotron"
"Tomiinek/Multilingual_Text_to_Speech" -> "tts-tutorial/survey"
"Tomiinek/Multilingual_Text_to_Speech" -> "ivanvovk/WaveGrad"
"Tomiinek/Multilingual_Text_to_Speech" -> "huawei-noah/Speech-Backbones"
"Tomiinek/Multilingual_Text_to_Speech" -> "auspicious3000/SpeechSplit"
"Tomiinek/Multilingual_Text_to_Speech" -> "ivanvovk/DurIAN"
"readbeyond/aeneas" -> "pettarin/forced-alignment-tools"
"readbeyond/aeneas" -> "lowerquality/gentle"
"readbeyond/aeneas" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"readbeyond/aeneas" -> "prosodylab/Prosodylab-Aligner"
"readbeyond/aeneas" -> "mozilla/DSAlign"
"readbeyond/aeneas" -> "bootphon/phonemizer"
"readbeyond/aeneas" -> "fatchord/WaveRNN"
"readbeyond/aeneas" -> "Rayhane-mamah/Tacotron-2"
"readbeyond/aeneas" -> "wiseman/py-webrtcvad"
"readbeyond/aeneas" -> "kan-bayashi/ParallelWaveGAN"
"readbeyond/aeneas" -> "NVIDIA/waveglow"
"readbeyond/aeneas" -> "r4victor/syncabook"
"readbeyond/aeneas" -> "r9y9/wavenet_vocoder"
"readbeyond/aeneas" -> "buriburisuri/speech-to-text-wavenet"
"readbeyond/aeneas" -> "YannickJadoul/Parselmouth"
"ideasman42/nerd-dictation" -> "papoteur-mga/elograf"
"ideasman42/nerd-dictation" -> "alphacep/vosk-api"
"ideasman42/nerd-dictation" -> "phil294/AHK_X11" ["e"=1]
"ideasman42/nerd-dictation" -> "cursorless-dev/cursorless" ["e"=1]
"ideasman42/nerd-dictation" -> "synesthesiam/voice2json" ["e"=1]
"ideasman42/nerd-dictation" -> "sstadick/hck" ["e"=1]
"ideasman42/nerd-dictation" -> "rvaiya/warpd" ["e"=1]
"ideasman42/nerd-dictation" -> "rhasspy/larynx" ["e"=1]
"ideasman42/nerd-dictation" -> "queer/boxxy" ["e"=1]
"ideasman42/nerd-dictation" -> "abb128/LiveCaptions" ["e"=1]
"ideasman42/nerd-dictation" -> "harporoeder/ebpfsnitch" ["e"=1]
"snakers4/silero-vad" -> "pyannote/pyannote-audio"
"snakers4/silero-vad" -> "wiseman/py-webrtcvad"
"snakers4/silero-vad" -> "jtkim-kaist/VAD"
"snakers4/silero-vad" -> "lhotse-speech/lhotse"
"snakers4/silero-vad" -> "facebookresearch/denoiser" ["e"=1]
"snakers4/silero-vad" -> "k2-fsa/k2"
"snakers4/silero-vad" -> "snakers4/silero-models"
"snakers4/silero-vad" -> "wq2012/awesome-diarization"
"snakers4/silero-vad" -> "speechbrain/speechbrain"
"snakers4/silero-vad" -> "asteroid-team/asteroid" ["e"=1]
"snakers4/silero-vad" -> "s3prl/s3prl"
"snakers4/silero-vad" -> "filippogiruzzi/voice_activity_detection"
"snakers4/silero-vad" -> "facebookresearch/WavAugment"
"snakers4/silero-vad" -> "asteroid-team/torch-audiomentations"
"snakers4/silero-vad" -> "aliutkus/speechmetrics" ["e"=1]
"qiniu/js-sdk" -> "qiniu/php-sdk" ["e"=1]
"qiniu/js-sdk" -> "qiniu/java-sdk"
"qiniu/js-sdk" -> "iwillwen/qiniu.js"
"qiniu/js-sdk" -> "qiniu/nodejs-sdk"
"qiniu/js-sdk" -> "gpake/qiniu-wxapp-sdk" ["e"=1]
"qiniu/js-sdk" -> "moxiecode/plupload" ["e"=1]
"qiniu/js-sdk" -> "qiniu/qshell"
"qiniu/js-sdk" -> "widuu/qiniu_ueditor_1.4.3"
"qiniu/js-sdk" -> "zgldh/qiniu-laravel-storage" ["e"=1]
"qiniu/js-sdk" -> "tmallfe/tmallfe.github.io" ["e"=1]
"qiniu/js-sdk" -> "lsxiao/qiniu4js"
"qiniu/js-sdk" -> "JacksonTian/eventproxy" ["e"=1]
"qiniu/js-sdk" -> "lenage/react-qiniu"
"qiniu/js-sdk" -> "qiniu/nodejs-sdk.v6" ["e"=1]
"qiniu/js-sdk" -> "guo-yu/koa-guide" ["e"=1]
"ddPn08/rvc-webui" -> "ddPn08/rvc-webui-colab"
"ddPn08/rvc-webui" -> "yantaisa11/Retrieval-based-Voice-Conversion-WebUI-JP-localization"
"ddPn08/rvc-webui" -> "ashen-sensored/sd_webui_gligen" ["e"=1]
"ddPn08/rvc-webui" -> "teftef6220/Voice_Separation_and_Selection"
"Jam3/voice-activity-detection" -> "kdavis-mozilla/vad.js"
"JoFrhwld/FAVE" -> "prosodylab/Prosodylab-Aligner"
"JoFrhwld/FAVE" -> "mlml/autovot"
"stemrollerapp/stemroller" -> "facebookresearch/demucs"
"stemrollerapp/stemroller" -> "meienberger/runtipi" ["e"=1]
"stemrollerapp/stemroller" -> "mikeroyal/Photogrammetry-Guide" ["e"=1]
"stemrollerapp/stemroller" -> "sensity-ai/dot" ["e"=1]
"stemrollerapp/stemroller" -> "webrcade/webrcade" ["e"=1]
"stemrollerapp/stemroller" -> "jetpack-io/devbox" ["e"=1]
"stemrollerapp/stemroller" -> "Music-and-Culture-Technology-Lab/omnizart" ["e"=1]
"stemrollerapp/stemroller" -> "serhack/pdf-diff" ["e"=1]
"stemrollerapp/stemroller" -> "pedrozath/coltrane" ["e"=1]
"stemrollerapp/stemroller" -> "erdewit/HiFiScan" ["e"=1]
"stemrollerapp/stemroller" -> "LuanRT/YouTube.js" ["e"=1]
"stemrollerapp/stemroller" -> "7thSamurai/steganography" ["e"=1]
"stemrollerapp/stemroller" -> "anufrievroman/calcure" ["e"=1]
"stemrollerapp/stemroller" -> "mprimi/portable-secret" ["e"=1]
"stemrollerapp/stemroller" -> "adriancooney/puppeteer-heap-snapshot" ["e"=1]
"zzw922cn/Automatic_Speech_Recognition" -> "pannous/tensorflow-speech-recognition"
"zzw922cn/Automatic_Speech_Recognition" -> "zzw922cn/awesome-speech-recognition-speech-synthesis-papers"
"zzw922cn/Automatic_Speech_Recognition" -> "buriburisuri/speech-to-text-wavenet"
"zzw922cn/Automatic_Speech_Recognition" -> "SeanNaren/deepspeech.pytorch"
"zzw922cn/Automatic_Speech_Recognition" -> "mravanelli/pytorch-kaldi"
"zzw922cn/Automatic_Speech_Recognition" -> "hirofumi0810/tensorflow_end2end_speech_recognition"
"zzw922cn/Automatic_Speech_Recognition" -> "awni/speech"
"zzw922cn/Automatic_Speech_Recognition" -> "PaddlePaddle/DeepSpeech"
"zzw922cn/Automatic_Speech_Recognition" -> "facebookresearch/wav2letter"
"zzw922cn/Automatic_Speech_Recognition" -> "srvk/eesen"
"zzw922cn/Automatic_Speech_Recognition" -> "jameslyons/python_speech_features"
"zzw922cn/Automatic_Speech_Recognition" -> "syhw/wer_are_we"
"zzw922cn/Automatic_Speech_Recognition" -> "pykaldi/pykaldi"
"zzw922cn/Automatic_Speech_Recognition" -> "audier/DeepSpeechRecognition"
"zzw922cn/Automatic_Speech_Recognition" -> "espnet/espnet"
"hmartiro/riffusion-app" -> "hmartiro/riffusion-inference"
"hmartiro/riffusion-app" -> "Harmonai-org/sample-generator"
"hmartiro/riffusion-app" -> "marcoppasini/musika"
"hmartiro/riffusion-app" -> "samim23/polymath"
"hmartiro/riffusion-app" -> "archinetai/audio-diffusion-pytorch"
"hmartiro/riffusion-app" -> "google-research/frame-interpolation" ["e"=1]
"hmartiro/riffusion-app" -> "VoltaML/voltaML-fast-stable-diffusion" ["e"=1]
"hmartiro/riffusion-app" -> "kakaobrain/karlo" ["e"=1]
"hmartiro/riffusion-app" -> "lkwq007/stablediffusion-infinity" ["e"=1]
"hmartiro/riffusion-app" -> "deforum-art/deforum-for-automatic1111-webui" ["e"=1]
"hmartiro/riffusion-app" -> "devilismyfriend/StableTuner" ["e"=1]
"hmartiro/riffusion-app" -> "ashawkey/stable-dreamfusion" ["e"=1]
"hmartiro/riffusion-app" -> "lucidrains/audiolm-pytorch"
"hmartiro/riffusion-app" -> "magenta/music-spectrogram-diffusion"
"hmartiro/riffusion-app" -> "SHI-Labs/Versatile-Diffusion" ["e"=1]
"archinetai/audio-diffusion-pytorch-trainer" -> "archinetai/audio-data-pytorch"
"coqui-ai/STT" -> "coqui-ai/TTS"
"coqui-ai/STT" -> "coqui-ai/STT-examples"
"coqui-ai/STT" -> "speechbrain/speechbrain"
"coqui-ai/STT" -> "coqui-ai/open-speech-corpora"
"coqui-ai/STT" -> "alphacep/vosk-api"
"coqui-ai/STT" -> "mozilla/TTS"
"coqui-ai/STT" -> "TensorSpeech/TensorFlowASR"
"coqui-ai/STT" -> "pyannote/pyannote-audio"
"coqui-ai/STT" -> "snakers4/silero-vad"
"coqui-ai/STT" -> "SpeechColab/GigaSpeech"
"coqui-ai/STT" -> "syhw/wer_are_we"
"coqui-ai/STT" -> "lhotse-speech/lhotse"
"coqui-ai/STT" -> "snakers4/silero-models"
"coqui-ai/STT" -> "espnet/espnet"
"coqui-ai/STT" -> "mozilla/DeepSpeech"
"teticio/audio-diffusion" -> "archinetai/audio-diffusion-pytorch"
"teticio/audio-diffusion" -> "LAION-AI/audio-dataset"
"teticio/audio-diffusion" -> "Harmonai-org/sample-generator"
"teticio/audio-diffusion" -> "Kinyugo/msanii"
"teticio/audio-diffusion" -> "marcoppasini/musika"
"teticio/audio-diffusion" -> "lucidrains/audiolm-pytorch"
"teticio/audio-diffusion" -> "archinetai/audio-diffusion-pytorch-trainer"
"teticio/audio-diffusion" -> "LAION-AI/CLAP"
"teticio/audio-diffusion" -> "haoheliu/AudioLDM"
"teticio/audio-diffusion" -> "csteinmetz1/auraloss"
"teticio/audio-diffusion" -> "archinetai/audio-ai-timeline"
"teticio/audio-diffusion" -> "acids-ircam/RAVE"
"teticio/audio-diffusion" -> "yangdongchao/Text-to-sound-Synthesis"
"teticio/audio-diffusion" -> "mindslab-ai/nuwave2"
"teticio/audio-diffusion" -> "Natooz/MidiTok" ["e"=1]
"githubharald/CTCWordBeamSearch" -> "githubharald/CTCDecoder"
"githubharald/CTCWordBeamSearch" -> "parlance/ctcdecode"
"githubharald/CTCWordBeamSearch" -> "corticph/prefix-beam-search"
"githubharald/CTCWordBeamSearch" -> "githubharald/SimpleHTR"
"githubharald/CTCWordBeamSearch" -> "HawkAaron/warp-transducer"
"githubharald/CTCWordBeamSearch" -> "awni/transducer"
"githubharald/CTCWordBeamSearch" -> "YiwenShaoStephen/pychain"
"githubharald/CTCWordBeamSearch" -> "srvk/eesen"
"githubharald/CTCWordBeamSearch" -> "lamhoangtung/LineHTR"
"githubharald/CTCWordBeamSearch" -> "HawkAaron/RNN-Transducer"
"githubharald/CTCWordBeamSearch" -> "thu-spmi/CAT"
"githubharald/CTCWordBeamSearch" -> "kensho-technologies/pyctcdecode"
"githubharald/CTCWordBeamSearch" -> "HawkAaron/E2E-ASR"
"githubharald/CTCWordBeamSearch" -> "Sundy1219/ctc_beam_search_lm"
"githubharald/CTCWordBeamSearch" -> "hirofumi0810/neural_sp"
"isletennos/MMVC_Trainer" -> "isletennos/MMVC_Client"
"isletennos/MMVC_Trainer" -> "w-okada/voice-changer"
"isletennos/MMVC_Trainer" -> "VOICEVOX/voicevox_engine"
"isletennos/MMVC_Trainer" -> "mmorise/ita-corpus"
"isletennos/MMVC_Trainer" -> "liujing04/Retrieval-based-Voice-Conversion-WebUI"
"isletennos/MMVC_Trainer" -> "ddPn08/rvc-webui"
"isletennos/MMVC_Trainer" -> "VOICEVOX/voicevox"
"isletennos/MMVC_Trainer" -> "suzune25254649/bakusoku_aviutl_plugin" ["e"=1]
"isletennos/MMVC_Trainer" -> "VOICEVOX/voicevox_core"
"isletennos/MMVC_Trainer" -> "Hiroshiba/become-yukarin"
"isletennos/MMVC_Trainer" -> "Hiroshiba/voicevox"
"isletennos/MMVC_Trainer" -> "ksasao/TTSController"
"isletennos/MMVC_Trainer" -> "m-hayabusa/VRChat-Exif-Writer"
"isletennos/MMVC_Trainer" -> "malaybaku/VMagicMirror" ["e"=1]
"isletennos/MMVC_Trainer" -> "sh-akira/VirtualMotionCapture" ["e"=1]
"letiantian/Pinyin2Hanzi" -> "letiantian/ChineseTone"
"letiantian/Pinyin2Hanzi" -> "LiuRoy/Pinyin_Demo"
"letiantian/Pinyin2Hanzi" -> "crownpku/Somiao-Pinyin" ["e"=1]
"letiantian/Pinyin2Hanzi" -> "liuhuanyong/Pinyin2Chinese"
"letiantian/Pinyin2Hanzi" -> "mozillazg/python-pinyin" ["e"=1]
"letiantian/Pinyin2Hanzi" -> "shibing624/pycorrector" ["e"=1]
"letiantian/Pinyin2Hanzi" -> "wdimmy/Automatic-Corpus-Generation" ["e"=1]
"letiantian/Pinyin2Hanzi" -> "mozillazg/pinyin-data"
"letiantian/Pinyin2Hanzi" -> "Kyubyong/neural_chinese_transliterator" ["e"=1]
"letiantian/Pinyin2Hanzi" -> "bojone/word-discovery" ["e"=1]
"letiantian/Pinyin2Hanzi" -> "taiqing/pinyin2hanzi" ["e"=1]
"letiantian/Pinyin2Hanzi" -> "sunpinyin/sunpinyin" ["e"=1]
"letiantian/Pinyin2Hanzi" -> "ccheng16/correction" ["e"=1]
"AndreyGuzhov/AudioCLIP" -> "descriptinc/lyrebird-wav2clip"
"AndreyGuzhov/AudioCLIP" -> "LAION-AI/CLAP"
"AndreyGuzhov/AudioCLIP" -> "LAION-AI/audio-dataset"
"AndreyGuzhov/AudioCLIP" -> "microsoft/CLAP"
"AndreyGuzhov/AudioCLIP" -> "ArrowLuo/CLIP4Clip" ["e"=1]
"AndreyGuzhov/AudioCLIP" -> "asteroid-team/torch-audiomentations"
"AndreyGuzhov/AudioCLIP" -> "RetroCirce/HTS-Audio-Transformer"
"AndreyGuzhov/AudioCLIP" -> "YuanGongND/ast"
"AndreyGuzhov/AudioCLIP" -> "facebookresearch/AudioMAE"
"AndreyGuzhov/AudioCLIP" -> "GeWu-Lab/awesome-audiovisual-learning" ["e"=1]
"AndreyGuzhov/AudioCLIP" -> "v-iashin/SpecVQGAN"
"AndreyGuzhov/AudioCLIP" -> "AndreyGuzhov/ESResNeXt-fbsp"
"AndreyGuzhov/AudioCLIP" -> "caillonantoine/RAVE"
"AndreyGuzhov/AudioCLIP" -> "lucidrains/audiolm-pytorch"
"AndreyGuzhov/AudioCLIP" -> "facebookresearch/SLIP" ["e"=1]
"israelg99/deepvoice" -> "Kyubyong/deepvoice3"
"israelg99/deepvoice" -> "sotelo/parrot"
"israelg99/deepvoice" -> "baidu-research/deep-voice"
"israelg99/deepvoice" -> "soroushmehr/sampleRNN_ICLR2017"
"thuhcsi/Crystal" -> "Jackiexiao/MTTS"
"thuhcsi/Crystal" -> "kakaobrain/g2pM"
"thuhcsi/Crystal" -> "BoragoCode/AttentionBasedProsodyPrediction"
"thuhcsi/Crystal" -> "thuhcsi/Crystal.TTVS"
"thuhcsi/Crystal" -> "Zeqiang-Lai/Prosody_Prediction"
"thuhcsi/Crystal" -> "Liu-Feng-deeplearning/TTS-frontend"
"thuhcsi/Crystal" -> "xcmyz/FastVocoder"
"sony/ai-research-code" -> "ws-choi/Conditioned-Source-Separation-LaSAFT"
"sony/ai-research-code" -> "Wendison/VQMIVC"
"sony/ai-research-code" -> "tky823/DNN-based_source_separation" ["e"=1]
"sony/ai-research-code" -> "liusongxiang/ppg-vc"
"sony/ai-research-code" -> "kuielab/mdx-net"
"sony/ai-research-code" -> "nussl/nussl" ["e"=1]
"sony/ai-research-code" -> "OlaWod/FreeVC"
"bigbrother666sh/shezhangbujianle" -> "Shawn-Inspur/Yuan-1.0"
"bigbrother666sh/shezhangbujianle" -> "bigbrother666sh/Awada"
"bigbrother666sh/shezhangbujianle" -> "Turing-Project/AntiFraudChatBot"
"bigbrother666sh/shezhangbujianle" -> "thu-coai/EVA" ["e"=1]
"bigbrother666sh/shezhangbujianle" -> "yangjianxin1/CPM" ["e"=1]
"bigbrother666sh/shezhangbujianle" -> "yangjianxin1/Firefly" ["e"=1]
"bigbrother666sh/shezhangbujianle" -> "esbatmop/MNBVC" ["e"=1]
"bigbrother666sh/shezhangbujianle" -> "wechaty/python-wechaty" ["e"=1]
"acids-ircam/RAVE" -> "acids-ircam/nn_tilde"
"acids-ircam/RAVE" -> "acids-ircam/rave_vst"
"acids-ircam/RAVE" -> "QosmoInc/neutone_sdk"
"acids-ircam/RAVE" -> "archinetai/audio-diffusion-pytorch"
"acids-ircam/RAVE" -> "Harmonai-org/sample-generator"
"acids-ircam/RAVE" -> "Torsion-Audio/Scyclone"
"acids-ircam/RAVE" -> "acids-ircam/ddsp_pytorch"
"acids-ircam/RAVE" -> "marcoppasini/musika"
"acids-ircam/RAVE" -> "archinetai/audio-ai-timeline"
"acids-ircam/RAVE" -> "teticio/audio-diffusion"
"acids-ircam/RAVE" -> "magenta/ddsp-vst"
"acids-ircam/RAVE" -> "magenta/ddsp" ["e"=1]
"acids-ircam/RAVE" -> "csteinmetz1/auraloss"
"acids-ircam/RAVE" -> "adobe-research/DeepAFx-ST"
"acids-ircam/RAVE" -> "jatinchowdhury18/RTNeural" ["e"=1]
"MubertAI/Mubert-Text-to-Music" -> "lkwq007/stablediffusion-infinity" ["e"=1]
"MubertAI/Mubert-Text-to-Music" -> "lucidrains/musiclm-pytorch"
"MubertAI/Mubert-Text-to-Music" -> "marcoppasini/musika"
"MubertAI/Mubert-Text-to-Music" -> "archinetai/audio-diffusion-pytorch"
"MubertAI/Mubert-Text-to-Music" -> "archinetai/audio-ai-timeline"
"MubertAI/Mubert-Text-to-Music" -> "riffusion/riffusion"
"MubertAI/Mubert-Text-to-Music" -> "nateraw/stable-diffusion-videos" ["e"=1]
"MubertAI/Mubert-Text-to-Music" -> "ashawkey/stable-dreamfusion" ["e"=1]
"MubertAI/Mubert-Text-to-Music" -> "microsoft/muzic"
"MubertAI/Mubert-Text-to-Music" -> "haoheliu/AudioLDM"
"MubertAI/Mubert-Text-to-Music" -> "Harmonai-org/sample-generator"
"MubertAI/Mubert-Text-to-Music" -> "XavierXiao/Dreambooth-Stable-Diffusion" ["e"=1]
"MubertAI/Mubert-Text-to-Music" -> "rinongal/textual_inversion" ["e"=1]
"MubertAI/Mubert-Text-to-Music" -> "lucidrains/audiolm-pytorch"
"MubertAI/Mubert-Text-to-Music" -> "ShivamShrirao/diffusers" ["e"=1]
"libai3/masr" -> "audier/DeepSpeechRecognition"
"libai3/masr" -> "xxbb1234021/speech_recognition"
"libai3/masr" -> "PaddlePaddle/DeepSpeech"
"libai3/masr" -> "sailist/ASRFrame"
"libai3/masr" -> "nl8590687/ASRT_SpeechRecognition"
"libai3/masr" -> "kaituoxu/Speech-Transformer"
"libai3/masr" -> "parlance/ctcdecode"
"libai3/masr" -> "chineseocr/darknet-ocr" ["e"=1]
"libai3/masr" -> "SeanNaren/deepspeech.pytorch"
"libai3/masr" -> "zw76859420/ASR_Theory"
"libai3/masr" -> "chineseocr/table-ocr" ["e"=1]
"libai3/masr" -> "Sierkinhane/crnn_chinese_characters_rec" ["e"=1]
"libai3/masr" -> "WenmuZhou/PSENet.pytorch" ["e"=1]
"libai3/masr" -> "mravanelli/pytorch-kaldi"
"libai3/masr" -> "daixiang789/tensorflow-examples"
"mozilla/DeepSpeech-examples" -> "mozilla/androidspeech"
"mozilla/DeepSpeech-examples" -> "rolczynski/Automatic-Speech-Recognition"
"mozilla/DeepSpeech-examples" -> "PaddlePaddle/DeepSpeech"
"mozilla/DeepSpeech-examples" -> "mozilla/DeepSpeech"
"mozilla/DeepSpeech-examples" -> "AASHISHAG/deepspeech-german"
"mozilla/DeepSpeech-examples" -> "mozilla/DSAlign"
"mozilla/DeepSpeech-examples" -> "coqui-ai/STT"
"mozilla/DeepSpeech-examples" -> "MainRo/deepspeech-server"
"mozilla/DeepSpeech-examples" -> "kpu/kenlm"
"mozilla/DeepSpeech-examples" -> "SeanNaren/deepspeech.pytorch"
"mozilla/DeepSpeech-examples" -> "tarekeldeeb/DeepSpeech-Quran"
"mozilla/DeepSpeech-examples" -> "alphacep/vosk-server"
"mozilla/DeepSpeech-examples" -> "AIWintermuteAI/DeepSpeech_RaspberryPi4_Hotword"
"mozilla/DeepSpeech-examples" -> "amsehili/auditok"
"mozilla/DeepSpeech-examples" -> "DanBmh/deepspeech-german"
"pkhungurn/talking-head-anime-3-demo" -> "pkhungurn/talking-head-anime-2-demo"
"pkhungurn/talking-head-anime-3-demo" -> "harlanhong/awesome-talking-head-generation" ["e"=1]
"pkhungurn/talking-head-anime-3-demo" -> "transpchan/Live3D-v2" ["e"=1]
"pkhungurn/talking-head-anime-3-demo" -> "mallorbc/whisper_mic" ["e"=1]
"pkhungurn/talking-head-anime-3-demo" -> "fishslot/video_loopback_for_webui" ["e"=1]
"pkhungurn/talking-head-anime-3-demo" -> "hysts/anime-face-detector" ["e"=1]
"pkhungurn/talking-head-anime-3-demo" -> "megvii-research/CoNR" ["e"=1]
"pkhungurn/talking-head-anime-3-demo" -> "pkhungurn/talking-head-anime-demo"
"pkhungurn/talking-head-anime-3-demo" -> "uezo/ChatdollKit"
"pkhungurn/talking-head-anime-3-demo" -> "SkyTNT/anime-segmentation" ["e"=1]
"pkhungurn/talking-head-anime-3-demo" -> "GunwooHan/EasyVtuber"
"pkhungurn/talking-head-anime-3-demo" -> "mattyamonaca/layerdivider" ["e"=1]
"pkhungurn/talking-head-anime-3-demo" -> "ShuhongChen/panic3d-anime-reconstruction" ["e"=1]
"pkhungurn/talking-head-anime-3-demo" -> "unity3d-jp/AnimeToolbox" ["e"=1]
"pkhungurn/talking-head-anime-3-demo" -> "isletennos/MMVC_Trainer"
"Harmonai-org/sample-generator" -> "archinetai/audio-diffusion-pytorch"
"Harmonai-org/sample-generator" -> "teticio/audio-diffusion"
"Harmonai-org/sample-generator" -> "marcoppasini/musika"
"Harmonai-org/sample-generator" -> "acids-ircam/RAVE"
"Harmonai-org/sample-generator" -> "sudosilico/sample-diffusion"
"Harmonai-org/sample-generator" -> "archinetai/audio-ai-timeline"
"Harmonai-org/sample-generator" -> "zqevans/audio-diffusion"
"Harmonai-org/sample-generator" -> "archinetai/audio-diffusion-pytorch-trainer"
"Harmonai-org/sample-generator" -> "haoheliu/AudioLDM"
"Harmonai-org/sample-generator" -> "lucidrains/audiolm-pytorch"
"Harmonai-org/sample-generator" -> "deforum/stable-diffusion" ["e"=1]
"Harmonai-org/sample-generator" -> "crowsonkb/k-diffusion" ["e"=1]
"Harmonai-org/sample-generator" -> "samim23/polymath"
"Harmonai-org/sample-generator" -> "magenta/music-spectrogram-diffusion"
"Harmonai-org/sample-generator" -> "LAION-AI/CLAP"
"NVIDIA/flowtron" -> "NVIDIA/mellotron"
"NVIDIA/flowtron" -> "jaywalnut310/glow-tts"
"NVIDIA/flowtron" -> "xcmyz/speech-synthesis-paper"
"NVIDIA/flowtron" -> "NVIDIA/waveglow"
"NVIDIA/flowtron" -> "jik876/hifi-gan"
"NVIDIA/flowtron" -> "kan-bayashi/ParallelWaveGAN"
"NVIDIA/flowtron" -> "mindslab-ai/cotatron"
"NVIDIA/flowtron" -> "ming024/FastSpeech2"
"NVIDIA/flowtron" -> "nii-yamagishilab/multi-speaker-tacotron"
"NVIDIA/flowtron" -> "as-ideas/ForwardTacotron"
"NVIDIA/flowtron" -> "xcmyz/FastSpeech"
"NVIDIA/flowtron" -> "seungwonpark/melgan"
"NVIDIA/flowtron" -> "descriptinc/melgan-neurips"
"NVIDIA/flowtron" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"NVIDIA/flowtron" -> "NVIDIA/radtts"
"huseinzol05/malay-dataset" -> "huseinzol05/malaya"
"pannous/tensorflow-speech-recognition" -> "zzw922cn/Automatic_Speech_Recognition"
"pannous/tensorflow-speech-recognition" -> "buriburisuri/speech-to-text-wavenet"
"pannous/tensorflow-speech-recognition" -> "pannous/caffe-speech-recognition"
"pannous/tensorflow-speech-recognition" -> "llSourcell/tensorflow_speech_recognition_demo"
"pannous/tensorflow-speech-recognition" -> "zzw922cn/awesome-speech-recognition-speech-synthesis-papers"
"pannous/tensorflow-speech-recognition" -> "SeanNaren/deepspeech.pytorch"
"pannous/tensorflow-speech-recognition" -> "jameslyons/python_speech_features"
"pannous/tensorflow-speech-recognition" -> "ibab/tensorflow-wavenet"
"pannous/tensorflow-speech-recognition" -> "Uberi/speech_recognition"
"pannous/tensorflow-speech-recognition" -> "baidu-research/warp-ctc" ["e"=1]
"pannous/tensorflow-speech-recognition" -> "ppwwyyxx/speaker-recognition"
"pannous/tensorflow-speech-recognition" -> "srvk/eesen"
"pannous/tensorflow-speech-recognition" -> "mozilla/DeepSpeech"
"pannous/tensorflow-speech-recognition" -> "baidu-research/ba-dls-deepspeech"
"pannous/tensorflow-speech-recognition" -> "hirofumi0810/tensorflow_end2end_speech_recognition"
"chrisdonahue/sheetsage" -> "cheriell/PM2S"
"interactiveaudiolab/penn" -> "maxrmorrison/torchcrepe"
"interactiveaudiolab/penn" -> "brentspell/torch-yin"
"TaylorSMarks/playsound" -> "hamiltron/py-simple-audio"
"TaylorSMarks/playsound" -> "spatialaudio/python-sounddevice" ["e"=1]
"TaylorSMarks/playsound" -> "jleb/pyaudio" ["e"=1]
"pettarin/forced-alignment-tools" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"pettarin/forced-alignment-tools" -> "prosodylab/Prosodylab-Aligner"
"pettarin/forced-alignment-tools" -> "lowerquality/gentle"
"pettarin/forced-alignment-tools" -> "readbeyond/aeneas"
"pettarin/forced-alignment-tools" -> "mozilla/DSAlign"
"pettarin/forced-alignment-tools" -> "Kyubyong/g2p"
"pettarin/forced-alignment-tools" -> "open-speech/speech-aligner"
"pettarin/forced-alignment-tools" -> "cmusphinx/g2p-seq2seq"
"pettarin/forced-alignment-tools" -> "xcmyz/FastSpeech"
"pettarin/forced-alignment-tools" -> "mozilla/LPCNet"
"pettarin/forced-alignment-tools" -> "JRMeyer/open-speech-corpora"
"pettarin/forced-alignment-tools" -> "JeremyCCHsu/Python-Wrapper-for-World-Vocoder"
"pettarin/forced-alignment-tools" -> "kan-bayashi/ParallelWaveGAN"
"pettarin/forced-alignment-tools" -> "google/REAPER"
"pettarin/forced-alignment-tools" -> "pykaldi/pykaldi"
"open-speech/speech-aligner" -> "aishell-foundation/DaCiDian"
"open-speech/speech-aligner" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"open-speech/speech-aligner" -> "Kyubyong/g2pC"
"open-speech/speech-aligner" -> "speechio/BigCiDian"
"open-speech/speech-aligner" -> "Jackiexiao/MTTS"
"open-speech/speech-aligner" -> "kakaobrain/g2pM"
"open-speech/speech-aligner" -> "thuhcsi/Crystal"
"open-speech/speech-aligner" -> "XiaoMi/kaldi-onnx"
"open-speech/speech-aligner" -> "KuangDD/zhvoice"
"open-speech/speech-aligner" -> "thu-spmi/CAT"
"open-speech/speech-aligner" -> "KuangDD/phkit"
"open-speech/speech-aligner" -> "speechio/chinese_text_normalization"
"open-speech/speech-aligner" -> "xcmyz/FastSpeech"
"open-speech/speech-aligner" -> "Kyubyong/g2p"
"open-speech/speech-aligner" -> "hirofumi0810/neural_sp"
"cmusphinx/pocketsphinx-android-demo" -> "cmusphinx/pocketsphinx-android"
"cmusphinx/pocketsphinx-android-demo" -> "cmusphinx/pocketsphinx"
"cmusphinx/pocketsphinx-android-demo" -> "cmusphinx/sphinxbase"
"cmusphinx/pocketsphinx-android-demo" -> "cmusphinx/pocketsphinx-ios-demo"
"cmusphinx/pocketsphinx-android-demo" -> "cmusphinx/sphinx4"
"cmusphinx/pocketsphinx-android-demo" -> "galrom/ContinuesVoiceRecognition"
"cmusphinx/pocketsphinx-android-demo" -> "cmusphinx/sphinxtrain"
"cmusphinx/pocketsphinx-android-demo" -> "cmusphinx/pocketsphinx-python"
"cmusphinx/pocketsphinx-android-demo" -> "cmusphinx/cmudict"
"cmusphinx/pocketsphinx-android-demo" -> "vikramezhil/DroidSpeech" ["e"=1]
"cmusphinx/pocketsphinx-android-demo" -> "cmusphinx/cmudict-tools"
"cmusphinx/pocketsphinx-android-demo" -> "alphacep/vosk-android-demo"
"PlayVoice/so-vits-svc" -> "PlayVoice/lora-svc"
"PlayVoice/so-vits-svc" -> "SUC-DriverOld/so-vits-svc-Chinese-Detaild-Documents"
"PlayVoice/so-vits-svc" -> "innnky/emotional-vits"
"PlayVoice/so-vits-svc" -> "bshall/soft-vc"
"PlayVoice/so-vits-svc" -> "StarStringStudio/so-vits-svc"
"dmort27/epitran" -> "dmort27/panphon"
"dmort27/epitran" -> "xinjli/allosaurus"
"dmort27/epitran" -> "bootphon/phonemizer"
"dmort27/epitran" -> "xinjli/transphone"
"dmort27/epitran" -> "kylebgorman/wikipron"
"dmort27/epitran" -> "Kyubyong/g2p"
"dmort27/epitran" -> "Kyubyong/css10"
"dmort27/epitran" -> "festvox/datasets-CMU_Wilderness"
"dmort27/epitran" -> "pettarin/ipapy"
"dmort27/epitran" -> "lingjzhu/CharsiuG2P"
"dmort27/epitran" -> "open-dict-data/ipa-dict"
"dmort27/epitran" -> "cmusphinx/g2p-seq2seq"
"dmort27/epitran" -> "xcmyz/FastSpeech"
"dmort27/epitran" -> "as-ideas/DeepPhonemizer"
"dmort27/epitran" -> "dmort27/allovera"
"igormq/ctc_tensorflow_example" -> "jonrein/tensorflow_CTC_example"
"igormq/ctc_tensorflow_example" -> "philipperemy/tensorflow-ctc-speech-recognition"
"igormq/ctc_tensorflow_example" -> "hirofumi0810/tensorflow_end2end_speech_recognition"
"igormq/ctc_tensorflow_example" -> "synckey/tensorflow_lstm_ctc_ocr" ["e"=1]
"igormq/ctc_tensorflow_example" -> "amaas/stanford-ctc"
"igormq/ctc_tensorflow_example" -> "igormq/asr-study"
"igormq/ctc_tensorflow_example" -> "awni/speech"
"igormq/ctc_tensorflow_example" -> "mindorii/kws" ["e"=1]
"igormq/ctc_tensorflow_example" -> "vrenkens/tfkaldi"
"igormq/ctc_tensorflow_example" -> "srvk/eesen"
"igormq/ctc_tensorflow_example" -> "vesis84/kaldi-io-for-python"
"igormq/ctc_tensorflow_example" -> "lingochamp/kaldi-ctc"
"igormq/ctc_tensorflow_example" -> "WindQAQ/listen-attend-and-spell"
"igormq/ctc_tensorflow_example" -> "yajiemiao/eesen"
"igormq/ctc_tensorflow_example" -> "baidu-research/ba-dls-deepspeech"
"mphilli/English-to-IPA" -> "aparrish/pronouncingpy"
"mphilli/English-to-IPA" -> "open-dict-data/ipa-dict"
"marcoppasini/musika" -> "magenta/music-spectrogram-diffusion"
"marcoppasini/musika" -> "teticio/audio-diffusion"
"marcoppasini/musika" -> "archinetai/audio-diffusion-pytorch"
"marcoppasini/musika" -> "csteinmetz1/auraloss"
"marcoppasini/musika" -> "YatingMusic/ddsp-singing-vocoders"
"marcoppasini/musika" -> "Harmonai-org/sample-generator"
"marcoppasini/musika" -> "acids-ircam/RAVE"
"marcoppasini/musika" -> "magenta/midi-ddsp"
"marcoppasini/musika" -> "Natooz/MidiTok" ["e"=1]
"marcoppasini/musika" -> "archinetai/audio-diffusion-pytorch-trainer"
"marcoppasini/musika" -> "acids-ircam/ddsp_pytorch"
"marcoppasini/musika" -> "archinetai/audio-ai-timeline"
"marcoppasini/musika" -> "adobe-research/DeepAFx-ST"
"marcoppasini/musika" -> "lucidrains/audiolm-pytorch"
"marcoppasini/musika" -> "sweetcocoa/pop2piano"
"wenet-e2e/speech-synthesis-paper" -> "guan-yuan/Awesome-Singing-Voice-Synthesis-and-Singing-Voice-Conversion"
"wenet-e2e/speech-synthesis-paper" -> "SJTMusicTeam/Muskits"
"wenet-e2e/speech-synthesis-paper" -> "coqui-ai/TTS-papers"
"wenet-e2e/speech-synthesis-paper" -> "Rongjiehuang/ProDiff"
"wenet-e2e/speech-synthesis-paper" -> "Rongjiehuang/GenerSpeech"
"wenet-e2e/speech-synthesis-paper" -> "huawei-noah/Speech-Backbones"
"wenet-e2e/speech-synthesis-paper" -> "tts-tutorial/survey"
"wenet-e2e/speech-synthesis-paper" -> "DigitalPhonetics/IMS-Toucan"
"wenet-e2e/speech-synthesis-paper" -> "ddlBoJack/Speech-Resources"
"wenet-e2e/speech-synthesis-paper" -> "keonlee9420/Comprehensive-Transformer-TTS"
"wenet-e2e/speech-synthesis-paper" -> "yl4579/StarGANv2-VC"
"wenet-e2e/speech-synthesis-paper" -> "wenet-e2e/wetts"
"wenet-e2e/speech-synthesis-paper" -> "wenet-e2e/speech-recognition-papers"
"wenet-e2e/speech-synthesis-paper" -> "WelkinYang/Learn2Sing2.0"
"wenet-e2e/speech-synthesis-paper" -> "heatz123/naturalspeech"
"k2-fsa/sherpa" -> "k2-fsa/icefall"
"k2-fsa/sherpa" -> "k2-fsa/sherpa-onnx"
"k2-fsa/sherpa" -> "k2-fsa/sherpa-ncnn"
"k2-fsa/sherpa" -> "k2-fsa/snowfall"
"k2-fsa/sherpa" -> "k2-fsa/k2"
"k2-fsa/sherpa" -> "lhotse-speech/lhotse"
"k2-fsa/sherpa" -> "csukuangfj/kaldifeat"
"k2-fsa/sherpa" -> "danpovey/fast_rnnt"
"k2-fsa/sherpa" -> "k2-fsa/multi_quantization"
"k2-fsa/sherpa" -> "wenet-e2e/wekws" ["e"=1]
"k2-fsa/sherpa" -> "tencent-ailab/pika"
"k2-fsa/sherpa-ncnn" -> "k2-fsa/sherpa-onnx"
"k2-fsa/sherpa-ncnn" -> "k2-fsa/sherpa"
"k2-fsa/sherpa-ncnn" -> "k2-fsa/icefall"
"k2-fsa/sherpa-ncnn" -> "chenkui164/FastASR"
"k2-fsa/sherpa-ncnn" -> "RapidAI/RapidASR"
"k2-fsa/sherpa-ncnn" -> "k2-fsa/snowfall"
"k2-fsa/sherpa-ncnn" -> "csukuangfj/kaldifeat"
"k2-fsa/sherpa-ncnn" -> "k2-fsa/k2"
"k2-fsa/sherpa-ncnn" -> "lhotse-speech/lhotse"
"k2-fsa/sherpa-ncnn" -> "FeiGeChuanShu/GFPGAN-ncnn"
"k2-fsa/sherpa-ncnn" -> "FeiGeChuanShu/ncnn_paddleocr" ["e"=1]
"k2-fsa/sherpa-ncnn" -> "alibaba-damo-academy/FunASR"
"k2-fsa/sherpa-ncnn" -> "pnnx/pnnx" ["e"=1]
"k2-fsa/sherpa-onnx" -> "k2-fsa/sherpa"
"k2-fsa/sherpa-onnx" -> "k2-fsa/sherpa-ncnn"
"k2-fsa/sherpa-onnx" -> "k2-fsa/icefall"
"k2-fsa/sherpa-onnx" -> "k2-fsa/multi_quantization"
"tramphero/kaldi" -> "alibaba/Alibaba-MIT-Speech"
"tramphero/kaldi" -> "lingochamp/kaldi-ctc"
"tramphero/kaldi" -> "audier/DeepSpeechRecognition"
"tramphero/kaldi" -> "srvk/eesen"
"tramphero/kaldi" -> "HawkAaron/warp-transducer"
"tramphero/kaldi" -> "xxbb1234021/speech_recognition"
"tramphero/kaldi" -> "k2-fsa/k2"
"tramphero/kaldi" -> "PaddlePaddle/DeepSpeech"
"tramphero/kaldi" -> "HawkAaron/RNN-Transducer"
"tramphero/kaldi" -> "facebookresearch/wav2letter"
"tramphero/kaldi" -> "kaituoxu/Speech-Transformer"
"tramphero/kaldi" -> "aishell-foundation/DaCiDian"
"tramphero/kaldi" -> "zw76859420/ASR_Syllable"
"tramphero/kaldi" -> "sailist/ASRFrame"
"tramphero/kaldi" -> "athena-team/athena" ["e"=1]
"guan-yuan/Awesome-Singing-Voice-Synthesis-and-Singing-Voice-Conversion" -> "PlayVoice/VI-SVC"
"guan-yuan/Awesome-Singing-Voice-Synthesis-and-Singing-Voice-Conversion" -> "WelkinYang/Learn2Sing2.0"
"guan-yuan/Awesome-Singing-Voice-Synthesis-and-Singing-Voice-Conversion" -> "M4Singer/M4Singer"
"guan-yuan/Awesome-Singing-Voice-Synthesis-and-Singing-Voice-Conversion" -> "nnsvs/nnsvs"
"guan-yuan/Awesome-Singing-Voice-Synthesis-and-Singing-Voice-Conversion" -> "SJTMusicTeam/Muskits"
"guan-yuan/Awesome-Singing-Voice-Synthesis-and-Singing-Voice-Conversion" -> "yl4579/PitchExtractor"
"guan-yuan/Awesome-Singing-Voice-Synthesis-and-Singing-Voice-Conversion" -> "YatingMusic/ddsp-singing-vocoders"
"guan-yuan/Awesome-Singing-Voice-Synthesis-and-Singing-Voice-Conversion" -> "Rongjiehuang/ProDiff"
"guan-yuan/Awesome-Singing-Voice-Synthesis-and-Singing-Voice-Conversion" -> "Rongjiehuang/FastDiff"
"guan-yuan/Awesome-Singing-Voice-Synthesis-and-Singing-Voice-Conversion" -> "chomeyama/SiFiGAN"
"guan-yuan/Awesome-Singing-Voice-Synthesis-and-Singing-Voice-Conversion" -> "SongRongLee/mir-svc"
"guan-yuan/Awesome-Singing-Voice-Synthesis-and-Singing-Voice-Conversion" -> "dhchoi99/NANSY"
"guan-yuan/Awesome-Singing-Voice-Synthesis-and-Singing-Voice-Conversion" -> "rishikksh20/Avocodo-pytorch"
"guan-yuan/Awesome-Singing-Voice-Synthesis-and-Singing-Voice-Conversion" -> "Rongjiehuang/Multi-Singer"
"s3prl/s3prl" -> "kan-bayashi/ParallelWaveGAN"
"s3prl/s3prl" -> "speechbrain/speechbrain"
"s3prl/s3prl" -> "jik876/hifi-gan"
"s3prl/s3prl" -> "espnet/espnet"
"s3prl/s3prl" -> "microsoft/UniSpeech"
"s3prl/s3prl" -> "facebookresearch/WavAugment"
"s3prl/s3prl" -> "ming024/FastSpeech2"
"s3prl/s3prl" -> "lhotse-speech/lhotse"
"s3prl/s3prl" -> "clovaai/voxceleb_trainer"
"s3prl/s3prl" -> "wenet-e2e/wenet"
"s3prl/s3prl" -> "k2-fsa/k2"
"s3prl/s3prl" -> "SpeechColab/GigaSpeech"
"s3prl/s3prl" -> "microsoft/NeuralSpeech"
"s3prl/s3prl" -> "aliutkus/speechmetrics" ["e"=1]
"s3prl/s3prl" -> "hirofumi0810/neural_sp"
"hueitan/javascript-sdk-design" -> "qiniu/js-sdk"
"hueitan/javascript-sdk-design" -> "sarbbottam/write-an-open-source-js-lib"
"hueitan/javascript-sdk-design" -> "kuitos/import-html-entry" ["e"=1]
"hueitan/javascript-sdk-design" -> "seriousben/embeddable-react-widget" ["e"=1]
"hueitan/javascript-sdk-design" -> "umijs/father" ["e"=1]
"hueitan/javascript-sdk-design" -> "kentcdodds/react-jest-workshop"
"hueitan/javascript-sdk-design" -> "mjavascript/mastering-modular-javascript" ["e"=1]
"w-okada/voice-changer" -> "liujing04/Retrieval-based-Voice-Conversion-WebUI"
"w-okada/voice-changer" -> "34j/so-vits-svc-fork"
"w-okada/voice-changer" -> "isletennos/MMVC_Trainer"
"w-okada/voice-changer" -> "ddPn08/rvc-webui"
"w-okada/voice-changer" -> "yxlllc/DDSP-SVC"
"w-okada/voice-changer" -> "yantaisa11/Retrieval-based-Voice-Conversion-WebUI-JP-localization"
"w-okada/voice-changer" -> "zhaohui8969/VST_NetProcess-"
"w-okada/voice-changer" -> "svc-develop-team/so-vits-svc"
"w-okada/voice-changer" -> "PlayVoice/so-vits-svc-5.0"
"w-okada/voice-changer" -> "uezo/ChatdollKit"
"w-okada/voice-changer" -> "PlayVoice/lora-svc"
"w-okada/voice-changer" -> "isletennos/MMVC_Client"
"w-okada/voice-changer" -> "fishaudio/fish-diffusion"
"w-okada/voice-changer" -> "Plachtaa/VITS-fast-fine-tuning"
"w-okada/voice-changer" -> "mmorise/ita-corpus"
"SayaSS/vits-finetuning" -> "AlexandaJerry/whisper-vits-japanese"
"SayaSS/vits-finetuning" -> "Plachtaa/VITS-fast-fine-tuning"
"SayaSS/vits-finetuning" -> "CjangCjengh/vits"
"SayaSS/vits-finetuning" -> "yxlllc/DDSP-SVC"
"SayaSS/vits-finetuning" -> "innnky/emotional-vits"
"SayaSS/vits-finetuning" -> "kslz/sound_dataset_tools2"
"SayaSS/vits-finetuning" -> "AlexandaJerry/vits-mandarin-biaobei"
"SayaSS/vits-finetuning" -> "CjangCjengh/MoeGoe_GUI"
"SayaSS/vits-finetuning" -> "rotten-work/vits-mandarin-windows"
"SayaSS/vits-finetuning" -> "NaruseMioShirakana/MoeSS"
"SayaSS/vits-finetuning" -> "PlayVoice/vits_chinese"
"SayaSS/vits-finetuning" -> "Paraworks/vits_with_chatgpt-gpt3"
"SayaSS/vits-finetuning" -> "CjangCjengh/TTSModels"
"SayaSS/vits-finetuning" -> "anonymous-pits/pits"
"SayaSS/vits-finetuning" -> "cjyaddone/ChatWaifu"
"liusongxiang/Large-Audio-Models" -> "hhguo/MSMC-TTS"
"liusongxiang/Large-Audio-Models" -> "mindslab-ai/phaseaug"
"liusongxiang/Large-Audio-Models" -> "Rongjiehuang/GenerSpeech"
"liusongxiang/Large-Audio-Models" -> "keonlee9420/DailyTalk"
"liusongxiang/Large-Audio-Models" -> "ncsoft/avocodo"
"microsoft/NeuralSpeech" -> "NVIDIA/BigVGAN"
"microsoft/NeuralSpeech" -> "jik876/hifi-gan"
"microsoft/NeuralSpeech" -> "ming024/FastSpeech2"
"microsoft/NeuralSpeech" -> "huawei-noah/Speech-Backbones"
"microsoft/NeuralSpeech" -> "jaywalnut310/glow-tts"
"microsoft/NeuralSpeech" -> "kan-bayashi/ParallelWaveGAN"
"microsoft/NeuralSpeech" -> "keonlee9420/Comprehensive-Transformer-TTS"
"microsoft/NeuralSpeech" -> "tts-tutorial/survey"
"microsoft/NeuralSpeech" -> "speechio/chinese_text_normalization"
"microsoft/NeuralSpeech" -> "NATSpeech/NATSpeech" ["e"=1]
"microsoft/NeuralSpeech" -> "KevinMIN95/StyleSpeech"
"microsoft/NeuralSpeech" -> "TencentGameMate/chinese_speech_pretrain"
"microsoft/NeuralSpeech" -> "lucidrains/audiolm-pytorch"
"microsoft/NeuralSpeech" -> "Rongjiehuang/ProDiff"
"microsoft/NeuralSpeech" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"google/uis-rnn" -> "wq2012/awesome-diarization"
"google/uis-rnn" -> "taylorlu/Speaker-Diarization"
"google/uis-rnn" -> "wq2012/SpectralCluster"
"google/uis-rnn" -> "HarryVolek/PyTorch_Speaker_Verification"
"google/uis-rnn" -> "pyannote/pyannote-audio"
"google/uis-rnn" -> "hitachi-speech/EEND"
"google/uis-rnn" -> "wiseman/py-webrtcvad"
"google/uis-rnn" -> "mravanelli/pytorch-kaldi"
"google/uis-rnn" -> "mravanelli/SincNet"
"google/uis-rnn" -> "philipperemy/deep-speaker"
"google/uis-rnn" -> "mindslab-ai/voicefilter" ["e"=1]
"google/uis-rnn" -> "Janghyun1230/Speaker_Verification"
"google/uis-rnn" -> "pykaldi/pykaldi"
"google/uis-rnn" -> "Snowdar/asv-subtools"
"google/uis-rnn" -> "WeidiXie/VGG-Speaker-Recognition"
"juanmc2005/StreamingSpeakerDiarization" -> "BUTSpeechFIT/VBx"
"juanmc2005/StreamingSpeakerDiarization" -> "hitachi-speech/EEND"
"juanmc2005/StreamingSpeakerDiarization" -> "nryant/dscore"
"juanmc2005/StreamingSpeakerDiarization" -> "wq2012/SpectralCluster"
"juanmc2005/StreamingSpeakerDiarization" -> "Xflick/EEND_PyTorch"
"juanmc2005/StreamingSpeakerDiarization" -> "desh2608/dover-lap"
"juanmc2005/StreamingSpeakerDiarization" -> "cvqluu/simple_diarizer"
"juanmc2005/StreamingSpeakerDiarization" -> "juanmc2005/rttm-viewer"
"juanmc2005/StreamingSpeakerDiarization" -> "nttcslab-sp/EEND-vector-clustering"
"juanmc2005/StreamingSpeakerDiarization" -> "wq2012/awesome-diarization"
"juanmc2005/StreamingSpeakerDiarization" -> "dihardchallenge/dihard3_baseline"
"juanmc2005/StreamingSpeakerDiarization" -> "asteroid-team/torch-audiomentations"
"juanmc2005/StreamingSpeakerDiarization" -> "taylorlu/Speaker-Diarization"
"praat/praat" -> "YannickJadoul/Parselmouth"
"praat/praat" -> "timmahrt/praatIO"
"praat/praat" -> "mmorise/World"
"praat/praat" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"praat/praat" -> "kylebgorman/textgrid"
"praat/praat" -> "covarep/covarep" ["e"=1]
"praat/praat" -> "NVIDIA/mellotron"
"praat/praat" -> "r9y9/pysptk"
"praat/praat" -> "bootphon/phonemizer"
"praat/praat" -> "marl/crepe" ["e"=1]
"praat/praat" -> "lowerquality/gentle"
"praat/praat" -> "pettarin/forced-alignment-tools"
"praat/praat" -> "prosodylab/Prosodylab-Aligner"
"praat/praat" -> "Kyubyong/g2p"
"praat/praat" -> "mozilla/LPCNet"
"GANtastic3/MaskCycleGAN-VC" -> "jackaduma/CycleGAN-VC3"
"GANtastic3/MaskCycleGAN-VC" -> "Wendison/VQMIVC"
"GANtastic3/MaskCycleGAN-VC" -> "yl4579/StarGANv2-VC"
"GANtastic3/MaskCycleGAN-VC" -> "mindslab-ai/assem-vc"
"GANtastic3/MaskCycleGAN-VC" -> "jackaduma/CycleGAN-VC2"
"KevinMIN95/StyleSpeech" -> "keonlee9420/StyleSpeech"
"KevinMIN95/StyleSpeech" -> "SungFeng-Huang/Meta-TTS"
"KevinMIN95/StyleSpeech" -> "keonlee9420/Comprehensive-Transformer-TTS"
"KevinMIN95/StyleSpeech" -> "WelkinYang/GradTTS"
"KevinMIN95/StyleSpeech" -> "Rongjiehuang/GenerSpeech"
"KevinMIN95/StyleSpeech" -> "NVIDIA/radtts"
"KevinMIN95/StyleSpeech" -> "thuhcsi/VAENAR-TTS"
"KevinMIN95/StyleSpeech" -> "ncsoft/avocodo"
"KevinMIN95/StyleSpeech" -> "keonlee9420/PortaSpeech"
"KevinMIN95/StyleSpeech" -> "huawei-noah/Speech-Backbones"
"KevinMIN95/StyleSpeech" -> "yerfor/SyntaSpeech"
"burchim/EfficientConformer" -> "tencent-ailab/3m-asr"
"burchim/EfficientConformer" -> "upskyy/Squeezeformer"
"burchim/EfficientConformer" -> "kssteven418/Squeezeformer"
"burchim/EfficientConformer" -> "csukuangfj/optimized_transducer"
"burchim/EfficientConformer" -> "jctian98/e2e_lfmmi"
"burchim/EfficientConformer" -> "danpovey/fast_rnnt"
"cvqluu/GE2E-Loss" -> "funcwj/ge2e-speaker-verification"
"patrickvonplaten/Wav2Vec2_PyCTCDecode" -> "farisalasmary/wav2vec2-kenlm"
"patrickvonplaten/Wav2Vec2_PyCTCDecode" -> "kensho-technologies/pyctcdecode"
"pykaldi/pykaldi" -> "vesis84/kaldi-io-for-python"
"pykaldi/pykaldi" -> "mravanelli/pytorch-kaldi"
"pykaldi/pykaldi" -> "k2-fsa/k2"
"pykaldi/pykaldi" -> "YoavRamon/awesome-kaldi"
"pykaldi/pykaldi" -> "jzlianglu/pykaldi2"
"pykaldi/pykaldi" -> "freewym/espresso"
"pykaldi/pykaldi" -> "lhotse-speech/lhotse"
"pykaldi/pykaldi" -> "srvk/eesen"
"pykaldi/pykaldi" -> "kaituoxu/Speech-Transformer"
"pykaldi/pykaldi" -> "YiwenShaoStephen/pychain"
"pykaldi/pykaldi" -> "nttcslab-sp/kaldiio"
"pykaldi/pykaldi" -> "alumae/kaldi-gstreamer-server"
"pykaldi/pykaldi" -> "XiaoMi/kaldi-onnx"
"pykaldi/pykaldi" -> "wiseman/py-webrtcvad"
"pykaldi/pykaldi" -> "espnet/espnet"
"aparrish/pronouncingpy" -> "mphilli/English-to-IPA"
"aparrish/pronouncingpy" -> "prosegrinder/python-cmudict"
"aparrish/pronouncingpy" -> "aparrish/pycorpora" ["e"=1]
"aparrish/pronouncingpy" -> "aparrish/rwet" ["e"=1]
"aparrish/pronouncingpy" -> "aparrish/gutenberg-poetry-corpus" ["e"=1]
"alphacep/vosk-server" -> "alphacep/vosk-api"
"alphacep/vosk-server" -> "alphacep/vosk"
"alphacep/vosk-server" -> "alphacep/vosk-asterisk"
"alphacep/vosk-server" -> "alumae/kaldi-gstreamer-server"
"alphacep/vosk-server" -> "YoavRamon/awesome-kaldi"
"alphacep/vosk-server" -> "k2-fsa/k2"
"alphacep/vosk-server" -> "daanzu/kaldi-active-grammar" ["e"=1]
"alphacep/vosk-server" -> "jcsilva/docker-kaldi-gstreamer-server"
"alphacep/vosk-server" -> "alphacep/vosk-android-demo"
"alphacep/vosk-server" -> "alphacep/kaldi-android-demo"
"alphacep/vosk-server" -> "pykaldi/pykaldi"
"alphacep/vosk-server" -> "SergeyShk/Speech-to-Text-Russian" ["e"=1]
"alphacep/vosk-server" -> "unispeech/unimrcp" ["e"=1]
"alphacep/vosk-server" -> "k2-fsa/icefall"
"alphacep/vosk-server" -> "danpovey/k2"
"tomlepaine/fast-wavenet" -> "ibab/tensorflow-wavenet"
"tomlepaine/fast-wavenet" -> "basveeling/wavenet"
"tomlepaine/fast-wavenet" -> "r9y9/wavenet_vocoder"
"tomlepaine/fast-wavenet" -> "buriburisuri/speech-to-text-wavenet"
"tomlepaine/fast-wavenet" -> "NVIDIA/nv-wavenet"
"tomlepaine/fast-wavenet" -> "vincentherrmann/pytorch-wavenet"
"tomlepaine/fast-wavenet" -> "Kyubyong/tacotron"
"tomlepaine/fast-wavenet" -> "usernaamee/keras-wavenet"
"tomlepaine/fast-wavenet" -> "openai/pixel-cnn" ["e"=1]
"tomlepaine/fast-wavenet" -> "Zeta36/tensorflow-tex-wavenet"
"tomlepaine/fast-wavenet" -> "soroushmehr/sampleRNN_ICLR2017"
"tomlepaine/fast-wavenet" -> "CSTR-Edinburgh/merlin"
"tomlepaine/fast-wavenet" -> "sotelo/parrot"
"tomlepaine/fast-wavenet" -> "keithito/tacotron"
"tomlepaine/fast-wavenet" -> "PrajitR/fast-pixel-cnn" ["e"=1]
"umeng/MultiFunctionAndroidDemo" -> "umeng/UMAndroidSdkDemo"
"umeng/MultiFunctionAndroidDemo" -> "MobClub/ShareSDK-for-Android"
"umeng/MultiFunctionAndroidDemo" -> "BuglyDevTeam/Bugly-Android-Demo"
"umeng/MultiFunctionAndroidDemo" -> "umeng/MultiFunctionAndroidMavenDemo-master"
"lukhy/masr" -> "zw76859420/ASR_Syllable"
"lukhy/masr" -> "tzyll/kaldi"
"lukhy/masr" -> "HawkAaron/RNN-Transducer"
"lukhy/masr" -> "Pelhans/ZASR_tensorflow"
"lukhy/masr" -> "audier/DeepSpeechRecognition"
"Suxiaogang/WeiboPicBed" -> "maysrp/webdir" ["e"=1]
"Suxiaogang/WeiboPicBed" -> "trytofix/hexo_weibo_image"
"Suxiaogang/WeiboPicBed" -> "dbbbit/ninja-search"
"Suxiaogang/WeiboPicBed" -> "Suxiaogang/doubanXbaidu"
"Suxiaogang/WeiboPicBed" -> "consatan/weibo_image_uploader" ["e"=1]
"Suxiaogang/WeiboPicBed" -> "yujiandong/simpleforum" ["e"=1]
"ilaria-manco/multimodal-ml-music" -> "ilaria-manco/muscall"
"ilaria-manco/multimodal-ml-music" -> "jeffreyjohnens/MetaMIDIDataset" ["e"=1]
"ilaria-manco/multimodal-ml-music" -> "LAION-AI/audio-dataset"
"ilaria-manco/multimodal-ml-music" -> "annahung31/EMOPIA" ["e"=1]
"ZhengkunTian/rnn-transducer" -> "HawkAaron/RNN-Transducer"
"ZhengkunTian/rnn-transducer" -> "HawkAaron/warp-transducer"
"ZhengkunTian/rnn-transducer" -> "noahchalifour/rnnt-speech-recognition"
"ZhengkunTian/rnn-transducer" -> "hirofumi0810/neural_sp"
"ZhengkunTian/rnn-transducer" -> "ZhengkunTian/OpenTransformer"
"ZhengkunTian/rnn-transducer" -> "awni/transducer"
"ZhengkunTian/rnn-transducer" -> "1ytic/warp-rnnt"
"ZhengkunTian/rnn-transducer" -> "HawkAaron/E2E-ASR"
"ZhengkunTian/rnn-transducer" -> "gentaiscool/end2end-asr-pytorch"
"ZhengkunTian/rnn-transducer" -> "cywang97/StreamingTransformer"
"ZhengkunTian/rnn-transducer" -> "sooftware/RNN-Transducer"
"ZhengkunTian/rnn-transducer" -> "kaituoxu/Speech-Transformer"
"ZhengkunTian/rnn-transducer" -> "tencent-ailab/pika"
"ZhengkunTian/rnn-transducer" -> "xingchensong/speech-recognition-papers"
"ZhengkunTian/rnn-transducer" -> "thu-spmi/CAT"
"Hiroshiba/voicevox" -> "Hiroshiba/voicevox_engine"
"Hiroshiba/voicevox" -> "Hiroshiba/voicevox_core"
"Hiroshiba/voicevox" -> "suzune25254649/bakusoku_aviutl_plugin" ["e"=1]
"Hiroshiba/voicevox" -> "mmorise/ita-corpus"
"Hiroshiba/voicevox" -> "isletennos/MMVC_Trainer"
"Hiroshiba/voicevox" -> "oov/aviutl_psdtoolkit" ["e"=1]
"Hiroshiba/voicevox" -> "VOICEVOX/voicevox_core"
"Hiroshiba/voicevox" -> "r9y9/ttslearn" ["e"=1]
"Hiroshiba/voicevox" -> "googlefonts/morisawa-biz-ud-gothic" ["e"=1]
"Hiroshiba/voicevox" -> "google/budoux" ["e"=1]
"Hiroshiba/voicevox" -> "Hiroshiba/become-yukarin"
"Hiroshiba/voicevox" -> "Hiroshiba/realtime-yukarin"
"Hiroshiba/voicevox" -> "isletennos/MMVC_Client"
"Hiroshiba/voicevox" -> "amate/InputPipePlugin" ["e"=1]
"Hiroshiba/voicevox" -> "googlefonts/morisawa-biz-ud-mincho" ["e"=1]
"auspicious3000/autovc" -> "auspicious3000/SpeechSplit"
"auspicious3000/autovc" -> "jjery2243542/adaptive_voice_conversion"
"auspicious3000/autovc" -> "liusongxiang/StarGAN-Voice-Conversion"
"auspicious3000/autovc" -> "kan-bayashi/ParallelWaveGAN"
"auspicious3000/autovc" -> "auspicious3000/AutoPST"
"auspicious3000/autovc" -> "Wendison/VQMIVC"
"auspicious3000/autovc" -> "jxzhanggg/nonparaSeq2seqVC_code"
"auspicious3000/autovc" -> "jik876/hifi-gan"
"auspicious3000/autovc" -> "descriptinc/melgan-neurips"
"auspicious3000/autovc" -> "jjery2243542/voice_conversion"
"auspicious3000/autovc" -> "NVIDIA/mellotron"
"auspicious3000/autovc" -> "mindslab-ai/cotatron"
"auspicious3000/autovc" -> "liusongxiang/ppg-vc"
"auspicious3000/autovc" -> "jaywalnut310/glow-tts"
"auspicious3000/autovc" -> "jackaduma/CycleGAN-VC2"
"VITA-Group/AutoSpeech" -> "jymsuper/SpeakerRecognition_tutorial"
"VITA-Group/AutoSpeech" -> "seongmin-kye/meta-SR"
"VITA-Group/AutoSpeech" -> "Jungjee/RawNet"
"VITA-Group/AutoSpeech" -> "google/speaker-id"
"zw76859420/ASR_WORD" -> "zw76859420/ASR_Syllable"
"zw76859420/ASR_WORD" -> "zw76859420/ASRT_SpeechRecognition"
"zw76859420/ASR_WORD" -> "EliasCai/speech_recognition_ctc"
"aliyun/aliyun-oss-android-sdk" -> "aliyun/alicloud-android-demo"
"aliyun/aliyun-oss-android-sdk" -> "BuglyDevTeam/Bugly-Android-Demo"
"aliyun/aliyun-oss-android-sdk" -> "oubowu/PinnedSectionItemDecoration" ["e"=1]
"widuu/qiniu_ueditor_1.4.3" -> "qiniu/php-sdk" ["e"=1]
"widuu/qiniu_ueditor_1.4.3" -> "abelyao/qiniu-file-for-typecho"
"widuu/qiniu_ueditor_1.4.3" -> "widuu/utf8_qiniu_ueditor"
"zw76859420/ASR_Theory" -> "zw76859420/ASR_Syllable"
"zw76859420/ASR_Theory" -> "kaituoxu/Speech-Transformer"
"zw76859420/ASR_Theory" -> "audier/DeepSpeechRecognition"
"zw76859420/ASR_Theory" -> "xxbb1234021/speech_recognition"
"zw76859420/ASR_Theory" -> "YoavRamon/awesome-kaldi"
"zw76859420/ASR_Theory" -> "zw76859420/ASR_WORD"
"zw76859420/ASR_Theory" -> "sailist/ASRFrame"
"zw76859420/ASR_Theory" -> "srvk/eesen"
"zw76859420/ASR_Theory" -> "nobody132/masr"
"zw76859420/ASR_Theory" -> "zzw922cn/awesome-speech-recognition-speech-synthesis-papers"
"zw76859420/ASR_Theory" -> "ZhengkunTian/OpenTransformer"
"zw76859420/ASR_Theory" -> "nl8590687/ASRT_SpeechRecognition"
"zw76859420/ASR_Theory" -> "mindorii/kws" ["e"=1]
"zw76859420/ASR_Theory" -> "Alexander-H-Liu/End-to-end-ASR-Pytorch"
"zw76859420/ASR_Theory" -> "zcaceres/spec_augment"
"Xflick/EEND_PyTorch" -> "nttcslab-sp/EEND-vector-clustering"
"Xflick/EEND_PyTorch" -> "hitachi-speech/EEND"
"Xflick/EEND_PyTorch" -> "zcxu-eric/AVA-AVD"
"amaas/stanford-ctc" -> "rakeshvar/rnn_ctc"
"amaas/stanford-ctc" -> "mohammadpz/CTC-Connectionist-Temporal-Classification"
"amaas/stanford-ctc" -> "rizar/attention-lvcsr"
"amaas/stanford-ctc" -> "srvk/eesen"
"amaas/stanford-ctc" -> "jonrein/tensorflow_CTC_example"
"amaas/stanford-ctc" -> "igormq/ctc_tensorflow_example"
"amaas/stanford-ctc" -> "skaae/Lasagne-CTC"
"amaas/stanford-ctc" -> "yajiemiao/eesen"
"amaas/stanford-ctc" -> "zxie/nn"
"amaas/stanford-ctc" -> "shawntan/theano-ctc"
"amaas/stanford-ctc" -> "sherjilozair/ctc"
"amaas/stanford-ctc" -> "lingochamp/kaldi-ctc"
"amaas/stanford-ctc" -> "thomasmesnard/CTC-LSTM"
"amaas/stanford-ctc" -> "baidu-research/ba-dls-deepspeech"
"facebookresearch/WavAugment" -> "asteroid-team/torch-audiomentations"
"facebookresearch/WavAugment" -> "aliutkus/speechmetrics" ["e"=1]
"facebookresearch/WavAugment" -> "iver56/audiomentations"
"facebookresearch/WavAugment" -> "facebookresearch/denoiser" ["e"=1]
"facebookresearch/WavAugment" -> "KinWaiCheuk/nnAudio"
"facebookresearch/WavAugment" -> "ivanvovk/WaveGrad"
"facebookresearch/WavAugment" -> "lhotse-speech/lhotse"
"facebookresearch/WavAugment" -> "facebookresearch/CPC_audio"
"facebookresearch/WavAugment" -> "csteinmetz1/auraloss"
"facebookresearch/WavAugment" -> "tencent-ailab/pika"
"facebookresearch/WavAugment" -> "DavidDiazGuerra/gpuRIR" ["e"=1]
"facebookresearch/WavAugment" -> "s3prl/s3prl"
"facebookresearch/WavAugment" -> "facebookresearch/libri-light"
"facebookresearch/WavAugment" -> "hirofumi0810/neural_sp"
"facebookresearch/WavAugment" -> "DemisEom/SpecAugment"
"hirofumi0810/tensorflow_end2end_speech_recognition" -> "hirofumi0810/asr_preprocessing"
"hirofumi0810/tensorflow_end2end_speech_recognition" -> "vrenkens/nabu"
"hirofumi0810/tensorflow_end2end_speech_recognition" -> "igormq/ctc_tensorflow_example"
"hirofumi0810/tensorflow_end2end_speech_recognition" -> "rizar/attention-lvcsr"
"hirofumi0810/tensorflow_end2end_speech_recognition" -> "awni/speech"
"hirofumi0810/tensorflow_end2end_speech_recognition" -> "HawkAaron/RNN-Transducer"
"hirofumi0810/tensorflow_end2end_speech_recognition" -> "philipperemy/tensorflow-ctc-speech-recognition"
"hirofumi0810/tensorflow_end2end_speech_recognition" -> "hirofumi0810/neural_sp"
"hirofumi0810/tensorflow_end2end_speech_recognition" -> "zw76859420/ASR_Syllable"
"hirofumi0810/tensorflow_end2end_speech_recognition" -> "rwth-i6/returnn"
"hirofumi0810/tensorflow_end2end_speech_recognition" -> "vrenkens/tfkaldi"
"hirofumi0810/tensorflow_end2end_speech_recognition" -> "Kyubyong/tacotron_asr"
"hirofumi0810/tensorflow_end2end_speech_recognition" -> "zzw922cn/Automatic_Speech_Recognition"
"hirofumi0810/tensorflow_end2end_speech_recognition" -> "thomasschmied/Speech_Recognition_with_Tensorflow"
"hirofumi0810/tensorflow_end2end_speech_recognition" -> "srvk/eesen"
"hitachi-speech/EEND" -> "nryant/dscore"
"hitachi-speech/EEND" -> "BUTSpeechFIT/VBx"
"hitachi-speech/EEND" -> "wq2012/SpectralCluster"
"hitachi-speech/EEND" -> "Xflick/EEND_PyTorch"
"hitachi-speech/EEND" -> "wq2012/awesome-diarization"
"hitachi-speech/EEND" -> "nttcslab-sp/EEND-vector-clustering"
"hitachi-speech/EEND" -> "desh2608/dover-lap"
"hitachi-speech/EEND" -> "yufan-aslp/AliMeeting"
"hitachi-speech/EEND" -> "manojpamk/pytorch_xvectors"
"hitachi-speech/EEND" -> "pyannote/pyannote-metrics"
"hitachi-speech/EEND" -> "juanmc2005/StreamingSpeakerDiarization"
"hitachi-speech/EEND" -> "taylorlu/Speaker-Diarization"
"hitachi-speech/EEND" -> "Snowdar/asv-subtools"
"hitachi-speech/EEND" -> "nttcslab-sp/kaldiio"
"hitachi-speech/EEND" -> "google/speaker-id"
"jymsuper/SpeakerRecognition_tutorial" -> "Walleclipse/Deep_Speaker-speaker_recognition_system"
"jymsuper/SpeakerRecognition_tutorial" -> "qqueing/DeepSpeaker-pytorch"
"jymsuper/SpeakerRecognition_tutorial" -> "astorfi/3D-convolutional-speaker-recognition-pytorch"
"jymsuper/SpeakerRecognition_tutorial" -> "cvqluu/TDNN"
"jymsuper/SpeakerRecognition_tutorial" -> "HarryVolek/PyTorch_Speaker_Verification"
"jymsuper/SpeakerRecognition_tutorial" -> "VITA-Group/AutoSpeech"
"jymsuper/SpeakerRecognition_tutorial" -> "Janghyun1230/Speaker_Verification"
"jymsuper/SpeakerRecognition_tutorial" -> "Dannynis/xvector_pytorch"
"jymsuper/SpeakerRecognition_tutorial" -> "bjfu-ai-institute/speaker-recognition-papers"
"jymsuper/SpeakerRecognition_tutorial" -> "yistLin/dvector"
"jymsuper/SpeakerRecognition_tutorial" -> "wangleiai/dVectorSpeakerRecognition"
"jymsuper/SpeakerRecognition_tutorial" -> "Aurora11111/speaker-recognition-pytorch"
"jymsuper/SpeakerRecognition_tutorial" -> "seongmin-kye/meta-SR"
"jymsuper/SpeakerRecognition_tutorial" -> "clovaai/voxceleb_trainer"
"jymsuper/SpeakerRecognition_tutorial" -> "taylorlu/Speaker-Diarization"
"mdangschat/ctc-asr" -> "HawkAaron/E2E-ASR"
"zcaceres/spec_augment" -> "DemisEom/SpecAugment"
"zcaceres/spec_augment" -> "shelling203/SpecAugment"
"zcaceres/spec_augment" -> "hirofumi0810/neural_sp"
"zcaceres/spec_augment" -> "kaituoxu/Speech-Transformer"
"zcaceres/spec_augment" -> "facebookresearch/WavAugment"
"zcaceres/spec_augment" -> "manojpamk/pytorch_xvectors"
"zcaceres/spec_augment" -> "kaituoxu/Listen-Attend-Spell"
"zcaceres/spec_augment" -> "lhotse-speech/lhotse"
"zcaceres/spec_augment" -> "iver56/audiomentations"
"zcaceres/spec_augment" -> "asteroid-team/torch-audiomentations"
"zcaceres/spec_augment" -> "gentaiscool/end2end-asr-pytorch"
"zcaceres/spec_augment" -> "HawkAaron/E2E-ASR"
"zcaceres/spec_augment" -> "clovaai/voxceleb_trainer"
"zcaceres/spec_augment" -> "cywang97/StreamingTransformer"
"zcaceres/spec_augment" -> "freewym/espresso"
"CjangCjengh/vits" -> "CjangCjengh/MoeGoe"
"CjangCjengh/vits" -> "CjangCjengh/TTSModels"
"CjangCjengh/vits" -> "innnky/emotional-vits"
"CjangCjengh/vits" -> "luoyily/MoeTTS"
"CjangCjengh/vits" -> "CjangCjengh/MoeGoe_GUI"
"CjangCjengh/vits" -> "jaywalnut310/vits"
"CjangCjengh/vits" -> "AlexandaJerry/whisper-vits-japanese"
"CjangCjengh/vits" -> "Plachtaa/VITS-fast-fine-tuning"
"CjangCjengh/vits" -> "SayaSS/vits-finetuning"
"CjangCjengh/vits" -> "IceKyrin/sovits_guide"
"CjangCjengh/vits" -> "CjangCjengh/tacotron2-japanese"
"CjangCjengh/vits" -> "NaruseMioShirakana/MoeSS"
"CjangCjengh/vits" -> "Minami-Yuduru/-ChatGPT_VITS"
"CjangCjengh/vits" -> "openvpi/audio-slicer"
"CjangCjengh/vits" -> "audeering/w2v2-how-to"
"Baidu-AIP/python-sdk" -> "Baidu-AIP/php-sdk"
"Baidu-AIP/python-sdk" -> "Baidu-AIP/nodejs-sdk"
"Baidu-AIP/python-sdk" -> "Baidu-AIP/QuickStart"
"Baidu-AIP/python-sdk" -> "Baidu-AIP/java-sdk"
"descriptinc/melgan-neurips" -> "seungwonpark/melgan"
"descriptinc/melgan-neurips" -> "kan-bayashi/ParallelWaveGAN"
"descriptinc/melgan-neurips" -> "mozilla/LPCNet"
"descriptinc/melgan-neurips" -> "jik876/hifi-gan"
"descriptinc/melgan-neurips" -> "NVIDIA/mellotron"
"descriptinc/melgan-neurips" -> "xcmyz/FastSpeech"
"descriptinc/melgan-neurips" -> "NVIDIA/waveglow"
"descriptinc/melgan-neurips" -> "auspicious3000/autovc"
"descriptinc/melgan-neurips" -> "auspicious3000/SpeechSplit"
"descriptinc/melgan-neurips" -> "r9y9/wavenet_vocoder"
"descriptinc/melgan-neurips" -> "soobinseo/Transformer-TTS"
"descriptinc/melgan-neurips" -> "yanggeng1995/GAN-TTS"
"descriptinc/melgan-neurips" -> "ivanvovk/WaveGrad"
"descriptinc/melgan-neurips" -> "jaywalnut310/glow-tts"
"descriptinc/melgan-neurips" -> "ming024/FastSpeech2"
"wq2012/SpectralCluster" -> "taylorlu/Speaker-Diarization"
"wq2012/SpectralCluster" -> "wq2012/awesome-diarization"
"wq2012/SpectralCluster" -> "hitachi-speech/EEND"
"wq2012/SpectralCluster" -> "google/uis-rnn"
"wq2012/SpectralCluster" -> "pyannote/pyannote-metrics"
"wq2012/SpectralCluster" -> "nryant/dscore"
"wq2012/SpectralCluster" -> "google/speaker-id"
"wq2012/SpectralCluster" -> "BUTSpeechFIT/VBx"
"wq2012/SpectralCluster" -> "FlorianKrey/DNC"
"wq2012/SpectralCluster" -> "juanmc2005/StreamingSpeakerDiarization"
"wq2012/SpectralCluster" -> "HarryVolek/PyTorch_Speaker_Verification"
"wq2012/SpectralCluster" -> "tango4j/Auto-Tuning-Spectral-Clustering"
"wq2012/SpectralCluster" -> "Jamiroquai88/VBDiarization"
"wq2012/SpectralCluster" -> "Snowdar/asv-subtools"
"wq2012/SpectralCluster" -> "pyannote/pyannote-audio"
"xcmyz/speech-synthesis-paper" -> "xcmyz/FastSpeech"
"xcmyz/speech-synthesis-paper" -> "mindslab-ai/cotatron"
"xcmyz/speech-synthesis-paper" -> "jaywalnut310/glow-tts"
"xcmyz/speech-synthesis-paper" -> "tts-tutorial/survey"
"xcmyz/speech-synthesis-paper" -> "Wendison/VQMIVC"
"xcmyz/speech-synthesis-paper" -> "jik876/hifi-gan"
"xcmyz/speech-synthesis-paper" -> "kan-bayashi/ParallelWaveGAN"
"xcmyz/speech-synthesis-paper" -> "thuhcsi/VAENAR-TTS"
"xcmyz/speech-synthesis-paper" -> "keonlee9420/Parallel-Tacotron2"
"xcmyz/speech-synthesis-paper" -> "keonlee9420/Comprehensive-Transformer-TTS"
"xcmyz/speech-synthesis-paper" -> "ming024/FastSpeech2"
"xcmyz/speech-synthesis-paper" -> "Tomiinek/Multilingual_Text_to_Speech"
"xcmyz/speech-synthesis-paper" -> "auspicious3000/SpeechSplit"
"xcmyz/speech-synthesis-paper" -> "keonlee9420/StyleSpeech"
"xcmyz/speech-synthesis-paper" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"mravanelli/pytorch-kaldi" -> "pykaldi/pykaldi"
"mravanelli/pytorch-kaldi" -> "espnet/espnet"
"mravanelli/pytorch-kaldi" -> "mravanelli/SincNet"
"mravanelli/pytorch-kaldi" -> "SeanNaren/deepspeech.pytorch"
"mravanelli/pytorch-kaldi" -> "speechbrain/speechbrain"
"mravanelli/pytorch-kaldi" -> "kaldi-asr/kaldi"
"mravanelli/pytorch-kaldi" -> "kaituoxu/Speech-Transformer"
"mravanelli/pytorch-kaldi" -> "facebookresearch/wav2letter"
"mravanelli/pytorch-kaldi" -> "Alexander-H-Liu/End-to-end-ASR-Pytorch"
"mravanelli/pytorch-kaldi" -> "vesis84/kaldi-io-for-python"
"mravanelli/pytorch-kaldi" -> "srvk/eesen"
"mravanelli/pytorch-kaldi" -> "syhw/wer_are_we"
"mravanelli/pytorch-kaldi" -> "zzw922cn/awesome-speech-recognition-speech-synthesis-papers"
"mravanelli/pytorch-kaldi" -> "k2-fsa/k2"
"mravanelli/pytorch-kaldi" -> "wiseman/py-webrtcvad"
"sealtalk/sealtalk-server" -> "sealtalk/sealtalk-web"
"sealtalk/sealtalk-server" -> "rongcloud/server-sdk-java"
"sealtalk/sealtalk-server" -> "sealtalk/sealtalk-desktop"
"sealtalk/sealtalk-server" -> "sealtalk/sealtalk-ios"
"sealtalk/sealtalk-server" -> "sealtalk/sealtalk-android" ["e"=1]
"ddlBoJack/Speech-Resources" -> "ddlBoJack/Awesome-Speech-Pretraining"
"ddlBoJack/Speech-Resources" -> "hit-thusz-RookieCJ/FullSubNet-plus" ["e"=1]
"ddlBoJack/Speech-Resources" -> "double22a/speech_dataset"
"ddlBoJack/Speech-Resources" -> "microsoft/UniSpeech"
"ddlBoJack/Speech-Resources" -> "cnlinxi/book-text-to-speech"
"ddlBoJack/Speech-Resources" -> "TencentGameMate/chinese_speech_pretrain"
"ddlBoJack/Speech-Resources" -> "liusongxiang/Large-Audio-Models"
"ddlBoJack/Speech-Resources" -> "Ryuk17/SpeechAlgorithms" ["e"=1]
"Hiroshiba/voicevox_core" -> "Hiroshiba/voicevox_engine"
"Hiroshiba/voicevox_core" -> "yamachu/VoicevoxEngineSharp"
"thooore/SpleeterGUI" -> "thooore/SpleeterCore"
"thooore/SpleeterGUI" -> "boy1dr/SpleeterGui"
"Paraworks/vits_with_chatgpt-gpt3" -> "Arkueid/Live2DMascot"
"Paraworks/vits_with_chatgpt-gpt3" -> "cjyaddone/ChatWaifuL2D"
"Paraworks/vits_with_chatgpt-gpt3" -> "cjyaddone/ChatWaifu"
"Paraworks/vits_with_chatgpt-gpt3" -> "NaruseMioShirakana/MoeSS"
"Paraworks/vits_with_chatgpt-gpt3" -> "Minami-Yuduru/-ChatGPT_VITS"
"Paraworks/vits_with_chatgpt-gpt3" -> "luoyily/MoeTTS"
"Paraworks/vits_with_chatgpt-gpt3" -> "CjangCjengh/TTSModels"
"Paraworks/vits_with_chatgpt-gpt3" -> "CjangCjengh/vits"
"Paraworks/vits_with_chatgpt-gpt3" -> "innnky/emotional-vits"
"Paraworks/vits_with_chatgpt-gpt3" -> "SayaSS/vits-finetuning"
"Paraworks/vits_with_chatgpt-gpt3" -> "innnky/MB-iSTFT-VITS"
"Paraworks/vits_with_chatgpt-gpt3" -> "Plachtaa/VITS-fast-fine-tuning"
"Paraworks/vits_with_chatgpt-gpt3" -> "AlexandaJerry/whisper-vits-japanese"
"Paraworks/vits_with_chatgpt-gpt3" -> "PlayVoice/vits_chinese"
"Paraworks/vits_with_chatgpt-gpt3" -> "AliceNavigator/AI-Vtuber-chatglm"
"cjyaddone/ChatWaifu" -> "cjyaddone/ChatWaifuL2D"
"cjyaddone/ChatWaifu" -> "MuBai-He/ChatWaifu-marai"
"cjyaddone/ChatWaifu" -> "Paraworks/vits_with_chatgpt-gpt3"
"cjyaddone/ChatWaifu" -> "CjangCjengh/TTSModels"
"cjyaddone/ChatWaifu" -> "Voine/ChatWaifu_Mobile"
"cjyaddone/ChatWaifu" -> "CjangCjengh/MoeGoe"
"cjyaddone/ChatWaifu" -> "SayaSS/vits-finetuning"
"cjyaddone/ChatWaifu" -> "Plachtaa/VITS-fast-fine-tuning"
"cjyaddone/ChatWaifu" -> "jaywalnut310/vits"
"cjyaddone/ChatWaifu" -> "CjangCjengh/vits"
"cjyaddone/ChatWaifu" -> "luoyily/MoeTTS"
"cjyaddone/ChatWaifu" -> "innnky/emotional-vits"
"cjyaddone/ChatWaifu" -> "CjangCjengh/MoeGoe_GUI"
"cjyaddone/ChatWaifu" -> "innnky/so-vits-svc"
"cjyaddone/ChatWaifu" -> "CjangCjengh/tacotron2-japanese"
"cjyaddone/ChatWaifuL2D" -> "cjyaddone/ChatWaifu"
"cjyaddone/ChatWaifuL2D" -> "MuBai-He/ChatWaifu-marai"
"cjyaddone/ChatWaifuL2D" -> "Paraworks/vits_with_chatgpt-gpt3"
"cjyaddone/ChatWaifuL2D" -> "CjangCjengh/TTSModels"
"cjyaddone/ChatWaifuL2D" -> "CjangCjengh/MoeGoe_GUI"
"cjyaddone/ChatWaifuL2D" -> "Minami-Yuduru/-ChatGPT_VITS"
"cjyaddone/ChatWaifuL2D" -> "CjangCjengh/MoeGoe"
"cjyaddone/ChatWaifuL2D" -> "CjangCjengh/vits"
"cjyaddone/ChatWaifuL2D" -> "Voine/ChatWaifu_Mobile"
"cjyaddone/ChatWaifuL2D" -> "luoyily/MoeTTS"
"cmusphinx/pocketsphinx-android" -> "cmusphinx/pocketsphinx-android-demo"
"cmusphinx/pocketsphinx-android" -> "cmusphinx/sphinxbase"
"cmusphinx/pocketsphinx-android" -> "cmusphinx/pocketsphinx-ios-demo"
"cmusphinx/pocketsphinx-android" -> "cmusphinx/cmudict-tools"
"cmusphinx/pocketsphinx-android" -> "cmusphinx/sphinxtrain"
"AlexandaJerry/whisper-vits-japanese" -> "SayaSS/vits-finetuning"
"AlexandaJerry/whisper-vits-japanese" -> "innnky/emotional-vits"
"AlexandaJerry/whisper-vits-japanese" -> "IceKyrin/sovits_guide"
"AlexandaJerry/whisper-vits-japanese" -> "CjangCjengh/vits"
"HarryVolek/PyTorch_Speaker_Verification" -> "Janghyun1230/Speaker_Verification"
"HarryVolek/PyTorch_Speaker_Verification" -> "qqueing/DeepSpeaker-pytorch"
"HarryVolek/PyTorch_Speaker_Verification" -> "google/uis-rnn"
"HarryVolek/PyTorch_Speaker_Verification" -> "wq2012/awesome-diarization"
"HarryVolek/PyTorch_Speaker_Verification" -> "philipperemy/deep-speaker"
"HarryVolek/PyTorch_Speaker_Verification" -> "taylorlu/Speaker-Diarization"
"HarryVolek/PyTorch_Speaker_Verification" -> "WeidiXie/VGG-Speaker-Recognition"
"HarryVolek/PyTorch_Speaker_Verification" -> "manojpamk/pytorch_xvectors"
"HarryVolek/PyTorch_Speaker_Verification" -> "funcwj/ge2e-speaker-verification"
"HarryVolek/PyTorch_Speaker_Verification" -> "mravanelli/SincNet"
"HarryVolek/PyTorch_Speaker_Verification" -> "clovaai/voxceleb_trainer"
"HarryVolek/PyTorch_Speaker_Verification" -> "jymsuper/SpeakerRecognition_tutorial"
"HarryVolek/PyTorch_Speaker_Verification" -> "wq2012/SpectralCluster"
"HarryVolek/PyTorch_Speaker_Verification" -> "Snowdar/asv-subtools"
"HarryVolek/PyTorch_Speaker_Verification" -> "mindslab-ai/voicefilter" ["e"=1]
"Kyubyong/expressive_tacotron" -> "syang1993/gst-tacotron"
"Kyubyong/expressive_tacotron" -> "syang1993/FFTNet"
"Kyubyong/expressive_tacotron" -> "npuichigo/waveglow"
"MelissaChen15/control-vc" -> "b04901014/UUVC"
"MelissaChen15/control-vc" -> "OlaWod/FreeVC"
"NVIDIA/mellotron" -> "seungwonpark/melgan"
"NVIDIA/mellotron" -> "xcmyz/FastSpeech"
"NVIDIA/mellotron" -> "NVIDIA/flowtron"
"NVIDIA/mellotron" -> "jaywalnut310/glow-tts"
"NVIDIA/mellotron" -> "kan-bayashi/ParallelWaveGAN"
"NVIDIA/mellotron" -> "auspicious3000/SpeechSplit"
"NVIDIA/mellotron" -> "mozilla/LPCNet"
"NVIDIA/mellotron" -> "jik876/hifi-gan"
"NVIDIA/mellotron" -> "KinglittleQ/GST-Tacotron"
"NVIDIA/mellotron" -> "descriptinc/melgan-neurips"
"NVIDIA/mellotron" -> "syang1993/gst-tacotron"
"NVIDIA/mellotron" -> "nii-yamagishilab/multi-speaker-tacotron"
"NVIDIA/mellotron" -> "soobinseo/Transformer-TTS"
"NVIDIA/mellotron" -> "NVIDIA/waveglow"
"NVIDIA/mellotron" -> "ming024/FastSpeech2"
"aalto-speech/speaker-diarization" -> "taylorlu/Speaker-Diarization"
"aalto-speech/speaker-diarization" -> "aalto-speech/AaltoASR"
"aalto-speech/speaker-diarization" -> "Jamiroquai88/VBDiarization"
"aalto-speech/speaker-diarization" -> "yinruiqing/change_detection"
"aalto-speech/speaker-diarization" -> "egonina/pycasp"
"aalto-speech/speaker-diarization" -> "wq2012/SpectralCluster"
"andabi/parallel-wavenet-vocoder" -> "bfs18/nsynth_wavenet"
"andabi/parallel-wavenet-vocoder" -> "azraelkuan/parallel_wavenet_vocoder"
"andabi/parallel-wavenet-vocoder" -> "kensun0/Parallel-Wavenet"
"andabi/parallel-wavenet-vocoder" -> "zhf459/P_wavenet_vocoder"
"andabi/parallel-wavenet-vocoder" -> "ksw0306/FloWaveNet"
"andabi/parallel-wavenet-vocoder" -> "ksw0306/ClariNet"
"andabi/parallel-wavenet-vocoder" -> "G-Wang/WaveRNN-Pytorch"
"azraelkuan/parallel_wavenet_vocoder" -> "andabi/parallel-wavenet-vocoder"
"azraelkuan/parallel_wavenet_vocoder" -> "bfs18/nsynth_wavenet"
"azraelkuan/parallel_wavenet_vocoder" -> "npuichigo/waveglow"
"azraelkuan/parallel_wavenet_vocoder" -> "kan-bayashi/PytorchWaveNetVocoder"
"azraelkuan/parallel_wavenet_vocoder" -> "ksw0306/ClariNet"
"azraelkuan/parallel_wavenet_vocoder" -> "A-Jacobson/tacotron2"
"azraelkuan/parallel_wavenet_vocoder" -> "geneing/WaveRNN-Pytorch"
"azraelkuan/parallel_wavenet_vocoder" -> "syang1993/FFTNet"
"azraelkuan/parallel_wavenet_vocoder" -> "h-meru/Tacotron-WaveRNN"
"azraelkuan/parallel_wavenet_vocoder" -> "azraelkuan/tensorflow_wavenet_vocoder"
"azraelkuan/parallel_wavenet_vocoder" -> "ksw0306/FloWaveNet"
"azraelkuan/parallel_wavenet_vocoder" -> "nii-yamagishilab/TSNetVocoder"
"azraelkuan/parallel_wavenet_vocoder" -> "azraelkuan/FFTNet"
"azraelkuan/parallel_wavenet_vocoder" -> "ksw0306/WaveVAE"
"basveeling/wavenet" -> "tomlepaine/fast-wavenet"
"basveeling/wavenet" -> "usernaamee/keras-wavenet"
"basveeling/wavenet" -> "ibab/tensorflow-wavenet"
"basveeling/wavenet" -> "bstriner/keras-adversarial" ["e"=1]
"basveeling/wavenet" -> "vincentherrmann/pytorch-wavenet"
"basveeling/wavenet" -> "r9y9/wavenet_vocoder"
"basveeling/wavenet" -> "sotelo/parrot"
"basveeling/wavenet" -> "buriburisuri/speech-to-text-wavenet"
"basveeling/wavenet" -> "soroushmehr/sampleRNN_ICLR2017"
"basveeling/wavenet" -> "matthiasplappert/keras-rl" ["e"=1]
"basveeling/wavenet" -> "jacobgil/keras-dcgan" ["e"=1]
"basveeling/wavenet" -> "coreylynch/async-rl" ["e"=1]
"basveeling/wavenet" -> "farizrahman4u/seq2seq" ["e"=1]
"basveeling/wavenet" -> "CSTR-Edinburgh/merlin"
"basveeling/wavenet" -> "Kyubyong/tacotron"
"bfs18/nsynth_wavenet" -> "andabi/parallel-wavenet-vocoder"
"bfs18/nsynth_wavenet" -> "kensun0/Parallel-Wavenet"
"bfs18/nsynth_wavenet" -> "zhf459/P_wavenet_vocoder"
"bfs18/nsynth_wavenet" -> "azraelkuan/parallel_wavenet_vocoder"
"hujinsen/StarGAN-Voice-Conversion" -> "liusongxiang/StarGAN-Voice-Conversion"
"hujinsen/StarGAN-Voice-Conversion" -> "leimao/Voice_Converter_CycleGAN"
"hujinsen/StarGAN-Voice-Conversion" -> "hujinsen/pytorch-StarGAN-VC"
"hujinsen/StarGAN-Voice-Conversion" -> "jjery2243542/adaptive_voice_conversion"
"hujinsen/StarGAN-Voice-Conversion" -> "jjery2243542/voice_conversion"
"hujinsen/StarGAN-Voice-Conversion" -> "jxzhanggg/nonparaSeq2seqVC_code"
"hujinsen/StarGAN-Voice-Conversion" -> "ryokamoi/ppg_vc"
"hujinsen/StarGAN-Voice-Conversion" -> "mazzzystar/randomCNN-voice-transfer"
"hujinsen/StarGAN-Voice-Conversion" -> "auspicious3000/autovc"
"hujinsen/StarGAN-Voice-Conversion" -> "JeremyCCHsu/vae-npvc"
"hujinsen/StarGAN-Voice-Conversion" -> "k2kobayashi/sprocket"
"hujinsen/StarGAN-Voice-Conversion" -> "SamuelBroughton/StarGAN-Voice-Conversion-2"
"hujinsen/StarGAN-Voice-Conversion" -> "jackaduma/CycleGAN-VC2"
"hujinsen/StarGAN-Voice-Conversion" -> "joansj/blow"
"hujinsen/StarGAN-Voice-Conversion" -> "jinhan/tacotron2-vae"
"innnky/emotional-vits" -> "audeering/w2v2-how-to"
"innnky/emotional-vits" -> "PlayVoice/vits_chinese"
"innnky/emotional-vits" -> "CjangCjengh/vits"
"innnky/emotional-vits" -> "CjangCjengh/TTSModels"
"innnky/emotional-vits" -> "luoyily/MoeTTS"
"innnky/emotional-vits" -> "AlexandaJerry/whisper-vits-japanese"
"innnky/emotional-vits" -> "Plachtaa/VITS-fast-fine-tuning"
"innnky/emotional-vits" -> "jaywalnut310/vits"
"innnky/emotional-vits" -> "MasayaKawamura/MB-iSTFT-VITS"
"innnky/emotional-vits" -> "CjangCjengh/MoeGoe"
"innnky/emotional-vits" -> "NaruseMioShirakana/MoeSS"
"innnky/emotional-vits" -> "w4123/GenshinVoice"
"innnky/emotional-vits" -> "IceKyrin/sovits_guide"
"innnky/emotional-vits" -> "CjangCjengh/MoeGoe_GUI"
"innnky/emotional-vits" -> "fishaudio/fish-diffusion"
"jxzhanggg/nonparaSeq2seqVC_code" -> "k2kobayashi/crank"
"jxzhanggg/nonparaSeq2seqVC_code" -> "dipjyoti92/SC-WaveRNN"
"jxzhanggg/nonparaSeq2seqVC_code" -> "jjery2243542/adaptive_voice_conversion"
"jxzhanggg/nonparaSeq2seqVC_code" -> "KunZhou9646/Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT"
"jxzhanggg/nonparaSeq2seqVC_code" -> "liusongxiang/ppg-vc"
"jxzhanggg/nonparaSeq2seqVC_code" -> "Wendison/VQMIVC"
"jxzhanggg/nonparaSeq2seqVC_code" -> "KunZhou9646/seq2seq-EVC"
"jxzhanggg/nonparaSeq2seqVC_code" -> "patrickltobing/cyclevae-vc-neuralvoco"
"jxzhanggg/nonparaSeq2seqVC_code" -> "jinhan/tacotron2-vae"
"jxzhanggg/nonparaSeq2seqVC_code" -> "tianrengao/SqueezeWave"
"jxzhanggg/nonparaSeq2seqVC_code" -> "yistLin/FragmentVC"
"jxzhanggg/nonparaSeq2seqVC_code" -> "bshall/ZeroSpeech"
"jxzhanggg/nonparaSeq2seqVC_code" -> "auspicious3000/autovc"
"jxzhanggg/nonparaSeq2seqVC_code" -> "joansj/blow"
"jxzhanggg/nonparaSeq2seqVC_code" -> "bigpon/vcc20_baseline_cyclevae"
"kan-bayashi/PytorchWaveNetVocoder" -> "azraelkuan/parallel_wavenet_vocoder"
"kan-bayashi/PytorchWaveNetVocoder" -> "mkotha/WaveRNN"
"kan-bayashi/PytorchWaveNetVocoder" -> "npuichigo/waveglow"
"kan-bayashi/PytorchWaveNetVocoder" -> "syang1993/gst-tacotron"
"kan-bayashi/PytorchWaveNetVocoder" -> "ksw0306/FloWaveNet"
"kan-bayashi/PytorchWaveNetVocoder" -> "mozilla/LPCNet"
"kan-bayashi/PytorchWaveNetVocoder" -> "JeremyCCHsu/Python-Wrapper-for-World-Vocoder"
"kan-bayashi/PytorchWaveNetVocoder" -> "bfs18/nsynth_wavenet"
"kan-bayashi/PytorchWaveNetVocoder" -> "k2kobayashi/sprocket"
"kan-bayashi/PytorchWaveNetVocoder" -> "andabi/parallel-wavenet-vocoder"
"kan-bayashi/PytorchWaveNetVocoder" -> "ksw0306/ClariNet"
"kan-bayashi/PytorchWaveNetVocoder" -> "kan-bayashi/ParallelWaveGAN"
"kan-bayashi/PytorchWaveNetVocoder" -> "r9y9/wavenet_vocoder"
"kan-bayashi/PytorchWaveNetVocoder" -> "descriptinc/melgan-neurips"
"kan-bayashi/PytorchWaveNetVocoder" -> "h-meru/Tacotron-WaveRNN"
"mkotha/WaveRNN" -> "ksw0306/ClariNet"
"mkotha/WaveRNN" -> "G-Wang/WaveRNN-Pytorch"
"mkotha/WaveRNN" -> "geneing/WaveRNN-Pytorch"
"mkotha/WaveRNN" -> "kan-bayashi/PytorchWaveNetVocoder"
"mkotha/WaveRNN" -> "h-meru/Tacotron-WaveRNN"
"mkotha/WaveRNN" -> "npuichigo/waveglow"
"mkotha/WaveRNN" -> "tiberiu44/TTS-Cube"
"mkotha/WaveRNN" -> "hrbigelow/ae-wavenet"
"mkotha/WaveRNN" -> "syang1993/gst-tacotron"
"mkotha/WaveRNN" -> "bfs18/nsynth_wavenet"
"mkotha/WaveRNN" -> "azraelkuan/parallel_wavenet_vocoder"
"mkotha/WaveRNN" -> "bshall/UniversalVocoding"
"mkotha/WaveRNN" -> "mozilla/LPCNet"
"mkotha/WaveRNN" -> "nii-yamagishilab/Extended_VQVAE"
"mkotha/WaveRNN" -> "ksw0306/WaveVAE"
"nii-yamagishilab/multi-speaker-tacotron" -> "mindslab-ai/cotatron"
"nii-yamagishilab/multi-speaker-tacotron" -> "Tomiinek/Multilingual_Text_to_Speech"
"nii-yamagishilab/multi-speaker-tacotron" -> "rishikksh20/vae_tacotron2"
"nii-yamagishilab/multi-speaker-tacotron" -> "tianrengao/SqueezeWave"
"nii-yamagishilab/multi-speaker-tacotron" -> "keonlee9420/Parallel-Tacotron2"
"nii-yamagishilab/multi-speaker-tacotron" -> "NVIDIA/mellotron"
"nii-yamagishilab/multi-speaker-tacotron" -> "jinhan/tacotron2-vae"
"nii-yamagishilab/multi-speaker-tacotron" -> "lochenchou/MOSNet" ["e"=1]
"nii-yamagishilab/multi-speaker-tacotron" -> "asuni/wavelet_prosody_toolkit"
"nii-yamagishilab/multi-speaker-tacotron" -> "jefflai108/pytorch-kaldi-neural-speaker-embeddings"
"nii-yamagishilab/multi-speaker-tacotron" -> "syang1993/gst-tacotron"
"nii-yamagishilab/multi-speaker-tacotron" -> "k2kobayashi/crank"
"pstuvwx/Deep_VoiceChanger" -> "Hiroshiba/realtime-yukarin"
"r9y9/nnmnkwii" -> "r9y9/nnmnkwii_gallery"
"r9y9/nnmnkwii" -> "r9y9/pysptk"
"r9y9/nnmnkwii" -> "r9y9/gantts"
"r9y9/nnmnkwii" -> "JeremyCCHsu/Python-Wrapper-for-World-Vocoder"
"r9y9/nnmnkwii" -> "jxzhanggg/nonparaSeq2seqVC_code"
"r9y9/nnmnkwii" -> "CSTR-Edinburgh/merlin"
"r9y9/nnmnkwii" -> "kan-bayashi/PytorchWaveNetVocoder"
"r9y9/nnmnkwii" -> "r9y9/nnsvs" ["e"=1]
"r9y9/nnmnkwii" -> "mmorise/World"
"r9y9/nnmnkwii" -> "soobinseo/Transformer-TTS"
"r9y9/nnmnkwii" -> "kan-bayashi/ParallelWaveGAN"
"r9y9/nnmnkwii" -> "r9y9/tacotron_pytorch"
"r9y9/nnmnkwii" -> "seungwonpark/melgan"
"r9y9/nnmnkwii" -> "Jackiexiao/MTTS"
"r9y9/nnmnkwii" -> "xcmyz/FastSpeech"
"rishikksh20/vae_tacotron2" -> "yanggeng1995/vae_tacotron"
"rishikksh20/vae_tacotron2" -> "jinhan/tacotron2-vae"
"ryokamoi/ppg_vc" -> "guanlongzhao/fac-via-ppg"
"ryokamoi/ppg_vc" -> "Kyubyong/cross_vc"
"sotelo/parrot" -> "soroushmehr/sampleRNN_ICLR2017"
"sotelo/parrot" -> "CSTR-Edinburgh/merlin"
"sotelo/parrot" -> "facebookresearch/loop"
"sotelo/parrot" -> "israelg99/deepvoice"
"sotelo/parrot" -> "azraelkuan/parallel_wavenet_vocoder"
"sotelo/parrot" -> "Zeta36/tensorflow-tex-wavenet"
"sotelo/parrot" -> "tomlepaine/fast-wavenet"
"sotelo/parrot" -> "ksw0306/ClariNet"
"sotelo/parrot" -> "JeremyCCHsu/Python-Wrapper-for-World-Vocoder"
"sotelo/parrot" -> "google/tacotron"
"sotelo/parrot" -> "Kyubyong/deepvoice3"
"sotelo/parrot" -> "mmorise/World"
"sotelo/parrot" -> "mozilla/LPCNet"
"sotelo/parrot" -> "tiberiu44/TTS-Cube"
"sotelo/parrot" -> "ksw0306/FloWaveNet"
"swasun/VQ-VAE-Speech" -> "hrbigelow/ae-wavenet"
"swasun/VQ-VAE-Speech" -> "DongyaoZhu/VQ-VAE-WaveNet"
"swasun/VQ-VAE-Speech" -> "bshall/ZeroSpeech"
"swasun/VQ-VAE-Speech" -> "JeremyCCHsu/vqvae-speech"
"swasun/VQ-VAE-Speech" -> "mkotha/WaveRNN"
"swasun/VQ-VAE-Speech" -> "ksw0306/WaveVAE"
"swasun/VQ-VAE-Speech" -> "yjlolo/vae-audio"
"swasun/VQ-VAE-Speech" -> "bshall/VectorQuantizedCPC"
"swasun/VQ-VAE-Speech" -> "ritheshkumar95/pytorch-vqvae" ["e"=1]
"swasun/VQ-VAE-Speech" -> "jaywalnut310/waveglow-vqvae"
"swasun/VQ-VAE-Speech" -> "joansj/blow"
"swasun/VQ-VAE-Speech" -> "syang1993/gst-tacotron"
"yanggeng1995/vae_tacotron" -> "rishikksh20/vae_tacotron2"
"yanggeng1995/vae_tacotron" -> "rishikksh20/gmvae_tacotron"
"zhf459/P_wavenet_vocoder" -> "maozhiqiang/wavernn"
"MycroftAI/mimic-recording-studio" -> "MycroftAI/mimic2"
"MycroftAI/mimic-recording-studio" -> "thorstenMueller/deep-learning-german-tts"
"MycroftAI/mimic-recording-studio" -> "DigitalPhonetics/IMS-Toucan"
"MycroftAI/mimic-recording-studio" -> "neonbjb/ocotillo"
"MycroftAI/mimic-recording-studio" -> "Edresson/YourTTS"
"MycroftAI/mimic-recording-studio" -> "thorstenMueller/Thorsten-Voice"
"haoheliu/audioldm_eval" -> "microsoft/CLAP"
"haoheliu/audioldm_eval" -> "haoheliu/diffres-python"
"haoheliu/audioldm_eval" -> "gudgud96/frechet-audio-distance"
"haoheliu/audioldm_eval" -> "Labbeti/aac-datasets"
"githubharald/SimpleHTR" -> "awslabs/handwritten-text-recognition-for-apache-mxnet"
"githubharald/SimpleHTR" -> "arthurflor23/handwritten-text-recognition"
"githubharald/SimpleHTR" -> "githubharald/CTCWordBeamSearch"
"githubharald/SimpleHTR" -> "sushant097/Handwritten-Line-Text-Recognition-using-Deep-Learning-with-Tensorflow"
"githubharald/SimpleHTR" -> "Breta01/handwriting-ocr"
"githubharald/SimpleHTR" -> "0x454447415244/HandwritingRecognitionSystem"
"githubharald/SimpleHTR" -> "lamhoangtung/LineHTR"
"githubharald/SimpleHTR" -> "githubharald/CTCDecoder"
"githubharald/SimpleHTR" -> "clovaai/deep-text-recognition-benchmark" ["e"=1]
"githubharald/SimpleHTR" -> "githubharald/WordSegmentation"
"githubharald/SimpleHTR" -> "githubharald/DeslantImg"
"githubharald/SimpleHTR" -> "solivr/tf-crnn" ["e"=1]
"githubharald/SimpleHTR" -> "Grzego/handwriting-generation" ["e"=1]
"githubharald/SimpleHTR" -> "githubharald/WordDetector"
"githubharald/SimpleHTR" -> "githubharald/WordDetectorNN"
"wenet-e2e/WenetSpeech" -> "SpeechColab/Leaderboard"
"wenet-e2e/WenetSpeech" -> "SpeechColab/GigaSpeech"
"wenet-e2e/WenetSpeech" -> "danpovey/fast_rnnt"
"wenet-e2e/WenetSpeech" -> "cywang97/StreamingTransformer"
"wenet-e2e/WenetSpeech" -> "jctian98/e2e_lfmmi"
"wenet-e2e/WenetSpeech" -> "k2-fsa/icefall"
"wenet-e2e/WenetSpeech" -> "mobvoi/wenet"
"wenet-e2e/WenetSpeech" -> "wenet-e2e/wenet-kws" ["e"=1]
"wenet-e2e/WenetSpeech" -> "speechio/chinese_text_normalization"
"wenet-e2e/WenetSpeech" -> "csukuangfj/kaldifeat"
"wenet-e2e/WenetSpeech" -> "speechio/BigCiDian"
"wenet-e2e/WenetSpeech" -> "k2-fsa/k2"
"wenet-e2e/WenetSpeech" -> "wenet-e2e/wekws" ["e"=1]
"wenet-e2e/WenetSpeech" -> "wenet-e2e/wetts"
"wenet-e2e/WenetSpeech" -> "wenet-e2e/speech-recognition-papers"
"githubharald/WordDetector" -> "githubharald/WordDetectorNN"
"githubharald/WordDetector" -> "harshavkumar/word_segmentation"
"qiniu/android-sdk" -> "qiniudemo/qiniu-lab-android"
"qiniu/android-sdk" -> "aliyun/aliyun-oss-android-sdk"
"google/tacotron" -> "sotelo/parrot"
"google/tacotron" -> "KinglittleQ/GST-Tacotron"
"google/tacotron" -> "keithito/tacotron"
"google/tacotron" -> "Kyubyong/tacotron"
"google/tacotron" -> "syang1993/gst-tacotron"
"google/tacotron" -> "mozilla/LPCNet"
"google/tacotron" -> "jinhan/tacotron2-vae"
"google/tacotron" -> "barronalex/Tacotron"
"google/tacotron" -> "google/sparrowhawk"
"google/tacotron" -> "thuhcsi/Crystal"
"google/tacotron" -> "Jackiexiao/MTTS"
"google/tacotron" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"google/tacotron" -> "Kyubyong/deepvoice3"
"google/tacotron" -> "Kyubyong/expressive_tacotron"
"google/tacotron" -> "NVIDIA/mellotron"
"YuanGongND/ssast" -> "YuanGongND/ast"
"YuanGongND/ssast" -> "RetroCirce/HTS-Audio-Transformer"
"YuanGongND/ssast" -> "facebookresearch/AudioMAE"
"YuanGongND/ssast" -> "YuanGongND/psla"
"YuanGongND/ssast" -> "nttcslab/msm-mae"
"YuanGongND/ssast" -> "kkoutini/PaSST"
"YuanGongND/ssast" -> "AlanBaade/MAE-AST-Public"
"YuanGongND/ssast" -> "YuanGongND/cav-mae"
"YuanGongND/ssast" -> "qiuqiangkong/torchlibrosa"
"YuanGongND/ssast" -> "nttcslab/byol-a"
"MobClub/ShareSDK-for-Android" -> "umeng/MultiFunctionAndroidDemo"
"MobClub/ShareSDK-for-Android" -> "xyzlf/ShareSDK"
"dpirch/libfvad" -> "Baidu-AIP/speech-vad-demo"
"dpirch/libfvad" -> "cpuimage/WebRTC_VAD" ["e"=1]
"dpirch/libfvad" -> "jtkim-kaist/VAD"
"dpirch/libfvad" -> "unispeech/unimrcp" ["e"=1]
"dpirch/libfvad" -> "wiseman/py-webrtcvad"
"dpirch/libfvad" -> "jitsi/jitsi-webrtc-vad-wrapper"
"dpirch/libfvad" -> "cotinyang/MRCP-Plugin-Demo" ["e"=1]
"dpirch/libfvad" -> "XiaoMi/kaldi-onnx"
"dpirch/libfvad" -> "hcmlab/vadnet"
"dpirch/libfvad" -> "voixen/voixen-vad"
"dpirch/libfvad" -> "shiweixingcn/vad"
"dpirch/libfvad" -> "mindorii/kws" ["e"=1]
"dpirch/libfvad" -> "snakers4/silero-vad"
"dpirch/libfvad" -> "shichaog/WebRTC-audio-processing" ["e"=1]
"dpirch/libfvad" -> "wangshub/python-vad"
"jerryuhoo/VTuberTalk" -> "innnky/emotional-vits"
"jerryuhoo/VTuberTalk" -> "PlayVoice/VI-SVC"
"jerryuhoo/VTuberTalk" -> "openvpi/audio-slicer"
"jerryuhoo/VTuberTalk" -> "luoyily/MoeTTS"
"jerryuhoo/VTuberTalk" -> "innnky/vits-japanese"
"jerryuhoo/VTuberTalk" -> "dtx525942103/vits-singing-voice-synthesis"
"jerryuhoo/VTuberTalk" -> "CjangCjengh/vits"
"jerryuhoo/VTuberTalk" -> "NaruseMioShirakana/MoeSS"
"jerryuhoo/VTuberTalk" -> "IceKyrin/sovits_guide"
"MoonInTheRiver/NeuralSVB" -> "SJTMusicTeam/Muskits"
"MoonInTheRiver/NeuralSVB" -> "Rongjiehuang/Multi-Singer"
"MoonInTheRiver/NeuralSVB" -> "M4Singer/M4Singer"
"MoonInTheRiver/NeuralSVB" -> "Rongjiehuang/ProDiff"
"MoonInTheRiver/NeuralSVB" -> "WelkinYang/Learn2Sing2.0"
"MoonInTheRiver/NeuralSVB" -> "YatingMusic/ddsp-singing-vocoders"
"MoonInTheRiver/NeuralSVB" -> "NVIDIA/BigVGAN"
"MoonInTheRiver/NeuralSVB" -> "zhangyongmao/VISinger2"
"MoonInTheRiver/NeuralSVB" -> "keonlee9420/DiffGAN-TTS"
"MoonInTheRiver/NeuralSVB" -> "NVIDIA/radtts"
"MoonInTheRiver/NeuralSVB" -> "Rongjiehuang/FastDiff"
"MoonInTheRiver/NeuralSVB" -> "yerfor/SyntaSpeech"
"MoonInTheRiver/NeuralSVB" -> "hhguo/MSMC-TTS"
"MoonInTheRiver/NeuralSVB" -> "hhguo/EA-SVC"
"MoonInTheRiver/NeuralSVB" -> "Rongjiehuang/GenerSpeech"
"rongcloud/websdk-demo" -> "rongcloud/rongcloud-web-im-sdk-v2"
"rongcloud/websdk-demo" -> "sealtalk/sealtalk-web"
"AlexandaJerry/vits-mandarin-biaobei" -> "rotten-work/vits-mandarin-windows"
"AlexandaJerry/vits-mandarin-biaobei" -> "UEhQZXI/vits_chinese"
"IceKyrin/sovits_f0_infer" -> "innnky/diff-svc"
"PlayVoice/VI-SVC" -> "guan-yuan/Awesome-Singing-Voice-Synthesis-and-Singing-Voice-Conversion"
"PlayVoice/VI-SVC" -> "OlaWod/FreeVC"
"PlayVoice/VI-SVC" -> "PlayVoice/VI-Speaker"
"PlayVoice/lora-svc" -> "PlayVoice/so-vits-svc-5.0"
"fishaudio/fish-diffusion" -> "innnky/diff-svc"
"fishaudio/fish-diffusion" -> "yxlllc/DDSP-SVC"
"fishaudio/fish-diffusion" -> "anonymous-pits/pits"
"fishaudio/fish-diffusion" -> "openvpi/diff-svc"
"fishaudio/fish-diffusion" -> "openvpi/audio-slicer"
"fishaudio/fish-diffusion" -> "zhaohui8969/VST_NetProcess-"
"fishaudio/fish-diffusion" -> "NaruseMioShirakana/MoeSS"
"fishaudio/fish-diffusion" -> "fishaudio/audio-preprocess"
"fishaudio/fish-diffusion" -> "WelkinYang/Learn2Sing2.0"
"fishaudio/fish-diffusion" -> "w4123/GenshinVoice"
"fishaudio/fish-diffusion" -> "M4Singer/M4Singer"
"fishaudio/fish-diffusion" -> "yxlllc/pc-ddsp"
"fishaudio/fish-diffusion" -> "xunmengshe/OpenUtau"
"fishaudio/fish-diffusion" -> "OlaWod/FreeVC"
"fishaudio/fish-diffusion" -> "MasayaKawamura/MB-iSTFT-VITS"
"flutydeer/audio-slicer" -> "openvpi/audio-slicer"
"flutydeer/audio-slicer" -> "fishaudio/fish-diffusion"
"openvpi/audio-slicer" -> "flutydeer/audio-slicer"
"openvpi/audio-slicer" -> "fishaudio/fish-diffusion"
"openvpi/audio-slicer" -> "yxlllc/DDSP-SVC"
"openvpi/audio-slicer" -> "IceKyrin/sovits_guide"
"openvpi/audio-slicer" -> "PlayVoice/lora-svc"
"openvpi/audio-slicer" -> "CjangCjengh/vits"
"openvpi/diff-svc" -> "fishaudio/audio-preprocess"
"xunmengshe/OpenUtau" -> "openvpi/diff-svc"
"yeyupiaoling/VoiceprintRecognition-Tensorflow" -> "yeyupiaoling/Kersa-Speaker-Recognition"
"yeyupiaoling/VoiceprintRecognition-Tensorflow" -> "yeyupiaoling/VoiceprintRecognition-Pytorch"
"yeyupiaoling/VoiceprintRecognition-Tensorflow" -> "SunYanCN/Voiceprint-Recognition"
"yeyupiaoling/VoiceprintRecognition-Tensorflow" -> "Kevinnan-teen/Speaker-Recognition"
"yeyupiaoling/VoiceprintRecognition-Tensorflow" -> "fighting41love/zhvoice"
"yeyupiaoling/VoiceprintRecognition-Tensorflow" -> "LCF2764/speaker-feature-extractor"
"yeyupiaoling/VoiceprintRecognition-Tensorflow" -> "yeyupiaoling/AudioClassification-Tensorflow"
"SiddGururani/Pytorch-TDNN" -> "kefirski/pytorch_TDNN"
"SiddGururani/Pytorch-TDNN" -> "Dannynis/xvector_pytorch"
"SiddGururani/Pytorch-TDNN" -> "jonasvdd/TDNN"
"SiddGururani/Pytorch-TDNN" -> "cvqluu/TDNN"
"jtkim-kaist/VAD" -> "hcmlab/vadnet"
"jtkim-kaist/VAD" -> "marsbroshok/VAD-python"
"jtkim-kaist/VAD" -> "wiseman/py-webrtcvad"
"jtkim-kaist/VAD" -> "jtkim-kaist/Speech-enhancement" ["e"=1]
"jtkim-kaist/VAD" -> "filippogiruzzi/voice_activity_detection"
"jtkim-kaist/VAD" -> "eesungkim/Voice_Activity_Detector"
"jtkim-kaist/VAD" -> "nicklashansen/voice-activity-detection"
"jtkim-kaist/VAD" -> "anicolson/DeepXi" ["e"=1]
"jtkim-kaist/VAD" -> "mindorii/kws" ["e"=1]
"jtkim-kaist/VAD" -> "snakers4/silero-vad"
"jtkim-kaist/VAD" -> "santi-pdp/segan" ["e"=1]
"jtkim-kaist/VAD" -> "fgnt/nara_wpe" ["e"=1]
"jtkim-kaist/VAD" -> "microsoft/AEC-Challenge" ["e"=1]
"jtkim-kaist/VAD" -> "aliutkus/speechmetrics" ["e"=1]
"jtkim-kaist/VAD" -> "nanahou/Awesome-Speech-Enhancement" ["e"=1]
"kefirski/pytorch_TDNN" -> "SiddGururani/Pytorch-TDNN"
"kefirski/pytorch_TDNN" -> "jonasvdd/TDNN"
"yuyq96/D-TDNN" -> "sasv-challenge/SASVC2022_Baseline" ["e"=1]
"Kurisu-Preston/AI-aqua-vc" -> "zhaohui8969/VST_NetProcess-"
"LiuRoy/Pinyin_Demo" -> "letiantian/Pinyin2Hanzi"
"LiuRoy/Pinyin_Demo" -> "crownpku/Somiao-Pinyin" ["e"=1]
"Baidu-AIP/speech-vad-demo" -> "dpirch/libfvad"
"Baidu-AIP/speech-vad-demo" -> "cpuimage/WebRTC_VAD" ["e"=1]
"Baidu-AIP/speech-vad-demo" -> "shichaog/WebRTC-audio-processing" ["e"=1]
"Baidu-AIP/speech-vad-demo" -> "open-speech/speech-aligner"
"Baidu-AIP/speech-vad-demo" -> "XiaoMi/kaldi-onnx"
"Baidu-AIP/speech-vad-demo" -> "cpuimage/WebRTC_NS" ["e"=1]
"Baidu-AIP/speech-vad-demo" -> "marsbroshok/VAD-python"
"Baidu-AIP/speech-vad-demo" -> "xiangxyq/xiangxyq_kaldi"
"Baidu-AIP/speech-vad-demo" -> "jtkim-kaist/VAD"
"Baidu-AIP/speech-vad-demo" -> "ZhengkunTian/OpenTransformer"
"Baidu-AIP/speech-vad-demo" -> "mobvoi/wenet"
"Baidu-AIP/speech-vad-demo" -> "xiongyihui/python-webrtc-audio-processing" ["e"=1]
"Baidu-AIP/speech-vad-demo" -> "wiseman/py-webrtcvad"
"Baidu-AIP/speech-vad-demo" -> "athena-team/athena-signal" ["e"=1]
"Baidu-AIP/speech-vad-demo" -> "hcmlab/vadnet"
"XzaiCloud/AI-Vtuber" -> "XzaiCloud/AI-Vtuber-Kun"
"XzaiCloud/AI-Vtuber" -> "cdfmlr/muvtuber"
"Diamondfan/CTC_pytorch" -> "lingochamp/kaldi-ctc"
"Diamondfan/CTC_pytorch" -> "awni/transducer"
"Diamondfan/CTC_pytorch" -> "hirofumi0810/neural_sp"
"Diamondfan/CTC_pytorch" -> "jinserk/pytorch-asr"
"Diamondfan/CTC_pytorch" -> "XiaoMi/kaldi-onnx"
"Diamondfan/CTC_pytorch" -> "gentaiscool/end2end-asr-pytorch"
"Diamondfan/CTC_pytorch" -> "HawkAaron/E2E-ASR"
"Diamondfan/CTC_pytorch" -> "ZhengkunTian/rnn-transducer"
"Diamondfan/CTC_pytorch" -> "kaituoxu/Speech-Transformer"
"Diamondfan/CTC_pytorch" -> "Sundy1219/eesen-for-thchs30"
"Diamondfan/CTC_pytorch" -> "HawkAaron/RNN-Transducer"
"Diamondfan/CTC_pytorch" -> "tbright17/kaldi-dnn-ali-gop"
"HawkAaron/E2E-ASR" -> "HawkAaron/RNN-Transducer"
"HawkAaron/E2E-ASR" -> "awni/transducer"
"HawkAaron/E2E-ASR" -> "HawkAaron/warp-transducer"
"HawkAaron/E2E-ASR" -> "mdangschat/ctc-asr"
"HawkAaron/E2E-ASR" -> "gentaiscool/end2end-asr-pytorch"
"HawkAaron/E2E-ASR" -> "ZhengkunTian/rnn-transducer"
"HawkAaron/E2E-ASR" -> "chenjiasheng/mwer"
"HawkAaron/E2E-ASR" -> "MarkWuNLP/SemanticMask"
"awni/speech" -> "HawkAaron/E2E-ASR"
"awni/speech" -> "hirofumi0810/neural_sp"
"awni/speech" -> "HawkAaron/RNN-Transducer"
"awni/speech" -> "HawkAaron/warp-transducer"
"awni/speech" -> "SeanNaren/deepspeech.pytorch"
"awni/speech" -> "hirofumi0810/tensorflow_end2end_speech_recognition"
"awni/speech" -> "srvk/eesen"
"awni/speech" -> "Alexander-H-Liu/End-to-end-ASR-Pytorch"
"awni/speech" -> "mravanelli/pytorch-kaldi"
"awni/speech" -> "awni/transducer"
"awni/speech" -> "kaituoxu/Speech-Transformer"
"awni/speech" -> "gentaiscool/end2end-asr-pytorch"
"awni/speech" -> "pykaldi/pykaldi"
"awni/speech" -> "syhw/wer_are_we"
"awni/speech" -> "ZhengkunTian/rnn-transducer"
"freewym/espresso" -> "YiwenShaoStephen/pychain"
"freewym/espresso" -> "hirofumi0810/neural_sp"
"freewym/espresso" -> "lhotse-speech/lhotse"
"freewym/espresso" -> "k2-fsa/k2"
"freewym/espresso" -> "jzlianglu/pykaldi2"
"freewym/espresso" -> "pykaldi/pykaldi"
"freewym/espresso" -> "tencent-ailab/pika"
"freewym/espresso" -> "mobvoi/wenet"
"freewym/espresso" -> "thu-spmi/CAT"
"freewym/espresso" -> "cywang97/StreamingTransformer"
"freewym/espresso" -> "kaituoxu/Speech-Transformer"
"freewym/espresso" -> "vesis84/kaldi-io-for-python"
"freewym/espresso" -> "SpeechColab/GigaSpeech"
"freewym/espresso" -> "mravanelli/pytorch-kaldi"
"freewym/espresso" -> "athena-team/athena" ["e"=1]
"githubharald/CTCDecoder" -> "githubharald/CTCWordBeamSearch"
"githubharald/CTCDecoder" -> "parlance/ctcdecode"
"githubharald/CTCDecoder" -> "corticph/prefix-beam-search"
"githubharald/CTCDecoder" -> "srvk/eesen"
"githubharald/CTCDecoder" -> "SeanNaren/warp-ctc" ["e"=1]
"githubharald/CTCDecoder" -> "HawkAaron/warp-transducer"
"githubharald/CTCDecoder" -> "hirofumi0810/neural_sp"
"githubharald/CTCDecoder" -> "thu-spmi/CAT"
"githubharald/CTCDecoder" -> "summerlvsong/Aggregation-Cross-Entropy" ["e"=1]
"githubharald/CTCDecoder" -> "mobvoi/wenet"
"githubharald/CTCDecoder" -> "kaituoxu/Speech-Transformer"
"githubharald/CTCDecoder" -> "YiwenShaoStephen/pychain"
"githubharald/CTCDecoder" -> "k2-fsa/k2"
"githubharald/CTCDecoder" -> "awni/transducer"
"githubharald/CTCDecoder" -> "Diamondfan/CTC_pytorch"
"srvk/eesen" -> "lingochamp/kaldi-ctc"
"srvk/eesen" -> "yajiemiao/eesen"
"srvk/eesen" -> "k2-fsa/k2"
"srvk/eesen" -> "thu-spmi/CAT"
"srvk/eesen" -> "mravanelli/pytorch-kaldi"
"srvk/eesen" -> "pykaldi/pykaldi"
"srvk/eesen" -> "vesis84/kaldi-io-for-python"
"srvk/eesen" -> "YiwenShaoStephen/pychain"
"srvk/eesen" -> "k2-fsa/icefall"
"srvk/eesen" -> "syhw/wer_are_we"
"srvk/eesen" -> "hirofumi0810/neural_sp"
"srvk/eesen" -> "aishell-foundation/DaCiDian"
"srvk/eesen" -> "HawkAaron/warp-transducer"
"srvk/eesen" -> "alumae/kaldi-gstreamer-server"
"srvk/eesen" -> "rizar/attention-lvcsr"
"rongcloud/server-sdk-java" -> "sealtalk/sealtalk-server"
"rongcloud/server-sdk-java" -> "rongcloud/demo-app-imlib-live-chatroom-android"
"rongcloud/server-sdk-java" -> "rongcloud/server-sdk-nodejs"
"hcmlab/vadnet" -> "jtkim-kaist/VAD"
"hcmlab/vadnet" -> "marsbroshok/VAD-python"
"hcmlab/vadnet" -> "filippogiruzzi/voice_activity_detection"
"hcmlab/vadnet" -> "nicklashansen/voice-activity-detection"
"hcmlab/vadnet" -> "mindorii/kws" ["e"=1]
"hcmlab/vadnet" -> "wiseman/py-webrtcvad"
"hcmlab/vadnet" -> "wangshub/python-vad"
"hcmlab/vadnet" -> "yongxuUSTC/sednn" ["e"=1]
"hcmlab/vadnet" -> "amsehili/auditok"
"hcmlab/vadnet" -> "snakers4/silero-vad"
"hcmlab/vadnet" -> "funcwj/setk" ["e"=1]
"hcmlab/vadnet" -> "haoxiangsnr/Wave-U-Net-for-Speech-Enhancement" ["e"=1]
"hcmlab/vadnet" -> "anicolson/DeepXi" ["e"=1]
"hcmlab/vadnet" -> "dpirch/libfvad"
"hcmlab/vadnet" -> "iariav/End-to-End-VAD"
"mozillazg/pinyin-data" -> "mozillazg/phrase-pinyin-data"
"mozillazg/pinyin-data" -> "letiantian/ChineseTone"
"mozillazg/pinyin-data" -> "mozillazg/python-pinyin" ["e"=1]
"mozillazg/pinyin-data" -> "mozillazg/go-pinyin" ["e"=1]
"mozillazg/pinyin-data" -> "kfcd/chaizi" ["e"=1]
"mozillazg/pinyin-data" -> "kakaobrain/g2pM"
"mozillazg/pinyin-data" -> "speechio/chinese_text_normalization"
"mozillazg/pinyin-data" -> "hjzin/PolyphoneDisambiguation"
"mozillazg/pinyin-data" -> "Kyubyong/g2pC"
"mozillazg/pinyin-data" -> "letiantian/Pinyin2Hanzi"
"mozillazg/pinyin-data" -> "TencentGameMate/chinese_speech_pretrain"
"mozillazg/pinyin-data" -> "aishell-foundation/DaCiDian"
"mozillazg/pinyin-data" -> "Jackiexiao/MTTS"
"mozillazg/pinyin-data" -> "xinglie/pinyin" ["e"=1]
"mozillazg/pinyin-data" -> "thuhcsi/Crystal"
"TaoRuijie/ECAPA-TDNN" -> "clovaai/voxceleb_trainer"
"TaoRuijie/ECAPA-TDNN" -> "zyzisyz/mfa_conformer"
"TaoRuijie/ECAPA-TDNN" -> "Snowdar/asv-subtools"
"TaoRuijie/ECAPA-TDNN" -> "wenet-e2e/wespeaker"
"TaoRuijie/ECAPA-TDNN" -> "TaoRuijie/Loss-Gated-Learning"
"TaoRuijie/ECAPA-TDNN" -> "ranchlai/speaker-verification"
"TaoRuijie/ECAPA-TDNN" -> "yeyupiaoling/VoiceprintRecognition-Pytorch"
"TaoRuijie/ECAPA-TDNN" -> "lawlict/ECAPA-TDNN"
"TaoRuijie/ECAPA-TDNN" -> "microsoft/UniSpeech"
"TaoRuijie/ECAPA-TDNN" -> "nikvaessen/w2v2-speaker"
"TaoRuijie/ECAPA-TDNN" -> "manojpamk/pytorch_xvectors"
"mobvoi/wenet" -> "cywang97/StreamingTransformer"
"mobvoi/wenet" -> "hirofumi0810/neural_sp"
"mobvoi/wenet" -> "xingchensong/speech-recognition-papers"
"mobvoi/wenet" -> "tencent-ailab/pika"
"mobvoi/wenet" -> "k2-fsa/k2"
"mobvoi/wenet" -> "ZhengkunTian/OpenTransformer"
"mobvoi/wenet" -> "thu-spmi/CAT"
"mobvoi/wenet" -> "speechio/chinese_text_normalization"
"mobvoi/wenet" -> "athena-team/athena" ["e"=1]
"mobvoi/wenet" -> "SpeechColab/GigaSpeech"
"mobvoi/wenet" -> "XiaoMi/kaldi-onnx"
"mobvoi/wenet" -> "lhotse-speech/lhotse"
"mobvoi/wenet" -> "kaituoxu/Speech-Transformer"
"mobvoi/wenet" -> "k2-fsa/icefall"
"mobvoi/wenet" -> "freewym/espresso"
"Breta01/handwriting-ocr" -> "awslabs/handwritten-text-recognition-for-apache-mxnet"
"Breta01/handwriting-ocr" -> "githubharald/SimpleHTR"
"Breta01/handwriting-ocr" -> "sushant097/Handwritten-Line-Text-Recognition-using-Deep-Learning-with-Tensorflow"
"Breta01/handwriting-ocr" -> "0x454447415244/HandwritingRecognitionSystem"
"Breta01/handwriting-ocr" -> "zacharywhitley/awesome-ocr" ["e"=1]
"Breta01/handwriting-ocr" -> "arthurflor23/handwritten-text-recognition"
"Breta01/handwriting-ocr" -> "Aniket025/Medical-Prescription-OCR"
"Breta01/handwriting-ocr" -> "arthurflor23/text-segmentation"
"Breta01/handwriting-ocr" -> "Grzego/handwriting-generation" ["e"=1]
"Breta01/handwriting-ocr" -> "johnsmithm/handwritten-tf-1.0"
"Breta01/handwriting-ocr" -> "githubharald/CTCWordBeamSearch"
"Breta01/handwriting-ocr" -> "ahmetozlu/signature_extractor"
"Breta01/handwriting-ocr" -> "emedvedev/attention-ocr" ["e"=1]
"Breta01/handwriting-ocr" -> "doxakis/form-segmentation"
"Breta01/handwriting-ocr" -> "cwig/start_follow_read"
"sealtalk/sealtalk-web" -> "sealtalk/sealtalk-server"
"sealtalk/sealtalk-web" -> "rongcloud/rongcloud-web-im-sdk-v2"
"sealtalk/sealtalk-web" -> "sealtalk/sealtalk-desktop"
"sealtalk/sealtalk-web" -> "rongcloud/websdk-demo"
"sealtalk/sealtalk-web" -> "rongcloud/rongcloud-web-im-widget"
"baidu-research/ba-dls-deepspeech" -> "NervanaSystems/deepspeech"
"baidu-research/ba-dls-deepspeech" -> "srvk/eesen"
"baidu-research/ba-dls-deepspeech" -> "SeanNaren/deepspeech.torch"
"baidu-research/ba-dls-deepspeech" -> "amaas/stanford-ctc"
"baidu-research/ba-dls-deepspeech" -> "igormq/asr-study"
"baidu-research/ba-dls-deepspeech" -> "lingochamp/kaldi-ctc"
"baidu-research/ba-dls-deepspeech" -> "cmusphinx/g2p-seq2seq"
"baidu-research/ba-dls-deepspeech" -> "pannous/tensorflow-speech-recognition"
"baidu-research/ba-dls-deepspeech" -> "igormq/ctc_tensorflow_example"
"baidu-research/ba-dls-deepspeech" -> "alumae/kaldi-gstreamer-server"
"baidu-research/ba-dls-deepspeech" -> "SeanNaren/deepspeech.pytorch"
"baidu-research/ba-dls-deepspeech" -> "samsungsds-rnd/deepspeech.mxnet"
"baidu-research/ba-dls-deepspeech" -> "dspavankumar/keras-kaldi"
"baidu-research/ba-dls-deepspeech" -> "syhw/wer_are_we"
"baidu-research/ba-dls-deepspeech" -> "PaddlePaddle/DeepSpeech"
"easemob/sdkexamples-android" -> "easemob/sdkdemoapp3.0_android"
"easemob/sdkexamples-android" -> "rongcloud/demo-app-android"
"easemob/sdkexamples-android" -> "rongcloud/demo-ui-android"
"rwth-i6/returnn" -> "rwth-i6/returnn-experiments"
"rwth-i6/returnn" -> "rwth-i6/sisyphus"
"rwth-i6/returnn" -> "cywang97/StreamingTransformer"
"rwth-i6/returnn" -> "HawkAaron/warp-transducer"
"rwth-i6/returnn" -> "rwth-i6/rasr"
"rwth-i6/returnn" -> "hirofumi0810/tensorflow_end2end_speech_recognition"
"rwth-i6/returnn" -> "hirofumi0810/neural_sp"
"rwth-i6/returnn" -> "rizar/attention-lvcsr"
"rwth-i6/returnn" -> "facebookresearch/libri-light"
"rwth-i6/returnn" -> "tencent-ailab/pika"
"rwth-i6/returnn" -> "jpuigcerver/Laia"
"rwth-i6/returnn" -> "freewym/espresso"
"rwth-i6/returnn" -> "YiwenShaoStephen/pychain"
"rwth-i6/returnn" -> "xingchensong/speech-recognition-papers"
"rwth-i6/returnn" -> "awni/transducer"
"kuangdd/ttskit" -> "ranchlai/mandarin-tts"
"kuangdd/ttskit" -> "lturing/tacotronv2_wavernn_chinese"
"kuangdd/ttskit" -> "Jackiexiao/zhtts"
"kuangdd/ttskit" -> "aidreamwin/TTS-Clone-Chinese"
"kuangdd/ttskit" -> "atomicoo/FCH-TTS"
"kuangdd/ttskit" -> "JasonWei512/Tacotron-2-Chinese"
"kuangdd/ttskit" -> "LuckyHookin/edge-TTS-record" ["e"=1]
"kuangdd/ttskit" -> "TensorSpeech/TensorFlowTTS"
"kuangdd/ttskit" -> "PaddlePaddle/Parakeet"
"kuangdd/ttskit" -> "cnlinxi/book-text-to-speech"
"kuangdd/ttskit" -> "fighting41love/zhvoice"
"kuangdd/ttskit" -> "chenkui164/FastASR"
"kuangdd/ttskit" -> "ming024/FastSpeech2"
"kuangdd/ttskit" -> "shibing624/parrots"
"kuangdd/ttskit" -> "PlayVoice/vits_chinese"
"fighting41love/zhvoice" -> "yeyupiaoling/VoiceprintRecognition-Pytorch"
"fighting41love/zhvoice" -> "yeyupiaoling/VoiceprintRecognition-Tensorflow"
"fighting41love/zhvoice" -> "hpc203/virtual_try_on_use_deep_learning"
"fighting41love/zhvoice" -> "double22a/speech_dataset"
"fighting41love/zhvoice" -> "yeyupiaoling/VoiceprintRecognition-PaddlePaddle" ["e"=1]
"fighting41love/zhvoice" -> "yeyupiaoling/VoiceprintRecognition-Keras"
"fighting41love/zhvoice" -> "microsoft/NeuralSpeech"
"yeyupiaoling/VoiceprintRecognition-Pytorch" -> "fighting41love/zhvoice"
"yeyupiaoling/VoiceprintRecognition-Pytorch" -> "yeyupiaoling/VoiceprintRecognition-Tensorflow"
"yeyupiaoling/VoiceprintRecognition-Pytorch" -> "TaoRuijie/ECAPA-TDNN"
"yeyupiaoling/VoiceprintRecognition-Pytorch" -> "yeyupiaoling/AudioClassification-Pytorch"
"yeyupiaoling/VoiceprintRecognition-Pytorch" -> "Kevinnan-teen/Speaker-Recognition"
"yeyupiaoling/VoiceprintRecognition-Pytorch" -> "yeyupiaoling/MASR" ["e"=1]
"yeyupiaoling/VoiceprintRecognition-Pytorch" -> "clovaai/voxceleb_trainer"
"yeyupiaoling/VoiceprintRecognition-Pytorch" -> "zyzisyz/mfa_conformer"
"yeyupiaoling/VoiceprintRecognition-Pytorch" -> "yeyupiaoling/PPASR" ["e"=1]
"yeyupiaoling/VoiceprintRecognition-Pytorch" -> "Snowdar/asv-subtools"
"yeyupiaoling/VoiceprintRecognition-Pytorch" -> "yeyupiaoling/VoiceprintRecognition-PaddlePaddle" ["e"=1]
"yeyupiaoling/VoiceprintRecognition-Pytorch" -> "TaoRuijie/Loss-Gated-Learning"
"cmusphinx/cmudict" -> "cmusphinx/g2p-seq2seq"
"cmusphinx/cmudict" -> "cmusphinx/cmudict-tools"
"cmusphinx/cmudict" -> "Kyubyong/g2p"
"cmusphinx/cmudict" -> "AdolfVonKleist/Phonetisaurus"
"cmusphinx/cmudict" -> "cmusphinx/sphinxbase"
"cmusphinx/cmudict" -> "sequitur-g2p/sequitur-g2p"
"cmusphinx/cmudict" -> "prosegrinder/python-cmudict"
"cmusphinx/cmudict" -> "cmusphinx/sphinxtrain"
"cmusphinx/cmudict" -> "Jackiexiao/MTTS"
"cmusphinx/cmudict" -> "kakaobrain/g2pM"
"cmusphinx/cmudict" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"cmusphinx/cmudict" -> "open-dict-data/ipa-dict"
"cmusphinx/cmudict" -> "speechio/chinese_text_normalization"
"cmusphinx/cmudict" -> "cmusphinx/pocketsphinx-android"
"cmusphinx/cmudict" -> "bootphon/phonemizer"
"idebtor/nowic" -> "hongshin/DiscreteMath"
"idebtor/nowic" -> "idebtor/JoyAI"
"idebtor/nowic" -> "hongshin/OperatingSystem"
"FlorianKrey/DNC" -> "nryant/dscore"
"FlorianKrey/DNC" -> "HuangZiliAndy/RPNSD"
"FlorianKrey/DNC" -> "DonkeyShot21/uis-rnn-sml"
"HawkAaron/warp-transducer" -> "HawkAaron/RNN-Transducer"
"HawkAaron/warp-transducer" -> "1ytic/warp-rnnt"
"HawkAaron/warp-transducer" -> "awni/transducer"
"HawkAaron/warp-transducer" -> "ZhengkunTian/rnn-transducer"
"HawkAaron/warp-transducer" -> "HawkAaron/E2E-ASR"
"HawkAaron/warp-transducer" -> "cywang97/StreamingTransformer"
"HawkAaron/warp-transducer" -> "noahchalifour/rnnt-speech-recognition"
"HawkAaron/warp-transducer" -> "YiwenShaoStephen/pychain"
"HawkAaron/warp-transducer" -> "hirofumi0810/neural_sp"
"HawkAaron/warp-transducer" -> "tencent-ailab/pika"
"HawkAaron/warp-transducer" -> "thu-spmi/CAT"
"HawkAaron/warp-transducer" -> "lingochamp/kaldi-ctc"
"HawkAaron/warp-transducer" -> "lhotse-speech/lhotse"
"HawkAaron/warp-transducer" -> "danpovey/fast_rnnt"
"HawkAaron/warp-transducer" -> "csukuangfj/optimized_transducer"
"Mashiro009/wenet-online-decoder-onnx" -> "Mashiro009/wenet-onnx"
"TeaPoly/CTC-OptimizedLoss" -> "chenjiasheng/mwer"
"alibaba-damo-academy/FunASR" -> "chenkui164/FastASR"
"alibaba-damo-academy/FunASR" -> "RapidAI/RapidASR"
"alibaba-damo-academy/FunASR" -> "SpeechColab/Leaderboard"
"alibaba-damo-academy/FunASR" -> "speechio/chinese_text_normalization"
"alibaba-damo-academy/FunASR" -> "csukuangfj/kaldifeat"
"alibaba-damo-academy/FunASR" -> "jctian98/e2e_lfmmi"
"alibaba-damo-academy/FunASR" -> "cywang97/StreamingTransformer"
"alibaba-damo-academy/FunASR" -> "thu-spmi/CAT"
"alibaba-damo-academy/FunASR" -> "wenet-e2e/wespeaker"
"alibaba-damo-academy/FunASR" -> "k2-fsa/icefall"
"alibaba-damo-academy/FunASR" -> "wenet-e2e/wekws" ["e"=1]
"alibaba-damo-academy/FunASR" -> "k2-fsa/k2"
"alibaba-damo-academy/FunASR" -> "mobvoi/wenet"
"alibaba-damo-academy/FunASR" -> "wenet-e2e/WeTextProcessing"
"alibaba-damo-academy/FunASR" -> "lhotse-speech/lhotse"
"awni/transducer" -> "HawkAaron/RNN-Transducer"
"awni/transducer" -> "HawkAaron/warp-transducer"
"awni/transducer" -> "HawkAaron/E2E-ASR"
"awni/transducer" -> "1ytic/warp-rnnt"
"awni/transducer" -> "ZhengkunTian/rnn-transducer"
"awni/transducer" -> "YiwenShaoStephen/pychain"
"awni/transducer" -> "danpovey/fast_rnnt"
"by2101/OpenASR" -> "ZhengkunTian/OpenTransformer"
"chenjiasheng/mwer" -> "TeaPoly/CTC-OptimizedLoss"
"chenkui164/FastASR" -> "RapidAI/RapidASR"
"chenkui164/FastASR" -> "alibaba-damo-academy/FunASR"
"chenkui164/FastASR" -> "k2-fsa/sherpa-ncnn"
"chenkui164/FastASR" -> "jctian98/e2e_lfmmi"
"chenkui164/FastASR" -> "yeyupiaoling/PunctuationModel" ["e"=1]
"chenkui164/FastASR" -> "speechio/chinese_text_normalization"
"chenkui164/FastASR" -> "huismiling/wenet_trt8" ["e"=1]
"chenkui164/FastASR" -> "TencentGameMate/chinese_speech_pretrain"
"chenkui164/FastASR" -> "k2-fsa/sherpa-onnx"
"chenkui164/FastASR" -> "danpovey/fast_rnnt"
"facebookresearch/CPC_audio" -> "andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning"
"facebookresearch/CPC_audio" -> "facebookresearch/WavAugment"
"facebookresearch/CPC_audio" -> "facebookresearch/libri-light"
"facebookresearch/CPC_audio" -> "iamyuanchung/Autoregressive-Predictive-Coding"
"facebookresearch/CPC_audio" -> "bootphon/zerospeech2021_baseline"
"facebookresearch/CPC_audio" -> "ivanvovk/WaveGrad"
"facebookresearch/CPC_audio" -> "awslabs/speech-representations"
"facebookresearch/CPC_audio" -> "bshall/VectorQuantizedCPC"
"facebookresearch/CPC_audio" -> "jefflai108/Contrastive-Predictive-Coding-PyTorch" ["e"=1]
"facebookresearch/CPC_audio" -> "hirofumi0810/neural_sp"
"facebookresearch/CPC_audio" -> "SpeechColab/GigaSpeech"
"facebookresearch/CPC_audio" -> "k2-fsa/snowfall"
"facebookresearch/CPC_audio" -> "microsoft/UniSpeech"
"hirofumi0810/neural_sp" -> "cywang97/StreamingTransformer"
"hirofumi0810/neural_sp" -> "ZhengkunTian/OpenTransformer"
"hirofumi0810/neural_sp" -> "mobvoi/wenet"
"hirofumi0810/neural_sp" -> "tencent-ailab/pika"
"hirofumi0810/neural_sp" -> "freewym/espresso"
"hirofumi0810/neural_sp" -> "kaituoxu/Speech-Transformer"
"hirofumi0810/neural_sp" -> "ZhengkunTian/rnn-transducer"
"hirofumi0810/neural_sp" -> "xingchensong/speech-recognition-papers"
"hirofumi0810/neural_sp" -> "YiwenShaoStephen/pychain"
"hirofumi0810/neural_sp" -> "HawkAaron/warp-transducer"
"hirofumi0810/neural_sp" -> "noahchalifour/rnnt-speech-recognition"
"hirofumi0810/neural_sp" -> "thu-spmi/CAT"
"hirofumi0810/neural_sp" -> "k2-fsa/k2"
"hirofumi0810/neural_sp" -> "SpeechColab/GigaSpeech"
"hirofumi0810/neural_sp" -> "lhotse-speech/lhotse"
"jctian98/e2e_lfmmi" -> "tencent-ailab/3m-asr"
"jctian98/e2e_lfmmi" -> "danpovey/fast_rnnt"
"jctian98/e2e_lfmmi" -> "cywang97/StreamingTransformer"
"jctian98/e2e_lfmmi" -> "burchim/EfficientConformer"
"jctian98/e2e_lfmmi" -> "tencent-ailab/pika"
"jctian98/e2e_lfmmi" -> "TeaPoly/CTC-OptimizedLoss"
"jctian98/e2e_lfmmi" -> "speechio/BigCiDian"
"jctian98/e2e_lfmmi" -> "YiwenShaoStephen/pychain"
"jctian98/e2e_lfmmi" -> "alibaba-damo-academy/FunASR"
"k2-fsa/snowfall" -> "k2-fsa/icefall"
"k2-fsa/snowfall" -> "danpovey/k2"
"k2-fsa/snowfall" -> "lhotse-speech/lhotse"
"k2-fsa/snowfall" -> "k2-fsa/sherpa"
"k2-fsa/snowfall" -> "pzelasko/lhotse"
"k2-fsa/snowfall" -> "k2-fsa/k2"
"k2-fsa/snowfall" -> "csukuangfj/kaldifeat"
"k2-fsa/snowfall" -> "csukuangfj/kaldilm"
"k2-fsa/snowfall" -> "YiwenShaoStephen/pychain"
"k2-fsa/snowfall" -> "danpovey/fast_rnnt"
"k2-fsa/snowfall" -> "theblackcat102/Online-Speech-Recognition"
"kaituoxu/E6870" -> "placebokkk/e6870"
"lhotse-speech/lhotse" -> "k2-fsa/k2"
"lhotse-speech/lhotse" -> "k2-fsa/icefall"
"lhotse-speech/lhotse" -> "k2-fsa/snowfall"
"lhotse-speech/lhotse" -> "tencent-ailab/pika"
"lhotse-speech/lhotse" -> "SpeechColab/GigaSpeech"
"lhotse-speech/lhotse" -> "k2-fsa/sherpa"
"lhotse-speech/lhotse" -> "vesis84/kaldi-io-for-python"
"lhotse-speech/lhotse" -> "Snowdar/asv-subtools"
"lhotse-speech/lhotse" -> "freewym/espresso"
"lhotse-speech/lhotse" -> "HawkAaron/warp-transducer"
"lhotse-speech/lhotse" -> "hirofumi0810/neural_sp"
"lhotse-speech/lhotse" -> "nttcslab-sp/kaldiio"
"lhotse-speech/lhotse" -> "mobvoi/wenet"
"lhotse-speech/lhotse" -> "facebookresearch/WavAugment"
"lhotse-speech/lhotse" -> "csukuangfj/kaldifeat"
"noahchalifour/rnnt-speech-recognition" -> "ZhengkunTian/rnn-transducer"
"noahchalifour/rnnt-speech-recognition" -> "theblackcat102/Online-Speech-Recognition"
"noahchalifour/rnnt-speech-recognition" -> "HawkAaron/warp-transducer"
"noahchalifour/rnnt-speech-recognition" -> "cywang97/StreamingTransformer"
"noahchalifour/rnnt-speech-recognition" -> "hirofumi0810/neural_sp"
"noahchalifour/rnnt-speech-recognition" -> "HawkAaron/RNN-Transducer"
"noahchalifour/rnnt-speech-recognition" -> "csukuangfj/optimized_transducer"
"noahchalifour/rnnt-speech-recognition" -> "tencent-ailab/pika"
"noahchalifour/rnnt-speech-recognition" -> "xingchensong/speech-recognition-papers"
"noahchalifour/rnnt-speech-recognition" -> "1ytic/warp-rnnt"
"noahchalifour/rnnt-speech-recognition" -> "ZhengkunTian/OpenTransformer"
"noahchalifour/rnnt-speech-recognition" -> "thu-spmi/CAT"
"noahchalifour/rnnt-speech-recognition" -> "iamjanvijay/rnnt_decoder_cuda"
"noahchalifour/rnnt-speech-recognition" -> "Z-yq/TensorflowASR"
"noahchalifour/rnnt-speech-recognition" -> "awni/transducer"
"nttcslab-sp/kaldiio" -> "vesis84/kaldi-io-for-python"
"nttcslab-sp/kaldiio" -> "YiwenShaoStephen/pychain"
"nttcslab-sp/kaldiio" -> "jzlianglu/pykaldi2"
"nttcslab-sp/kaldiio" -> "lhotse-speech/lhotse"
"nttcslab-sp/kaldiio" -> "k2-fsa/snowfall"
"nttcslab-sp/kaldiio" -> "hitachi-speech/EEND"
"nttcslab-sp/kaldiio" -> "nryant/dscore"
"nttcslab-sp/kaldiio" -> "k2-fsa/k2"
"nttcslab-sp/kaldiio" -> "thu-spmi/CAT"
"nttcslab-sp/kaldiio" -> "hirofumi0810/neural_sp"
"opendcd/opendcd" -> "jpuigcerver/kaldi-decoders"
"philipperemy/speaker-change-detection" -> "hedonistrh/SpChangeDetect"
"philipperemy/speaker-change-detection" -> "yinruiqing/change_detection"
"rwth-i6/returnn-experiments" -> "rwth-i6/returnn"
"rwth-i6/returnn-experiments" -> "rwth-i6/sisyphus"
"rwth-i6/returnn-experiments" -> "nttcslab-sp/torchain"
"rwth-i6/returnn-experiments" -> "cywang97/StreamingTransformer"
"rwth-i6/returnn-experiments" -> "YiwenShaoStephen/pychain_example"
"rwth-i6/returnn-experiments" -> "YiwenShaoStephen/pychain"
"speechio/chinese_text_normalization" -> "speechio/BigCiDian"
"speechio/chinese_text_normalization" -> "kakaobrain/g2pM"
"speechio/chinese_text_normalization" -> "aishell-foundation/DaCiDian"
"speechio/chinese_text_normalization" -> "mobvoi/wenet"
"speechio/chinese_text_normalization" -> "cywang97/StreamingTransformer"
"speechio/chinese_text_normalization" -> "alibaba-damo-academy/FunASR"
"speechio/chinese_text_normalization" -> "wenet-e2e/WeTextProcessing"
"speechio/chinese_text_normalization" -> "thuhcsi/Crystal"
"speechio/chinese_text_normalization" -> "TencentGameMate/chinese_speech_pretrain"
"speechio/chinese_text_normalization" -> "SpeechColab/Leaderboard"
"speechio/chinese_text_normalization" -> "xcmyz/speech-synthesis-paper"
"speechio/chinese_text_normalization" -> "k2-fsa/k2"
"speechio/chinese_text_normalization" -> "k2-fsa/icefall"
"speechio/chinese_text_normalization" -> "SpeechColab/GigaSpeech"
"speechio/chinese_text_normalization" -> "wenet-e2e/wespeaker"
"tencent-ailab/pika" -> "cywang97/StreamingTransformer"
"tencent-ailab/pika" -> "xingchensong/speech-recognition-papers"
"tencent-ailab/pika" -> "thu-spmi/CAT"
"tencent-ailab/pika" -> "hirofumi0810/neural_sp"
"tencent-ailab/pika" -> "lhotse-speech/lhotse"
"tencent-ailab/pika" -> "ZhengkunTian/OpenTransformer"
"tencent-ailab/pika" -> "mobvoi/wenet"
"tencent-ailab/pika" -> "k2-fsa/k2"
"tencent-ailab/pika" -> "YiwenShaoStephen/pychain"
"tencent-ailab/pika" -> "1ytic/warp-rnnt"
"tencent-ailab/pika" -> "SpeechColab/GigaSpeech"
"tencent-ailab/pika" -> "danpovey/fast_rnnt"
"tencent-ailab/pika" -> "HawkAaron/warp-transducer"
"tencent-ailab/pika" -> "athena-team/athena" ["e"=1]
"tencent-ailab/pika" -> "XiaoMi/kaldi-onnx"
"timmahrt/praatIO" -> "timmahrt/ProMo"
"timmahrt/praatIO" -> "jcvasquezc/DisVoice" ["e"=1]
"timmahrt/praatIO" -> "YannickJadoul/Parselmouth"
"timmahrt/praatIO" -> "lingjzhu/charsiu"
"timmahrt/praatIO" -> "hbuschme/TextGridTools"
"timmahrt/praatIO" -> "kylebgorman/textgrid"
"timmahrt/praatIO" -> "r9y9/pysptk"
"voithru/voice-activity-detection" -> "voithru/wav2vec2_finetune"
"voithru/voice-activity-detection" -> "voithru/asr-text_classification-pipeline"
"wq2012/awesome-diarization" -> "google/uis-rnn"
"wq2012/awesome-diarization" -> "taylorlu/Speaker-Diarization"
"wq2012/awesome-diarization" -> "pyannote/pyannote-audio"
"wq2012/awesome-diarization" -> "wq2012/SpectralCluster"
"wq2012/awesome-diarization" -> "hitachi-speech/EEND"
"wq2012/awesome-diarization" -> "HarryVolek/PyTorch_Speaker_Verification"
"wq2012/awesome-diarization" -> "Snowdar/asv-subtools"
"wq2012/awesome-diarization" -> "clovaai/voxceleb_trainer"
"wq2012/awesome-diarization" -> "nryant/dscore"
"wq2012/awesome-diarization" -> "philipperemy/deep-speaker"
"wq2012/awesome-diarization" -> "BUTSpeechFIT/VBx"
"wq2012/awesome-diarization" -> "mravanelli/SincNet"
"wq2012/awesome-diarization" -> "resemble-ai/Resemblyzer"
"wq2012/awesome-diarization" -> "wiseman/py-webrtcvad"
"wq2012/awesome-diarization" -> "WeidiXie/VGG-Speaker-Recognition"
"xingchensong/speech-recognition-papers" -> "cywang97/StreamingTransformer"
"xingchensong/speech-recognition-papers" -> "tencent-ailab/pika"
"xingchensong/speech-recognition-papers" -> "mobvoi/wenet"
"xingchensong/speech-recognition-papers" -> "hirofumi0810/neural_sp"
"xingchensong/speech-recognition-papers" -> "SpeechColab/GigaSpeech"
"xingchensong/speech-recognition-papers" -> "thu-spmi/CAT"
"xingchensong/speech-recognition-papers" -> "speechio/BigCiDian"
"xingchensong/speech-recognition-papers" -> "XiaoMi/kaldi-onnx"
"xingchensong/speech-recognition-papers" -> "YiwenShaoStephen/pychain"
"xingchensong/speech-recognition-papers" -> "tencent-ailab/3m-asr"
"xingchensong/speech-recognition-papers" -> "theblackcat102/Online-Speech-Recognition"
"xingchensong/speech-recognition-papers" -> "ZhengkunTian/OpenTransformer"
"xingchensong/speech-recognition-papers" -> "cornerfarmer/ctc_segmentation"
"xingchensong/speech-recognition-papers" -> "HawkAaron/warp-transducer"
"xingchensong/speech-recognition-papers" -> "chenjiasheng/mwer"
"yinruiqing/change_detection" -> "philipperemy/speaker-change-detection"
"yufan-aslp/AliMeeting" -> "desh2608/dover-lap"
"EFord36/normalise" -> "speech-io/chinese_text_normalization"
"SUC-DriverOld/so-vits-svc-Chinese-Detaild-Documents" -> "MaxMax2016/so-vits-svc"
"SUC-DriverOld/so-vits-svc-Chinese-Detaild-Documents" -> "PlayVoice/so-vits-svc"
"linhdvu14/vggvox-speaker-identification" -> "a-nagrani/VGGVox"
"linhdvu14/vggvox-speaker-identification" -> "GauravWaghmare/Speaker-Identification"
"MasayaKawamura/MB-iSTFT-VITS" -> "anonymous-pits/pits"
"MasayaKawamura/MB-iSTFT-VITS" -> "yl4579/StyleTTS"
"MasayaKawamura/MB-iSTFT-VITS" -> "chomeyama/SiFiGAN"
"MasayaKawamura/MB-iSTFT-VITS" -> "heatz123/naturalspeech"
"MasayaKawamura/MB-iSTFT-VITS" -> "NVIDIA/radtts"
"MasayaKawamura/MB-iSTFT-VITS" -> "Rongjiehuang/GenerSpeech"
"MasayaKawamura/MB-iSTFT-VITS" -> "OlaWod/FreeVC"
"MasayaKawamura/MB-iSTFT-VITS" -> "hhguo/MSMC-TTS"
"MasayaKawamura/MB-iSTFT-VITS" -> "NVIDIA/BigVGAN"
"MasayaKawamura/MB-iSTFT-VITS" -> "PlayVoice/vits_chinese"
"MasayaKawamura/MB-iSTFT-VITS" -> "keonlee9420/Comprehensive-Transformer-TTS"
"MasayaKawamura/MB-iSTFT-VITS" -> "rendchevi/nix-tts"
"MasayaKawamura/MB-iSTFT-VITS" -> "PlayVoice/VI-SVC"
"MasayaKawamura/MB-iSTFT-VITS" -> "keonlee9420/PortaSpeech"
"MasayaKawamura/MB-iSTFT-VITS" -> "liusongxiang/ppg-vc"
"GunwooHan/EasyVtuber" -> "yuyuyzl/EasyVtuber"
"GunwooHan/EasyVtuber" -> "zzsza/Boostcamp-AI-Tech-Product-Serving" ["e"=1]
"GunwooHan/EasyVtuber" -> "pkhungurn/talking-head-anime-3-demo"
"GunwooHan/EasyVtuber" -> "KR-HappyFace/KoDALLE"
"GunwooHan/EasyVtuber" -> "shugen002/shader"
"GunwooHan/EasyVtuber" -> "Boostcamp-JoHan4Park/Interview-Study"
"alphacep/vosk" -> "alphacep/vosk-server"
"alphacep/vosk" -> "alphacep/kaldi-android-demo"
"NVIDIA/OpenSeq2Seq" -> "mravanelli/pytorch-kaldi"
"NVIDIA/OpenSeq2Seq" -> "syhw/wer_are_we"
"NVIDIA/OpenSeq2Seq" -> "NVIDIA/waveglow"
"NVIDIA/OpenSeq2Seq" -> "freewym/espresso"
"NVIDIA/OpenSeq2Seq" -> "facebookresearch/wav2letter"
"NVIDIA/OpenSeq2Seq" -> "SeanNaren/deepspeech.pytorch"
"NVIDIA/OpenSeq2Seq" -> "HawkAaron/warp-transducer"
"NVIDIA/OpenSeq2Seq" -> "tensorflow/lingvo" ["e"=1]
"NVIDIA/OpenSeq2Seq" -> "espnet/espnet"
"NVIDIA/OpenSeq2Seq" -> "pykaldi/pykaldi"
"NVIDIA/OpenSeq2Seq" -> "Rayhane-mamah/Tacotron-2"
"NVIDIA/OpenSeq2Seq" -> "mozilla/LPCNet"
"NVIDIA/OpenSeq2Seq" -> "zzw922cn/awesome-speech-recognition-speech-synthesis-papers"
"NVIDIA/OpenSeq2Seq" -> "kaituoxu/Speech-Transformer"
"NVIDIA/OpenSeq2Seq" -> "athena-team/athena" ["e"=1]
"CjangCjengh/MoeGoe_GUI" -> "CjangCjengh/MoeGoe"
"CjangCjengh/MoeGoe_GUI" -> "CjangCjengh/TTSModels"
"CjangCjengh/MoeGoe_GUI" -> "CjangCjengh/vits"
"CjangCjengh/MoeGoe_GUI" -> "luoyily/MoeTTS"
"CjangCjengh/MoeGoe_GUI" -> "innnky/emotional-vits"
"CjangCjengh/MoeGoe_GUI" -> "SayaSS/vits-finetuning"
"CjangCjengh/MoeGoe_GUI" -> "TheKOG/Gal-Voice-Bot"
"CjangCjengh/MoeGoe_GUI" -> "NaruseMioShirakana/MoeSS"
"CjangCjengh/MoeGoe_GUI" -> "Plachtaa/VITS-fast-fine-tuning"
"CjangCjengh/MoeGoe_GUI" -> "cjyaddone/ChatWaifuL2D"
"CjangCjengh/MoeGoe_GUI" -> "audeering/w2v2-how-to"
"CjangCjengh/MoeGoe_GUI" -> "Francis-Komizu/VITS"
"CjangCjengh/MoeGoe_GUI" -> "alphanemeless/VITS_TXT_to_Audio"
"CjangCjengh/MoeGoe_GUI" -> "jaywalnut310/vits"
"CjangCjengh/MoeGoe_GUI" -> "IceKyrin/sovits_guide"
"thorstenMueller/deep-learning-german-tts" -> "rhasspy/gruut"
"thorstenMueller/deep-learning-german-tts" -> "german-asr/megs"
"thorstenMueller/deep-learning-german-tts" -> "monatis/german-tts"
"thorstenMueller/deep-learning-german-tts" -> "erogol/TTS-papers"
"sciooga/v2ex-plus" -> "bynil/sov2ex"
"sciooga/v2ex-plus" -> "jkeylu/v2ex.ext" ["e"=1]
"sciooga/v2ex-plus" -> "Suxiaogang/WeiboPicBed"
"sciooga/v2ex-plus" -> "VitoVan/v2excellent.js"
"sciooga/v2ex-plus" -> "Feiox/useless-websites" ["e"=1]
"sciooga/v2ex-plus" -> "kokdemo/v2ex.k"
"sciooga/v2ex-plus" -> "satorioh/google-enhancer"
"sciooga/v2ex-plus" -> "Zhiqing-Lee/js-printer" ["e"=1]
"RElbers/info-nce-pytorch" -> "Wendison/VQMIVC"
"Kyubyong/css10" -> "Tomiinek/Multilingual_Text_to_Speech"
"Kyubyong/css10" -> "NVIDIA/mellotron"
"Kyubyong/css10" -> "ivanvovk/WaveGrad"
"Kyubyong/css10" -> "bootphon/phonemizer"
"Kyubyong/css10" -> "KinglittleQ/GST-Tacotron"
"Kyubyong/css10" -> "jaywalnut310/glow-tts"
"Kyubyong/css10" -> "xcmyz/speech-synthesis-paper"
"Kyubyong/css10" -> "syang1993/gst-tacotron"
"Kyubyong/css10" -> "JRMeyer/open-speech-corpora"
"Kyubyong/css10" -> "tts-tutorial/survey"
"Kyubyong/css10" -> "DigitalPhonetics/IMS-Toucan"
"Kyubyong/css10" -> "seungwonpark/melgan"
"Kyubyong/css10" -> "tianrengao/SqueezeWave"
"Kyubyong/css10" -> "yanggeng1995/GAN-TTS"
"Kyubyong/css10" -> "NVIDIA/BigVGAN"
"rongcloud/server-sdk-php" -> "rongcloud/demo-server-php"
"julius-speech/julius" -> "julius-speech/dictation-kit"
"julius-speech/julius" -> "cmusphinx/pocketsphinx"
"julius-speech/julius" -> "kaldi-asr/kaldi"
"julius-speech/julius" -> "alumae/kaldi-gstreamer-server"
"julius-speech/julius" -> "srvk/eesen"
"julius-speech/julius" -> "espnet/espnet"
"julius-speech/julius" -> "mmorise/World"
"julius-speech/julius" -> "facebookresearch/wav2letter"
"julius-speech/julius" -> "PaddlePaddle/DeepSpeech"
"julius-speech/julius" -> "julius-speech/segmentation-kit"
"julius-speech/julius" -> "cmusphinx/sphinxbase"
"julius-speech/julius" -> "pannous/tensorflow-speech-recognition"
"julius-speech/julius" -> "flashlight/wav2letter"
"julius-speech/julius" -> "mozilla/DeepSpeech"
"julius-speech/julius" -> "SeanNaren/deepspeech.pytorch"
"sealtalk/sealtalk-ios" -> "rongcloud/demo-app-imlib-live-chatroom-ios"
"sealtalk/sealtalk-ios" -> "rongcloud/callkit-ios"
"YannickJadoul/Parselmouth" -> "praat/praat"
"YannickJadoul/Parselmouth" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"YannickJadoul/Parselmouth" -> "timmahrt/praatIO"
"YannickJadoul/Parselmouth" -> "kan-bayashi/ParallelWaveGAN"
"YannickJadoul/Parselmouth" -> "JeremyCCHsu/Python-Wrapper-for-World-Vocoder"
"YannickJadoul/Parselmouth" -> "drfeinberg/PraatScripts" ["e"=1]
"YannickJadoul/Parselmouth" -> "bootphon/phonemizer"
"YannickJadoul/Parselmouth" -> "google/REAPER"
"YannickJadoul/Parselmouth" -> "marl/crepe" ["e"=1]
"YannickJadoul/Parselmouth" -> "NVIDIA/BigVGAN"
"YannickJadoul/Parselmouth" -> "ivanvovk/WaveGrad"
"YannickJadoul/Parselmouth" -> "xcmyz/FastSpeech"
"YannickJadoul/Parselmouth" -> "ming024/FastSpeech2"
"YannickJadoul/Parselmouth" -> "descriptinc/melgan-neurips"
"YannickJadoul/Parselmouth" -> "jik876/hifi-gan"
"xyzlf/ShareSDK" -> "MobClub/ShareSDK-for-Android"
"AASHISHAG/deepspeech-german" -> "uhh-lt/kaldi-tuda-de"
"AASHISHAG/deepspeech-german" -> "thorstenMueller/deep-learning-german-tts"
"AASHISHAG/deepspeech-german" -> "DanBmh/deepspeech-german"
"AASHISHAG/deepspeech-german" -> "ynop/audiomate"
"AASHISHAG/deepspeech-german" -> "ynop/deepspeech-german"
"AASHISHAG/deepspeech-german" -> "gooofy/zamia-speech"
"AASHISHAG/deepspeech-german" -> "mozilla/DeepSpeech-examples"
"AASHISHAG/deepspeech-german" -> "coqui-ai/STT"
"AASHISHAG/deepspeech-german" -> "AASHISHAG/DeepSpeech-API"
"Shawn-Inspur/Yuan-1.0" -> "bigbrother666sh/shezhangbujianle"
"Shawn-Inspur/Yuan-1.0" -> "Turing-Project/AntiFraudChatBot"
"Shawn-Inspur/Yuan-1.0" -> "thu-coai/EVA" ["e"=1]
"Shawn-Inspur/Yuan-1.0" -> "OpenBMB/BMInf" ["e"=1]
"Shawn-Inspur/Yuan-1.0" -> "clue-ai/PromptCLUE" ["e"=1]
"Shawn-Inspur/Yuan-1.0" -> "OpenBMB/CPM-Live" ["e"=1]
"Shawn-Inspur/Yuan-1.0" -> "microsoft/Megatron-DeepSpeed" ["e"=1]
"Shawn-Inspur/Yuan-1.0" -> "TsinghuaAI/CPM-1-Generate" ["e"=1]
"letiantian/ChineseTone" -> "letiantian/Pinyin2Hanzi"
"letiantian/ChineseTone" -> "hjzin/PolyphoneDisambiguation"
"letiantian/ChineseTone" -> "mozillazg/pinyin-data"
"letiantian/ChineseTone" -> "mozillazg/phrase-pinyin-data"
"liuxp0827/govpr" -> "liuxp0827/Jvpr"
"liuxp0827/govpr" -> "ibillxia/VoicePrintReco"
"liuxp0827/govpr" -> "dake/openVP"
"LAION-AI/CLAP" -> "LAION-AI/audio-dataset"
"LAION-AI/CLAP" -> "microsoft/CLAP"
"LAION-AI/CLAP" -> "archinetai/audio-diffusion-pytorch"
"LAION-AI/CLAP" -> "haoheliu/AudioLDM"
"LAION-AI/CLAP" -> "facebookresearch/AudioMAE"
"LAION-AI/CLAP" -> "lucidrains/audiolm-pytorch"
"LAION-AI/CLAP" -> "liusongxiang/Large-Audio-Models"
"LAION-AI/CLAP" -> "XinhaoMei/WavCaps"
"LAION-AI/CLAP" -> "archinetai/audio-ai-timeline"
"LAION-AI/CLAP" -> "haoheliu/audioldm_eval"
"LAION-AI/CLAP" -> "AndreyGuzhov/AudioCLIP"
"LAION-AI/CLAP" -> "NVIDIA/BigVGAN"
"LAION-AI/CLAP" -> "mindslab-ai/phaseaug"
"LAION-AI/CLAP" -> "seungheondoh/music-text-representation"
"LAION-AI/CLAP" -> "YuanGongND/ssast"
"descriptinc/lyrebird-wav2clip" -> "AndreyGuzhov/AudioCLIP"
"descriptinc/lyrebird-wav2clip" -> "Spijkervet/CLMR" ["e"=1]
"Deepest-Project/MelNet" -> "resemble-ai/MelNet"
"Deepest-Project/MelNet" -> "seungwonpark/melgan"
"Deepest-Project/MelNet" -> "mindslab-ai/cotatron"
"Deepest-Project/MelNet" -> "jaeyeun97/MelNet"
"Deepest-Project/MelNet" -> "WelkinYang/GradTTS"
"Deepest-Project/MelNet" -> "thuhcsi/VAENAR-TTS"
"Deepest-Project/MelNet" -> "YuvalBecker/MelNet"
"alokprasad/LPCTron" -> "MlWoo/LPCNet"
"alokprasad/LPCTron" -> "mozilla/LPCNet"
"alokprasad/LPCTron" -> "geneing/WaveRNN-Pytorch"
"rishikksh20/VocGAN" -> "ivanvovk/WaveGrad"
"rishikksh20/VocGAN" -> "rishikksh20/FastSpeech2"
"rishikksh20/VocGAN" -> "keonlee9420/Parallel-Tacotron2"
"rishikksh20/VocGAN" -> "LEEYOONHYUNG/BVAE-TTS"
"rishikksh20/VocGAN" -> "rishikksh20/TFGAN"
"rishikksh20/VocGAN" -> "thuhcsi/VAENAR-TTS"
"rishikksh20/VocGAN" -> "jik876/hifi-gan"
"rishikksh20/VocGAN" -> "ivanvovk/DurIAN"
"rishikksh20/VocGAN" -> "seungwonpark/melgan"
"rishikksh20/VocGAN" -> "lmnt-com/wavegrad"
"rishikksh20/VocGAN" -> "k2kobayashi/crank"
"rishikksh20/VocGAN" -> "lmnt-com/diffwave"
"rishikksh20/VocGAN" -> "huawei-noah/Speech-Backbones"
"rishikksh20/VocGAN" -> "kan-bayashi/ParallelWaveGAN"
"rishikksh20/VocGAN" -> "HGU-DLLAB/Korean-FastSpeech2-Pytorch"
"seungwonpark/melgan" -> "descriptinc/melgan-neurips"
"seungwonpark/melgan" -> "kan-bayashi/ParallelWaveGAN"
"seungwonpark/melgan" -> "NVIDIA/mellotron"
"seungwonpark/melgan" -> "xcmyz/FastSpeech"
"seungwonpark/melgan" -> "jaywalnut310/glow-tts"
"seungwonpark/melgan" -> "mozilla/LPCNet"
"seungwonpark/melgan" -> "tianrengao/SqueezeWave"
"seungwonpark/melgan" -> "yanggeng1995/GAN-TTS"
"seungwonpark/melgan" -> "rishikksh20/VocGAN"
"seungwonpark/melgan" -> "soobinseo/Transformer-TTS"
"seungwonpark/melgan" -> "jik876/hifi-gan"
"seungwonpark/melgan" -> "xcmyz/speech-synthesis-paper"
"seungwonpark/melgan" -> "NVIDIA/waveglow"
"seungwonpark/melgan" -> "syang1993/gst-tacotron"
"seungwonpark/melgan" -> "ming024/FastSpeech2"
"double22a/speech_dataset" -> "wenet-e2e/wekws" ["e"=1]
"double22a/speech_dataset" -> "ddlBoJack/Speech-Resources"
"double22a/speech_dataset" -> "wenet-e2e/wespeaker"
"double22a/speech_dataset" -> "speechio/chinese_text_normalization"
"double22a/speech_dataset" -> "xingchensong/speech-recognition-papers"
"double22a/speech_dataset" -> "chenkui164/FastASR"
"double22a/speech_dataset" -> "kakaobrain/g2pM"
"RapidAI/RapidASR" -> "chenkui164/FastASR"
"RapidAI/RapidASR" -> "alibaba-damo-academy/FunASR"
"RapidAI/RapidASR" -> "Mashiro009/wenet-online-decoder-onnx"
"RapidAI/RapidASR" -> "k2-fsa/sherpa-onnx"
"mailong25/self-supervised-speech-recognition" -> "kensho-technologies/pyctcdecode"
"mailong25/self-supervised-speech-recognition" -> "hirofumi0810/neural_sp"
"mailong25/self-supervised-speech-recognition" -> "m3hrdadfi/soxan" ["e"=1]
"mailong25/self-supervised-speech-recognition" -> "dangvansam98/demo_vietasr"
"mailong25/self-supervised-speech-recognition" -> "patrickvonplaten/Wav2Vec2_PyCTCDecode"
"mailong25/self-supervised-speech-recognition" -> "eastonYi/wav2vec"
"mailong25/self-supervised-speech-recognition" -> "thu-spmi/CAT"
"mailong25/self-supervised-speech-recognition" -> "YoavRamon/awesome-kaldi"
"mailong25/self-supervised-speech-recognition" -> "cywang97/StreamingTransformer"
"mailong25/self-supervised-speech-recognition" -> "lumaku/ctc-segmentation"
"mailong25/self-supervised-speech-recognition" -> "khanld/ASR-Wav2vec-Finetune"
"mailong25/self-supervised-speech-recognition" -> "jonatasgrosman/huggingsound"
"mailong25/self-supervised-speech-recognition" -> "facebookresearch/WavAugment"
"mailong25/self-supervised-speech-recognition" -> "jonatasgrosman/wav2vec2-sprint"
"mailong25/self-supervised-speech-recognition" -> "ZhengkunTian/OpenTransformer"
"midas-research/audino" -> "k2-fsa/k2"
"midas-research/audino" -> "lumaku/ctc-segmentation"
"midas-research/audino" -> "hirofumi0810/neural_sp"
"midas-research/audino" -> "novoic/surfboard" ["e"=1]
"midas-research/audino" -> "mobvoi/wenet"
"midas-research/audino" -> "JRMeyer/open-speech-corpora"
"midas-research/audino" -> "wq2012/awesome-diarization"
"midas-research/audino" -> "freewym/espresso"
"midas-research/audino" -> "iankur/ContextNet"
"midas-research/audino" -> "TensorSpeech/TensorFlowASR"
"midas-research/audino" -> "asteroid-team/torch-audiomentations"
"midas-research/audino" -> "k2-fsa/icefall"
"midas-research/audino" -> "Kyubyong/css10"
"midas-research/audino" -> "snakers4/silero-vad"
"midas-research/audino" -> "facebookresearch/WavAugment"
"ZhengkunTian/OpenTransformer" -> "hirofumi0810/neural_sp"
"ZhengkunTian/OpenTransformer" -> "cywang97/StreamingTransformer"
"ZhengkunTian/OpenTransformer" -> "kaituoxu/Speech-Transformer"
"ZhengkunTian/OpenTransformer" -> "tencent-ailab/pika"
"ZhengkunTian/OpenTransformer" -> "mobvoi/wenet"
"ZhengkunTian/OpenTransformer" -> "ZhengkunTian/rnn-transducer"
"ZhengkunTian/OpenTransformer" -> "thu-spmi/CAT"
"ZhengkunTian/OpenTransformer" -> "SpeechColab/GigaSpeech"
"ZhengkunTian/OpenTransformer" -> "by2101/OpenASR"
"ZhengkunTian/OpenTransformer" -> "gentaiscool/end2end-asr-pytorch"
"ZhengkunTian/OpenTransformer" -> "YiwenShaoStephen/pychain"
"ZhengkunTian/OpenTransformer" -> "aishell-foundation/DaCiDian"
"ZhengkunTian/OpenTransformer" -> "xingchensong/speech-recognition-papers"
"ZhengkunTian/OpenTransformer" -> "k2-fsa/k2"
"ZhengkunTian/OpenTransformer" -> "speechio/BigCiDian"
"jackaduma/CycleGAN-VC2" -> "jackaduma/CycleGAN-VC3"
"jackaduma/CycleGAN-VC2" -> "liusongxiang/StarGAN-Voice-Conversion"
"jackaduma/CycleGAN-VC2" -> "auspicious3000/autovc"
"jackaduma/CycleGAN-VC2" -> "SamuelBroughton/StarGAN-Voice-Conversion-2"
"jackaduma/CycleGAN-VC2" -> "auspicious3000/SpeechSplit"
"jackaduma/CycleGAN-VC2" -> "leimao/Voice-Converter-CycleGAN"
"jackaduma/CycleGAN-VC2" -> "GANtastic3/MaskCycleGAN-VC"
"jackaduma/CycleGAN-VC2" -> "KuangDD/zhrtvc"
"jackaduma/CycleGAN-VC2" -> "jjery2243542/adaptive_voice_conversion"
"jackaduma/CycleGAN-VC2" -> "hujinsen/pytorch-StarGAN-VC"
"jackaduma/CycleGAN-VC2" -> "yl4579/StarGANv2-VC"
"jackaduma/CycleGAN-VC2" -> "lturing/tacotronv2_wavernn_chinese"
"jackaduma/CycleGAN-VC2" -> "jxzhanggg/nonparaSeq2seqVC_code"
"jackaduma/CycleGAN-VC2" -> "hujinsen/StarGAN-Voice-Conversion"
"jackaduma/CycleGAN-VC2" -> "KunZhou9646/emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0"
"jackaduma/CycleGAN-VC3" -> "jackaduma/CycleGAN-VC2"
"jackaduma/CycleGAN-VC3" -> "GANtastic3/MaskCycleGAN-VC"
"jackaduma/LAS_Mandarin_PyTorch" -> "kaituoxu/Listen-Attend-Spell"
"yinkalario/General-Purpose-Sound-Recognition-Demo" -> "yinkalario/Sound-Event-Detection-AudioSet"
"yinkalario/General-Purpose-Sound-Recognition-Demo" -> "microsoft/WavText5K"
"yinkalario/General-Purpose-Sound-Recognition-Demo" -> "Labbeti/aac-datasets"
"yinkalario/General-Purpose-Sound-Recognition-Demo" -> "Kikyo-16/Sound_event_detection" ["e"=1]
"MrNothing/AI-Blocks" -> "andabi/deep-voice-conversion"
"MrNothing/AI-Blocks" -> "buriburisuri/speech-to-text-wavenet"
"MrNothing/AI-Blocks" -> "junyanz/iGAN" ["e"=1]
"MrNothing/AI-Blocks" -> "AKSHAYUBHAT/DeepVideoAnalytics"
"MrNothing/AI-Blocks" -> "PAIR-code/facets" ["e"=1]
"MrNothing/AI-Blocks" -> "oarriaga/face_classification"
"MrNothing/AI-Blocks" -> "OpenNMT/OpenNMT" ["e"=1]
"MrNothing/AI-Blocks" -> "uber/horovod" ["e"=1]
"MrNothing/AI-Blocks" -> "uber/pyro" ["e"=1]
"MrNothing/AI-Blocks" -> "yunjey/StarGAN" ["e"=1]
"MrNothing/AI-Blocks" -> "DmitryUlyanov/deep-image-prior" ["e"=1]
"MrNothing/AI-Blocks" -> "tensorflow/tfjs-core" ["e"=1]
"MrNothing/AI-Blocks" -> "NVIDIA/pix2pixHD" ["e"=1]
"MrNothing/AI-Blocks" -> "deepmind/sonnet" ["e"=1]
"MrNothing/AI-Blocks" -> "lengstrom/fast-style-transfer" ["e"=1]
"SpeechColab/GigaSpeech" -> "cywang97/StreamingTransformer"
"SpeechColab/GigaSpeech" -> "lhotse-speech/lhotse"
"SpeechColab/GigaSpeech" -> "k2-fsa/k2"
"SpeechColab/GigaSpeech" -> "xingchensong/speech-recognition-papers"
"SpeechColab/GigaSpeech" -> "tencent-ailab/pika"
"SpeechColab/GigaSpeech" -> "k2-fsa/icefall"
"SpeechColab/GigaSpeech" -> "mobvoi/wenet"
"SpeechColab/GigaSpeech" -> "hirofumi0810/neural_sp"
"SpeechColab/GigaSpeech" -> "SpeechColab/Leaderboard"
"SpeechColab/GigaSpeech" -> "ZhengkunTian/OpenTransformer"
"SpeechColab/GigaSpeech" -> "facebookresearch/libri-light"
"SpeechColab/GigaSpeech" -> "facebookresearch/voxpopuli"
"SpeechColab/GigaSpeech" -> "YiwenShaoStephen/pychain"
"SpeechColab/GigaSpeech" -> "speechio/chinese_text_normalization"
"SpeechColab/GigaSpeech" -> "wenet-e2e/WenetSpeech"
"espnet/interspeech2019-tutorial" -> "cywang97/StreamingTransformer"
"jitsi/jiwer" -> "belambert/asr-evaluation"
"jitsi/jiwer" -> "kensho-technologies/pyctcdecode"
"jitsi/jiwer" -> "zszyellow/WER-in-python"
"jitsi/jiwer" -> "lumaku/ctc-segmentation"
"jitsi/jiwer" -> "parlance/ctcdecode"
"jitsi/jiwer" -> "coqui-ai/open-speech-corpora"
"jitsi/jiwer" -> "nicklashansen/voice-activity-detection"
"k2-fsa/icefall" -> "k2-fsa/k2"
"k2-fsa/icefall" -> "lhotse-speech/lhotse"
"k2-fsa/icefall" -> "k2-fsa/sherpa"
"k2-fsa/icefall" -> "k2-fsa/snowfall"
"k2-fsa/icefall" -> "k2-fsa/sherpa-onnx"
"k2-fsa/icefall" -> "csukuangfj/kaldifeat"
"k2-fsa/icefall" -> "SpeechColab/Leaderboard"
"k2-fsa/icefall" -> "SpeechColab/GigaSpeech"
"k2-fsa/icefall" -> "tencent-ailab/pika"
"k2-fsa/icefall" -> "danpovey/fast_rnnt"
"k2-fsa/icefall" -> "thu-spmi/CAT"
"k2-fsa/icefall" -> "jctian98/e2e_lfmmi"
"k2-fsa/icefall" -> "wenet-e2e/wespeaker"
"k2-fsa/icefall" -> "hirofumi0810/neural_sp"
"k2-fsa/icefall" -> "cywang97/StreamingTransformer"
"kensho-technologies/pyctcdecode" -> "parlance/ctcdecode"
"kensho-technologies/pyctcdecode" -> "patrickvonplaten/Wav2Vec2_PyCTCDecode"
"kensho-technologies/pyctcdecode" -> "lumaku/ctc-segmentation"
"kensho-technologies/pyctcdecode" -> "mailong25/self-supervised-speech-recognition"
"kensho-technologies/pyctcdecode" -> "SpeechColab/GigaSpeech"
"kensho-technologies/pyctcdecode" -> "cywang97/StreamingTransformer"
"kensho-technologies/pyctcdecode" -> "burchim/EfficientConformer"
"kensho-technologies/pyctcdecode" -> "thu-spmi/CAT"
"kensho-technologies/pyctcdecode" -> "danpovey/fast_rnnt"
"kensho-technologies/pyctcdecode" -> "hirofumi0810/neural_sp"
"kensho-technologies/pyctcdecode" -> "k2-fsa/k2"
"kensho-technologies/pyctcdecode" -> "k2-fsa/icefall"
"kensho-technologies/pyctcdecode" -> "farisalasmary/wav2vec2-kenlm"
"kensho-technologies/pyctcdecode" -> "tencent-ailab/3m-asr"
"kensho-technologies/pyctcdecode" -> "k2-fsa/sherpa"
"nanoporetech/fast-ctc-decode" -> "TeaPoly/CTC-OptimizedLoss"
"nanoporetech/fast-ctc-decode" -> "danpovey/fast_rnnt"
"nanoporetech/fast-ctc-decode" -> "athena-team/athena-decoder"
"taylorlu/Speaker-Diarization" -> "wq2012/awesome-diarization"
"taylorlu/Speaker-Diarization" -> "wq2012/SpectralCluster"
"taylorlu/Speaker-Diarization" -> "google/uis-rnn"
"taylorlu/Speaker-Diarization" -> "WeidiXie/VGG-Speaker-Recognition"
"taylorlu/Speaker-Diarization" -> "HarryVolek/PyTorch_Speaker_Verification"
"taylorlu/Speaker-Diarization" -> "aalto-speech/speaker-diarization"
"taylorlu/Speaker-Diarization" -> "hitachi-speech/EEND"
"taylorlu/Speaker-Diarization" -> "DonkeyShot21/uis-rnn-sml"
"taylorlu/Speaker-Diarization" -> "Jamiroquai88/VBDiarization"
"taylorlu/Speaker-Diarization" -> "pyannote/pyannote-audio"
"taylorlu/Speaker-Diarization" -> "nryant/dscore"
"taylorlu/Speaker-Diarization" -> "google/speaker-id"
"taylorlu/Speaker-Diarization" -> "FlorianKrey/DNC"
"taylorlu/Speaker-Diarization" -> "pyannote/pyannote-metrics"
"taylorlu/Speaker-Diarization" -> "Janghyun1230/Speaker_Verification"
"thu-spmi/CAT" -> "YiwenShaoStephen/pychain"
"thu-spmi/CAT" -> "tencent-ailab/pika"
"thu-spmi/CAT" -> "k2-fsa/k2"
"thu-spmi/CAT" -> "XiaoMi/kaldi-onnx"
"thu-spmi/CAT" -> "mobvoi/wenet"
"thu-spmi/CAT" -> "danpovey/fast_rnnt"
"thu-spmi/CAT" -> "cywang97/StreamingTransformer"
"thu-spmi/CAT" -> "xingchensong/speech-recognition-papers"
"thu-spmi/CAT" -> "hirofumi0810/neural_sp"
"thu-spmi/CAT" -> "HawkAaron/warp-transducer"
"thu-spmi/CAT" -> "athena-team/athena-decoder"
"thu-spmi/CAT" -> "ZhengkunTian/OpenTransformer"
"thu-spmi/CAT" -> "k2-fsa/icefall"
"thu-spmi/CAT" -> "1ytic/warp-rnnt"
"thu-spmi/CAT" -> "csukuangfj/kaldifeat"
"zeroQiaoba/ivector-xvector" -> "manojpamk/pytorch_xvectors"
"zeroQiaoba/ivector-xvector" -> "hsn-zeinali/x-vector-kaldi-tf"
"zeroQiaoba/ivector-xvector" -> "Dannynis/xvector_pytorch"
"AliceNavigator/AI-Vtuber-chatglm" -> "whiteeat/ai-vtuber-alpha"
"easemob/emchat-server-examples" -> "easemob/web-im"
"easemob/emchat-server-examples" -> "easemob/sdkexamples-android"
"easemob/emchat-server-examples" -> "easemob/sdkdemoapp3.0_android"
"easemob/emchat-server-examples" -> "rongcloud/server-sdk-php"
"easemob/emchat-server-examples" -> "jpush/jpush-api-java-client"
"easemob/emchat-server-examples" -> "huangfangyi/FanXin3.0" ["e"=1]
"easemob/emchat-server-examples" -> "sealtalk/sealtalk-server"
"corticph/prefix-beam-search" -> "githubharald/CTCDecoder"
"corticph/prefix-beam-search" -> "githubharald/CTCWordBeamSearch"
"corticph/prefix-beam-search" -> "danpovey/fast_rnnt"
"Renovamen/Speech-and-Text" -> "shibing624/parrots"
"Renovamen/Speech-and-Text" -> "liangstein/Chinese-speech-to-text"
"Renovamen/Speech-and-Text" -> "opensourceteams/google-sdk-speech-to-text"
"Renovamen/Speech-and-Text" -> "Zws-China/VoiceToWords"
"Z-yq/TensorflowASR" -> "TensorSpeech/TensorFlowASR"
"Z-yq/TensorflowASR" -> "hirofumi0810/neural_sp"
"Z-yq/TensorflowASR" -> "ZhengkunTian/OpenTransformer"
"Z-yq/TensorflowASR" -> "tencent-ailab/pika"
"Z-yq/TensorflowASR" -> "noahchalifour/rnnt-speech-recognition"
"Z-yq/TensorflowASR" -> "yeyupiaoling/MASR" ["e"=1]
"Z-yq/TensorflowASR" -> "mobvoi/wenet"
"Z-yq/TensorflowASR" -> "cywang97/StreamingTransformer"
"Z-yq/TensorflowASR" -> "Z-yq/TensorflowTTS"
"Z-yq/TensorflowASR" -> "thu-spmi/CAT"
"Z-yq/TensorflowASR" -> "yeyupiaoling/PaddlePaddle-DeepSpeech" ["e"=1]
"Z-yq/TensorflowASR" -> "danpovey/fast_rnnt"
"Z-yq/TensorflowASR" -> "HawkAaron/warp-transducer"
"Z-yq/TensorflowASR" -> "athena-team/athena" ["e"=1]
"Z-yq/TensorflowASR" -> "kaituoxu/Listen-Attend-Spell"
"sailist/ASRFrame" -> "libai3/masr"
"sailist/ASRFrame" -> "audier/DeepSpeechRecognition"
"sailist/ASRFrame" -> "zw76859420/ASR_Theory"
"sailist/ASRFrame" -> "Pelhans/ZASR_tensorflow"
"sailist/ASRFrame" -> "xxbb1234021/speech_recognition"
"sailist/ASRFrame" -> "nobody132/masr"
"microsoft/CLAP" -> "LAION-AI/audio-dataset"
"microsoft/CLAP" -> "haoheliu/audioldm_eval"
"microsoft/CLAP" -> "akoepke/audio-retrieval-benchmark"
"microsoft/CLAP" -> "mindslab-ai/phaseaug"
"microsoft/CLAP" -> "LAION-AI/CLAP"
"microsoft/CLAP" -> "Rongjiehuang/GenerSpeech"
"microsoft/CLAP" -> "Labbeti/aac-datasets"
"microsoft/CLAP" -> "facebookresearch/AudioMAE"
"qiniu/QSunSync" -> "qiniu/csharp-sdk"
"cdfmlr/muvtuber" -> "whiteeat/ai-vtuber-alpha"
"cdfmlr/muvtuber" -> "XzaiCloud/AI-Vtuber"
"cdfmlr/muvtuber" -> "XzaiCloud/AI-Vtuber-Kun"
"ina-foss/inaSpeechSegmenter" -> "taylorlu/Speaker-Diarization"
"ina-foss/inaSpeechSegmenter" -> "wq2012/awesome-diarization"
"ina-foss/inaSpeechSegmenter" -> "amsehili/auditok"
"ina-foss/inaSpeechSegmenter" -> "philipperemy/deep-speaker"
"ina-foss/inaSpeechSegmenter" -> "pyannote/pyannote-audio"
"ina-foss/inaSpeechSegmenter" -> "filippogiruzzi/voice_activity_detection"
"ina-foss/inaSpeechSegmenter" -> "wiseman/py-webrtcvad"
"ina-foss/inaSpeechSegmenter" -> "lumaku/ctc-segmentation"
"ina-foss/inaSpeechSegmenter" -> "qlemaire22/speech-music-detection"
"ina-foss/inaSpeechSegmenter" -> "jtkim-kaist/VAD"
"ina-foss/inaSpeechSegmenter" -> "snakers4/silero-vad"
"ina-foss/inaSpeechSegmenter" -> "aalto-speech/speaker-diarization"
"ina-foss/inaSpeechSegmenter" -> "wq2012/SpectralCluster"
"ina-foss/inaSpeechSegmenter" -> "Snowdar/asv-subtools"
"ina-foss/inaSpeechSegmenter" -> "WeidiXie/VGG-Speaker-Recognition"
"shibing624/parrots" -> "CynthiaSuwi/ASR-for-Chinese-Pipeline"
"shibing624/parrots" -> "Z-yq/TensorflowTTS"
"astorfi/3D-convolutional-speaker-recognition-pytorch" -> "qqueing/DeepSpeaker-pytorch"
"astorfi/3D-convolutional-speaker-recognition-pytorch" -> "jymsuper/SpeakerRecognition_tutorial"
"astorfi/3D-convolutional-speaker-recognition-pytorch" -> "Walleclipse/Deep_Speaker-speaker_recognition_system"
"astorfi/3D-convolutional-speaker-recognition-pytorch" -> "HarryVolek/PyTorch_Speaker_Verification"
"astorfi/3D-convolutional-speaker-recognition-pytorch" -> "funcwj/ge2e-speaker-verification"
"just-ai/aimybox-android-assistant" -> "just-ai/aimybox-android-sdk"
"just-ai/aimybox-android-assistant" -> "just-ai/jaicf-kotlin"
"PlayVoice/vits_chinese" -> "heatz123/naturalspeech"
"PlayVoice/vits_chinese" -> "anonymous-pits/pits"
"PlayVoice/vits_chinese" -> "MasayaKawamura/MB-iSTFT-VITS"
"PlayVoice/vits_chinese" -> "wenet-e2e/wetts"
"PlayVoice/vits_chinese" -> "innnky/emotional-vits"
"PlayVoice/vits_chinese" -> "Executedone/Chinese-FastSpeech2"
"PlayVoice/vits_chinese" -> "b04901014/MQTTS"
"PlayVoice/vits_chinese" -> "Plachtaa/VITS-fast-fine-tuning"
"PlayVoice/vits_chinese" -> "liusongxiang/ppg-vc"
"PlayVoice/vits_chinese" -> "yl4579/StyleTTS"
"PlayVoice/vits_chinese" -> "Zz-ww/VITS-BigVGAN-SpanPSP-Chinese"
"PlayVoice/vits_chinese" -> "keonlee9420/Comprehensive-Transformer-TTS"
"PlayVoice/vits_chinese" -> "rhasspy/larynx2"
"PlayVoice/vits_chinese" -> "yl4579/StyleTTS-VC"
"PlayVoice/vits_chinese" -> "chomeyama/SiFiGAN"
"dathudeptrai/TensorflowTTS" -> "jaywalnut310/glow-tts"
"dathudeptrai/TensorflowTTS" -> "liusongxiang/efficient_tts"
"dathudeptrai/TensorflowTTS" -> "xcmyz/FastSpeech"
"dathudeptrai/TensorflowTTS" -> "erogol/TTS-papers"
"dathudeptrai/TensorflowTTS" -> "tianrengao/SqueezeWave"
"dathudeptrai/TensorflowTTS" -> "xcmyz/speech-synthesis-paper"
"dathudeptrai/TensorflowTTS" -> "tulasiram58827/TTS_TFLite"
"dathudeptrai/TensorflowTTS" -> "ivanvovk/DurIAN"
"dathudeptrai/TensorflowTTS" -> "kan-bayashi/ParallelWaveGAN"
"dathudeptrai/TensorflowTTS" -> "LEEYOONHYUNG/BVAE-TTS"
"dathudeptrai/TensorflowTTS" -> "k2kobayashi/crank"
"dathudeptrai/TensorflowTTS" -> "descriptinc/melgan-neurips"
"dathudeptrai/TensorflowTTS" -> "yanggeng1995/FB-MelGAN"
"dathudeptrai/TensorflowTTS" -> "zzw922cn/LPC_for_TTS"
"dathudeptrai/TensorflowTTS" -> "begeekmyfriend/tacotron2"
"Zeta36/tensorflow-tex-wavenet" -> "Zeta36/tensorflow-image-wavenet"
"Zeta36/tensorflow-tex-wavenet" -> "tomlepaine/fast-wavenet"
"Zeta36/tensorflow-tex-wavenet" -> "sotelo/parrot"
"Zeta36/tensorflow-tex-wavenet" -> "soroushmehr/sampleRNN_ICLR2017"
"Zeta36/tensorflow-tex-wavenet" -> "ibab/tensorflow-wavenet"
"junzew/HanTTS" -> "Jackiexiao/MTTS"
"junzew/HanTTS" -> "hgneng/ekho"
"junzew/HanTTS" -> "thuhcsi/Crystal"
"junzew/HanTTS" -> "JasonWei512/Tacotron-2-Chinese"
"junzew/HanTTS" -> "alexram1313/text-to-speech-sample"
"junzew/HanTTS" -> "mindslab-ai/cotatron"
"revsic/torch-nansypp" -> "chomeyama/SiFiGAN"
"llSourcell/tensorflow_speech_recognition_demo" -> "pannous/tensorflow-speech-recognition"
"llSourcell/tensorflow_speech_recognition_demo" -> "mickvanhulst/tf_chatbot_lotr"
"llSourcell/tensorflow_speech_recognition_demo" -> "igormq/ctc_tensorflow_example"
"llSourcell/tensorflow_speech_recognition_demo" -> "pannous/caffe-speech-recognition"
"llSourcell/tensorflow_speech_recognition_demo" -> "lingochamp/kaldi-ctc"
"llSourcell/tensorflow_speech_recognition_demo" -> "hirofumi0810/tensorflow_end2end_speech_recognition"
"cmusphinx/pocketsphinx-python" -> "cmusphinx/sphinxbase"
"cmusphinx/pocketsphinx-python" -> "bambocher/pocketsphinx-python"
"cmusphinx/pocketsphinx-python" -> "cmusphinx/sphinxtrain"
"cmusphinx/pocketsphinx-python" -> "cmusphinx/pocketsphinx"
"cmusphinx/pocketsphinx-python" -> "cmusphinx/pocketsphinx-android"
"cmusphinx/pocketsphinx-python" -> "cmusphinx/pocketsphinx-android-demo"
"qiniu/nodejs-sdk" -> "qiniu/js-sdk"
"qiniu/nodejs-sdk" -> "node-modules/qn"
"zw76859420/ASR_Syllable" -> "zw76859420/ASR_WORD"
"zw76859420/ASR_Syllable" -> "zw76859420/ASR_Theory"
"zw76859420/ASR_Syllable" -> "Sundy1219/eesen-for-thchs30"
"zw76859420/ASR_Syllable" -> "mdangschat/ctc-asr"
"XzaiCloud/AI-Vtuber-Kun" -> "XzaiCloud/AI-Vtuber"
"Pelhans/ZASR_tensorflow" -> "lezasantaizi/from_video_get_ASR_traindata"
"plison/opendial" -> "cuayahuitl/SimpleDS"
"plison/opendial" -> "UFAL-DSG/alex"
"ALIZE-Speaker-Recognition/alize-core" -> "ALIZE-Speaker-Recognition/LIA_RAL"
"ALIZE-Speaker-Recognition/alize-core" -> "ALIZE-Speaker-Recognition/android-alize"
"DmitryUlyanov/neural-style-audio-tf" -> "rupeshs/neuralsongstyle"
"DmitryUlyanov/neural-style-audio-tf" -> "vadim-v-lebedev/audio_style_tranfer"
"DmitryUlyanov/neural-style-audio-tf" -> "DmitryUlyanov/neural-style-audio-torch"
"DmitryUlyanov/neural-style-audio-tf" -> "pkmital/time-domain-neural-audio-style-transfer"
"DmitryUlyanov/neural-style-audio-tf" -> "alishdipani/Neural-Style-Transfer-Audio"
"DmitryUlyanov/neural-style-audio-tf" -> "soroushmehr/sampleRNN_ICLR2017"
"DmitryUlyanov/neural-style-audio-tf" -> "sumuzhao/CycleGAN-Music-Style-Transfer" ["e"=1]
"DmitryUlyanov/neural-style-audio-tf" -> "sotelo/parrot"
"DmitryUlyanov/neural-style-audio-tf" -> "JeremyCCHsu/Python-Wrapper-for-World-Vocoder"
"DmitryUlyanov/neural-style-audio-tf" -> "frhrdr/generative_audio"
"Kyubyong/g2p" -> "bootphon/phonemizer"
"Kyubyong/g2p" -> "cmusphinx/g2p-seq2seq"
"Kyubyong/g2p" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"Kyubyong/g2p" -> "kakaobrain/g2pM"
"Kyubyong/g2p" -> "xcmyz/FastSpeech"
"Kyubyong/g2p" -> "jaywalnut310/glow-tts"
"Kyubyong/g2p" -> "kan-bayashi/ParallelWaveGAN"
"Kyubyong/g2p" -> "mozilla/LPCNet"
"Kyubyong/g2p" -> "ming024/FastSpeech2"
"Kyubyong/g2p" -> "jik876/hifi-gan"
"Kyubyong/g2p" -> "speechio/chinese_text_normalization"
"Kyubyong/g2p" -> "syang1993/gst-tacotron"
"Kyubyong/g2p" -> "NVIDIA/mellotron"
"Kyubyong/g2p" -> "soobinseo/Transformer-TTS"
"Kyubyong/g2p" -> "AdolfVonKleist/Phonetisaurus"
"RicherMans/PLDA" -> "RaviSoji/plda"
"RicherMans/PLDA" -> "rajathkmp/speaker-verification"
"barronalex/Tacotron" -> "Kyubyong/tacotron"
"barronalex/Tacotron" -> "candlewill/Tacotron-2"
"barronalex/Tacotron" -> "riverphoenix/tacotron2"
"barronalex/Tacotron" -> "Kyubyong/tacotron_asr"
"dspavankumar/keras-kaldi" -> "vrenkens/tfkaldi"
"dspavankumar/keras-kaldi" -> "JRMeyer/multi-task-kaldi"
"dspavankumar/keras-kaldi" -> "gooofy/py-kaldi-asr"
"hsn-zeinali/x-vector-kaldi-tf" -> "mycrazycracy/tf-kaldi-speaker"
"hsn-zeinali/x-vector-kaldi-tf" -> "qqueing/SR_with_kaldi"
"hsn-zeinali/x-vector-kaldi-tf" -> "zeroQiaoba/ivector-xvector"
"hsn-zeinali/x-vector-kaldi-tf" -> "vesis84/kaldi-io-for-python"
"hsn-zeinali/x-vector-kaldi-tf" -> "idiap/kaldi-ivector"
"lawlict/ECAPA-TDNN" -> "ranchlai/speaker-verification"
"qiuqiangkong/torchlibrosa" -> "qiuqiangkong/audioset_tagging_cnn"
"qiuqiangkong/torchlibrosa" -> "KinWaiCheuk/nnAudio"
"qiuqiangkong/torchlibrosa" -> "asteroid-team/torch-audiomentations"
"qiuqiangkong/torchlibrosa" -> "Spijkervet/torchaudio-augmentations" ["e"=1]
"qiuqiangkong/torchlibrosa" -> "iver56/audiomentations"
"qiuqiangkong/torchlibrosa" -> "YuanGongND/ssast"
"qiuqiangkong/torchlibrosa" -> "kkoutini/PaSST"
"qiuqiangkong/torchlibrosa" -> "YuanGongND/ast"
"qiuqiangkong/torchlibrosa" -> "qiuqiangkong/panns_inference"
"qiuqiangkong/torchlibrosa" -> "qiuqiangkong/audioset_classification" ["e"=1]
"qiuqiangkong/torchlibrosa" -> "facebookresearch/AudioMAE"
"qiuqiangkong/torchlibrosa" -> "facebookresearch/WavAugment"
"qiuqiangkong/torchlibrosa" -> "yinkalario/Two-Stage-Polyphonic-Sound-Event-Detection-and-Localization" ["e"=1]
"qiuqiangkong/torchlibrosa" -> "microsoft/CLAP"
"qiuqiangkong/torchlibrosa" -> "ryanwongsa/kaggle-birdsong-recognition" ["e"=1]
"vrenkens/tfkaldi" -> "yajiemiao/eesen"
"vrenkens/tfkaldi" -> "dspavankumar/keras-kaldi"
"vrenkens/tfkaldi" -> "vrenkens/nabu"
"vrenkens/tfkaldi" -> "v0lta/Listen-attend-and-spell"
"vrenkens/tfkaldi" -> "lingochamp/kaldi-ctc"
"vrenkens/tfkaldi" -> "rizar/attention-lvcsr"
"vrenkens/tfkaldi" -> "srvk/eesen"
"vrenkens/tfkaldi" -> "YiwenShaoStephen/pychain"
"vrenkens/tfkaldi" -> "fernandodelacalle/ResNet-Kaldi-Tensorflow-ASR"
"VOICEVOX/voicevox_core" -> "VOICEVOX/voicevox_engine"
"VOICEVOX/voicevox_core" -> "VOICEVOX/voicevox"
"VOICEVOX/voicevox_core" -> "Hiroshiba/voicevox_engine"
"VOICEVOX/voicevox_core" -> "isletennos/MMVC_Trainer"
"VOICEVOX/voicevox_core" -> "Hiroshiba/voicevox"
"VOICEVOX/voicevox_core" -> "malaybaku/VMagicMirror" ["e"=1]
"VOICEVOX/voicevox_core" -> "jiro4989/ojosama" ["e"=1]
"VOICEVOX/voicevox_core" -> "yamachu/VoicevoxEngineSharp"
"mozillazg/phrase-pinyin-data" -> "mozillazg/pinyin-data"
"mozillazg/phrase-pinyin-data" -> "kakaobrain/g2pM"
"mozillazg/phrase-pinyin-data" -> "speechio/chinese_text_normalization"
"mozillazg/phrase-pinyin-data" -> "letiantian/ChineseTone"
"mozillazg/phrase-pinyin-data" -> "Kyubyong/g2pC"
"mozillazg/phrase-pinyin-data" -> "aishell-foundation/DaCiDian"
"mozillazg/phrase-pinyin-data" -> "speechio/BigCiDian"
"mozillazg/phrase-pinyin-data" -> "hjzin/PolyphoneDisambiguation"
"alphanemeless/VITS_TXT_to_Audio" -> "audeering/w2v2-how-to"
"cywang97/StreamingTransformer" -> "tencent-ailab/pika"
"cywang97/StreamingTransformer" -> "hirofumi0810/neural_sp"
"cywang97/StreamingTransformer" -> "xingchensong/speech-recognition-papers"
"cywang97/StreamingTransformer" -> "mobvoi/wenet"
"cywang97/StreamingTransformer" -> "ZhengkunTian/OpenTransformer"
"cywang97/StreamingTransformer" -> "SpeechColab/GigaSpeech"
"cywang97/StreamingTransformer" -> "HawkAaron/warp-transducer"
"cywang97/StreamingTransformer" -> "noahchalifour/rnnt-speech-recognition"
"cywang97/StreamingTransformer" -> "YiwenShaoStephen/pychain"
"cywang97/StreamingTransformer" -> "jctian98/e2e_lfmmi"
"cywang97/StreamingTransformer" -> "thu-spmi/CAT"
"cywang97/StreamingTransformer" -> "speechio/chinese_text_normalization"
"cywang97/StreamingTransformer" -> "theblackcat102/Online-Speech-Recognition"
"cywang97/StreamingTransformer" -> "1ytic/warp-rnnt"
"cywang97/StreamingTransformer" -> "athena-team/athena-decoder"
"felixfuyihui/AISHELL-4" -> "desh2608/dover-lap"
"felixfuyihui/AISHELL-4" -> "yufan-aslp/AliMeeting"
"CZ26/CycleTransGAN-EVC" -> "KunZhou9646/Emovox"
"PlayVoice/so-vits-svc-5.0" -> "PlayVoice/lora-svc"
"PlayVoice/so-vits-svc-5.0" -> "yxlllc/DDSP-SVC"
"PlayVoice/so-vits-svc-5.0" -> "w-okada/voice-changer"
"janvarev/Irene-Voice-Assistant" -> "Oknolaz/vasisualy"
"janvarev/Irene-Voice-Assistant" -> "timhok/IreneVA-hassio-script-trigger-plugin"
"janvarev/Irene-Voice-Assistant" -> "snakers4/silero-models"
"janvarev/Irene-Voice-Assistant" -> "EnjiRouz/Voice-Assistant-App"
"janvarev/Irene-Voice-Assistant" -> "snakers4/open_stt" ["e"=1]
"janvarev/Irene-Voice-Assistant" -> "RajSolai/TextSnatcher" ["e"=1]
"janvarev/Irene-Voice-Assistant" -> "SergeyShk/Speech-to-Text-Russian" ["e"=1]
"janvarev/Irene-Voice-Assistant" -> "NyanNyanovich/nyan"
"janvarev/Irene-Voice-Assistant" -> "0x7o/text2keywords"
"janvarev/Irene-Voice-Assistant" -> "putnik/mycroft-russian"
"janvarev/Irene-Voice-Assistant" -> "ericmurphyxyz/rofi-wifi-menu" ["e"=1]
"r9y9/nnmnkwii_gallery" -> "r9y9/nnmnkwii"
"CMsmartvoice/One-Shot-Voice-Cloning" -> "SungFeng-Huang/Meta-TTS"
"CMsmartvoice/One-Shot-Voice-Cloning" -> "Labmem-Zhouyx/CDFSE_FastSpeech2"
"CMsmartvoice/One-Shot-Voice-Cloning" -> "dunky11/voicesmith"
"CMsmartvoice/One-Shot-Voice-Cloning" -> "KevinMIN95/StyleSpeech"
"CMsmartvoice/One-Shot-Voice-Cloning" -> "Rongjiehuang/GenerSpeech"
"CMsmartvoice/One-Shot-Voice-Cloning" -> "keonlee9420/StyleSpeech"
"CMsmartvoice/One-Shot-Voice-Cloning" -> "keonlee9420/DailyTalk"
"easemob/easeui" -> "easemob/sdkdemoapp3.0_android"
"easemob/easeui" -> "umeng/UMAndroidSdkDemo"
"easemob/easeui" -> "easemob/kefu-android-demo"
"Rongjiehuang/GenerSpeech" -> "Rongjiehuang/ProDiff"
"Rongjiehuang/GenerSpeech" -> "Rongjiehuang/TranSpeech"
"Rongjiehuang/GenerSpeech" -> "b04901014/MQTTS"
"Rongjiehuang/GenerSpeech" -> "KevinMIN95/StyleSpeech"
"Rongjiehuang/GenerSpeech" -> "yangdongchao/Text-to-sound-Synthesis"
"Rongjiehuang/GenerSpeech" -> "liusongxiang/Large-Audio-Models"
"YuanGongND/ast" -> "YuanGongND/ssast"
"YuanGongND/ast" -> "kkoutini/PaSST"
"YuanGongND/ast" -> "qiuqiangkong/audioset_tagging_cnn"
"YuanGongND/ast" -> "YuanGongND/psla"
"YuanGongND/ast" -> "RetroCirce/HTS-Audio-Transformer"
"YuanGongND/ast" -> "facebookresearch/AudioMAE"
"YuanGongND/ast" -> "asteroid-team/torch-audiomentations"
"YuanGongND/ast" -> "karolpiczak/ESC-50"
"YuanGongND/ast" -> "google-research/leaf-audio"
"YuanGongND/ast" -> "iver56/audiomentations"
"YuanGongND/ast" -> "qiuqiangkong/torchlibrosa"
"YuanGongND/ast" -> "s3prl/s3prl"
"YuanGongND/ast" -> "sooftware/conformer"
"YuanGongND/ast" -> "nttcslab/byol-a"
"YuanGongND/ast" -> "KinWaiCheuk/nnAudio"
"node-modules/qn" -> "iwillwen/node-qiniu"
"crouchred/speaker-recognition-py3" -> "ppwwyyxx/speaker-recognition"
"crouchred/speaker-recognition-py3" -> "orchidas/Speaker-Recognition"
"crouchred/speaker-recognition-py3" -> "Walleclipse/Deep_Speaker-speaker_recognition_system"
"crouchred/speaker-recognition-py3" -> "wangleiai/dVectorSpeakerRecognition"
"crouchred/speaker-recognition-py3" -> "qqueing/DeepSpeaker-pytorch"
"crouchred/speaker-recognition-py3" -> "linhdvu14/vggvox-speaker-identification"
"crouchred/speaker-recognition-py3" -> "scelesticsiva/speaker_recognition_GMM_UBM"
"crouchred/speaker-recognition-py3" -> "astorfi/3D-convolutional-speaker-recognition"
"crouchred/speaker-recognition-py3" -> "GauravWaghmare/Speaker-Identification"
"crouchred/speaker-recognition-py3" -> "philipperemy/deep-speaker"
"crouchred/speaker-recognition-py3" -> "jymsuper/SpeakerRecognition_tutorial"
"crouchred/speaker-recognition-py3" -> "WeidiXie/VGG-Speaker-Recognition"
"crouchred/speaker-recognition-py3" -> "genzen2103/Speaker-Recognition-System-using-GMM"
"crouchred/speaker-recognition-py3" -> "Anwarvic/Speaker-Recognition"
"crouchred/speaker-recognition-py3" -> "idiap/kaldi-ivector"
"jimbozhang/kaldi-gop" -> "tbright17/kaldi-dnn-ali-gop"
"jimbozhang/kaldi-gop" -> "sweekarsud/Goodness-of-Pronunciation"
"jimbozhang/kaldi-gop" -> "jimbozhang/speechocean762"
"jimbozhang/kaldi-gop" -> "danijel3/KaldiWebrtcServer"
"jimbozhang/kaldi-gop" -> "dialogflow/asr-server"
"sigsep/norbert" -> "pfnet-research/meta-tasnet"
"sigsep/norbert" -> "francesclluis/source-separation-wavenet" ["e"=1]
"umeng/UMAndroidSdkDemo" -> "umeng/MultiFunctionAndroidDemo"
"BoragoCode/AttentionBasedProsodyPrediction" -> "Liu-Feng-deeplearning/TTS-frontend"
"BoragoCode/AttentionBasedProsodyPrediction" -> "Zeqiang-Lai/Prosody_Prediction"
"BoragoCode/AttentionBasedProsodyPrediction" -> "XierHacker/Model_Fusion_Based_Prosody_Prediction"
"Walleclipse/Deep_Speaker-speaker_recognition_system" -> "qqueing/DeepSpeaker-pytorch"
"Walleclipse/Deep_Speaker-speaker_recognition_system" -> "philipperemy/deep-speaker"
"Walleclipse/Deep_Speaker-speaker_recognition_system" -> "jymsuper/SpeakerRecognition_tutorial"
"Walleclipse/Deep_Speaker-speaker_recognition_system" -> "WeidiXie/VGG-Speaker-Recognition"
"Walleclipse/Deep_Speaker-speaker_recognition_system" -> "astorfi/3D-convolutional-speaker-recognition-pytorch"
"Walleclipse/Deep_Speaker-speaker_recognition_system" -> "a-nagrani/VGGVox"
"Walleclipse/Deep_Speaker-speaker_recognition_system" -> "bjfu-ai-institute/speaker-recognition-papers"
"Walleclipse/Deep_Speaker-speaker_recognition_system" -> "houzhengzhang/speaker_recognition"
"Walleclipse/Deep_Speaker-speaker_recognition_system" -> "crouchred/speaker-recognition-py3"
"Walleclipse/Deep_Speaker-speaker_recognition_system" -> "wangleiai/dVectorSpeakerRecognition"
"Walleclipse/Deep_Speaker-speaker_recognition_system" -> "Janghyun1230/Speaker_Verification"
"Walleclipse/Deep_Speaker-speaker_recognition_system" -> "andabi/voice-vector"
"Walleclipse/Deep_Speaker-speaker_recognition_system" -> "HarryVolek/PyTorch_Speaker_Verification"
"Walleclipse/Deep_Speaker-speaker_recognition_system" -> "linhdvu14/vggvox-speaker-identification"
"Walleclipse/Deep_Speaker-speaker_recognition_system" -> "Suhee05/Text-Independent-Speaker-Verification"
"nikvaessen/w2v2-speaker" -> "zyzisyz/mfa_conformer"
"Jamiroquai88/VBDiarization" -> "nryant/dscore"
"Jamiroquai88/VBDiarization" -> "taylorlu/Speaker-Diarization"
"Jamiroquai88/VBDiarization" -> "danpovey/pocolm"
"rhdunn/espeak" -> "espeak-ng/espeak-ng"
"opendmm/opendmm" -> "dbbbit/ninja-search"
"magenta/music-spectrogram-diffusion" -> "YatingMusic/ddsp-singing-vocoders"
"magenta/music-spectrogram-diffusion" -> "marcoppasini/musika"
"magenta/music-spectrogram-diffusion" -> "magenta/midi-ddsp"
"magenta/music-spectrogram-diffusion" -> "magenta/symbolic-music-diffusion" ["e"=1]
"magenta/music-spectrogram-diffusion" -> "Kinyugo/msanii"
"magenta/music-spectrogram-diffusion" -> "dada-bots/dadaGP" ["e"=1]
"magenta/music-spectrogram-diffusion" -> "dvruette/figaro" ["e"=1]
"magenta/music-spectrogram-diffusion" -> "sony/DiffRoll"
"magenta/music-spectrogram-diffusion" -> "salu133445/muspy" ["e"=1]
"magenta/music-spectrogram-diffusion" -> "NVIDIA/BigVGAN"
"magenta/music-spectrogram-diffusion" -> "YatingMusic/MuseMorphose" ["e"=1]
"magenta/music-spectrogram-diffusion" -> "archinetai/audio-diffusion-pytorch"
"magenta/music-spectrogram-diffusion" -> "YatingMusic/miditoolkit" ["e"=1]
"magenta/music-spectrogram-diffusion" -> "chrisdonahue/sheetsage"
"magenta/music-spectrogram-diffusion" -> "microsoft/CLAP"
"mindslab-ai/assem-vc" -> "mindslab-ai/cotatron"
"mindslab-ai/assem-vc" -> "Wendison/VQMIVC"
"mindslab-ai/assem-vc" -> "liusongxiang/ppg-vc"
"mindslab-ai/assem-vc" -> "mindslab-ai/univnet"
"mindslab-ai/assem-vc" -> "mindslab-ai/nuwave"
"mindslab-ai/assem-vc" -> "mindslab-ai/phaseaug"
"mindslab-ai/assem-vc" -> "keonlee9420/StyleSpeech"
"mindslab-ai/assem-vc" -> "mindslab-ai/wavegrad2"
"mindslab-ai/assem-vc" -> "yl4579/StarGANv2-VC"
"mindslab-ai/assem-vc" -> "thuhcsi/VAENAR-TTS"
"mindslab-ai/assem-vc" -> "hhguo/EA-SVC"
"mindslab-ai/assem-vc" -> "jxzhanggg/nonparaSeq2seqVC_code"
"mindslab-ai/assem-vc" -> "SungFeng-Huang/Meta-TTS"
"mindslab-ai/assem-vc" -> "k2kobayashi/crank"
"mindslab-ai/assem-vc" -> "mindslab-ai/nuwave2"
"Atul-Anand-Jha/Speaker-Identification-Python" -> "GauravWaghmare/Speaker-Identification"
"Atul-Anand-Jha/Speaker-Identification-Python" -> "oscarknagg/voicemap"
"Atul-Anand-Jha/Speaker-Identification-Python" -> "abhijeet3922/Speaker-identification-using-GMMs"
"Atul-Anand-Jha/Speaker-Identification-Python" -> "ppwwyyxx/speaker-recognition"
"Atul-Anand-Jha/Speaker-Identification-Python" -> "orchidas/Speaker-Recognition"
"Atul-Anand-Jha/Speaker-Identification-Python" -> "Walleclipse/Deep_Speaker-speaker_recognition_system"
"Atul-Anand-Jha/Speaker-Identification-Python" -> "Janghyun1230/Speaker_Verification"
"Atul-Anand-Jha/Speaker-Identification-Python" -> "crouchred/speaker-recognition-py3"
"Atul-Anand-Jha/Speaker-Identification-Python" -> "HarryVolek/PyTorch_Speaker_Verification"
"Atul-Anand-Jha/Speaker-Identification-Python" -> "SuperKogito/Voice-based-speaker-identification"
"WeidiXie/VGG-Speaker-Recognition" -> "taylorlu/Speaker-Diarization"
"WeidiXie/VGG-Speaker-Recognition" -> "a-nagrani/VGGVox"
"WeidiXie/VGG-Speaker-Recognition" -> "HarryVolek/PyTorch_Speaker_Verification"
"WeidiXie/VGG-Speaker-Recognition" -> "mycrazycracy/tf-kaldi-speaker"
"WeidiXie/VGG-Speaker-Recognition" -> "philipperemy/deep-speaker"
"WeidiXie/VGG-Speaker-Recognition" -> "qqueing/DeepSpeaker-pytorch"
"WeidiXie/VGG-Speaker-Recognition" -> "Walleclipse/Deep_Speaker-speaker_recognition_system"
"WeidiXie/VGG-Speaker-Recognition" -> "funcwj/ge2e-speaker-verification"
"WeidiXie/VGG-Speaker-Recognition" -> "mravanelli/SincNet"
"WeidiXie/VGG-Speaker-Recognition" -> "hsn-zeinali/x-vector-kaldi-tf"
"WeidiXie/VGG-Speaker-Recognition" -> "linhdvu14/vggvox-speaker-identification"
"WeidiXie/VGG-Speaker-Recognition" -> "Jungjee/RawNet"
"WeidiXie/VGG-Speaker-Recognition" -> "manojpamk/pytorch_xvectors"
"WeidiXie/VGG-Speaker-Recognition" -> "Snowdar/asv-subtools"
"WeidiXie/VGG-Speaker-Recognition" -> "Janghyun1230/Speaker_Verification"
"iamyuanchung/VQ-APC" -> "Alexander-H-Liu/NPC"
"RaviSoji/plda" -> "RicherMans/PLDA"
"RaviSoji/plda" -> "cvqluu/TDNN"
"RaviSoji/plda" -> "iiscleap/NeuralPlda"
"RaviSoji/plda" -> "manojpamk/pytorch_xvectors"
"RaviSoji/plda" -> "juanmc2005/torch-plda"
"RaviSoji/plda" -> "vzxxbacq/PLDA"
"RaviSoji/plda" -> "zeroQiaoba/ivector-xvector"
"rizar/attention-lvcsr" -> "hirofumi0810/tensorflow_end2end_speech_recognition"
"rizar/attention-lvcsr" -> "yajiemiao/eesen"
"rizar/attention-lvcsr" -> "dmitriy-serdyuk/kaldi-python"
"rizar/attention-lvcsr" -> "amaas/stanford-ctc"
"rizar/attention-lvcsr" -> "srvk/eesen"
"rizar/attention-lvcsr" -> "vrenkens/tfkaldi"
"rizar/attention-lvcsr" -> "mila-udem/blocks-examples" ["e"=1]
"rizar/attention-lvcsr" -> "yajiemiao/kaldipdnn"
"rizar/attention-lvcsr" -> "lingochamp/kaldi-ctc"
"rizar/attention-lvcsr" -> "rwth-i6/returnn"
"rizar/attention-lvcsr" -> "YiwenShaoStephen/pychain"
"rizar/attention-lvcsr" -> "vrenkens/nabu"
"rizar/attention-lvcsr" -> "NervanaSystems/deepspeech"
"rizar/attention-lvcsr" -> "SeanNaren/deepspeech.torch"
"facebookresearch/loop" -> "sotelo/parrot"
"facebookresearch/loop" -> "CSTR-Edinburgh/merlin"
"facebookresearch/loop" -> "r9y9/deepvoice3_pytorch"
"facebookresearch/loop" -> "kan-bayashi/PytorchWaveNetVocoder"
"facebookresearch/loop" -> "r9y9/gantts"
"facebookresearch/loop" -> "Kyubyong/deepvoice3"
"facebookresearch/loop" -> "Kyubyong/tacotron"
"facebookresearch/loop" -> "mozilla/LPCNet"
"facebookresearch/loop" -> "mmorise/World"
"facebookresearch/loop" -> "ksw0306/FloWaveNet"
"facebookresearch/loop" -> "r9y9/nnmnkwii"
"facebookresearch/loop" -> "NVIDIA/nv-wavenet"
"facebookresearch/loop" -> "tomlepaine/fast-wavenet"
"facebookresearch/loop" -> "soroushmehr/sampleRNN_ICLR2017"
"facebookresearch/loop" -> "syang1993/gst-tacotron"
"RayeRen/acad-homepage.github.io" -> "RayeRen/rayeren.github.io"
"RayeRen/acad-homepage.github.io" -> "yaoyao-liu/minimal-light"
"RayeRen/acad-homepage.github.io" -> "KevinMIN95/StyleSpeech"
"glam-imperial/EmotionalConversionStarGAN" -> "KunZhou9646/Emovox"
"glam-imperial/EmotionalConversionStarGAN" -> "KunZhou9646/controllable_evc_code"
"glam-imperial/EmotionalConversionStarGAN" -> "KrishnaDN/speech-emotion-recognition-using-self-attention" ["e"=1]
"rupeshs/neuralsongstyle" -> "DmitryUlyanov/neural-style-audio-tf"
"rupeshs/neuralsongstyle" -> "vadim-v-lebedev/audio_style_tranfer"
"Jeeseung-Park/Styleformer" -> "mindslab-ai/pnlp-mixer"
"Jeeseung-Park/Styleformer" -> "younggeun-kim/NCSR"
"Jeeseung-Park/Styleformer" -> "younggeun-kim/Styleformer"
"AIFanatic/google-offline-speech-recognition" -> "biemster/gasr"
"AIFanatic/google-offline-speech-recognition" -> "biemster/asr"
"rhasspy/gruut" -> "as-ideas/DeepPhonemizer"
"zszyellow/WER-in-python" -> "belambert/asr-evaluation"
"zszyellow/WER-in-python" -> "jitsi/asr-wer"
"iver56/audiomentations" -> "asteroid-team/torch-audiomentations"
"iver56/audiomentations" -> "facebookresearch/WavAugment"
"iver56/audiomentations" -> "qiuqiangkong/audioset_tagging_cnn"
"iver56/audiomentations" -> "qiuqiangkong/torchlibrosa"
"iver56/audiomentations" -> "KinWaiCheuk/nnAudio"
"iver56/audiomentations" -> "aliutkus/speechmetrics" ["e"=1]
"iver56/audiomentations" -> "microsoft/DNS-Challenge" ["e"=1]
"iver56/audiomentations" -> "s3prl/s3prl"
"iver56/audiomentations" -> "YuanGongND/ast"
"iver56/audiomentations" -> "LCAV/pyroomacoustics" ["e"=1]
"iver56/audiomentations" -> "DemisEom/SpecAugment"
"iver56/audiomentations" -> "pytorch/audio"
"iver56/audiomentations" -> "zcaceres/spec_augment"
"iver56/audiomentations" -> "google-research/leaf-audio"
"iver56/audiomentations" -> "speechbrain/speechbrain"
"ddlBoJack/Awesome-Speech-Pretraining" -> "ddlBoJack/Speech-Resources"
"biemster/gasr" -> "biemster/gtts"
"biemster/gasr" -> "AIFanatic/google-offline-speech-recognition"
"biemster/gasr" -> "biemster/asr"
"1ytic/warp-rnnt" -> "HawkAaron/warp-transducer"
"1ytic/warp-rnnt" -> "awni/transducer"
"1ytic/warp-rnnt" -> "tencent-ailab/pika"
"1ytic/warp-rnnt" -> "ZhengkunTian/rnn-transducer"
"1ytic/warp-rnnt" -> "cywang97/StreamingTransformer"
"1ytic/warp-rnnt" -> "HawkAaron/RNN-Transducer"
"1ytic/warp-rnnt" -> "hirofumi0810/neural_sp"
"1ytic/warp-rnnt" -> "csukuangfj/optimized_transducer"
"1ytic/warp-rnnt" -> "thu-spmi/CAT"
"1ytic/warp-rnnt" -> "danpovey/fast_rnnt"
"1ytic/warp-rnnt" -> "YiwenShaoStephen/pychain"
"1ytic/warp-rnnt" -> "noahchalifour/rnnt-speech-recognition"
"1ytic/warp-rnnt" -> "HawkAaron/E2E-ASR"
"1ytic/warp-rnnt" -> "theblackcat102/Online-Speech-Recognition"
"1ytic/warp-rnnt" -> "iamjanvijay/rnnt_decoder_cuda"
"JRMeyer/open-speech-corpora" -> "gooofy/zamia-speech"
"JRMeyer/open-speech-corpora" -> "Kyubyong/g2p"
"JRMeyer/open-speech-corpora" -> "freewym/espresso"
"JRMeyer/open-speech-corpora" -> "Kyubyong/css10"
"JRMeyer/open-speech-corpora" -> "jim-schwoebel/voice_datasets" ["e"=1]
"JRMeyer/open-speech-corpora" -> "xingchensong/speech-recognition-papers"
"JRMeyer/open-speech-corpora" -> "ynop/audiomate"
"JRMeyer/open-speech-corpora" -> "Helsinki-NLP/prosody"
"JRMeyer/open-speech-corpora" -> "mobvoi/wenet"
"JRMeyer/open-speech-corpora" -> "aliutkus/speechmetrics" ["e"=1]
"JRMeyer/open-speech-corpora" -> "robmsmt/ASR_Audio_Data_Links"
"JRMeyer/open-speech-corpora" -> "facebookresearch/WavAugment"
"JRMeyer/open-speech-corpora" -> "vesis84/kaldi-io-for-python"
"JRMeyer/open-speech-corpora" -> "hirofumi0810/neural_sp"
"JRMeyer/open-speech-corpora" -> "cywang97/StreamingTransformer"
"YiwenShaoStephen/pychain" -> "YiwenShaoStephen/pychain_example"
"YiwenShaoStephen/pychain" -> "jzlianglu/pykaldi2"
"YiwenShaoStephen/pychain" -> "thu-spmi/CAT"
"YiwenShaoStephen/pychain" -> "idiap/pkwrap"
"YiwenShaoStephen/pychain" -> "danpovey/k2"
"YiwenShaoStephen/pychain" -> "freewym/espresso"
"YiwenShaoStephen/pychain" -> "XiaoMi/kaldi-onnx"
"YiwenShaoStephen/pychain" -> "tencent-ailab/pika"
"YiwenShaoStephen/pychain" -> "facebookresearch/gtn"
"YiwenShaoStephen/pychain" -> "HawkAaron/warp-transducer"
"YiwenShaoStephen/pychain" -> "nttcslab-sp/kaldiio"
"YiwenShaoStephen/pychain" -> "hirofumi0810/neural_sp"
"YiwenShaoStephen/pychain" -> "awni/transducer"
"YiwenShaoStephen/pychain" -> "cywang97/StreamingTransformer"
"YiwenShaoStephen/pychain" -> "k2-fsa/snowfall"
"danpovey/fast_rnnt" -> "csukuangfj/optimized_transducer"
"danpovey/fast_rnnt" -> "csukuangfj/transducer-loss-benchmarking"
"danpovey/fast_rnnt" -> "csukuangfj/kaldifeat"
"gentaiscool/end2end-asr-pytorch" -> "kaituoxu/Speech-Transformer"
"gentaiscool/end2end-asr-pytorch" -> "HawkAaron/E2E-ASR"
"gentaiscool/end2end-asr-pytorch" -> "ZhengkunTian/rnn-transducer"
"gentaiscool/end2end-asr-pytorch" -> "hirofumi0810/neural_sp"
"gentaiscool/end2end-asr-pytorch" -> "ZhengkunTian/OpenTransformer"
"gentaiscool/end2end-asr-pytorch" -> "sooftware/End-to-End-Speech-Recognition-Models"
"gentaiscool/end2end-asr-pytorch" -> "thu-spmi/CAT"
"gentaiscool/end2end-asr-pytorch" -> "kaituoxu/Listen-Attend-Spell"
"gentaiscool/end2end-asr-pytorch" -> "Alexander-H-Liu/End-to-end-ASR-Pytorch"
"gentaiscool/end2end-asr-pytorch" -> "by2101/OpenASR"
"gentaiscool/end2end-asr-pytorch" -> "Diamondfan/CTC_pytorch"
"gentaiscool/end2end-asr-pytorch" -> "openspeech-team/openspeech"
"gentaiscool/end2end-asr-pytorch" -> "awni/speech"
"gentaiscool/end2end-asr-pytorch" -> "zcaceres/spec_augment"
"gentaiscool/end2end-asr-pytorch" -> "foamliu/Speech-Transformer"
"heatz123/naturalspeech" -> "anonymous-pits/pits"
"heatz123/naturalspeech" -> "PlayVoice/vits_chinese"
"heatz123/naturalspeech" -> "yl4579/StyleTTS-VC"
"heatz123/naturalspeech" -> "MasayaKawamura/MB-iSTFT-VITS"
"heatz123/naturalspeech" -> "wenet-e2e/wetts"
"heatz123/naturalspeech" -> "imdanboy/jets"
"heatz123/naturalspeech" -> "b04901014/MQTTS"
"heatz123/naturalspeech" -> "hcy71o/TransferTTS"
"heatz123/naturalspeech" -> "NVIDIA/radtts"
"heatz123/naturalspeech" -> "keonlee9420/Comprehensive-Transformer-TTS"
"heatz123/naturalspeech" -> "collabora/spear-tts-pytorch"
"heatz123/naturalspeech" -> "KevinMIN95/StyleSpeech"
"heatz123/naturalspeech" -> "keonlee9420/Comprehensive-E2E-TTS"
"iamyuanchung/Autoregressive-Predictive-Coding" -> "andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning"
"iamyuanchung/Autoregressive-Predictive-Coding" -> "facebookresearch/CPC_audio"
"iamyuanchung/Autoregressive-Predictive-Coding" -> "iamyuanchung/VQ-APC"
"iamyuanchung/Autoregressive-Predictive-Coding" -> "Alexander-H-Liu/NPC"
"iamyuanchung/Autoregressive-Predictive-Coding" -> "facebookresearch/libri-light"
"jzlianglu/pykaldi2" -> "YiwenShaoStephen/pychain"
"jzlianglu/pykaldi2" -> "danpovey/k2"
"jzlianglu/pykaldi2" -> "nttcslab-sp/kaldiio"
"jzlianglu/pykaldi2" -> "pykaldi/pykaldi"
"jzlianglu/pykaldi2" -> "vesis84/kaldi-io-for-python"
"jzlianglu/pykaldi2" -> "aishell-foundation/DaCiDian"
"jzlianglu/pykaldi2" -> "athena-team/athena-decoder"
"nicklashansen/voice-activity-detection" -> "filippogiruzzi/voice_activity_detection"
"nicklashansen/voice-activity-detection" -> "voithru/voice-activity-detection"
"nicklashansen/voice-activity-detection" -> "hcmlab/vadnet"
"nicklashansen/voice-activity-detection" -> "jymsuper/VAD_tutorial"
"nicklashansen/voice-activity-detection" -> "jtkim-kaist/VAD"
"nicklashansen/voice-activity-detection" -> "eesungkim/Voice_Activity_Detector"
"theblackcat102/Online-Speech-Recognition" -> "noahchalifour/rnnt-speech-recognition"
"theblackcat102/Online-Speech-Recognition" -> "k2-fsa/snowfall"
"theblackcat102/Online-Speech-Recognition" -> "cywang97/StreamingTransformer"
"theblackcat102/Online-Speech-Recognition" -> "athena-team/athena-decoder"
"theblackcat102/Online-Speech-Recognition" -> "xingchensong/speech-recognition-papers"
"theblackcat102/Online-Speech-Recognition" -> "hirofumi0810/neural_sp"
"theblackcat102/Online-Speech-Recognition" -> "HawkAaron/RNN-Transducer"
"theblackcat102/Online-Speech-Recognition" -> "tencent-ailab/pika"
"theblackcat102/Online-Speech-Recognition" -> "HaoranMiao/streaming-attention"
"caillonantoine/RAVE" -> "Fyfe93/RAVE-audition"
"caillonantoine/RAVE" -> "ben-hayes/neural-waveshaping-synthesis"
"caillonantoine/RAVE" -> "acids-ircam/nn_tilde"
"caillonantoine/RAVE" -> "acids-ircam/ddsp_pytorch"
"caillonantoine/RAVE" -> "csteinmetz1/steerable-nafx"
"caillonantoine/RAVE" -> "moiseshorta/MelSpecVAE"
"caillonantoine/RAVE" -> "acids-ircam/rave_vst"
"caillonantoine/RAVE" -> "ilaria-manco/word2wave"
"caillonantoine/RAVE" -> "galgreshler/Catch-A-Waveform"
"caillonantoine/RAVE" -> "rncm-prism/prism-samplernn"
"caillonantoine/RAVE" -> "QosmoInc/neutone_sdk"
"caillonantoine/RAVE" -> "hyakuchiki/diffsynth"
"parlance/ctcdecode" -> "githubharald/CTCDecoder"
"parlance/ctcdecode" -> "kensho-technologies/pyctcdecode"
"parlance/ctcdecode" -> "githubharald/CTCWordBeamSearch"
"parlance/ctcdecode" -> "nanoporetech/fast-ctc-decode"
"parlance/ctcdecode" -> "hirofumi0810/neural_sp"
"parlance/ctcdecode" -> "thu-spmi/CAT"
"parlance/ctcdecode" -> "kpu/kenlm"
"parlance/ctcdecode" -> "SeanNaren/warp-ctc" ["e"=1]
"parlance/ctcdecode" -> "srvk/eesen"
"parlance/ctcdecode" -> "YiwenShaoStephen/pychain"
"parlance/ctcdecode" -> "k2-fsa/k2"
"parlance/ctcdecode" -> "SeanNaren/deepspeech.pytorch"
"parlance/ctcdecode" -> "HawkAaron/warp-transducer"
"parlance/ctcdecode" -> "lumaku/ctc-segmentation"
"parlance/ctcdecode" -> "corticph/prefix-beam-search"
"YuanGongND/psla" -> "YuanGongND/ssast"
"YuanGongND/psla" -> "kkoutini/PaSST"
"YuanGongND/psla" -> "turpaultn/DESED" ["e"=1]
"YuanGongND/psla" -> "qiuqiangkong/panns_inference"
"microsoft/UniSpeech" -> "s3prl/s3prl"
"microsoft/UniSpeech" -> "wenet-e2e/wespeaker"
"microsoft/UniSpeech" -> "ddlBoJack/Speech-Resources"
"microsoft/UniSpeech" -> "nttcslab-sp/EEND-vector-clustering"
"microsoft/UniSpeech" -> "BUTSpeechFIT/VBx"
"microsoft/UniSpeech" -> "SpeechColab/GigaSpeech"
"microsoft/UniSpeech" -> "nryant/dscore"
"microsoft/UniSpeech" -> "Snowdar/asv-subtools"
"microsoft/UniSpeech" -> "nttcslab-sp/kaldiio"
"microsoft/UniSpeech" -> "vesis84/kaldi-io-for-python"
"microsoft/UniSpeech" -> "ddlBoJack/Awesome-Speech-Pretraining"
"vlomme/Multi-Tacotron-Voice-Cloning" -> "nsu-ai/russian_g2p" ["e"=1]
"vlomme/Multi-Tacotron-Voice-Cloning" -> "SforAiDl/Neural-Voice-Cloning-With-Few-Samples"
"vlomme/Multi-Tacotron-Voice-Cloning" -> "deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning"
"vlomme/Multi-Tacotron-Voice-Cloning" -> "snakers4/open_stt" ["e"=1]
"vlomme/Multi-Tacotron-Voice-Cloning" -> "Tomiinek/Multilingual_Text_to_Speech"
"vlomme/Multi-Tacotron-Voice-Cloning" -> "snakers4/russian_stt_text_normalization" ["e"=1]
"vlomme/Multi-Tacotron-Voice-Cloning" -> "NVIDIA/flowtron"
"awslabs/handwritten-text-recognition-for-apache-mxnet" -> "sushant097/Handwritten-Line-Text-Recognition-using-Deep-Learning-with-Tensorflow"
"awslabs/handwritten-text-recognition-for-apache-mxnet" -> "arthurflor23/handwritten-text-recognition"
"awslabs/handwritten-text-recognition-for-apache-mxnet" -> "githubharald/SimpleHTR"
"awslabs/handwritten-text-recognition-for-apache-mxnet" -> "Breta01/handwriting-ocr"
"awslabs/handwritten-text-recognition-for-apache-mxnet" -> "ThomasDelteil/HandwrittenTextRecognition_MXNet"
"awslabs/handwritten-text-recognition-for-apache-mxnet" -> "lamhoangtung/LineHTR"
"awslabs/handwritten-text-recognition-for-apache-mxnet" -> "githubharald/WordDetector"
"awslabs/handwritten-text-recognition-for-apache-mxnet" -> "0x454447415244/HandwritingRecognitionSystem"
"awslabs/handwritten-text-recognition-for-apache-mxnet" -> "cwig/start_follow_read"
"awslabs/handwritten-text-recognition-for-apache-mxnet" -> "lquirosd/P2PaLA" ["e"=1]
"awslabs/handwritten-text-recognition-for-apache-mxnet" -> "githubharald/WordDetectorNN"
"awslabs/handwritten-text-recognition-for-apache-mxnet" -> "githubharald/CTCWordBeamSearch"
"awslabs/handwritten-text-recognition-for-apache-mxnet" -> "amzn/convolutional-handwriting-gan" ["e"=1]
"awslabs/handwritten-text-recognition-for-apache-mxnet" -> "Grzego/handwriting-generation" ["e"=1]
"awslabs/handwritten-text-recognition-for-apache-mxnet" -> "zacharywhitley/awesome-ocr" ["e"=1]
"TaoRuijie/Loss-Gated-Learning" -> "TaoRuijie/AVCleanse"
"julius-speech/dictation-kit" -> "julius-speech/segmentation-kit"
"WICWIU/WICWIU" -> "callee2006/HGUNeuralNetworks"
"WICWIU/WICWIU" -> "callee2006/2019-Winter-HGU-Machine-Learing-Camp"
"hmartiro/riffusion-inference" -> "hmartiro/riffusion-app"
"hmartiro/riffusion-inference" -> "archinetai/audio-diffusion-pytorch"
"hmartiro/riffusion-inference" -> "teticio/audio-diffusion"
"hmartiro/riffusion-inference" -> "LAION-AI/audio-dataset"
"hmartiro/riffusion-inference" -> "NVIDIA/BigVGAN"
"hmartiro/riffusion-inference" -> "magenta/music-spectrogram-diffusion"
"hmartiro/riffusion-inference" -> "marcoppasini/musika"
"hmartiro/riffusion-inference" -> "LAION-AI/CLAP"
"hmartiro/riffusion-inference" -> "Rongjiehuang/FastDiff"
"hmartiro/riffusion-inference" -> "Harmonai-org/sample-generator"
"hmartiro/riffusion-inference" -> "csteinmetz1/auraloss"
"hmartiro/riffusion-inference" -> "ilaria-manco/multimodal-ml-music"
"hmartiro/riffusion-inference" -> "dome272/Paella" ["e"=1]
"hmartiro/riffusion-inference" -> "chavinlo/riffusion-manipulation"
"acids-ircam/creative_ml" -> "fcaspe/ddx7"
"LAION-AI/audio-dataset" -> "LAION-AI/CLAP"
"LAION-AI/audio-dataset" -> "microsoft/CLAP"
"LAION-AI/audio-dataset" -> "facebookresearch/AudioMAE"
"LAION-AI/audio-dataset" -> "yangdongchao/Text-to-sound-Synthesis"
"LAION-AI/audio-dataset" -> "liusongxiang/Large-Audio-Models"
"LAION-AI/audio-dataset" -> "teticio/audio-diffusion"
"LAION-AI/audio-dataset" -> "lucidrains/natural-speech-pytorch"
"LAION-AI/audio-dataset" -> "archinetai/audio-diffusion-pytorch"
"LAION-AI/audio-dataset" -> "XinhaoMei/WavCaps"
"LAION-AI/audio-dataset" -> "lucidrains/audiolm-pytorch"
"LAION-AI/audio-dataset" -> "haoheliu/audioldm_eval"
"LAION-AI/audio-dataset" -> "NVIDIA/BigVGAN"
"LAION-AI/audio-dataset" -> "gudgud96/frechet-audio-distance"
"LAION-AI/audio-dataset" -> "liuxubo717/sound_generation"
"jonatasgrosman/huggingsound" -> "jonatasgrosman/wav2vec2-sprint"
"jonatasgrosman/huggingsound" -> "jonatasgrosman/asrecognition"
"jonatasgrosman/huggingsound" -> "oliverguhr/wav2vec2-live"
"jonatasgrosman/huggingsound" -> "huggingface/speechbox" ["e"=1]
"jonatasgrosman/huggingsound" -> "lumaku/ctc-segmentation"
"jonatasgrosman/huggingsound" -> "ccoreilly/wav2vec2-service"
"jonatasgrosman/huggingsound" -> "patrickvonplaten/Wav2Vec2_PyCTCDecode"
"jonatasgrosman/huggingsound" -> "Edresson/Wav2Vec-Wrapper"
"jonatasgrosman/huggingsound" -> "kensho-technologies/pyctcdecode"
"jonatasgrosman/huggingsound" -> "mailong25/self-supervised-speech-recognition"
"jonatasgrosman/huggingsound" -> "farisalasmary/wav2vec2-kenlm"
"kssteven418/Squeezeformer" -> "upskyy/Squeezeformer"
"kssteven418/Squeezeformer" -> "burchim/EfficientConformer"
"kssteven418/Squeezeformer" -> "jctian98/e2e_lfmmi"
"kssteven418/Squeezeformer" -> "k2-fsa/icefall"
"kssteven418/Squeezeformer" -> "tencent-ailab/3m-asr"
"lucidrains/conformer" -> "sooftware/conformer"
"lucidrains/conformer" -> "burchim/EfficientConformer"
"lucidrains/conformer" -> "TensorSpeech/TensorFlowASR"
"lucidrains/natural-speech-pytorch" -> "keonlee9420/Comprehensive-E2E-TTS"
"lucidrains/natural-speech-pytorch" -> "yerfor/SyntaSpeech"
"lucidrains/natural-speech-pytorch" -> "keonlee9420/StyleSpeech"
"lucidrains/natural-speech-pytorch" -> "rishikksh20/Avocodo-pytorch"
"lucidrains/natural-speech-pytorch" -> "mindslab-ai/phaseaug"
"lucidrains/natural-speech-pytorch" -> "keonlee9420/Cross-Speaker-Emotion-Transfer"
"lucidrains/natural-speech-pytorch" -> "NVIDIA/radtts"
"lucidrains/natural-speech-pytorch" -> "KevinMIN95/StyleSpeech"
"lucidrains/natural-speech-pytorch" -> "dunky11/voicesmith"
"lucidrains/natural-speech-pytorch" -> "NVIDIA/BigVGAN"
"lucidrains/natural-speech-pytorch" -> "rendchevi/nix-tts"
"lucidrains/natural-speech-pytorch" -> "Rongjiehuang/FastDiff"
"lucidrains/natural-speech-pytorch" -> "BridgetteSong/ExpressiveTacotron"
"lucidrains/natural-speech-pytorch" -> "yl4579/StyleTTS"
"lumaku/ctc-segmentation" -> "cywang97/StreamingTransformer"
"lumaku/ctc-segmentation" -> "YiwenShaoStephen/pychain"
"lumaku/ctc-segmentation" -> "cornerfarmer/ctc_segmentation"
"lumaku/ctc-segmentation" -> "sarulab-speech/jtubespeech"
"lumaku/ctc-segmentation" -> "kensho-technologies/pyctcdecode"
"lumaku/ctc-segmentation" -> "hirofumi0810/neural_sp"
"lumaku/ctc-segmentation" -> "csukuangfj/kaldifeat"
"lumaku/ctc-segmentation" -> "facebookresearch/voxpopuli"
"lumaku/ctc-segmentation" -> "burchim/EfficientConformer"
"xingchensong/Speech-Transformer-tf2.0" -> "foamliu/Speech-Transformer"
"easemob/sdkdemoapp3.0_android" -> "easemob/easeui"
"easemob/sdkdemoapp3.0_android" -> "easemob/sdkexamples-android"
"zhaohui8969/VST_NetProcess-" -> "Kurisu-Preston/AI-aqua-vc"
"zhaohui8969/VST_NetProcess-" -> "openvpi/diff-svc"
"zhaohui8969/VST_NetProcess-" -> "fishaudio/fish-diffusion"
"zhaohui8969/VST_NetProcess-" -> "IceKyrin/sovits_f0_infer"
"zhaohui8969/VST_NetProcess-" -> "NaruseMioShirakana/MoeSS"
"zhaohui8969/VST_NetProcess-" -> "yxlllc/DDSP-SVC"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "kan-bayashi/ParallelWaveGAN"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "pettarin/forced-alignment-tools"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "ming024/FastSpeech2"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "Kyubyong/g2p"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "jik876/hifi-gan"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "bootphon/phonemizer"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "xcmyz/FastSpeech"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "jaywalnut310/glow-tts"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "open-speech/speech-aligner"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "xcmyz/speech-synthesis-paper"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "Jackiexiao/MTTS"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "mozilla/LPCNet"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "kakaobrain/g2pM"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "speechio/chinese_text_normalization"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "lowerquality/gentle"
"OlaWod/FreeVC" -> "MelissaChen15/control-vc"
"OlaWod/FreeVC" -> "PlayVoice/VI-SVC"
"OlaWod/FreeVC" -> "bshall/soft-vc"
"OlaWod/FreeVC" -> "yl4579/StyleTTS-VC"
"OlaWod/FreeVC" -> "MasayaKawamura/MB-iSTFT-VITS"
"OlaWod/FreeVC" -> "liusongxiang/ppg-vc"
"OlaWod/FreeVC" -> "mindslab-ai/assem-vc"
"OlaWod/FreeVC" -> "anonymous-pits/pits"
"OlaWod/FreeVC" -> "yl4579/StyleTTS"
"OlaWod/FreeVC" -> "Rongjiehuang/GenerSpeech"
"OlaWod/FreeVC" -> "yl4579/StarGANv2-VC"
"OlaWod/FreeVC" -> "NVIDIA/radtts"
"OlaWod/FreeVC" -> "bshall/hubert"
"OlaWod/FreeVC" -> "Wendison/VQMIVC"
"OlaWod/FreeVC" -> "M4Singer/M4Singer"
"as-ideas/DeepPhonemizer" -> "rhasspy/gruut"
"as-ideas/DeepPhonemizer" -> "CUNY-CL/wikipron"
"as-ideas/DeepPhonemizer" -> "keonlee9420/StyleSpeech"
"as-ideas/DeepPhonemizer" -> "liusongxiang/Large-Audio-Models"
"as-ideas/DeepPhonemizer" -> "keonlee9420/DailyTalk"
"as-ideas/DeepPhonemizer" -> "keonlee9420/Comprehensive-Transformer-TTS"
"as-ideas/DeepPhonemizer" -> "Edresson/SC-GlowTTS"
"as-ideas/DeepPhonemizer" -> "NVIDIA/NeMo-text-processing"
"as-ideas/DeepPhonemizer" -> "mindslab-ai/phaseaug"
"as-ideas/DeepPhonemizer" -> "lingjzhu/CharsiuG2P"
"as-ideas/DeepPhonemizer" -> "mindslab-ai/assem-vc"
"b04901014/MQTTS" -> "Rongjiehuang/GenerSpeech"
"bootphon/phonemizer" -> "Kyubyong/g2p"
"bootphon/phonemizer" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"bootphon/phonemizer" -> "kan-bayashi/ParallelWaveGAN"
"bootphon/phonemizer" -> "jik876/hifi-gan"
"bootphon/phonemizer" -> "jaywalnut310/glow-tts"
"bootphon/phonemizer" -> "ming024/FastSpeech2"
"bootphon/phonemizer" -> "NVIDIA/BigVGAN"
"bootphon/phonemizer" -> "mozilla/LPCNet"
"bootphon/phonemizer" -> "xcmyz/speech-synthesis-paper"
"bootphon/phonemizer" -> "dmort27/epitran"
"bootphon/phonemizer" -> "aliutkus/speechmetrics" ["e"=1]
"bootphon/phonemizer" -> "xcmyz/FastSpeech"
"bootphon/phonemizer" -> "NVIDIA/mellotron"
"bootphon/phonemizer" -> "facebookresearch/WavAugment"
"bootphon/phonemizer" -> "Kyubyong/css10"
"bshall/hubert" -> "bshall/soft-vc"
"bshall/hubert" -> "bshall/acoustic-model"
"keonlee9420/Comprehensive-Transformer-TTS" -> "keonlee9420/StyleSpeech"
"keonlee9420/Comprehensive-Transformer-TTS" -> "KevinMIN95/StyleSpeech"
"keonlee9420/Comprehensive-Transformer-TTS" -> "keonlee9420/Parallel-Tacotron2"
"keonlee9420/Comprehensive-Transformer-TTS" -> "keonlee9420/Comprehensive-E2E-TTS"
"keonlee9420/Comprehensive-Transformer-TTS" -> "NVIDIA/radtts"
"keonlee9420/Comprehensive-Transformer-TTS" -> "keonlee9420/DiffGAN-TTS"
"keonlee9420/Comprehensive-Transformer-TTS" -> "keonlee9420/PortaSpeech"
"keonlee9420/Comprehensive-Transformer-TTS" -> "rishikksh20/iSTFTNet-pytorch"
"keonlee9420/Comprehensive-Transformer-TTS" -> "mindslab-ai/univnet"
"keonlee9420/Comprehensive-Transformer-TTS" -> "Rongjiehuang/GenerSpeech"
"keonlee9420/Comprehensive-Transformer-TTS" -> "keonlee9420/STYLER"
"keonlee9420/Comprehensive-Transformer-TTS" -> "keonlee9420/Expressive-FastSpeech2"
"keonlee9420/Comprehensive-Transformer-TTS" -> "xcmyz/speech-synthesis-paper"
"keonlee9420/Comprehensive-Transformer-TTS" -> "DigitalPhonetics/IMS-Toucan"
"keonlee9420/Comprehensive-Transformer-TTS" -> "tts-tutorial/survey"
"keonlee9420/Cross-Speaker-Emotion-Transfer" -> "keonlee9420/StyleSpeech"
"keonlee9420/Cross-Speaker-Emotion-Transfer" -> "ubisoft/ubisoft-laforge-daft-exprt"
"lingjzhu/CharsiuG2P" -> "xinjli/transphone"
"quadrismegistus/prosodic" -> "quadrismegistus/poesy"
"quadrismegistus/prosodic" -> "Helsinki-NLP/prosody"
"quadrismegistus/prosodic" -> "quadrismegistus/litlab-poetry"
"quadrismegistus/prosodic" -> "hyperreality/Poetry-Tools"
"quadrismegistus/prosodic" -> "manexagirrezabal/zeuscansion"
"tts-tutorial/survey" -> "xcmyz/speech-synthesis-paper"
"tts-tutorial/survey" -> "LEEYOONHYUNG/BVAE-TTS"
"tts-tutorial/survey" -> "keonlee9420/StyleSpeech"
"tts-tutorial/survey" -> "jaywalnut310/glow-tts"
"tts-tutorial/survey" -> "thuhcsi/VAENAR-TTS"
"tts-tutorial/survey" -> "keonlee9420/Comprehensive-Transformer-TTS"
"tts-tutorial/survey" -> "kakaobrain/g2pM"
"tts-tutorial/survey" -> "jik876/hifi-gan"
"tts-tutorial/survey" -> "Wendison/VQMIVC"
"tts-tutorial/survey" -> "KevinMIN95/StyleSpeech"
"tts-tutorial/survey" -> "facebookresearch/BinauralSpeechSynthesis"
"tts-tutorial/survey" -> "NVIDIA/BigVGAN"
"tts-tutorial/survey" -> "facebookresearch/vocoder-benchmark"
"tts-tutorial/survey" -> "nii-yamagishilab/project-NN-Pytorch-scripts"
"tts-tutorial/survey" -> "facebookresearch/speech-resynthesis"
"Kyubyong/g2pC" -> "kakaobrain/g2pM"
"Kyubyong/g2pC" -> "BoragoCode/AttentionBasedProsodyPrediction"
"Kyubyong/g2pC" -> "open-speech/speech-aligner"
"Kyubyong/g2pC" -> "thuhcsi/Crystal"
"Kyubyong/g2pC" -> "hjzin/PolyphoneDisambiguation"
"Kyubyong/g2pC" -> "speechio/BigCiDian"
"Kyubyong/g2pC" -> "speechio/chinese_text_normalization"
"Kyubyong/g2pC" -> "Yablon/auorange"
"facebookresearch/voxpopuli" -> "facebookresearch/libri-light"
"facebookresearch/voxpopuli" -> "cywang97/StreamingTransformer"
"facebookresearch/voxpopuli" -> "SpeechColab/GigaSpeech"
"facebookresearch/voxpopuli" -> "lumaku/ctc-segmentation"
"facebookresearch/voxpopuli" -> "kahne/SpeechTransProgress" ["e"=1]
"facebookresearch/voxpopuli" -> "facebookresearch/WavAugment"
"facebookresearch/voxpopuli" -> "tencent-ailab/pika"
"facebookresearch/voxpopuli" -> "google-research-datasets/cvss" ["e"=1]
"facebookresearch/voxpopuli" -> "thu-spmi/CAT"
"facebookresearch/voxpopuli" -> "facebookresearch/covost" ["e"=1]
"facebookresearch/voxpopuli" -> "xingchensong/speech-recognition-papers"
"facebookresearch/voxpopuli" -> "hirofumi0810/neural_sp"
"facebookresearch/voxpopuli" -> "andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning"
"facebookresearch/voxpopuli" -> "k2-fsa/k2"
"facebookresearch/voxpopuli" -> "facebookresearch/CPC_audio"
"nttcslab/byol-a" -> "edufonseca/uclser20"
"nttcslab/byol-a" -> "Spijkervet/CLMR" ["e"=1]
"KuangDD/phkit" -> "KuangDD/aukit"
"KuangDD/phkit" -> "kakaobrain/g2pM"
"andyweiqiu/asr-ios-local" -> "andyweiqiu/SpeechRecognition"
"lingochamp/kaldi-ctc" -> "srvk/eesen"
"lingochamp/kaldi-ctc" -> "yajiemiao/eesen"
"lingochamp/kaldi-ctc" -> "HawkAaron/warp-transducer"
"lingochamp/kaldi-ctc" -> "tbright17/kaldi-dnn-ali-gop"
"lingochamp/kaldi-ctc" -> "aishell-foundation/DaCiDian"
"lingochamp/kaldi-ctc" -> "thu-spmi/CAT"
"lingochamp/kaldi-ctc" -> "XiaoMi/kaldi-onnx"
"lingochamp/kaldi-ctc" -> "Diamondfan/CTC_pytorch"
"lingochamp/kaldi-ctc" -> "HawkAaron/RNN-Transducer"
"lingochamp/kaldi-ctc" -> "YiwenShaoStephen/pychain"
"lingochamp/kaldi-ctc" -> "vesis84/kaldi-io-for-python"
"lingochamp/kaldi-ctc" -> "vrenkens/tfkaldi"
"lingochamp/kaldi-ctc" -> "cywang97/StreamingTransformer"
"lingochamp/kaldi-ctc" -> "awni/transducer"
"lingochamp/kaldi-ctc" -> "mindorii/kws" ["e"=1]
"vesis84/kaldi-io-for-python" -> "nttcslab-sp/kaldiio"
"vesis84/kaldi-io-for-python" -> "pykaldi/pykaldi"
"vesis84/kaldi-io-for-python" -> "lhotse-speech/lhotse"
"vesis84/kaldi-io-for-python" -> "XiaoMi/kaldi-onnx"
"vesis84/kaldi-io-for-python" -> "YiwenShaoStephen/pychain"
"vesis84/kaldi-io-for-python" -> "hsn-zeinali/x-vector-kaldi-tf"
"vesis84/kaldi-io-for-python" -> "k2-fsa/k2"
"vesis84/kaldi-io-for-python" -> "jzlianglu/pykaldi2"
"vesis84/kaldi-io-for-python" -> "YoavRamon/awesome-kaldi"
"vesis84/kaldi-io-for-python" -> "Snowdar/asv-subtools"
"vesis84/kaldi-io-for-python" -> "mravanelli/pytorch-kaldi"
"vesis84/kaldi-io-for-python" -> "santi-pdp/pase"
"vesis84/kaldi-io-for-python" -> "srvk/eesen"
"vesis84/kaldi-io-for-python" -> "tbright17/kaldi-dnn-ali-gop"
"vesis84/kaldi-io-for-python" -> "mravanelli/SincNet"
"ahmetozlu/signature_extractor" -> "gnbaron/signature-recognition"
"ahmetozlu/signature_extractor" -> "EnzoSeason/signature_detection"
"ahmetozlu/signature_extractor" -> "amaljoseph/Signature-Verification_System_using_YOLOv5-and-CycleGAN"
"ahmetozlu/signature_extractor" -> "luizgh/sigver_wiwd"
"ahmetozlu/signature_extractor" -> "luizgh/sigver"
"ahmetozlu/signature_extractor" -> "watersink/ocrsegment" ["e"=1]
"ahmetozlu/signature_extractor" -> "Aftaab99/OfflineSignatureVerification"
"YoavRamon/awesome-kaldi" -> "pykaldi/pykaldi"
"YoavRamon/awesome-kaldi" -> "vesis84/kaldi-io-for-python"
"YoavRamon/awesome-kaldi" -> "alumae/kaldi-gstreamer-server"
"YoavRamon/awesome-kaldi" -> "k2-fsa/k2"
"YoavRamon/awesome-kaldi" -> "mravanelli/pytorch-kaldi"
"YoavRamon/awesome-kaldi" -> "gooofy/zamia-speech"
"YoavRamon/awesome-kaldi" -> "XiaoMi/kaldi-onnx"
"YoavRamon/awesome-kaldi" -> "wq2012/awesome-diarization"
"YoavRamon/awesome-kaldi" -> "xingchensong/speech-recognition-papers"
"YoavRamon/awesome-kaldi" -> "lhotse-speech/lhotse"
"YoavRamon/awesome-kaldi" -> "daanzu/kaldi-active-grammar" ["e"=1]
"YoavRamon/awesome-kaldi" -> "cywang97/StreamingTransformer"
"YoavRamon/awesome-kaldi" -> "freewym/espresso"
"YoavRamon/awesome-kaldi" -> "gooofy/py-kaldi-asr"
"YoavRamon/awesome-kaldi" -> "YiwenShaoStephen/pychain"
"yangdongchao/text-to-sound-synthesis-demo" -> "yangdongchao/Text-to-sound-Synthesis"
"yeyupiaoling/AudioClassification-PaddlePaddle" -> "HighCWu/denoising-diffusion-paddle"
"tbright17/kaldi-dnn-ali-gop" -> "jimbozhang/kaldi-gop"
"tbright17/kaldi-dnn-ali-gop" -> "sweekarsud/Goodness-of-Pronunciation"
"tbright17/kaldi-dnn-ali-gop" -> "lingochamp/kaldi-ctc"
"tbright17/kaldi-dnn-ali-gop" -> "mcvkhaos/GOP-LSTM"
"tbright17/kaldi-dnn-ali-gop" -> "XiaoMi/kaldi-onnx"
"tbright17/kaldi-dnn-ali-gop" -> "vesis84/kaldi-io-for-python"
"tbright17/kaldi-dnn-ali-gop" -> "robin1001/xdecoder"
"tbright17/kaldi-dnn-ali-gop" -> "dialogflow/asr-server"
"NervanaSystems/deepspeech" -> "baidu-research/ba-dls-deepspeech"
"NervanaSystems/deepspeech" -> "SeanNaren/deepspeech.torch"
"NervanaSystems/deepspeech" -> "rizar/attention-lvcsr"
"NervanaSystems/deepspeech" -> "zzw922cn/awesome-speech-recognition-papers"
"NervanaSystems/deepspeech" -> "yajiemiao/eesen"
"NervanaSystems/deepspeech" -> "patyork/python-deep-speech"
"guanlongzhao/fac-via-ppg" -> "guanlongzhao/ppg-gmm"
"guanlongzhao/fac-via-ppg" -> "cjerry1243/TransferLearning-CLVC"
"guanlongzhao/fac-via-ppg" -> "ryokamoi/ppg_vc"
"guanlongzhao/fac-via-ppg" -> "hhguo/EA-SVC"
"liusongxiang/ppg-vc" -> "Wendison/VQMIVC"
"liusongxiang/ppg-vc" -> "hhguo/EA-SVC"
"liusongxiang/ppg-vc" -> "mindslab-ai/assem-vc"
"liusongxiang/ppg-vc" -> "jxzhanggg/nonparaSeq2seqVC_code"
"liusongxiang/ppg-vc" -> "yl4579/StarGANv2-VC"
"liusongxiang/ppg-vc" -> "thuhcsi/VAENAR-TTS"
"liusongxiang/ppg-vc" -> "facebookresearch/speech-resynthesis"
"liusongxiang/ppg-vc" -> "OlaWod/FreeVC"
"liusongxiang/ppg-vc" -> "hhguo/MSMC-TTS"
"liusongxiang/ppg-vc" -> "NVIDIA/BigVGAN"
"liusongxiang/ppg-vc" -> "YatingMusic/ddsp-singing-vocoders"
"liusongxiang/ppg-vc" -> "SungFeng-Huang/Meta-TTS"
"liusongxiang/ppg-vc" -> "bshall/soft-vc"
"liusongxiang/ppg-vc" -> "yistLin/FragmentVC"
"facebookresearch/gtn" -> "YiwenShaoStephen/pychain"
"facebookresearch/gtn" -> "danpovey/k2"
"facebookresearch/gtn" -> "k2-fsa/snowfall"
"facebookresearch/gtn" -> "HawkAaron/warp-transducer"
"facebookresearch/gtn" -> "k2-fsa/k2"
"facebookresearch/gtn" -> "facebookresearch/gtn_applications"
"facebookresearch/gtn" -> "nttcslab-sp/kaldiio"
"facebookresearch/gtn" -> "hirofumi0810/neural_sp"
"facebookresearch/gtn" -> "awni/transducer"
"facebookresearch/gtn" -> "1ytic/warp-rnnt"
"facebookresearch/gtn" -> "tencent-ailab/pika"
"facebookresearch/gtn" -> "cornerfarmer/ctc_segmentation"
"facebookresearch/gtn" -> "danpovey/fast_rnnt"
"facebookresearch/gtn" -> "SpeechColab/GigaSpeech"
"facebookresearch/gtn" -> "mobvoi/wenet"
"Kinyugo/msanii" -> "archinetai/audio-data-pytorch"
"Kinyugo/msanii" -> "ben-hayes/neural-waveshaping-synthesis"
"Kinyugo/msanii" -> "archinetai/audio-diffusion-pytorch-trainer"
"sushant097/Handwritten-Line-Text-Recognition-using-Deep-Learning-with-Tensorflow" -> "arthurflor23/handwritten-text-recognition"
"sushant097/Handwritten-Line-Text-Recognition-using-Deep-Learning-with-Tensorflow" -> "awslabs/handwritten-text-recognition-for-apache-mxnet"
"sushant097/Handwritten-Line-Text-Recognition-using-Deep-Learning-with-Tensorflow" -> "lamhoangtung/LineHTR"
"sushant097/Handwritten-Line-Text-Recognition-using-Deep-Learning-with-Tensorflow" -> "githubharald/SimpleHTR"
"sushant097/Handwritten-Line-Text-Recognition-using-Deep-Learning-with-Tensorflow" -> "0x454447415244/HandwritingRecognitionSystem"
"sushant097/Handwritten-Line-Text-Recognition-using-Deep-Learning-with-Tensorflow" -> "arthurflor23/text-segmentation"
"sushant097/Handwritten-Line-Text-Recognition-using-Deep-Learning-with-Tensorflow" -> "huyhoang17/CRNN_CTC_English_Handwriting_Recognition"
"sushant097/Handwritten-Line-Text-Recognition-using-Deep-Learning-with-Tensorflow" -> "Breta01/handwriting-ocr"
"sushant097/Handwritten-Line-Text-Recognition-using-Deep-Learning-with-Tensorflow" -> "sushant097/Devnagari-Handwritten-Word-Recongition-with-Deep-Learning"
"sushant097/Handwritten-Line-Text-Recognition-using-Deep-Learning-with-Tensorflow" -> "cwig/start_follow_read"
"sushant097/Handwritten-Line-Text-Recognition-using-Deep-Learning-with-Tensorflow" -> "githubharald/WordDetectorNN"
"sushant097/Handwritten-Line-Text-Recognition-using-Deep-Learning-with-Tensorflow" -> "githubharald/WordSegmentation"
"sushant097/Handwritten-Line-Text-Recognition-using-Deep-Learning-with-Tensorflow" -> "giovanniguidi/Seq-2-Seq-OCR"
"hccho2/Tacotron-Wavenet-Vocoder-Korean" -> "hccho2/Tacotron2-Wavenet-Korean-TTS"
"pannous/caffe-speech-recognition" -> "pannous/tensorflow-speech-recognition"
"pannous/caffe-speech-recognition" -> "amaas/stanford-ctc"
"pannous/caffe-speech-recognition" -> "yajiemiao/kaldipdnn"
"pannous/caffe-speech-recognition" -> "rakeshvar/rnn_ctc"
"pannous/caffe-speech-recognition" -> "yajiemiao/eesen"
"pannous/caffe-speech-recognition" -> "igormq/ctc_tensorflow_example"
"pannous/caffe-speech-recognition" -> "NervanaSystems/deepspeech"
"pannous/caffe-speech-recognition" -> "patyork/python-deep-speech"
"Koischizo/AI-Vtuber" -> "ardha27/AI-Waifu-Vtuber"
"Koischizo/AI-Vtuber" -> "adi-panda/Kuebiko"
"JeffreyCA/spleeter-web" -> "VVasanth/Spleeter_Unofficial_TF20_MobileApp"
"JeffreyCA/spleeter-web" -> "gvne/spleeterpp" ["e"=1]
"kuielab/mdx-net" -> "kuielab/mdx-net-submission"
"kuielab/mdx-net" -> "haoheliu/2021-ISMIR-MSS-Challenge-CWS-PResUNet"
"kuielab/mdx-net" -> "ws-choi/ISMIR2020_U_Nets_SVS"
"chomeyama/SiFiGAN" -> "chomeyama/HN-UnifiedSourceFilterGAN"
"chomeyama/SiFiGAN" -> "revsic/torch-nansypp"
"huawei-noah/Speech-Backbones" -> "WelkinYang/GradTTS"
"huawei-noah/Speech-Backbones" -> "keonlee9420/DiffGAN-TTS"
"huawei-noah/Speech-Backbones" -> "NVIDIA/BigVGAN"
"huawei-noah/Speech-Backbones" -> "KevinMIN95/StyleSpeech"
"huawei-noah/Speech-Backbones" -> "Rongjiehuang/ProDiff"
"huawei-noah/Speech-Backbones" -> "facebookresearch/speech-resynthesis"
"huawei-noah/Speech-Backbones" -> "thuhcsi/VAENAR-TTS"
"huawei-noah/Speech-Backbones" -> "Rongjiehuang/FastDiff"
"huawei-noah/Speech-Backbones" -> "Wendison/VQMIVC"
"huawei-noah/Speech-Backbones" -> "tencent-ailab/bddm"
"huawei-noah/Speech-Backbones" -> "dhchoi99/NANSY"
"huawei-noah/Speech-Backbones" -> "jxzhanggg/nonparaSeq2seqVC_code"
"huawei-noah/Speech-Backbones" -> "LEEYOONHYUNG/BVAE-TTS"
"huawei-noah/Speech-Backbones" -> "keonlee9420/Comprehensive-Transformer-TTS"
"huawei-noah/Speech-Backbones" -> "lmnt-com/diffwave"
"sarulab-speech/jtubespeech" -> "laboroai/LaboroTVSpeech"
"sarulab-speech/jtubespeech" -> "lumaku/ctc-segmentation"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "Sharad24/Neural-Voice-Cloning-with-Few-Samples"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "r9y9/deepvoice3_pytorch"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "liusongxiang/StarGAN-Voice-Conversion"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "jxzhanggg/nonparaSeq2seqVC_code"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "seungwonpark/melgan"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "NVIDIA/mellotron"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "nii-yamagishilab/multi-speaker-tacotron"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "Tomiinek/Multilingual_Text_to_Speech"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "r9y9/gantts"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "syang1993/gst-tacotron"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "IEEE-NITK/Neural-Voice-Cloning"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "descriptinc/melgan-neurips"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "KuangDD/zhvoice"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "vlomme/Multi-Tacotron-Voice-Cloning"
"gnbaron/signature-recognition" -> "luizgh/sigver_wiwd"
"gnbaron/signature-recognition" -> "ahmetozlu/signature_extractor"
"festvox/flite" -> "festvox/festival"
"festvox/flite" -> "MycroftAI/mimic1"
"festvox/flite" -> "festvox/festvox"
"festvox/flite" -> "espeak-ng/espeak-ng"
"festvox/flite" -> "numediart/MBROLA"
"festvox/flite" -> "festvox/speech_tools"
"festvox/flite" -> "festvox/datasets-CMU_Wilderness"
"festvox/flite" -> "dmort27/epitran"
"festvox/flite" -> "happyalu/Flite-TTS-Engine-for-Android"
"festvox/flite" -> "Kyubyong/g2p"
"festvox/flite" -> "r9y9/nnmnkwii"
"festvox/flite" -> "Idlak/idlak"
"festvox/flite" -> "AdolfVonKleist/Phonetisaurus"
"festvox/flite" -> "CSTR-Edinburgh/merlin"
"Francis-Komizu/VITS" -> "Francis-Komizu/Sovits"
"Francis-Komizu/VITS" -> "Francis-Komizu/VITS-Bilingual"
"Francis-Komizu/VITS" -> "luoyily/MoeTTS"
"andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning" -> "iamyuanchung/Autoregressive-Predictive-Coding"
"andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning" -> "facebookresearch/CPC_audio"
"andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning" -> "hirofumi0810/neural_sp"
"andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning" -> "santi-pdp/pase"
"andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning" -> "xingchensong/speech-recognition-papers"
"andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning" -> "awslabs/speech-representations"
"andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning" -> "facebookresearch/WavAugment"
"andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning" -> "facebookresearch/libri-light"
"andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning" -> "cywang97/StreamingTransformer"
"andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning" -> "jjery2243542/adaptive_voice_conversion"
"andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning" -> "manojpamk/pytorch_xvectors"
"andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning" -> "facebookresearch/voxpopuli"
"andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning" -> "vesis84/kaldi-io-for-python"
"ben-hayes/neural-waveshaping-synthesis" -> "acids-ircam/ddsp_pytorch"
"ben-hayes/neural-waveshaping-synthesis" -> "fcaspe/ddx7"
"ben-hayes/neural-waveshaping-synthesis" -> "caillonantoine/RAVE"
"ben-hayes/neural-waveshaping-synthesis" -> "torchsynth/torchsynth"
"ben-hayes/neural-waveshaping-synthesis" -> "ben-hayes/neural-field-synth"
"ben-hayes/neural-waveshaping-synthesis" -> "magenta/midi-ddsp"
"ben-hayes/neural-waveshaping-synthesis" -> "csteinmetz1/auraloss"
"bshall/VectorQuantizedCPC" -> "bshall/ZeroSpeech"
"bshall/VectorQuantizedCPC" -> "Wendison/VQMIVC"
"bshall/VectorQuantizedCPC" -> "facebookresearch/speech-resynthesis"
"bshall/VectorQuantizedCPC" -> "LEEYOONHYUNG/BVAE-TTS"
"bshall/VectorQuantizedCPC" -> "k2kobayashi/crank"
"bshall/VectorQuantizedCPC" -> "keonlee9420/STYLER"
"facebookresearch/libri-light" -> "facebookresearch/voxpopuli"
"facebookresearch/libri-light" -> "SpeechColab/GigaSpeech"
"facebookresearch/libri-light" -> "facebookresearch/CPC_audio"
"facebookresearch/libri-light" -> "facebookresearch/WavAugment"
"facebookresearch/libri-light" -> "cywang97/StreamingTransformer"
"facebookresearch/libri-light" -> "iamyuanchung/Autoregressive-Predictive-Coding"
"facebookresearch/libri-light" -> "andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning"
"facebookresearch/libri-light" -> "hirofumi0810/neural_sp"
"facebookresearch/libri-light" -> "awslabs/speech-representations"
"facebookresearch/libri-light" -> "freewym/espresso"
"facebookresearch/libri-light" -> "YiwenShaoStephen/pychain"
"facebookresearch/libri-light" -> "facebookresearch/covost" ["e"=1]
"facebookresearch/libri-light" -> "rwth-i6/returnn"
"facebookresearch/libri-light" -> "NVIDIA/BigVGAN"
"facebookresearch/libri-light" -> "awni/transducer"
"gtn-org/gtn" -> "facebookresearch/gtn_applications"
"mindslab-ai/nuwave2" -> "mindslab-ai/nuwave"
"mindslab-ai/nuwave2" -> "mindslab-ai/phaseaug"
"mindslab-ai/nuwave2" -> "rishikksh20/HiFiplusplus-pytorch"
"mindslab-ai/nuwave2" -> "haoheliu/ssr_eval"
"mindslab-ai/nuwave2" -> "ncsoft/avocodo"
"mindslab-ai/nuwave2" -> "mindslab-ai/assem-vc"
"santi-pdp/pase" -> "mravanelli/SincNet"
"santi-pdp/pase" -> "vesis84/kaldi-io-for-python"
"santi-pdp/pase" -> "andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning"
"santi-pdp/pase" -> "facebookresearch/WavAugment"
"santi-pdp/pase" -> "nttcslab-sp/kaldiio"
"santi-pdp/pase" -> "hirofumi0810/neural_sp"
"santi-pdp/pase" -> "facebookresearch/CPC_audio"
"santi-pdp/pase" -> "iamyuanchung/Autoregressive-Predictive-Coding"
"santi-pdp/pase" -> "microsoft/UniSpeech"
"santi-pdp/pase" -> "descriptinc/melgan-neurips"
"santi-pdp/pase" -> "andi611/Mockingjay-Speech-Representation"
"santi-pdp/pase" -> "mravanelli/pytorch-kaldi"
"santi-pdp/pase" -> "anicolson/DeepXi" ["e"=1]
"santi-pdp/pase" -> "freewym/espresso"
"santi-pdp/pase" -> "mpariente/asteroid" ["e"=1]
"wenet-e2e/WeTextProcessing" -> "speechio/chinese_text_normalization"
"wenet-e2e/WeTextProcessing" -> "wenet-e2e/wespeaker"
"auspicious3000/SpeechSplit" -> "auspicious3000/autovc"
"auspicious3000/SpeechSplit" -> "auspicious3000/AutoPST"
"auspicious3000/SpeechSplit" -> "Wendison/VQMIVC"
"auspicious3000/SpeechSplit" -> "mindslab-ai/cotatron"
"auspicious3000/SpeechSplit" -> "NVIDIA/mellotron"
"auspicious3000/SpeechSplit" -> "jjery2243542/adaptive_voice_conversion"
"auspicious3000/SpeechSplit" -> "jxzhanggg/nonparaSeq2seqVC_code"
"auspicious3000/SpeechSplit" -> "kan-bayashi/ParallelWaveGAN"
"auspicious3000/SpeechSplit" -> "xcmyz/speech-synthesis-paper"
"auspicious3000/SpeechSplit" -> "descriptinc/melgan-neurips"
"auspicious3000/SpeechSplit" -> "jaywalnut310/glow-tts"
"auspicious3000/SpeechSplit" -> "jik876/hifi-gan"
"auspicious3000/SpeechSplit" -> "liusongxiang/StarGAN-Voice-Conversion"
"auspicious3000/SpeechSplit" -> "mindslab-ai/assem-vc"
"auspicious3000/SpeechSplit" -> "syang1993/gst-tacotron"
"bshall/ZeroSpeech" -> "bshall/VectorQuantizedCPC"
"bshall/ZeroSpeech" -> "bigpon/vcc20_baseline_cyclevae"
"bshall/ZeroSpeech" -> "hhguo/EA-SVC"
"bshall/ZeroSpeech" -> "k2kobayashi/crank"
"bshall/ZeroSpeech" -> "jxzhanggg/nonparaSeq2seqVC_code"
"bshall/ZeroSpeech" -> "facebookresearch/speech-resynthesis"
"bshall/ZeroSpeech" -> "ericwudayi/SkipVQVC"
"bshall/ZeroSpeech" -> "jjery2243542/adaptive_voice_conversion"
"bshall/ZeroSpeech" -> "guanlongzhao/fac-via-ppg"
"bshall/ZeroSpeech" -> "Wendison/VQMIVC"
"bshall/ZeroSpeech" -> "yistLin/FragmentVC"
"bshall/ZeroSpeech" -> "thuhcsi/VAENAR-TTS"
"bshall/ZeroSpeech" -> "SamuelBroughton/StarGAN-Voice-Conversion-2"
"bshall/ZeroSpeech" -> "bshall/UniversalVocoding"
"bshall/ZeroSpeech" -> "swasun/VQ-VAE-Speech"
"Anwarvic/Speaker-Recognition" -> "Anwarvic/Stanford_CS224n--NLP-with-Deep-Learning"
"Anwarvic/Speaker-Recognition" -> "Anwarvic/Deep-Learning-Nanodegree-Udacity-2019"
"Anwarvic/Speaker-Recognition" -> "wangleiai/dVectorSpeakerRecognition"
"Anwarvic/Speaker-Recognition" -> "Anwarvic/Deep-Learning-Specialization-2017--Coursera"
"Anwarvic/Speaker-Recognition" -> "vvestman/pytorch-ivectors"
"abhijeet3922/Speaker-identification-using-GMMs" -> "genzen2103/Speaker-Recognition-System-using-GMM"
"k2kobayashi/crank" -> "patrickltobing/cyclevae-vc-neuralvoco"
"k2kobayashi/crank" -> "jxzhanggg/nonparaSeq2seqVC_code"
"k2kobayashi/crank" -> "bigpon/vcc20_baseline_cyclevae"
"k2kobayashi/crank" -> "yistLin/FragmentVC"
"k2kobayashi/crank" -> "ericwudayi/SkipVQVC"
"k2kobayashi/crank" -> "Wendison/VQMIVC"
"k2kobayashi/crank" -> "lochenchou/MOSNet" ["e"=1]
"k2kobayashi/crank" -> "bshall/ZeroSpeech"
"anonymous-pits/pits" -> "heatz123/naturalspeech"
"anonymous-pits/pits" -> "mindslab-ai/phaseaug"
"anonymous-pits/pits" -> "MasayaKawamura/MB-iSTFT-VITS"
"anonymous-pits/pits" -> "yl4579/StyleTTS-VC"
"anonymous-pits/pits" -> "yl4579/StyleTTS"
"anonymous-pits/pits" -> "ncsoft/avocodo"
"anonymous-pits/pits" -> "Rongjiehuang/GenerSpeech"
"anonymous-pits/pits" -> "junjun3518/alias-free-torch"
"yl4579/StyleTTS" -> "yl4579/StyleTTS-VC"
"yl4579/StyleTTS" -> "yl4579/PitchExtractor"
"yl4579/StyleTTS" -> "mindslab-ai/phaseaug"
"yl4579/StyleTTS" -> "yl4579/AuxiliaryASR"
"yl4579/StyleTTS" -> "anonymous-pits/pits"
"yl4579/StyleTTS" -> "yl4579/PL-BERT"
"XiaoMi/kaldi-onnx" -> "YiwenShaoStephen/pychain"
"XiaoMi/kaldi-onnx" -> "thu-spmi/CAT"
"XiaoMi/kaldi-onnx" -> "xiangxyq/minimize-chain-decoder"
"XiaoMi/kaldi-onnx" -> "vesis84/kaldi-io-for-python"
"XiaoMi/kaldi-onnx" -> "tencent-ailab/pika"
"XiaoMi/kaldi-onnx" -> "xiangxyq/xiangxyq_kaldi"
"XiaoMi/kaldi-onnx" -> "andyweiqiu/asr-ios-local"
"XiaoMi/kaldi-onnx" -> "xingchensong/speech-recognition-papers"
"XiaoMi/kaldi-onnx" -> "speechio/BigCiDian"
"XiaoMi/kaldi-onnx" -> "alphacep/kaldi-android-demo"
"XiaoMi/kaldi-onnx" -> "mobvoi/wenet"
"XiaoMi/kaldi-onnx" -> "tencent-ailab/3m-asr"
"XiaoMi/kaldi-onnx" -> "k2-fsa/k2"
"XiaoMi/kaldi-onnx" -> "aishell-foundation/DaCiDian"
"XiaoMi/kaldi-onnx" -> "mindorii/kws" ["e"=1]
"numediart/MBROLA" -> "numediart/MBROLA-voices"
"numediart/MBROLA" -> "numediart/MBROLATOR"
"kuielab/mdx-net-submission" -> "kuielab/mdx-net"
"kuielab/mdx-net-submission" -> "adefossez/mdx21_demucs"
"kuielab/mdx-net-submission" -> "yoyololicon/music-demixing-challenge-ismir-2021-entry"
"seaniezhao/torch_npss" -> "MTG/WGANSing"
"seaniezhao/torch_npss" -> "M4Singer/M4Singer"
"seaniezhao/torch_npss" -> "xushengyuan/Fastsinging"
"flashlight/wav2letter" -> "flashlight/flashlight" ["e"=1]
"flashlight/wav2letter" -> "thu-spmi/CAT"
"flashlight/wav2letter" -> "facebookresearch/libri-light"
"flashlight/wav2letter" -> "SpeechColab/GigaSpeech"
"flashlight/wav2letter" -> "k2-fsa/k2"
"flashlight/wav2letter" -> "k2-fsa/icefall"
"flashlight/wav2letter" -> "cywang97/StreamingTransformer"
"flashlight/wav2letter" -> "athena-team/athena" ["e"=1]
"flashlight/wav2letter" -> "csukuangfj/kaldifeat"
"flashlight/wav2letter" -> "Z-yq/TensorflowASR"
"flashlight/wav2letter" -> "mailong25/self-supervised-speech-recognition"
"flashlight/wav2letter" -> "double22a/speech_dataset"
"flashlight/wav2letter" -> "lhotse-speech/lhotse"
"flashlight/wav2letter" -> "TensorSpeech/TensorFlowASR"
"flashlight/wav2letter" -> "espnet/interspeech2019-tutorial"
"lmnt-com/diffwave" -> "ivanvovk/WaveGrad"
"lmnt-com/diffwave" -> "lmnt-com/wavegrad"
"lmnt-com/diffwave" -> "jik876/hifi-gan"
"lmnt-com/diffwave" -> "NVIDIA/BigVGAN"
"lmnt-com/diffwave" -> "kan-bayashi/ParallelWaveGAN"
"lmnt-com/diffwave" -> "huawei-noah/Speech-Backbones"
"lmnt-com/diffwave" -> "rishikksh20/VocGAN"
"lmnt-com/diffwave" -> "keonlee9420/Parallel-Tacotron2"
"lmnt-com/diffwave" -> "maxrmorrison/torchcrepe"
"lmnt-com/diffwave" -> "facebookresearch/vocoder-benchmark"
"lmnt-com/diffwave" -> "mindslab-ai/cotatron"
"lmnt-com/diffwave" -> "jaywalnut310/glow-tts"
"lmnt-com/diffwave" -> "YatingMusic/ddsp-singing-vocoders"
"lmnt-com/diffwave" -> "xcmyz/speech-synthesis-paper"
"lmnt-com/diffwave" -> "auspicious3000/SpeechSplit"
"sweetcocoa/pop2piano" -> "marcoppasini/musika"
"sweetcocoa/pop2piano" -> "vb000/Waveformer" ["e"=1]
"sweetcocoa/pop2piano" -> "sh-lee-prml/BigVGAN"
"sweetcocoa/pop2piano" -> "magenta/music-spectrogram-diffusion"
"nryant/dscore" -> "BUTSpeechFIT/VBx"
"nryant/dscore" -> "hitachi-speech/EEND"
"nryant/dscore" -> "joonson/voxconverse"
"nryant/dscore" -> "HuangZiliAndy/RPNSD"
"nryant/dscore" -> "FlorianKrey/DNC"
"nryant/dscore" -> "pyannote/pyannote-metrics"
"nryant/dscore" -> "Jamiroquai88/VBDiarization"
"nryant/dscore" -> "desh2608/dover-lap"
"nryant/dscore" -> "wq2012/SpectralCluster"
"nryant/dscore" -> "tango4j/Auto-Tuning-Spectral-Clustering"
"nryant/dscore" -> "Xflick/EEND_PyTorch"
"nryant/dscore" -> "juanmc2005/StreamingSpeakerDiarization"
"nryant/dscore" -> "google/speaker-id"
"nryant/dscore" -> "nttcslab-sp/EEND-vector-clustering"
"Hiroshiba/yukarin" -> "Hiroshiba/realtime-yukarin"
"Hiroshiba/yukarin" -> "Hiroshiba/become-yukarin"
"MlWoo/LPCNet" -> "alokprasad/LPCTron"
"MlWoo/LPCNet" -> "h-meru/Tacotron-WaveRNN"
"SpeechColab/Leaderboard" -> "csukuangfj/kaldifeat"
"SpeechColab/Leaderboard" -> "k2-fsa/icefall"
"SpeechColab/Leaderboard" -> "alibaba-damo-academy/FunASR"
"SpeechColab/Leaderboard" -> "SpeechColab/GigaSpeech"
"SpeechColab/Leaderboard" -> "wenet-e2e/wekws" ["e"=1]
"SpeechColab/Leaderboard" -> "speechio/chinese_text_normalization"
"SpeechColab/Leaderboard" -> "wenet-e2e/WenetSpeech"
"SpeechColab/Leaderboard" -> "cywang97/StreamingTransformer"
"SpeechColab/Leaderboard" -> "k2-fsa/k2"
"SpeechColab/Leaderboard" -> "speechio/BigCiDian"
"SpeechColab/Leaderboard" -> "ddlBoJack/Speech-Resources"
"SpeechColab/Leaderboard" -> "thu-spmi/CAT"
"SpeechColab/Leaderboard" -> "mobvoi/wenet"
"SpeechColab/Leaderboard" -> "jctian98/e2e_lfmmi"
"SpeechColab/Leaderboard" -> "wenet-e2e/WeTextProcessing"
"begeekmyfriend/tacotron2" -> "begeekmyfriend/WaveRNN"
"begeekmyfriend/tacotron2" -> "begeekmyfriend/Tacotron-2"
"imdanboy/jets" -> "keonlee9420/Comprehensive-E2E-TTS"
"kakaobrain/g2pM" -> "Kyubyong/g2pC"
"kakaobrain/g2pM" -> "speechio/chinese_text_normalization"
"kakaobrain/g2pM" -> "thuhcsi/Crystal"
"kakaobrain/g2pM" -> "Jackiexiao/MTTS"
"kakaobrain/g2pM" -> "KuangDD/phkit"
"kakaobrain/g2pM" -> "speechio/BigCiDian"
"kakaobrain/g2pM" -> "tts-tutorial/survey"
"kakaobrain/g2pM" -> "Helsinki-NLP/prosody"
"kakaobrain/g2pM" -> "GitYCC/g2pW"
"kakaobrain/g2pM" -> "Kyubyong/g2p"
"kakaobrain/g2pM" -> "thuhcsi/FlatTN"
"kakaobrain/g2pM" -> "Zeqiang-Lai/Prosody_Prediction"
"kakaobrain/g2pM" -> "open-speech/cn-text-normalizer"
"kakaobrain/g2pM" -> "xcmyz/FastVocoder"
"kakaobrain/g2pM" -> "xcmyz/speech-synthesis-paper"
"manojpamk/pytorch_xvectors" -> "Snowdar/asv-subtools"
"manojpamk/pytorch_xvectors" -> "cvqluu/TDNN"
"manojpamk/pytorch_xvectors" -> "yistLin/dvector"
"manojpamk/pytorch_xvectors" -> "KrishnaDN/x-vector-pytorch"
"manojpamk/pytorch_xvectors" -> "Dannynis/xvector_pytorch"
"manojpamk/pytorch_xvectors" -> "qqueing/DeepSpeaker-pytorch"
"manojpamk/pytorch_xvectors" -> "zeroQiaoba/ivector-xvector"
"manojpamk/pytorch_xvectors" -> "hitachi-speech/EEND"
"manojpamk/pytorch_xvectors" -> "HarryVolek/PyTorch_Speaker_Verification"
"manojpamk/pytorch_xvectors" -> "cvqluu/Factorized-TDNN"
"manojpamk/pytorch_xvectors" -> "clovaai/voxceleb_trainer"
"manojpamk/pytorch_xvectors" -> "RaviSoji/plda"
"manojpamk/pytorch_xvectors" -> "Jungjee/RawNet"
"manojpamk/pytorch_xvectors" -> "iiscleap/NeuralPlda"
"manojpamk/pytorch_xvectors" -> "WeidiXie/VGG-Speaker-Recognition"
"hrbigelow/ae-wavenet" -> "swasun/VQ-VAE-Speech"
"hrbigelow/ae-wavenet" -> "DongyaoZhu/VQ-VAE-WaveNet"
"hrbigelow/ae-wavenet" -> "mkotha/WaveRNN"
"torchsynth/torchsynth" -> "ben-hayes/neural-waveshaping-synthesis"
"torchsynth/torchsynth" -> "andreasjansson/fmsynth"
"torchsynth/torchsynth" -> "csteinmetz1/auraloss"
"torchsynth/torchsynth" -> "acids-ircam/ddsp_pytorch"
"torchsynth/torchsynth" -> "hyakuchiki/diffsynth"
"torchsynth/torchsynth" -> "magenta/midi-ddsp"
"torchsynth/torchsynth" -> "acids-ircam/flow_synthesizer" ["e"=1]
"yistLin/FragmentVC" -> "yistLin/universal-vocoder"
"yistLin/FragmentVC" -> "k2kobayashi/crank"
"yistLin/FragmentVC" -> "howard1337/S2VC"
"yistLin/FragmentVC" -> "ericwudayi/SkipVQVC"
"yistLin/FragmentVC" -> "cyhuang-tw/AdaIN-VC"
"yistLin/FragmentVC" -> "KimythAnly/AGAIN-VC"
"yistLin/FragmentVC" -> "jxzhanggg/nonparaSeq2seqVC_code"
"yistLin/FragmentVC" -> "Wendison/VQMIVC"
"yistLin/FragmentVC" -> "tzuhsien/Voice-conversion-evaluation"
"yistLin/FragmentVC" -> "yistLin/dvector"
"yistLin/FragmentVC" -> "mindslab-ai/cotatron"
"yistLin/FragmentVC" -> "bshall/ZeroSpeech"
"yistLin/FragmentVC" -> "liusongxiang/ppg-vc"
"gooofy/zamia-speech" -> "gooofy/py-kaldi-asr"
"gooofy/zamia-speech" -> "daanzu/kaldi-active-grammar" ["e"=1]
"gooofy/zamia-speech" -> "YoavRamon/awesome-kaldi"
"gooofy/zamia-speech" -> "JRMeyer/open-speech-corpora"
"gooofy/zamia-speech" -> "alumae/gst-kaldi-nnet2-online"
"gooofy/zamia-speech" -> "jzlianglu/pykaldi2"
"gooofy/zamia-speech" -> "alumae/kaldi-gstreamer-server"
"gooofy/zamia-speech" -> "pykaldi/pykaldi"
"gooofy/zamia-speech" -> "alphacep/kaldi-android-demo"
"gooofy/zamia-speech" -> "jcsilva/docker-kaldi-gstreamer-server"
"gooofy/zamia-speech" -> "hirofumi0810/neural_sp"
"gooofy/zamia-speech" -> "gooofy/kaldi-adapt-lm"
"gooofy/zamia-speech" -> "jimbozhang/kaldi-gop"
"gooofy/zamia-speech" -> "robmsmt/ASR_Audio_Data_Links"
"gooofy/zamia-speech" -> "SpeechColab/GigaSpeech"
"liasece/sd-webui-train-tools" -> "Zuntan03/CharFramework"
"open-dict-data/ipa-dict" -> "CUNY-CL/wikipron"
"open-dict-data/ipa-dict" -> "open-dsl-dict/ipa-dict-dsl"
"open-dict-data/ipa-dict" -> "dmort27/epitran"
"open-dict-data/ipa-dict" -> "JoseLlarena/Britfone"
"open-dict-data/ipa-dict" -> "mphilli/English-to-IPA"
"open-dict-data/ipa-dict" -> "menelik3/cmudict-ipa"
"open-dict-data/ipa-dict" -> "Kyubyong/g2p"
"open-dict-data/ipa-dict" -> "cmusphinx/cmudict"
"open-dict-data/ipa-dict" -> "xinjli/allosaurus"
"open-dict-data/ipa-dict" -> "as-ideas/DeepPhonemizer"
"open-dict-data/ipa-dict" -> "bootphon/phonemizer"
"open-dict-data/ipa-dict" -> "uiuc-sst/g2ps"
"albertaparicio/tfg-voice-conversion" -> "shamidreza/dnnmapper"
"albertaparicio/tfg-voice-conversion" -> "Kyubyong/cross_vc"
"jjery2243542/voice_conversion" -> "jjery2243542/adaptive_voice_conversion"
"jjery2243542/voice_conversion" -> "liusongxiang/StarGAN-Voice-Conversion"
"jjery2243542/voice_conversion" -> "joansj/blow"
"jjery2243542/voice_conversion" -> "andi611/ZeroSpeech-TTS-without-T"
"jjery2243542/voice_conversion" -> "JeremyCCHsu/vae-npvc"
"jjery2243542/voice_conversion" -> "auspicious3000/autovc"
"jjery2243542/voice_conversion" -> "leimao/Voice_Converter_CycleGAN"
"jjery2243542/voice_conversion" -> "bigpon/vcc20_baseline_cyclevae"
"jjery2243542/voice_conversion" -> "BogiHsu/Voice-Conversion"
"jjery2243542/voice_conversion" -> "ericwudayi/SkipVQVC"
"jjery2243542/voice_conversion" -> "hujinsen/pytorch-StarGAN-VC"
"jjery2243542/voice_conversion" -> "hujinsen/StarGAN-Voice-Conversion"
"jjery2243542/voice_conversion" -> "bshall/ZeroSpeech"
"jjery2243542/voice_conversion" -> "k2kobayashi/sprocket"
"jjery2243542/voice_conversion" -> "wnhsu/FactorizedHierarchicalVAE"
"vanstorm9/AI-Vocaloid-Kit-V2" -> "vanstorm9/AI-vocaloid-kit"
"yeyupiaoling/AudioClassification-Pytorch" -> "yeyupiaoling/AudioClassification-PaddlePaddle"
"SongRongLee/mir-svc" -> "hhguo/EA-SVC"
"SongRongLee/mir-svc" -> "johndpope/Singing-Voice-Conversion-with-conditional-VAW-GAN"
"arthurflor23/handwritten-text-recognition" -> "sushant097/Handwritten-Line-Text-Recognition-using-Deep-Learning-with-Tensorflow"
"arthurflor23/handwritten-text-recognition" -> "awslabs/handwritten-text-recognition-for-apache-mxnet"
"arthurflor23/handwritten-text-recognition" -> "githubharald/SimpleHTR"
"arthurflor23/handwritten-text-recognition" -> "arthurflor23/text-segmentation"
"arthurflor23/handwritten-text-recognition" -> "caltechlibrary/handprint"
"arthurflor23/handwritten-text-recognition" -> "githubharald/WordDetector"
"arthurflor23/handwritten-text-recognition" -> "githubharald/WordSegmentation"
"arthurflor23/handwritten-text-recognition" -> "0x454447415244/HandwritingRecognitionSystem"
"arthurflor23/handwritten-text-recognition" -> "githubharald/WordDetectorNN"
"arthurflor23/handwritten-text-recognition" -> "githubharald/DeslantImg"
"arthurflor23/handwritten-text-recognition" -> "Breta01/handwriting-ocr"
"arthurflor23/handwritten-text-recognition" -> "him4318/Transformer-ocr"
"arthurflor23/handwritten-text-recognition" -> "lamhoangtung/LineHTR"
"arthurflor23/handwritten-text-recognition" -> "vloison/Handwritten_Text_Recognition"
"arthurflor23/handwritten-text-recognition" -> "arthurflor23/spelling-correction"
"Suhee05/Text-Independent-Speaker-Verification" -> "funcwj/ge2e-speaker-verification"
"BUTSpeechFIT/VBx" -> "nryant/dscore"
"BUTSpeechFIT/VBx" -> "hitachi-speech/EEND"
"BUTSpeechFIT/VBx" -> "desh2608/dover-lap"
"BUTSpeechFIT/VBx" -> "juanmc2005/StreamingSpeakerDiarization"
"BUTSpeechFIT/VBx" -> "Snowdar/asv-subtools"
"BUTSpeechFIT/VBx" -> "yufan-aslp/AliMeeting"
"BUTSpeechFIT/VBx" -> "phonexiaresearch/VBx-training-recipe"
"BUTSpeechFIT/VBx" -> "BUTSpeechFIT/EEND"
"BUTSpeechFIT/VBx" -> "Xflick/EEND_PyTorch"
"BUTSpeechFIT/VBx" -> "nttcslab-sp/EEND-vector-clustering"
"BUTSpeechFIT/VBx" -> "wq2012/SpectralCluster"
"BUTSpeechFIT/VBx" -> "tango4j/Auto-Tuning-Spectral-Clustering"
"BUTSpeechFIT/VBx" -> "BUTSpeechFIT/speakerbeam" ["e"=1]
"BUTSpeechFIT/VBx" -> "wq2012/awesome-diarization"
"BUTSpeechFIT/VBx" -> "Jamiroquai88/VBDiarization"
"wenet-e2e/wespeaker" -> "Snowdar/asv-subtools"
"wenet-e2e/wespeaker" -> "zyzisyz/mfa_conformer"
"wenet-e2e/wespeaker" -> "wenet-e2e/wekws" ["e"=1]
"wenet-e2e/wespeaker" -> "wenet-e2e/wetts"
"wenet-e2e/wespeaker" -> "csukuangfj/kaldifeat"
"wenet-e2e/wespeaker" -> "k2-fsa/icefall"
"wenet-e2e/wespeaker" -> "felixfuyihui/AISHELL-4"
"wenet-e2e/wespeaker" -> "hitachi-speech/EEND"
"wenet-e2e/wespeaker" -> "TaoRuijie/ECAPA-TDNN"
"wenet-e2e/wespeaker" -> "wenet-e2e/WeTextProcessing"
"wenet-e2e/wespeaker" -> "alibaba-damo-academy/FunASR"
"wenet-e2e/wespeaker" -> "speechio/chinese_text_normalization"
"wenet-e2e/wespeaker" -> "microsoft/UniSpeech"
"wenet-e2e/wespeaker" -> "clovaai/voxceleb_trainer"
"Edresson/YourTTS" -> "keonlee9420/StyleSpeech"
"Edresson/YourTTS" -> "NVIDIA/BigVGAN"
"Edresson/YourTTS" -> "liusongxiang/ppg-vc"
"Edresson/YourTTS" -> "HLTSingapore/Emotional-Speech-Data"
"Edresson/YourTTS" -> "Wendison/VQMIVC"
"Edresson/YourTTS" -> "yl4579/StarGANv2-VC"
"Edresson/YourTTS" -> "keonlee9420/Comprehensive-Transformer-TTS"
"Edresson/YourTTS" -> "NVIDIA/radtts"
"Edresson/YourTTS" -> "KevinMIN95/StyleSpeech"
"Edresson/YourTTS" -> "NATSpeech/NATSpeech" ["e"=1]
"Edresson/YourTTS" -> "jik876/hifi-gan"
"Edresson/YourTTS" -> "DigitalPhonetics/IMS-Toucan"
"Edresson/YourTTS" -> "SungFeng-Huang/Meta-TTS"
"Edresson/YourTTS" -> "auspicious3000/autovc"
"Edresson/YourTTS" -> "rendchevi/nix-tts"
"hccho2/Tacotron2-Wavenet-Korean-TTS" -> "hccho2/Tacotron-Wavenet-Vocoder-Korean"
"hccho2/Tacotron2-Wavenet-Korean-TTS" -> "hccho2/Tacotron-Wavenet-Vocoder"
"hccho2/Tacotron2-Wavenet-Korean-TTS" -> "carpedm20/multi-speaker-tacotron-tensorflow"
"ardha27/AI-Waifu-Vtuber" -> "Koischizo/AI-Vtuber"
"ardha27/AI-Waifu-Vtuber" -> "HRNPH/AIwaifu"
"ardha27/AI-Waifu-Vtuber" -> "adi-panda/Kuebiko"
"ardha27/AI-Waifu-Vtuber" -> "whiteeat/ai-vtuber-alpha"
"dake/openVP" -> "ALIZE-Speaker-Recognition/LIA_RAL"
"dake/openVP" -> "ibillxia/VoicePrintReco"
"acids-ircam/nn_tilde" -> "acids-ircam/rave_vst"
"acids-ircam/nn_tilde" -> "Fyfe93/RAVE-audition"
"acids-ircam/nn_tilde" -> "acids-ircam/RAVE"
"acids-ircam/nn_tilde" -> "caillonantoine/RAVE"
"acids-ircam/nn_tilde" -> "QosmoInc/neutone_sdk"
"acids-ircam/nn_tilde" -> "AlexHarker/FrameLib"
"acids-ircam/nn_tilde" -> "victor-shepardson/rave-supercollider"
"acids-ircam/rave_vst" -> "acids-ircam/nn_tilde"
"acids-ircam/rave_vst" -> "acids-ircam/RAVE"
"acids-ircam/rave_vst" -> "Fyfe93/RAVE-audition"
"acids-ircam/rave_vst" -> "caillonantoine/RAVE"
"acids-ircam/rave_vst" -> "fcaspe/ddx7"
"maxfrenzel/SampleVAE" -> "maxfrenzel/SpectrogramVAE"
"sp-nitech/SPTK" -> "sp-nitech/diffsptk"
"audio-captioning/dcase-2020-baseline" -> "audio-captioning/audio-captioning-papers"
"enlyth/sd-webui-riffusion" -> "chavinlo/riffusion-manipulation"
"KrishnaDN/x-vector-pytorch" -> "manojpamk/pytorch_xvectors"
"KrishnaDN/x-vector-pytorch" -> "cvqluu/TDNN"
"jcsilva/docker-kaldi-gstreamer-server" -> "alumae/kaldi-gstreamer-server"
"jcsilva/docker-kaldi-gstreamer-server" -> "alumae/gst-kaldi-nnet2-online"
"jcsilva/docker-kaldi-gstreamer-server" -> "Kaljurand/dictate.js"
"jcsilva/docker-kaldi-gstreamer-server" -> "Kaljurand/K6nele"
"jcsilva/docker-kaldi-gstreamer-server" -> "alumae/kaldi-offline-transcriber"
"jcsilva/docker-kaldi-gstreamer-server" -> "dialogflow/asr-server"
"jcsilva/docker-kaldi-gstreamer-server" -> "XiaoMi/kaldi-onnx"
"jcsilva/docker-kaldi-gstreamer-server" -> "gooofy/zamia-speech"
"jcsilva/docker-kaldi-gstreamer-server" -> "uhh-lt/kaldi-tuda-de"
"jcsilva/docker-kaldi-gstreamer-server" -> "gooofy/py-kaldi-asr"
"jcsilva/docker-kaldi-gstreamer-server" -> "YoavRamon/awesome-kaldi"
"jcsilva/docker-kaldi-gstreamer-server" -> "lingochamp/kaldi-ctc"
"jcsilva/docker-kaldi-gstreamer-server" -> "alphacep/kaldi-websocket-python"
"npuichigo/waveglow" -> "azraelkuan/parallel_wavenet_vocoder"
"npuichigo/waveglow" -> "ksw0306/WaveVAE"
"npuichigo/waveglow" -> "h-meru/Tacotron-WaveRNN"
"npuichigo/waveglow" -> "ksw0306/ClariNet"
"npuichigo/waveglow" -> "npuichigo/voicenet"
"npuichigo/waveglow" -> "ksw0306/FloWaveNet"
"npuichigo/waveglow" -> "mkotha/WaveRNN"
"npuichigo/waveglow" -> "nii-yamagishilab/TSNetVocoder"
"npuichigo/waveglow" -> "tiberiu44/TTS-Cube"
"npuichigo/waveglow" -> "HaiFengZeng/clari_wavenet_vocoder"
"npuichigo/waveglow" -> "kan-bayashi/PytorchWaveNetVocoder"
"npuichigo/waveglow" -> "syang1993/FFTNet"
"sigsep/sigsep-mus-eval" -> "sigsep/sigsep-mus-db"
"sigsep/sigsep-mus-eval" -> "craffel/mir_eval" ["e"=1]
"WelkinYang/Learn2Sing2.0" -> "YatingMusic/ddsp-singing-vocoders"
"WelkinYang/Learn2Sing2.0" -> "SJTMusicTeam/Muskits"
"WelkinYang/Learn2Sing2.0" -> "zhangyongmao/VISinger2"
"WelkinYang/Learn2Sing2.0" -> "M4Singer/M4Singer"
"WelkinYang/Learn2Sing2.0" -> "So-Fann/VISinger"
"aidreamwin/TTS-Clone-Chinese" -> "KuangDD/zhrtvc"
"aidreamwin/TTS-Clone-Chinese" -> "Jackiexiao/zhtts"
"aidreamwin/TTS-Clone-Chinese" -> "jackaduma/CycleGAN-VC2"
"aidreamwin/TTS-Clone-Chinese" -> "xingmegshuo/zhrtvc"
"aidreamwin/TTS-Clone-Chinese" -> "JasonWei512/Tacotron-2-Chinese"
"Dannynis/xvector_pytorch" -> "SiddGururani/Pytorch-TDNN"
"Dannynis/xvector_pytorch" -> "manojpamk/pytorch_xvectors"
"Dannynis/xvector_pytorch" -> "funcwj/ge2e-speaker-verification"
"Dannynis/xvector_pytorch" -> "zeroQiaoba/ivector-xvector"
"mycrazycracy/tf-kaldi-speaker" -> "hsn-zeinali/x-vector-kaldi-tf"
"mycrazycracy/tf-kaldi-speaker" -> "mycrazycracy/speaker-embedding-with-phonetic-information"
"mycrazycracy/tf-kaldi-speaker" -> "WeidiXie/VGG-Speaker-Recognition"
"mycrazycracy/tf-kaldi-speaker" -> "mycrazycracy/Backends-for-SRE19"
"mycrazycracy/tf-kaldi-speaker" -> "RicherMans/PLDA"
"mycrazycracy/tf-kaldi-speaker" -> "linhdvu14/vggvox-speaker-identification"
"mycrazycracy/tf-kaldi-speaker" -> "jefflai108/pytorch-kaldi-neural-speaker-embeddings"
"Helsinki-NLP/prosody" -> "asuni/wavelet_prosody_toolkit"
"Helsinki-NLP/prosody" -> "kakaobrain/g2pM"
"Helsinki-NLP/prosody" -> "BoragoCode/AttentionBasedProsodyPrediction"
"Helsinki-NLP/prosody" -> "Zeqiang-Lai/Prosody_Prediction"
"Helsinki-NLP/prosody" -> "yanggeng1995/FB-MelGAN"
"Helsinki-NLP/prosody" -> "Jackiexiao/MTTS"
"Helsinki-NLP/prosody" -> "keonlee9420/STYLER"
"Helsinki-NLP/prosody" -> "ivanvovk/DurIAN"
"L0SG/WaveFlow" -> "Deepest-Project/AlignTTS"
"ksw0306/ClariNet" -> "ksw0306/FloWaveNet"
"ksw0306/ClariNet" -> "azraelkuan/parallel_wavenet_vocoder"
"ksw0306/ClariNet" -> "npuichigo/waveglow"
"ksw0306/ClariNet" -> "mkotha/WaveRNN"
"ksw0306/ClariNet" -> "geneing/WaveRNN-Pytorch"
"ksw0306/ClariNet" -> "ksw0306/WaveVAE"
"ksw0306/ClariNet" -> "mozilla/LPCNet"
"ksw0306/ClariNet" -> "bshall/UniversalVocoding"
"ksw0306/ClariNet" -> "andabi/parallel-wavenet-vocoder"
"ksw0306/ClariNet" -> "tiberiu44/TTS-Cube"
"ksw0306/ClariNet" -> "soobinseo/Transformer-TTS"
"ksw0306/ClariNet" -> "bfs18/nsynth_wavenet"
"ksw0306/ClariNet" -> "tianrengao/SqueezeWave"
"ksw0306/ClariNet" -> "syang1993/gst-tacotron"
"ksw0306/ClariNet" -> "kan-bayashi/PytorchWaveNetVocoder"
"soobinseo/Transformer-TTS" -> "xcmyz/FastSpeech"
"soobinseo/Transformer-TTS" -> "jik876/hifi-gan"
"soobinseo/Transformer-TTS" -> "kan-bayashi/ParallelWaveGAN"
"soobinseo/Transformer-TTS" -> "NVIDIA/mellotron"
"soobinseo/Transformer-TTS" -> "jaywalnut310/glow-tts"
"soobinseo/Transformer-TTS" -> "seungwonpark/melgan"
"soobinseo/Transformer-TTS" -> "mozilla/LPCNet"
"soobinseo/Transformer-TTS" -> "descriptinc/melgan-neurips"
"soobinseo/Transformer-TTS" -> "xcmyz/speech-synthesis-paper"
"soobinseo/Transformer-TTS" -> "ksw0306/ClariNet"
"soobinseo/Transformer-TTS" -> "as-ideas/TransformerTTS"
"soobinseo/Transformer-TTS" -> "ksw0306/FloWaveNet"
"soobinseo/Transformer-TTS" -> "yanggeng1995/GAN-TTS"
"soobinseo/Transformer-TTS" -> "ming024/FastSpeech2"
"soobinseo/Transformer-TTS" -> "tianrengao/SqueezeWave"
"KinglittleQ/GST-Tacotron" -> "syang1993/gst-tacotron"
"KinglittleQ/GST-Tacotron" -> "rishikksh20/vae_tacotron2"
"KinglittleQ/GST-Tacotron" -> "jinhan/tacotron2-vae"
"KinglittleQ/GST-Tacotron" -> "xcmyz/FastSpeech"
"KinglittleQ/GST-Tacotron" -> "Wendison/VQMIVC"
"KinglittleQ/GST-Tacotron" -> "NVIDIA/mellotron"
"KinglittleQ/GST-Tacotron" -> "jinhan/tacotron2-gst"
"KinglittleQ/GST-Tacotron" -> "Emotional-Text-to-Speech/dl-for-emo-tts"
"KinglittleQ/GST-Tacotron" -> "rishikksh20/gmvae_tacotron"
"KinglittleQ/GST-Tacotron" -> "mindslab-ai/cotatron"
"KinglittleQ/GST-Tacotron" -> "keonlee9420/Expressive-FastSpeech2"
"KinglittleQ/GST-Tacotron" -> "kan-bayashi/ParallelWaveGAN"
"KinglittleQ/GST-Tacotron" -> "keonlee9420/Parallel-Tacotron2"
"KinglittleQ/GST-Tacotron" -> "yanggeng1995/vae_tacotron"
"hash2430/pitchtron" -> "emotiontts/emotiontts_open_db"
"hash2430/pitchtron" -> "dipjyoti92/SC-WaveRNN"
"kkoutini/PaSST" -> "RetroCirce/HTS-Audio-Transformer"
"kkoutini/PaSST" -> "YuanGongND/ast"
"kkoutini/PaSST" -> "YuanGongND/psla"
"kkoutini/PaSST" -> "YuanGongND/ssast"
"kkoutini/PaSST" -> "fschmid56/EfficientAT"
"kkoutini/PaSST" -> "qiuqiangkong/audioset_tagging_cnn"
"kkoutini/PaSST" -> "facebookresearch/AudioMAE"
"kkoutini/PaSST" -> "RetroCirce/Zero_Shot_Audio_Source_Separation"
"kkoutini/PaSST" -> "DCASE-REPO/DESED_task" ["e"=1]
"kkoutini/PaSST" -> "qiuqiangkong/panns_inference"
"kkoutini/PaSST" -> "liuxubo717/sound_generation"
"kkoutini/PaSST" -> "qiuqiangkong/torchlibrosa"
"kkoutini/PaSST" -> "soham97/awesome-sound_event_detection"
"yistLin/dvector" -> "cyhuang-tw/AdaIN-VC"
"yistLin/dvector" -> "manojpamk/pytorch_xvectors"
"yistLin/dvector" -> "yistLin/universal-vocoder"
"yistLin/dvector" -> "yistLin/FragmentVC"
"yistLin/dvector" -> "howard1337/S2VC"
"yistLin/dvector" -> "yistLin/human-evaluation"
"Tarteel-io/tarteel-ml" -> "tarekeldeeb/DeepSpeech-Quran"
"jadevaibhav/Signature-verification-using-deep-learning" -> "luizgh/sigver"
"usernaamee/keras-wavenet" -> "basveeling/wavenet"
"usernaamee/keras-wavenet" -> "tomlepaine/fast-wavenet"
"usernaamee/keras-wavenet" -> "phreeza/keras-GAN" ["e"=1]
"usernaamee/keras-wavenet" -> "huyouare/WaveNet-Theano"
"GSByeon/multi-speaker-tacotron-tensorflow" -> "kastnerkyle/multi-speaker-tacotron-tensorflow"
"GSByeon/multi-speaker-tacotron-tensorflow" -> "carpedm20/multi-speaker-tacotron-tensorflow"
"csukuangfj/transducer-loss-benchmarking" -> "csukuangfj/optimized_transducer"
"ivanvovk/DurIAN" -> "yanggeng1995/Multi-band-WaveRNN"
"ivanvovk/DurIAN" -> "liusongxiang/efficient_tts"
"ivanvovk/DurIAN" -> "Deepest-Project/AlignTTS"
"ivanvovk/DurIAN" -> "keonlee9420/Parallel-Tacotron2"
"ivanvovk/DurIAN" -> "entn-at/DurIAN-1"
"ivanvovk/DurIAN" -> "WelkinYang/GradTTS"
"ivanvovk/DurIAN" -> "thuhcsi/VAENAR-TTS"
"kylebgorman/textgrid" -> "hbuschme/TextGridTools"
"kylebgorman/textgrid" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"nwpuaslp/ASR_Course" -> "kaituoxu/E6870"
"nwpuaslp/ASR_Course" -> "csukuangfj/kaldifeat"
"nwpuaslp/ASR_Course" -> "placebokkk/e6870"
"sweekarsud/Goodness-of-Pronunciation" -> "mcvkhaos/GOP-LSTM"
"sweekarsud/Goodness-of-Pronunciation" -> "tzyll/goparrot"
"sweekarsud/Goodness-of-Pronunciation" -> "JazminVidal/gop-dnn-epadb"
"sweekarsud/Goodness-of-Pronunciation" -> "tbright17/kaldi-dnn-ali-gop"
"wangshub/python-vad" -> "marsbroshok/VAD-python"
"wangshub/python-vad" -> "filippogiruzzi/voice_activity_detection"
"wangshub/python-vad" -> "mounalab/LSTM-RNN-VAD"
"wangshub/python-vad" -> "hcmlab/vadnet"
"wangshub/python-vad" -> "mindorii/kws" ["e"=1]
"wangshub/python-vad" -> "eesungkim/Voice_Activity_Detector"
"wangshub/python-vad" -> "xiyihong/webRTC-"
"nnsvs/nnsvs" -> "guan-yuan/Awesome-Singing-Voice-Synthesis-and-Singing-Voice-Conversion"
"nnsvs/nnsvs" -> "oatsu-gh/ENUNU" ["e"=1]
"nnsvs/nnsvs" -> "SJTMusicTeam/Muskits"
"nnsvs/nnsvs" -> "chomeyama/SiFiGAN"
"nnsvs/nnsvs" -> "WelkinYang/Learn2Sing2.0"
"nnsvs/nnsvs" -> "M4Singer/M4Singer"
"nnsvs/nnsvs" -> "PlayVoice/VI-SVC"
"nnsvs/nnsvs" -> "chomeyama/HN-UnifiedSourceFilterGAN"
"aishell-foundation/DaCiDian" -> "speechio/BigCiDian"
"aishell-foundation/DaCiDian" -> "open-speech/speech-aligner"
"aishell-foundation/DaCiDian" -> "speechio/chinese_text_normalization"
"aishell-foundation/DaCiDian" -> "tencent-ailab/pika"
"aishell-foundation/DaCiDian" -> "YiwenShaoStephen/pychain"
"aishell-foundation/DaCiDian" -> "cywang97/StreamingTransformer"
"aishell-foundation/DaCiDian" -> "ZhengkunTian/OpenTransformer"
"aishell-foundation/DaCiDian" -> "jzlianglu/pykaldi2"
"aishell-foundation/DaCiDian" -> "XiaoMi/kaldi-onnx"
"aishell-foundation/DaCiDian" -> "lingochamp/kaldi-ctc"
"aishell-foundation/DaCiDian" -> "shiyuzh2007/ASR"
"aishell-foundation/DaCiDian" -> "HawkAaron/warp-transducer"
"aishell-foundation/DaCiDian" -> "xingchensong/speech-recognition-papers"
"aishell-foundation/DaCiDian" -> "kakaobrain/g2pM"
"aishell-foundation/DaCiDian" -> "mobvoi/wenet"
"rakeshvar/rnn_ctc" -> "shawntan/theano-ctc"
"rakeshvar/rnn_ctc" -> "amaas/stanford-ctc"
"rakeshvar/rnn_ctc" -> "mohammadpz/CTC-Connectionist-Temporal-Classification"
"rakeshvar/rnn_ctc" -> "sherjilozair/ctc"
"rakeshvar/rnn_ctc" -> "aaron-xichen/cnn-lstm-ctc"
"cnlinxi/book-text-to-speech" -> "ddlBoJack/Speech-Resources"
"cnlinxi/book-text-to-speech" -> "TencentGameMate/chinese_speech_pretrain"
"cnlinxi/book-text-to-speech" -> "speechio/chinese_text_normalization"
"cnlinxi/book-text-to-speech" -> "kakaobrain/g2pM"
"cnlinxi/book-text-to-speech" -> "wenet-e2e/wetts"
"cnlinxi/book-text-to-speech" -> "tts-tutorial/survey"
"cnlinxi/book-text-to-speech" -> "jik876/hifi-gan"
"cnlinxi/book-text-to-speech" -> "yerfor/SyntaSpeech"
"cnlinxi/book-text-to-speech" -> "keonlee9420/Comprehensive-Transformer-TTS"
"cnlinxi/book-text-to-speech" -> "xcmyz/FastVocoder"
"cnlinxi/book-text-to-speech" -> "thuhcsi/Crystal"
"cnlinxi/book-text-to-speech" -> "microsoft/NeuralSpeech"
"cnlinxi/book-text-to-speech" -> "Rongjiehuang/FastDiff"
"cnlinxi/book-text-to-speech" -> "SungFeng-Huang/Meta-TTS"
"cnlinxi/book-text-to-speech" -> "xcmyz/speech-synthesis-paper"
"dipjyoti92/SC-WaveRNN" -> "jxzhanggg/nonparaSeq2seqVC_code"
"dipjyoti92/SC-WaveRNN" -> "geneing/WaveRNN-Pytorch"
"dipjyoti92/SC-WaveRNN" -> "keonlee9420/WaveGrad2"
"cvqluu/TDNN" -> "cvqluu/Factorized-TDNN"
"cvqluu/TDNN" -> "jonasvdd/TDNN"
"cvqluu/TDNN" -> "manojpamk/pytorch_xvectors"
"cvqluu/TDNN" -> "SiddGururani/Pytorch-TDNN"
"cvqluu/TDNN" -> "RaviSoji/plda"
"cvqluu/TDNN" -> "KrishnaDN/x-vector-pytorch"
"cvqluu/TDNN" -> "funcwj/ge2e-speaker-verification"
"cvqluu/TDNN" -> "jymsuper/SpeakerRecognition_tutorial"
"cvqluu/TDNN" -> "Dannynis/xvector_pytorch"
"cvqluu/TDNN" -> "yuyq96/D-TDNN"
"cvqluu/TDNN" -> "kefirski/pytorch_TDNN"
"cvqluu/TDNN" -> "Jamiroquai88/VBDiarization"
"marsbroshok/VAD-python" -> "hcmlab/vadnet"
"marsbroshok/VAD-python" -> "jtkim-kaist/VAD"
"marsbroshok/VAD-python" -> "wiseman/py-webrtcvad"
"marsbroshok/VAD-python" -> "mwv/vad"
"marsbroshok/VAD-python" -> "wangshub/python-vad"
"marsbroshok/VAD-python" -> "filippogiruzzi/voice_activity_detection"
"marsbroshok/VAD-python" -> "qqueing/DeepSpeaker-pytorch"
"marsbroshok/VAD-python" -> "mounalab/LSTM-RNN-VAD"
"marsbroshok/VAD-python" -> "amsehili/auditok"
"marsbroshok/VAD-python" -> "Baidu-AIP/speech-vad-demo"
"marsbroshok/VAD-python" -> "vBaiCai/python-pesq" ["e"=1]
"marsbroshok/VAD-python" -> "eesungkim/Voice_Activity_Detector"
"marsbroshok/VAD-python" -> "nicklashansen/voice-activity-detection"
"marsbroshok/VAD-python" -> "Janghyun1230/Speaker_Verification"
"marsbroshok/VAD-python" -> "xiongyihui/python-webrtc-audio-processing" ["e"=1]
"AdolfVonKleist/Phonetisaurus" -> "cmusphinx/g2p-seq2seq"
"AdolfVonKleist/Phonetisaurus" -> "sequitur-g2p/sequitur-g2p"
"AdolfVonKleist/Phonetisaurus" -> "Kyubyong/g2p"
"AdolfVonKleist/Phonetisaurus" -> "bootphon/phonemizer"
"AdolfVonKleist/Phonetisaurus" -> "cmusphinx/cmudict"
"AdolfVonKleist/Phonetisaurus" -> "google/sparrowhawk"
"AdolfVonKleist/Phonetisaurus" -> "uiuc-sst/g2ps"
"AdolfVonKleist/Phonetisaurus" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"AdolfVonKleist/Phonetisaurus" -> "YoavRamon/awesome-kaldi"
"AdolfVonKleist/Phonetisaurus" -> "HawkAaron/warp-transducer"
"AdolfVonKleist/Phonetisaurus" -> "JRMeyer/open-speech-corpora"
"AdolfVonKleist/Phonetisaurus" -> "lingjzhu/charsiu"
"AdolfVonKleist/Phonetisaurus" -> "gooofy/zamia-speech"
"AdolfVonKleist/Phonetisaurus" -> "lhotse-speech/lhotse"
"AdolfVonKleist/Phonetisaurus" -> "lowerquality/gentle"
"HLTSingapore/Emotional-Speech-Data" -> "keonlee9420/StyleSpeech"
"HLTSingapore/Emotional-Speech-Data" -> "KunZhou9646/Mixed_Emotions"
"HLTSingapore/Emotional-Speech-Data" -> "SuperKogito/SER-datasets" ["e"=1]
"HLTSingapore/Emotional-Speech-Data" -> "keonlee9420/STYLER"
"HLTSingapore/Emotional-Speech-Data" -> "NVIDIA/BigVGAN"
"HLTSingapore/Emotional-Speech-Data" -> "keonlee9420/Expressive-FastSpeech2"
"KunZhou9646/seq2seq-EVC" -> "KunZhou9646/controllable_evc_code"
"KunZhou9646/seq2seq-EVC" -> "KunZhou9646/Emovox"
"KunZhou9646/seq2seq-EVC" -> "KunZhou9646/Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT"
"qiniu/csharp-sdk" -> "icattlecoder/qiniu-csharp-sdk"
"cmusphinx/sphinxbase" -> "cmusphinx/sphinxtrain"
"cmusphinx/sphinxbase" -> "cmusphinx/pocketsphinx"
"cmusphinx/sphinxbase" -> "cmusphinx/pocketsphinx-python"
"cmusphinx/sphinxbase" -> "cmusphinx/pocketsphinx-android"
"cmusphinx/sphinxbase" -> "cmusphinx/sphinx4"
"cmusphinx/sphinxbase" -> "cmusphinx/cmudict-tools"
"cmusphinx/sphinxbase" -> "cmusphinx/cmudict"
"cmusphinx/sphinxbase" -> "cmusphinx/pocketsphinx-android-demo"
"cmusphinx/sphinxbase" -> "cmusphinx/pocketsphinx-ios-demo"
"cmusphinx/sphinxbase" -> "cmusphinx/g2p-seq2seq"
"cmusphinx/sphinxbase" -> "bambocher/pocketsphinx-python"
"sequitur-g2p/sequitur-g2p" -> "AdolfVonKleist/Phonetisaurus"
"sequitur-g2p/sequitur-g2p" -> "cmusphinx/g2p-seq2seq"
"SJTMusicTeam/Muskits" -> "WelkinYang/Learn2Sing2.0"
"SJTMusicTeam/Muskits" -> "YatingMusic/ddsp-singing-vocoders"
"SJTMusicTeam/Muskits" -> "CODEJIN/HiFiSinger"
"SJTMusicTeam/Muskits" -> "M4Singer/M4Singer"
"SJTMusicTeam/Muskits" -> "SJTMusicTeam/SVS_system"
"SJTMusicTeam/Muskits" -> "Rongjiehuang/Multi-Singer"
"SJTMusicTeam/Muskits" -> "MoonInTheRiver/NeuralSVB"
"SJTMusicTeam/Muskits" -> "neosapience/mlp-singer"
"SJTMusicTeam/Muskits" -> "hhguo/EA-SVC"
"SJTMusicTeam/Muskits" -> "zhangyongmao/VISinger2"
"SJTMusicTeam/Muskits" -> "guan-yuan/Awesome-Singing-Voice-Synthesis-and-Singing-Voice-Conversion"
"SJTMusicTeam/Muskits" -> "nnsvs/nnsvs"
"SJTMusicTeam/Muskits" -> "wenet-e2e/wetts"
"just-ai/jaicf-kotlin" -> "aimybox/aimybox-android-assistant"
"just-ai/jaicf-kotlin" -> "just-ai/aimybox-android-assistant"
"just-ai/jaicf-kotlin" -> "just-ai/panda"
"just-ai/jaicf-kotlin" -> "just-ai/jaicf-jaicp-caila-template"
"just-ai/jaicf-kotlin" -> "just-ai/jaicf-template"
"just-ai/jaicf-kotlin" -> "just-ai/alice-jaicf-template"
"numediart/MBROLA-voices" -> "numediart/MBROLA"
"numediart/MBROLA-voices" -> "numediart/MBROLATOR"
"mindslab-ai/nuwave" -> "mindslab-ai/nuwave2"
"mindslab-ai/nuwave" -> "mindslab-ai/wavegrad2"
"mindslab-ai/nuwave" -> "junjun3518/alias-free-torch"
"mindslab-ai/nuwave" -> "mindslab-ai/phaseaug"
"mindslab-ai/nuwave" -> "mindslab-ai/assem-vc"
"mindslab-ai/nuwave" -> "rishikksh20/NU-Wave-pytorch"
"mindslab-ai/nuwave" -> "mindslab-ai/pnlp-mixer"
"mindslab-ai/nuwave" -> "haoheliu/ssr_eval"
"mindslab-ai/nuwave" -> "ncsoft/avocodo"
"mindslab-ai/nuwave" -> "mindslab-ai/univnet"
"mindslab-ai/nuwave" -> "sp-uhh/sgmse" ["e"=1]
"mindslab-ai/nuwave" -> "tencent-ailab/bddm"
"orchidas/Speaker-Recognition" -> "crouchred/speaker-recognition-py3"
"jaywalnut310/glow-tts" -> "jik876/hifi-gan"
"jaywalnut310/glow-tts" -> "LEEYOONHYUNG/BVAE-TTS"
"jaywalnut310/glow-tts" -> "kan-bayashi/ParallelWaveGAN"
"jaywalnut310/glow-tts" -> "xcmyz/speech-synthesis-paper"
"jaywalnut310/glow-tts" -> "NVIDIA/mellotron"
"jaywalnut310/glow-tts" -> "ming024/FastSpeech2"
"jaywalnut310/glow-tts" -> "tts-tutorial/survey"
"jaywalnut310/glow-tts" -> "seungwonpark/melgan"
"jaywalnut310/glow-tts" -> "thuhcsi/VAENAR-TTS"
"jaywalnut310/glow-tts" -> "xcmyz/FastSpeech"
"jaywalnut310/glow-tts" -> "ivanvovk/WaveGrad"
"jaywalnut310/glow-tts" -> "NVIDIA/BigVGAN"
"jaywalnut310/glow-tts" -> "NVIDIA/flowtron"
"jaywalnut310/glow-tts" -> "Tomiinek/Multilingual_Text_to_Speech"
"jaywalnut310/glow-tts" -> "mindslab-ai/cotatron"
"MachineLP/TensorFlowTTS_chinese" -> "Liu-Feng-deeplearning/TTS-frontend"
"MachineLP/TensorFlowTTS_chinese" -> "Zeqiang-Lai/Prosody_Prediction"
"MachineLP/TensorFlowTTS_chinese" -> "Riroaki/Chinese-Rhythm-Predictor"
"auspicious3000/contentvec" -> "auspicious3000/AutoPST"
"auspicious3000/contentvec" -> "bshall/soft-vc"
"auspicious3000/contentvec" -> "biggytruck/SpeechSplit2"
"auspicious3000/contentvec" -> "M4Singer/M4Singer"
"auspicious3000/contentvec" -> "OlaWod/FreeVC"
"auspicious3000/contentvec" -> "yl4579/StyleTTS-VC"
"auspicious3000/contentvec" -> "YatingMusic/ddsp-singing-vocoders"
"auspicious3000/contentvec" -> "hhguo/MSMC-TTS"
"auspicious3000/contentvec" -> "auspicious3000/SpeechSplit"
"tyiannak/paura" -> "tyiannak/basic_audio_analysis"
"upskyy/Squeezeformer" -> "kssteven418/Squeezeformer"
"upskyy/Squeezeformer" -> "burchim/EfficientConformer"
"amaurycrickx/recognito" -> "Adirockzz95/Piwho"
"amaurycrickx/recognito" -> "ppwwyyxx/speaker-recognition"
"amaurycrickx/recognito" -> "ALIZE-Speaker-Recognition/LIA_RAL"
"jitsi/jitsi-webrtc-vad-wrapper" -> "orctom/vad4j"
"speechio/BigCiDian" -> "aishell-foundation/DaCiDian"
"speechio/BigCiDian" -> "speechio/chinese_text_normalization"
"speechio/BigCiDian" -> "kakaobrain/g2pM"
"speechio/BigCiDian" -> "xingchensong/speech-recognition-papers"
"speechio/BigCiDian" -> "YiwenShaoStephen/pychain"
"speechio/BigCiDian" -> "tencent-ailab/3m-asr"
"speechio/BigCiDian" -> "XiaoMi/kaldi-onnx"
"speechio/BigCiDian" -> "jctian98/e2e_lfmmi"
"speechio/BigCiDian" -> "cywang97/StreamingTransformer"
"CODEJIN/HiFiSinger" -> "CODEJIN/PWGAN_for_HiFiSinger"
"CODEJIN/HiFiSinger" -> "SJTMusicTeam/Muskits"
"CODEJIN/HiFiSinger" -> "M4Singer/M4Singer"
"lucidrains/naturalspeech2-pytorch" -> "yl4579/StyleTTS"
"lucidrains/naturalspeech2-pytorch" -> "wenet-e2e/wetts"
"lucidrains/naturalspeech2-pytorch" -> "liusongxiang/Large-Audio-Models"
"lucidrains/naturalspeech2-pytorch" -> "NVIDIA/BigVGAN"
"lucidrains/naturalspeech2-pytorch" -> "hhguo/MSMC-TTS"
"lucidrains/naturalspeech2-pytorch" -> "anonymous-pits/pits"
"lucidrains/naturalspeech2-pytorch" -> "keonlee9420/DailyTalk"
"festvox/datasets-CMU_Wilderness" -> "lingjzhu/CharsiuG2P"
"festvox/datasets-CMU_Wilderness" -> "xinjli/transphone"
"kaituoxu/Listen-Attend-Spell" -> "kaituoxu/Speech-Transformer"
"kaituoxu/Listen-Attend-Spell" -> "jackaduma/LAS_Mandarin_PyTorch"
"kaituoxu/Listen-Attend-Spell" -> "AzizCode92/Listen-Attend-and-Spell-Pytorch"
"kaituoxu/Listen-Attend-Spell" -> "Alexander-H-Liu/End-to-end-ASR-Pytorch"
"kaituoxu/Listen-Attend-Spell" -> "ZhengkunTian/rnn-transducer"
"kaituoxu/Listen-Attend-Spell" -> "WindQAQ/listen-attend-and-spell"
"kaituoxu/Listen-Attend-Spell" -> "HawkAaron/RNN-Transducer"
"kaituoxu/Listen-Attend-Spell" -> "zcaceres/spec_augment"
"kaituoxu/Listen-Attend-Spell" -> "jzlianglu/pykaldi2"
"kaituoxu/Listen-Attend-Spell" -> "gentaiscool/end2end-asr-pytorch"
"kaituoxu/Listen-Attend-Spell" -> "ZhengkunTian/Speech-Tranformer-Pytorch"
"kaituoxu/Listen-Attend-Spell" -> "ZhengkunTian/OpenTransformer"
"foamliu/Tacotron2-Mandarin" -> "JasonWei512/wavenet_vocoder"
"foamliu/Tacotron2-Mandarin" -> "Joee1995/tacotron2-mandarin-griffin-lim"
"foamliu/Tacotron2-Mandarin" -> "JasonWei512/Tacotron-2-Chinese"
"yanggeng1995/GAN-TTS" -> "seungwonpark/melgan"
"yanggeng1995/GAN-TTS" -> "ivanvovk/WaveGrad"
"yanggeng1995/GAN-TTS" -> "thuhcsi/VAENAR-TTS"
"yanggeng1995/GAN-TTS" -> "descriptinc/melgan-neurips"
"yanggeng1995/GAN-TTS" -> "rishikksh20/vae_tacotron2"
"yanggeng1995/GAN-TTS" -> "soobinseo/Transformer-TTS"
"yanggeng1995/GAN-TTS" -> "ivanvovk/DurIAN"
"yanggeng1995/GAN-TTS" -> "rishikksh20/VocGAN"
"yanggeng1995/GAN-TTS" -> "xcmyz/speech-synthesis-paper"
"yanggeng1995/GAN-TTS" -> "mbinkowski/DeepSpeechDistances"
"yanggeng1995/GAN-TTS" -> "jaywalnut310/glow-tts"
"yanggeng1995/GAN-TTS" -> "MTG/WGANSing"
"yanggeng1995/GAN-TTS" -> "kan-bayashi/ParallelWaveGAN"
"yanggeng1995/GAN-TTS" -> "syang1993/gst-tacotron"
"AlexHarker/FrameLib" -> "AlexHarker/HISSTools_Library"
"yangdongchao/Text-to-sound-Synthesis" -> "Rongjiehuang/GenerSpeech"
"yangdongchao/Text-to-sound-Synthesis" -> "YatingMusic/ddsp-singing-vocoders"
"yangdongchao/Text-to-sound-Synthesis" -> "mindslab-ai/phaseaug"
"yangdongchao/Text-to-sound-Synthesis" -> "KevinMIN95/StyleSpeech"
"yangdongchao/Text-to-sound-Synthesis" -> "tts-tutorial/interspeech2022"
"yangdongchao/Text-to-sound-Synthesis" -> "liusongxiang/Large-Audio-Models"
"yangdongchao/Text-to-sound-Synthesis" -> "Rongjiehuang/FastDiff"
"yangdongchao/Text-to-sound-Synthesis" -> "v-iashin/SpecVQGAN"
"yangdongchao/Text-to-sound-Synthesis" -> "microsoft/CLAP"
"yangdongchao/Text-to-sound-Synthesis" -> "hhguo/MSMC-TTS"
"yangdongchao/Text-to-sound-Synthesis" -> "yangdongchao/text-to-sound-synthesis-demo"
"yangdongchao/Text-to-sound-Synthesis" -> "SungFeng-Huang/Meta-TTS"
"yangdongchao/Text-to-sound-Synthesis" -> "huawei-noah/Speech-Backbones"
"yangdongchao/Text-to-sound-Synthesis" -> "b04901014/MQTTS"
"yangdongchao/Text-to-sound-Synthesis" -> "Rongjiehuang/ProDiff"
"RetroCirce/Zero_Shot_Audio_Source_Separation" -> "RetroCirce/HTS-Audio-Transformer"
"RetroCirce/Zero_Shot_Audio_Source_Separation" -> "gudgud96/frechet-audio-distance"
"audeering/w2v2-how-to" -> "innnky/emotional-vits"
"audeering/w2v2-how-to" -> "anonymous-pits/pits"
"audeering/w2v2-how-to" -> "MasayaKawamura/MB-iSTFT-VITS"
"audeering/w2v2-how-to" -> "AlexandaJerry/whisper-vits-japanese"
"audeering/w2v2-how-to" -> "PlayVoice/vits_chinese"
"audeering/w2v2-how-to" -> "alphanemeless/VITS_TXT_to_Audio"
"audeering/w2v2-how-to" -> "yl4579/StyleTTS"
"MainRo/deepspeech-server" -> "ashwan1/django-deepspeech-server"
"savoirfairelinux/num2words" -> "bootphon/phonemizer"
"savoirfairelinux/num2words" -> "daanzu/kaldi-active-grammar" ["e"=1]
"savoirfairelinux/num2words" -> "jitsi/jiwer"
"savoirfairelinux/num2words" -> "jfilter/clean-text" ["e"=1]
"savoirfairelinux/num2words" -> "SpeechColab/GigaSpeech"
"savoirfairelinux/num2words" -> "coqui-ai/open-speech-corpora"
"savoirfairelinux/num2words" -> "jonatasgrosman/huggingsound"
"savoirfairelinux/num2words" -> "AdolfVonKleist/Phonetisaurus"
"savoirfairelinux/num2words" -> "lhotse-speech/lhotse"
"savoirfairelinux/num2words" -> "Kyubyong/g2p"
"DigitalPhonetics/IMS-Toucan" -> "keonlee9420/Comprehensive-Transformer-TTS"
"DigitalPhonetics/IMS-Toucan" -> "NVIDIA/BigVGAN"
"DigitalPhonetics/IMS-Toucan" -> "keonlee9420/Comprehensive-E2E-TTS"
"DigitalPhonetics/IMS-Toucan" -> "anonymous-pits/pits"
"DigitalPhonetics/IMS-Toucan" -> "wenet-e2e/wetts"
"DigitalPhonetics/IMS-Toucan" -> "rishikksh20/Avocodo-pytorch"
"DigitalPhonetics/IMS-Toucan" -> "dunky11/voicesmith"
"DigitalPhonetics/IMS-Toucan" -> "Edresson/YourTTS"
"DigitalPhonetics/IMS-Toucan" -> "KevinMIN95/StyleSpeech"
"DigitalPhonetics/IMS-Toucan" -> "keonlee9420/DailyTalk"
"DigitalPhonetics/IMS-Toucan" -> "wenet-e2e/speech-synthesis-paper"
"DigitalPhonetics/IMS-Toucan" -> "tencent-ailab/bddm"
"DigitalPhonetics/IMS-Toucan" -> "rendchevi/nix-tts"
"DigitalPhonetics/IMS-Toucan" -> "Rongjiehuang/GenerSpeech"
"DigitalPhonetics/IMS-Toucan" -> "Kyubyong/css10"
"YatingMusic/ddsp-singing-vocoders" -> "WelkinYang/Learn2Sing2.0"
"YatingMusic/ddsp-singing-vocoders" -> "SJTMusicTeam/Muskits"
"YatingMusic/ddsp-singing-vocoders" -> "Rongjiehuang/FastDiff"
"YatingMusic/ddsp-singing-vocoders" -> "sp-nitech/diffsptk"
"YatingMusic/ddsp-singing-vocoders" -> "Rongjiehuang/ProDiff"
"YatingMusic/ddsp-singing-vocoders" -> "yangdongchao/Text-to-sound-Synthesis"
"YatingMusic/ddsp-singing-vocoders" -> "magenta/midi-ddsp"
"YatingMusic/ddsp-singing-vocoders" -> "hhguo/MSMC-TTS"
"YatingMusic/ddsp-singing-vocoders" -> "hhguo/EA-SVC"
"YatingMusic/ddsp-singing-vocoders" -> "maxrmorrison/torchcrepe"
"as-ideas/ForwardTacotron" -> "jaywalnut310/glow-tts"
"as-ideas/ForwardTacotron" -> "xcmyz/FastSpeech"
"as-ideas/ForwardTacotron" -> "as-ideas/TransformerTTS"
"as-ideas/ForwardTacotron" -> "xcmyz/speech-synthesis-paper"
"as-ideas/ForwardTacotron" -> "seungwonpark/melgan"
"as-ideas/ForwardTacotron" -> "mindslab-ai/cotatron"
"as-ideas/ForwardTacotron" -> "NVIDIA/mellotron"
"as-ideas/ForwardTacotron" -> "NVIDIA/flowtron"
"as-ideas/ForwardTacotron" -> "Tomiinek/Multilingual_Text_to_Speech"
"as-ideas/ForwardTacotron" -> "jik876/hifi-gan"
"as-ideas/ForwardTacotron" -> "tianrengao/SqueezeWave"
"as-ideas/ForwardTacotron" -> "kan-bayashi/ParallelWaveGAN"
"as-ideas/ForwardTacotron" -> "syang1993/gst-tacotron"
"as-ideas/ForwardTacotron" -> "auspicious3000/SpeechSplit"
"as-ideas/ForwardTacotron" -> "mozilla/LPCNet"
"yl4579/StarGANv2-VC" -> "liusongxiang/ppg-vc"
"yl4579/StarGANv2-VC" -> "Wendison/VQMIVC"
"yl4579/StarGANv2-VC" -> "mindslab-ai/assem-vc"
"yl4579/StarGANv2-VC" -> "yl4579/PitchExtractor"
"yl4579/StarGANv2-VC" -> "GANtastic3/MaskCycleGAN-VC"
"yl4579/StarGANv2-VC" -> "yl4579/StyleTTS-VC"
"yl4579/StarGANv2-VC" -> "OlaWod/FreeVC"
"yl4579/StarGANv2-VC" -> "bshall/soft-vc"
"yl4579/StarGANv2-VC" -> "NVIDIA/BigVGAN"
"yl4579/StarGANv2-VC" -> "Edresson/YourTTS"
"yl4579/StarGANv2-VC" -> "yl4579/StyleTTS"
"yl4579/StarGANv2-VC" -> "NVIDIA/radtts"
"yl4579/StarGANv2-VC" -> "rishikksh20/Avocodo-pytorch"
"yl4579/StarGANv2-VC" -> "auspicious3000/autovc"
"yl4579/StarGANv2-VC" -> "jackaduma/CycleGAN-VC2"
"oscarknagg/voicemap" -> "PiotrTa/Huawei-Challenge-Speaker-Identification"
"oscarknagg/voicemap" -> "Atul-Anand-Jha/Speaker-Identification-Python"
"belambert/asr-evaluation" -> "zszyellow/WER-in-python"
"belambert/asr-evaluation" -> "jitsi/asr-wer"
"belambert/asr-evaluation" -> "jitsi/jiwer"
"belambert/asr-evaluation" -> "belambert/edit-distance"
"belambert/asr-evaluation" -> "gooofy/zamia-speech"
"belambert/asr-evaluation" -> "dspavankumar/keras-kaldi"
"dominoanty/SpeakerRecognition" -> "scelesticsiva/speaker_recognition_GMM_UBM"
"joansj/blow" -> "ericwudayi/SkipVQVC"
"joansj/blow" -> "jxzhanggg/nonparaSeq2seqVC_code"
"vrenkens/nabu" -> "v0lta/Listen-attend-and-spell"
"vrenkens/nabu" -> "hirofumi0810/tensorflow_end2end_speech_recognition"
"csteinmetz1/steerable-nafx" -> "caillonantoine/RAVE"
"neosapience/mlp-singer" -> "SoonbeomChoi/BEGANSing"
"neosapience/mlp-singer" -> "SJTMusicTeam/Muskits"
"neosapience/mlp-singer" -> "Rongjiehuang/Multi-Singer"
"neosapience/mlp-singer" -> "M4Singer/M4Singer"
"auspicious3000/AutoPST" -> "auspicious3000/SpeechSplit"
"auspicious3000/AutoPST" -> "auspicious3000/autovc"
"auspicious3000/AutoPST" -> "auspicious3000/contentvec"
"auspicious3000/AutoPST" -> "ebadawy/voice_conversion"
"auspicious3000/AutoPST" -> "Wendison/VQMIVC"
"auspicious3000/AutoPST" -> "cyhuang-tw/AdaIN-VC"
"auspicious3000/AutoPST" -> "facebookresearch/speech-resynthesis"
"auspicious3000/AutoPST" -> "MelissaChen15/control-vc"
"auspicious3000/AutoPST" -> "bshall/VectorQuantizedCPC"
"auspicious3000/AutoPST" -> "thuhcsi/VAENAR-TTS"
"joonson/voxconverse" -> "nryant/dscore"
"pyannote/pyannote-core" -> "pyannote/pyannote-database"
"pyannote/pyannote-core" -> "pyannote/pyannote-metrics"
"haoheliu/2021-ISMIR-MSS-Challenge-CWS-PResUNet" -> "kuielab/mdx-net"
"cuayahuitl/SimpleDS" -> "plison/opendial"
"cuayahuitl/SimpleDS" -> "UFAL-DSG/alex"
"LEEYOONHYUNG/BVAE-TTS" -> "thuhcsi/VAENAR-TTS"
"LEEYOONHYUNG/BVAE-TTS" -> "xcmyz/FastVocoder"
"LEEYOONHYUNG/BVAE-TTS" -> "Deepest-Project/AlignTTS"
"ksw0306/FloWaveNet" -> "ksw0306/ClariNet"
"ksw0306/FloWaveNet" -> "npuichigo/waveglow"
"ksw0306/FloWaveNet" -> "mozilla/LPCNet"
"ksw0306/FloWaveNet" -> "azraelkuan/parallel_wavenet_vocoder"
"ksw0306/FloWaveNet" -> "andabi/parallel-wavenet-vocoder"
"ksw0306/FloWaveNet" -> "kan-bayashi/PytorchWaveNetVocoder"
"ksw0306/FloWaveNet" -> "NVIDIA/waveglow"
"ksw0306/FloWaveNet" -> "soobinseo/Transformer-TTS"
"ksw0306/FloWaveNet" -> "syang1993/gst-tacotron"
"ksw0306/FloWaveNet" -> "mkotha/WaveRNN"
"ksw0306/FloWaveNet" -> "L0SG/WaveFlow"
"ksw0306/FloWaveNet" -> "tianrengao/SqueezeWave"
"ksw0306/FloWaveNet" -> "geneing/WaveRNN-Pytorch"
"ksw0306/FloWaveNet" -> "NVIDIA/nv-wavenet"
"ksw0306/FloWaveNet" -> "descriptinc/melgan-neurips"
"lingjzhu/charsiu" -> "lingjzhu/CharsiuG2P"
"lingjzhu/charsiu" -> "thuhcsi/NeuFA"
"mindslab-ai/cotatron" -> "mindslab-ai/assem-vc"
"mindslab-ai/cotatron" -> "nii-yamagishilab/multi-speaker-tacotron"
"mindslab-ai/cotatron" -> "xcmyz/speech-synthesis-paper"
"mindslab-ai/cotatron" -> "LEEYOONHYUNG/BVAE-TTS"
"mindslab-ai/cotatron" -> "thuhcsi/VAENAR-TTS"
"mindslab-ai/cotatron" -> "auspicious3000/SpeechSplit"
"mindslab-ai/cotatron" -> "jaywalnut310/glow-tts"
"mindslab-ai/cotatron" -> "dipjyoti92/SC-WaveRNN"
"mindslab-ai/cotatron" -> "Wendison/VQMIVC"
"mindslab-ai/cotatron" -> "jinhan/tacotron2-vae"
"sh-lee-prml/BigVGAN" -> "WX-Wei/HarmoF0"
"WuYiming6526/HARD" -> "Torsion-Audio/Scyclone"
"WuYiming6526/HARD" -> "rodrigodzf/NeuralResonatorVST"
"bshall/soft-vc" -> "bshall/hubert"
"bshall/soft-vc" -> "OlaWod/FreeVC"
"bshall/soft-vc" -> "yl4579/StyleTTS-VC"
"bshall/soft-vc" -> "NVIDIA/radtts"
"bshall/soft-vc" -> "bshall/acoustic-model"
"bshall/soft-vc" -> "auspicious3000/contentvec"
"bshall/soft-vc" -> "liusongxiang/ppg-vc"
"bshall/soft-vc" -> "dhchoi99/NANSY"
"bshall/soft-vc" -> "Francis-Komizu/Sovits"
"bshall/soft-vc" -> "mindslab-ai/assem-vc"
"bshall/soft-vc" -> "WelkinYang/Learn2Sing2.0"
"bshall/soft-vc" -> "yl4579/StarGANv2-VC"
"bshall/soft-vc" -> "PlayVoice/VI-SVC"
"ide8/tacotron2" -> "jinhan/tacotron2-vae"
"sooftware/End-to-End-Speech-Recognition-Models" -> "sooftware/deepspeech2" ["e"=1]
"r9y9/pysptk" -> "JeremyCCHsu/Python-Wrapper-for-World-Vocoder"
"r9y9/pysptk" -> "r9y9/nnmnkwii"
"r9y9/pysptk" -> "mmorise/World"
"r9y9/pysptk" -> "k2kobayashi/sprocket"
"r9y9/pysptk" -> "auspicious3000/SpeechSplit"
"r9y9/pysptk" -> "kan-bayashi/ParallelWaveGAN"
"r9y9/pysptk" -> "lochenchou/MOSNet" ["e"=1]
"r9y9/pysptk" -> "r9y9/gantts"
"r9y9/pysptk" -> "mkotha/WaveRNN"
"r9y9/pysptk" -> "mozilla/LPCNet"
"r9y9/pysptk" -> "bigpon/vcc20_baseline_cyclevae"
"r9y9/pysptk" -> "jxzhanggg/nonparaSeq2seqVC_code"
"r9y9/pysptk" -> "NVIDIA/mellotron"
"r9y9/pysptk" -> "kan-bayashi/PytorchWaveNetVocoder"
"r9y9/pysptk" -> "descriptinc/melgan-neurips"
"LeoniusChen/Attentions-in-Tacotron" -> "thuhcsi/tacotron"
"LeoniusChen/Attentions-in-Tacotron" -> "bshall/Tacotron"
"LeoniusChen/Attentions-in-Tacotron" -> "thuhcsi/VAENAR-TTS"
"bshall/Tacotron" -> "LeoniusChen/Attentions-in-Tacotron"
"bshall/Tacotron" -> "thuhcsi/VAENAR-TTS"
"r9y9/tacotron_pytorch" -> "syang1993/gst-tacotron"
"r9y9/tacotron_pytorch" -> "soobinseo/Transformer-TTS"
"r9y9/tacotron_pytorch" -> "KinglittleQ/GST-Tacotron"
"r9y9/tacotron_pytorch" -> "r9y9/gantts"
"r9y9/tacotron_pytorch" -> "kan-bayashi/PytorchWaveNetVocoder"
"r9y9/tacotron_pytorch" -> "ttaoREtw/Tacotron-pytorch"
"r9y9/tacotron_pytorch" -> "azraelkuan/parallel_wavenet_vocoder"
"r9y9/tacotron_pytorch" -> "r9y9/Tacotron-2"
"ASzot/vq-vae-audio" -> "pelillian/deep-learning-kd-diagnosis"
"SarthakYadav/leaf-pytorch" -> "denfed/leaf-audio-pytorch"
"deepgram/kur" -> "baidu-research/ba-dls-deepspeech"
"deepgram/kur" -> "NervanaSystems/deepspeech"
"deepgram/kur" -> "jakebian/quiver" ["e"=1]
"deepgram/kur" -> "openai/cleverhans" ["e"=1]
"deepgram/kur" -> "srvk/eesen-transcriber"
"deepgram/kur" -> "merantix/picasso" ["e"=1]
"cmusphinx/g2p-seq2seq" -> "Kyubyong/g2p"
"cmusphinx/g2p-seq2seq" -> "cmusphinx/cmudict"
"cmusphinx/g2p-seq2seq" -> "AdolfVonKleist/Phonetisaurus"
"cmusphinx/g2p-seq2seq" -> "sequitur-g2p/sequitur-g2p"
"cmusphinx/g2p-seq2seq" -> "srvk/eesen"
"cmusphinx/g2p-seq2seq" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"cmusphinx/g2p-seq2seq" -> "kakaobrain/g2pM"
"cmusphinx/g2p-seq2seq" -> "pykaldi/pykaldi"
"cmusphinx/g2p-seq2seq" -> "YoavRamon/awesome-kaldi"
"cmusphinx/g2p-seq2seq" -> "alumae/kaldi-gstreamer-server"
"cmusphinx/g2p-seq2seq" -> "aishell-foundation/DaCiDian"
"cmusphinx/g2p-seq2seq" -> "bootphon/phonemizer"
"cmusphinx/g2p-seq2seq" -> "speechio/chinese_text_normalization"
"cmusphinx/g2p-seq2seq" -> "pettarin/forced-alignment-tools"
"cmusphinx/g2p-seq2seq" -> "vesis84/kaldi-io-for-python"
"zhenghuatan/rVADfast" -> "zhenghuatan/rVAD"
"iamjanvijay/rnnt" -> "iamjanvijay/rnnt_decoder_cuda"
"uhh-lt/kaldi-tuda-de" -> "uhh-lt/kaldi-model-server"
"uhh-lt/kaldi-tuda-de" -> "ynop/deepspeech-german"
"RetroCirce/HTS-Audio-Transformer" -> "YuanGongND/ssast"
"RetroCirce/HTS-Audio-Transformer" -> "kkoutini/PaSST"
"RetroCirce/HTS-Audio-Transformer" -> "RetroCirce/Zero_Shot_Audio_Source_Separation"
"RetroCirce/HTS-Audio-Transformer" -> "soham97/awesome-sound_event_detection"
"RetroCirce/HTS-Audio-Transformer" -> "fschmid56/EfficientAT"
"RetroCirce/HTS-Audio-Transformer" -> "YuanGongND/psla"
"RetroCirce/HTS-Audio-Transformer" -> "liuxubo717/SimPFs_Spectral"
"RetroCirce/HTS-Audio-Transformer" -> "YuanGongND/ast"
"RetroCirce/HTS-Audio-Transformer" -> "frednam93/FDY-SED" ["e"=1]
"pyannote/pyannote-metrics" -> "pyannote/pyannote-core"
"pyannote/pyannote-metrics" -> "nryant/dscore"
"pyannote/pyannote-metrics" -> "wq2012/SpectralCluster"
"pyannote/pyannote-metrics" -> "hitachi-speech/EEND"
"pyannote/pyannote-metrics" -> "pyannote/pyannote-database"
"pyannote/pyannote-metrics" -> "wq2012/SimpleDER"
"pyannote/pyannote-metrics" -> "yinruiqing/change_detection"
"pyannote/pyannote-metrics" -> "joonson/voxconverse"
"pyannote/pyannote-metrics" -> "phonexiaresearch/VBx-training-recipe"
"pyannote/pyannote-metrics" -> "tango4j/Auto-Tuning-Spectral-Clustering"
"pyannote/pyannote-metrics" -> "Jamiroquai88/VBDiarization"
"pyannote/pyannote-metrics" -> "HuangZiliAndy/RPNSD"
"pyannote/pyannote-metrics" -> "BUTSpeechFIT/VBx"
"marcoppasini/MelGAN-VC" -> "jjery2243542/adaptive_voice_conversion"
"marcoppasini/MelGAN-VC" -> "mazzzystar/randomCNN-voice-transfer"
"marcoppasini/MelGAN-VC" -> "SamuelBroughton/StarGAN-Voice-Conversion-2"
"marcoppasini/MelGAN-VC" -> "jxzhanggg/nonparaSeq2seqVC_code"
"marcoppasini/MelGAN-VC" -> "KunZhou9646/emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0"
"marcoppasini/MelGAN-VC" -> "liusongxiang/StarGAN-Voice-Conversion"
"marcoppasini/MelGAN-VC" -> "JeremyCCHsu/vae-npvc"
"marcoppasini/MelGAN-VC" -> "ebadawy/voice_conversion"
"marcoppasini/MelGAN-VC" -> "bigpon/vcc20_baseline_cyclevae"
"marcoppasini/MelGAN-VC" -> "jackaduma/CycleGAN-VC2"
"marcoppasini/MelGAN-VC" -> "himajin2045/voice-conversion"
"marcoppasini/MelGAN-VC" -> "leimao/Voice-Converter-CycleGAN"
"marcoppasini/MelGAN-VC" -> "k2kobayashi/crank"
"NVIDIA/radtts" -> "NVIDIA/BigVGAN"
"NVIDIA/radtts" -> "KevinMIN95/StyleSpeech"
"NVIDIA/radtts" -> "mindslab-ai/phaseaug"
"NVIDIA/radtts" -> "choiHkk/CVAEJETS"
"NVIDIA/radtts" -> "keonlee9420/Comprehensive-Transformer-TTS"
"NVIDIA/radtts" -> "ncsoft/avocodo"
"NVIDIA/radtts" -> "keonlee9420/DailyTalk"
"NVIDIA/radtts" -> "brentspell/hifi-gan-bwe"
"NVIDIA/radtts" -> "keonlee9420/Comprehensive-E2E-TTS"
"NVIDIA/radtts" -> "Rongjiehuang/GenerSpeech"
"NVIDIA/radtts" -> "MasayaKawamura/MB-iSTFT-VITS"
"TensorSpeech/TensorflowTTS" -> "thuhcsi/Crystal"
"TensorSpeech/TensorflowTTS" -> "xcmyz/speech-synthesis-paper"
"TensorSpeech/TensorflowTTS" -> "ivanvovk/DurIAN"
"TensorSpeech/TensorflowTTS" -> "janvainer/speedyspeech"
"TensorSpeech/TensorflowTTS" -> "seungwonpark/melgan"
"TensorSpeech/TensorflowTTS" -> "mindslab-ai/cotatron"
"v0lta/Listen-attend-and-spell" -> "vrenkens/nabu"
"festvox/festvox" -> "festvox/speech_tools"
"Aftaab99/OfflineSignatureVerification" -> "sounakdey/SigNet"
"Aftaab99/OfflineSignatureVerification" -> "luizgh/sigver"
"Aftaab99/OfflineSignatureVerification" -> "vjayd/Signature-verification-using-Siamese-CNN"
"Aftaab99/OfflineSignatureVerification" -> "luizgh/sigver_wiwd"
"Aftaab99/OfflineSignatureVerification" -> "netrapathak/Offline-Signature-Verification"
"eesungkim/Voice_Activity_Detector" -> "mounalab/LSTM-RNN-VAD"
"eesungkim/Voice_Activity_Detector" -> "jtkim-kaist/VAD"
"eesungkim/Voice_Activity_Detector" -> "nicklashansen/voice-activity-detection"
"eesungkim/Voice_Activity_Detector" -> "iariav/End-to-End-VAD"
"githubharald/DeslantImg" -> "arthurflor23/text-segmentation"
"janvainer/speedyspeech" -> "tianrengao/SqueezeWave"
"janvainer/speedyspeech" -> "LEEYOONHYUNG/BVAE-TTS"
"janvainer/speedyspeech" -> "lmnt-com/wavegrad"
"janvainer/speedyspeech" -> "ivanvovk/WaveGrad"
"janvainer/speedyspeech" -> "mindslab-ai/cotatron"
"janvainer/speedyspeech" -> "keonlee9420/Comprehensive-Transformer-TTS"
"janvainer/speedyspeech" -> "erogol/TTS-papers"
"janvainer/speedyspeech" -> "tts-tutorial/survey"
"janvainer/speedyspeech" -> "jaywalnut310/glow-tts"
"janvainer/speedyspeech" -> "keonlee9420/StyleSpeech"
"janvainer/speedyspeech" -> "rishikksh20/VocGAN"
"janvainer/speedyspeech" -> "Deepest-Project/AlignTTS"
"janvainer/speedyspeech" -> "dipjyoti92/SC-WaveRNN"
"tarekeldeeb/DeepSpeech-Quran" -> "Tarteel-io/tarteel-ml"
"huseinzol05/malaya-speech" -> "huseinzol05/malaya"
"huseinzol05/malaya-speech" -> "keonlee9420/StyleSpeech"
"huseinzol05/malaya-speech" -> "hhguo/MSMC-TTS"
"huseinzol05/malaya-speech" -> "thuhcsi/VAENAR-TTS"
"TaiChunYen/Pytorch-CycleGAN-VC2" -> "onejiin/CycleGAN-VC2"
"TaiChunYen/Pytorch-CycleGAN-VC2" -> "SamuelBroughton/StarGAN-Voice-Conversion-2"
"TaiChunYen/Pytorch-CycleGAN-VC2" -> "ariacat3366/pytorch-StarGAN-VC2-implementation"
"numediart/EmoV-DB" -> "glam-imperial/EmotionalConversionStarGAN"
"wnhsu/FactorizedHierarchicalVAE" -> "wnhsu/ScalableFHVAE"
"wnhsu/FactorizedHierarchicalVAE" -> "wnhsu/SpeechVAE"
"wnhsu/FactorizedHierarchicalVAE" -> "jjery2243542/voice_conversion"
"yjlolo/vae-audio" -> "yjlolo/gmvae-synth"
"facebookresearch/AudioMAE" -> "YuanGongND/ssast"
"facebookresearch/AudioMAE" -> "microsoft/CLAP"
"facebookresearch/AudioMAE" -> "LAION-AI/audio-dataset"
"facebookresearch/AudioMAE" -> "XinhaoMei/WavCaps"
"facebookresearch/AudioMAE" -> "LAION-AI/CLAP"
"facebookresearch/AudioMAE" -> "asteroid-team/torch-audiomentations"
"facebookresearch/AudioMAE" -> "kkoutini/PaSST"
"facebookresearch/AudioMAE" -> "mindslab-ai/phaseaug"
"facebookresearch/AudioMAE" -> "AlanBaade/MAE-AST-Public"
"facebookresearch/AudioMAE" -> "nttcslab/msm-mae"
"facebookresearch/AudioMAE" -> "NVIDIA/BigVGAN"
"facebookresearch/AudioMAE" -> "liusongxiang/Large-Audio-Models"
"facebookresearch/AudioMAE" -> "YuanGongND/ast"
"facebookresearch/AudioMAE" -> "lucidrains/audiolm-pytorch"
"facebookresearch/AudioMAE" -> "gudgud96/frechet-audio-distance"
"thomasschmied/Speech_Recognition_with_Tensorflow" -> "WindQAQ/listen-attend-and-spell"
"thomasschmied/Speech_Recognition_with_Tensorflow" -> "hirofumi0810/tensorflow_end2end_speech_recognition"
"thomasschmied/Speech_Recognition_with_Tensorflow" -> "HawkAaron/RNN-Transducer"
"thomasschmied/Speech_Recognition_with_Tensorflow" -> "vrenkens/nabu"
"thomasschmied/Speech_Recognition_with_Tensorflow" -> "kaituoxu/Listen-Attend-Spell"
"thomasschmied/Speech_Recognition_with_Tensorflow" -> "shiyuzh2007/ASR"
"thomasschmied/Speech_Recognition_with_Tensorflow" -> "srvk/eesen"
"thomasschmied/Speech_Recognition_with_Tensorflow" -> "rwth-i6/returnn"
"thomasschmied/Speech_Recognition_with_Tensorflow" -> "rwth-i6/returnn-experiments"
"thomasschmied/Speech_Recognition_with_Tensorflow" -> "awni/speech"
"thomasschmied/Speech_Recognition_with_Tensorflow" -> "Kyubyong/css10"
"dhchoi99/NANSY" -> "revsic/torch-nansypp"
"dhchoi99/NANSY" -> "ncsoft/avocodo"
"junjun3518/alias-free-torch" -> "mindslab-ai/wavegrad2"
"junjun3518/alias-free-torch" -> "mindslab-ai/phaseaug"
"mindslab-ai/pnlp-mixer" -> "junjun3518/alias-free-torch"
"mindslab-ai/univnet" -> "mindslab-ai/phaseaug"
"mindslab-ai/univnet" -> "thuhcsi/VAENAR-TTS"
"mindslab-ai/univnet" -> "mindslab-ai/assem-vc"
"mindslab-ai/univnet" -> "junjun3518/alias-free-torch"
"mindslab-ai/univnet" -> "mindslab-ai/wavegrad2"
"mindslab-ai/univnet" -> "keonlee9420/VAENAR-TTS"
"mindslab-ai/univnet" -> "keonlee9420/Comprehensive-Transformer-TTS"
"mindslab-ai/univnet" -> "keonlee9420/StyleSpeech"
"mindslab-ai/univnet" -> "keonlee9420/Parallel-Tacotron2"
"mindslab-ai/univnet" -> "LEEYOONHYUNG/BVAE-TTS"
"wqt2019/tacotron-2_wavernn" -> "wqt2019/tacotron-2_melgan"
"mindslab-ai/phaseaug" -> "ncsoft/avocodo"
"mindslab-ai/phaseaug" -> "junjun3518/alias-free-torch"
"mindslab-ai/phaseaug" -> "liusongxiang/Large-Audio-Models"
"alphacep/kaldi-android-demo" -> "alphacep/kaldi-android"
"alphacep/kaldi-android-demo" -> "XiaoMi/kaldi-onnx"
"alphacep/kaldi-android-demo" -> "xiangxyq/minimize-chain-decoder"
"alphacep/kaldi-android-demo" -> "alphacep/kaldi-websocket-python"
"alphacep/kaldi-android-demo" -> "jpuigcerver/kaldi-decoders"
"alphacep/kaldi-android-demo" -> "dialogflow/asr-server"
"alphacep/kaldi-android-demo" -> "jcsilva/docker-kaldi-android"
"UFAL-DSG/pykaldi" -> "UFAL-DSG/alex-asr"
"jcsilva/docker-kaldi-android" -> "alphacep/kaldi-android"
"sudosilico/sample-diffusion" -> "diontimmer/sample-diffusion-gui"
"sudosilico/sample-diffusion" -> "sudosilico/sample-diffusion-app"
"dmort27/panphon" -> "dmort27/epitran"
"dmort27/panphon" -> "pettarin/ipapy"
"dmort27/panphon" -> "phoible/dev"
"dmort27/panphon" -> "xinjli/transphone"
"dmort27/panphon" -> "uiuc-sst/g2ps"
"bshall/UniversalVocoding" -> "geneing/WaveRNN-Pytorch"
"bshall/UniversalVocoding" -> "bshall/Tacotron"
"bshall/UniversalVocoding" -> "tianrengao/SqueezeWave"
"bshall/UniversalVocoding" -> "yistLin/universal-vocoder"
"bshall/UniversalVocoding" -> "ksw0306/WaveVAE"
"bshall/UniversalVocoding" -> "ivanvovk/WaveGrad"
"bshall/UniversalVocoding" -> "G-Wang/WaveRNN-Pytorch"
"bshall/UniversalVocoding" -> "ksw0306/ClariNet"
"bshall/UniversalVocoding" -> "bshall/ZeroSpeech"
"bshall/UniversalVocoding" -> "ivanvovk/DurIAN"
"bshall/UniversalVocoding" -> "mkotha/WaveRNN"
"bshall/UniversalVocoding" -> "LeoniusChen/Attentions-in-Tacotron"
"SeanNaren/deepspeech.torch" -> "NervanaSystems/deepspeech"
"SeanNaren/deepspeech.torch" -> "baidu-research/ba-dls-deepspeech"
"oliverguhr/wav2vec2-live" -> "Edresson/Wav2Vec-Wrapper"
"oliverguhr/wav2vec2-live" -> "patrickvonplaten/Wav2Vec2_PyCTCDecode"
"oliverguhr/wav2vec2-live" -> "jonatasgrosman/huggingsound"
"oliverguhr/wav2vec2-live" -> "jonatasgrosman/wav2vec2-sprint"
"oliverguhr/wav2vec2-live" -> "chuachinhon/wav2vec2_transformers"
"oliverguhr/wav2vec2-live" -> "kensho-technologies/pyctcdecode"
"csukuangfj/kaldifeat" -> "csukuangfj/transducer-loss-benchmarking"
"csukuangfj/kaldifeat" -> "danpovey/fast_rnnt"
"csukuangfj/kaldifeat" -> "SpeechColab/Leaderboard"
"csukuangfj/kaldifeat" -> "k2-fsa/snowfall"
"csukuangfj/kaldifeat" -> "csukuangfj/kaldilm"
"csukuangfj/kaldifeat" -> "k2-fsa/icefall"
"SuperKogito/Voice-based-gender-recognition" -> "x4nth055/gender-recognition-by-voice"
"SuperKogito/Voice-based-gender-recognition" -> "SuperKogito/Voice-based-speaker-identification"
"v-iashin/SpecVQGAN" -> "PeihaoChen/regnet"
"v-iashin/SpecVQGAN" -> "yangdongchao/Text-to-sound-Synthesis"
"v-iashin/SpecVQGAN" -> "microsoft/CLAP"
"jonrein/tensorflow_CTC_example" -> "igormq/ctc_tensorflow_example"
"jonrein/tensorflow_CTC_example" -> "amaas/stanford-ctc"
"jonrein/tensorflow_CTC_example" -> "aaron-xichen/cnn-lstm-ctc"
"jonrein/tensorflow_CTC_example" -> "vrenkens/tfkaldi"
"voixen/voixen-vad" -> "Snirpo/node-vad"
"voixen/voixen-vad" -> "Jam3/voice-activity-detection"
"adi-panda/Kuebiko" -> "Koischizo/AI-Vtuber"
"adi-panda/Kuebiko" -> "ardha27/AI-Waifu-Vtuber"
"adi-panda/Kuebiko" -> "ponlponl123/-Prototype-AIVTuber"
"thuhcsi/VAENAR-TTS" -> "LEEYOONHYUNG/BVAE-TTS"
"thuhcsi/VAENAR-TTS" -> "keonlee9420/VAENAR-TTS"
"thuhcsi/VAENAR-TTS" -> "LeoniusChen/Attentions-in-Tacotron"
"thuhcsi/VAENAR-TTS" -> "keonlee9420/StyleSpeech"
"thuhcsi/VAENAR-TTS" -> "hhguo/MSMC-TTS"
"deepsound-project/samplernn-pytorch" -> "soroushmehr/sampleRNN_ICLR2017"
"deepsound-project/samplernn-pytorch" -> "richardassar/SampleRNN_torch"
"deepsound-project/samplernn-pytorch" -> "deepsound-project/pggan-pytorch"
"deepsound-project/samplernn-pytorch" -> "Unisound/SampleRNN"
"deepsound-project/samplernn-pytorch" -> "gcunhase/samplernn-pytorch"
"deepsound-project/samplernn-pytorch" -> "rncm-prism/prism-samplernn"
"deepsound-project/samplernn-pytorch" -> "Cortexelus/dadabots_sampleRNN"
"luizgh/sigver" -> "luizgh/sigver_wiwd"
"luizgh/sigver" -> "luizgh/adversarial_signatures"
"luizgh/sigver" -> "jadevaibhav/Signature-verification-using-deep-learning"
"luizgh/sigver" -> "Aftaab99/OfflineSignatureVerification"
"luizgh/sigver" -> "EB324/signature_verification"
"pzelasko/lhotse" -> "danpovey/k2"
"soroushmehr/sampleRNN_ICLR2017" -> "sotelo/parrot"
"soroushmehr/sampleRNN_ICLR2017" -> "richardassar/SampleRNN_torch"
"soroushmehr/sampleRNN_ICLR2017" -> "deepsound-project/samplernn-pytorch"
"soroushmehr/sampleRNN_ICLR2017" -> "Unisound/SampleRNN"
"soroushmehr/sampleRNN_ICLR2017" -> "tomlepaine/fast-wavenet"
"soroushmehr/sampleRNN_ICLR2017" -> "CSTR-Edinburgh/merlin"
"soroushmehr/sampleRNN_ICLR2017" -> "Zeta36/tensorflow-tex-wavenet"
"soroushmehr/sampleRNN_ICLR2017" -> "barronalex/Tacotron"
"soroushmehr/sampleRNN_ICLR2017" -> "ibab/tensorflow-wavenet"
"soroushmehr/sampleRNN_ICLR2017" -> "facebookresearch/loop"
"soroushmehr/sampleRNN_ICLR2017" -> "basveeling/wavenet"
"soroushmehr/sampleRNN_ICLR2017" -> "ZVK/sampleRNN_ICLR2017"
"soroushmehr/sampleRNN_ICLR2017" -> "kundan2510/pixelCNN" ["e"=1]
"soroushmehr/sampleRNN_ICLR2017" -> "Kyubyong/tacotron"
"soroushmehr/sampleRNN_ICLR2017" -> "israelg99/deepvoice"
"JingShing/Encryptor-Decryptor" -> "JingShing/Sorry-NovelAI"
"huseinzol05/malaya" -> "huseinzol05/malay-dataset"
"huseinzol05/malaya" -> "huseinzol05/malaya-speech"
"asuni/wavelet_prosody_toolkit" -> "Helsinki-NLP/prosody"
"githubharald/WordDetectorNN" -> "githubharald/WordDetector"
"callee2006/2019-Winter-HGU-Machine-Learing-Camp" -> "callee2006/MachineLearning"
"callee2006/2019-Winter-HGU-Machine-Learing-Camp" -> "WICWIU/WICWIU"
"callee2006/2019-Winter-HGU-Machine-Learing-Camp" -> "hongshin/OperatingSystem"
"richardassar/SampleRNN_torch" -> "soroushmehr/sampleRNN_ICLR2017"
"richardassar/SampleRNN_torch" -> "Unisound/SampleRNN"
"richardassar/SampleRNN_torch" -> "deepsound-project/samplernn-pytorch"
"mozilla/DSAlign" -> "pettarin/forced-alignment-tools"
"mozilla/DSAlign" -> "lumaku/ctc-segmentation"
"mozilla/DSAlign" -> "prosodylab/Prosodylab-Aligner"
"mozilla/DSAlign" -> "mlcommons/peoples-speech"
"Anwarvic/Deep-Learning-Nanodegree-Udacity-2019" -> "Anwarvic/Stanford_CS224n--NLP-with-Deep-Learning"
"Anwarvic/Deep-Learning-Nanodegree-Udacity-2019" -> "Anwarvic/Deep-Learning-Specialization-2017--Coursera"
"soham97/awesome-sound_event_detection" -> "RetroCirce/HTS-Audio-Transformer"
"soham97/awesome-sound_event_detection" -> "sithu31296/audio-tagging"
"soham97/awesome-sound_event_detection" -> "fgnt/pb_sed" ["e"=1]
"naotokui/RhythmVAE_M4L" -> "naotokui/M4L-CreativeGAN-Rhythm"
"naotokui/RhythmVAE_M4L" -> "naotokui/VAE_Rhythm_Generator"
"naotokui/RhythmVAE_M4L" -> "QosmoInc/neutone_sdk"
"naotokui/RhythmVAE_M4L" -> "naotokui/CreativeGAN-Rhythm"
"naotokui/RhythmVAE_M4L" -> "maxfrenzel/SpectrogramVAE"
"naotokui/RhythmVAE_M4L" -> "v7b1/sigmund_64bit-version" ["e"=1]
"pkmital/time-domain-neural-audio-style-transfer" -> "anonymousiclr2018/Style-Transfer-for-Musical-Audio"
"ccoreilly/LocalSTT" -> "alphacep/vosk-android-service"
"ccoreilly/LocalSTT" -> "Kaljurand/K6nele-service"
"Moon0316/T2A" -> "ddlBoJack/MT4SSL"
"facebookresearch/speech-resynthesis" -> "Wendison/VQMIVC"
"facebookresearch/speech-resynthesis" -> "bshall/VectorQuantizedCPC"
"facebookresearch/speech-resynthesis" -> "huawei-noah/Speech-Backbones"
"facebookresearch/speech-resynthesis" -> "thuhcsi/VAENAR-TTS"
"facebookresearch/speech-resynthesis" -> "keonlee9420/StyleSpeech"
"facebookresearch/speech-resynthesis" -> "bshall/ZeroSpeech"
"facebookresearch/speech-resynthesis" -> "k2kobayashi/crank"
"facebookresearch/speech-resynthesis" -> "liusongxiang/ppg-vc"
"facebookresearch/speech-resynthesis" -> "hhguo/MSMC-TTS"
"facebookresearch/speech-resynthesis" -> "keonlee9420/DailyTalk"
"facebookresearch/speech-resynthesis" -> "keonlee9420/STYLER"
"facebookresearch/speech-resynthesis" -> "LEEYOONHYUNG/BVAE-TTS"
"facebookresearch/speech-resynthesis" -> "keonlee9420/Comprehensive-E2E-TTS"
"facebookresearch/speech-resynthesis" -> "howard1337/S2VC"
"srvk/lm_build" -> "gooofy/kaldi-adapt-lm"
"zhenghuatan/rVAD" -> "zhenghuatan/rVADfast"
"zhangyongmao/VISinger2" -> "WelkinYang/Learn2Sing2.0"
"zhangyongmao/VISinger2" -> "PlayVoice/VI-SVS"
"zhangyongmao/VISinger2" -> "So-Fann/VISinger"
"luizgh/sigver_wiwd" -> "luizgh/sigver"
"luizgh/sigver_wiwd" -> "sounakdey/SigNet"
"luizgh/sigver_wiwd" -> "luizgh/adversarial_signatures"
"luizgh/sigver_wiwd" -> "jadevaibhav/Signature-verification-using-deep-learning"
"luizgh/sigver_wiwd" -> "Aftaab99/OfflineSignatureVerification"
"luizgh/sigver_wiwd" -> "jeongmincha/Online-Signature-Verification"
"luizgh/sigver_wiwd" -> "netrapathak/Offline-Signature-Verification"
"luizgh/sigver_wiwd" -> "guilhermefloriani/signature-recognition"
"luizgh/sigver_wiwd" -> "gnbaron/signature-recognition"
"Kyubyong/cross_vc" -> "ryokamoi/ppg_vc"
"Kyubyong/cross_vc" -> "JeremyCCHsu/vae-npvc"
"sounakdey/SigNet" -> "netrapathak/Offline-Signature-Verification"
"sounakdey/SigNet" -> "luizgh/sigver_wiwd"
"sounakdey/SigNet" -> "sounakdey/Lenet_nicicon"
"sounakdey/SigNet" -> "Aftaab99/OfflineSignatureVerification"
"tianrengao/SqueezeWave" -> "bshall/UniversalVocoding"
"tianrengao/SqueezeWave" -> "jxzhanggg/nonparaSeq2seqVC_code"
"tianrengao/SqueezeWave" -> "janvainer/speedyspeech"
"tianrengao/SqueezeWave" -> "seungwonpark/melgan"
"tianrengao/SqueezeWave" -> "jaywalnut310/glow-tts"
"tianrengao/SqueezeWave" -> "nii-yamagishilab/multi-speaker-tacotron"
"tianrengao/SqueezeWave" -> "mozilla/LPCNet"
"tianrengao/SqueezeWave" -> "facebookresearch/vocoder-benchmark"
"tianrengao/SqueezeWave" -> "geneing/WaveRNN-Pytorch"
"tianrengao/SqueezeWave" -> "LEEYOONHYUNG/BVAE-TTS"
"tianrengao/SqueezeWave" -> "Yablon/auorange"
"tianrengao/SqueezeWave" -> "ivanvovk/WaveGrad"
"tianrengao/SqueezeWave" -> "L0SG/WaveFlow"
"tianrengao/SqueezeWave" -> "yanggeng1995/FB-MelGAN"
"tianrengao/SqueezeWave" -> "NVIDIA/mellotron"
"Torsion-Audio/Scyclone" -> "rodrigodzf/NeuralResonatorVST"
"Torsion-Audio/Scyclone" -> "WuYiming6526/HARD"
"arthurflor23/text-segmentation" -> "Samir55/Image2Lines"
"iamjanvijay/rnnt_decoder_cuda" -> "iamjanvijay/rnnt"
"iamjanvijay/rnnt_decoder_cuda" -> "csukuangfj/optimized_transducer"
"nii-yamagishilab/project-NN-Pytorch-scripts" -> "asvspoof-challenge/2021" ["e"=1]
"nii-yamagishilab/project-NN-Pytorch-scripts" -> "yzyouzhang/AIR-ASVspoof" ["e"=1]
"nii-yamagishilab/project-NN-Pytorch-scripts" -> "clovaai/aasist" ["e"=1]
"nii-yamagishilab/project-NN-Pytorch-scripts" -> "sasv-challenge/SASVC2022_Baseline" ["e"=1]
"nii-yamagishilab/project-NN-Pytorch-scripts" -> "LEEYOONHYUNG/BVAE-TTS"
"nii-yamagishilab/project-NN-Pytorch-scripts" -> "tts-tutorial/survey"
"nii-yamagishilab/project-NN-Pytorch-scripts" -> "thuhcsi/VAENAR-TTS"
"nii-yamagishilab/project-NN-Pytorch-scripts" -> "jxzhanggg/nonparaSeq2seqVC_code"
"nii-yamagishilab/project-NN-Pytorch-scripts" -> "hhguo/MSMC-TTS"
"nii-yamagishilab/project-NN-Pytorch-scripts" -> "liusongxiang/ppg-vc"
"dunky11/voicesmith" -> "yl4579/StyleTTS"
"dunky11/voicesmith" -> "lucidrains/natural-speech-pytorch"
"dunky11/voicesmith" -> "NVIDIA/radtts"
"dunky11/voicesmith" -> "Rongjiehuang/GenerSpeech"
"dunky11/voicesmith" -> "polvanrijn/VoiceMe"
"dunky11/voicesmith" -> "rendchevi/nix-tts"
"dunky11/voicesmith" -> "keonlee9420/DailyTalk"
"dunky11/voicesmith" -> "CMsmartvoice/One-Shot-Voice-Cloning"
"dunky11/voicesmith" -> "keonlee9420/Comprehensive-Transformer-TTS"
"adobe-research/DeepAFx-ST" -> "YatingMusic/ddsp-singing-vocoders"
"adobe-research/DeepAFx-ST" -> "csteinmetz1/micro-tcn"
"adobe-research/DeepAFx-ST" -> "csteinmetz1/pymixconsole" ["e"=1]
"adobe-research/DeepAFx-ST" -> "csteinmetz1/steerable-nafx"
"adobe-research/DeepAFx-ST" -> "jatinchowdhury18/RTNeural" ["e"=1]
"adobe-research/DeepAFx-ST" -> "acids-ircam/ddsp_pytorch"
"qiuqiangkong/panns_inference" -> "YuanGongND/psla"
"qiuqiangkong/panns_inference" -> "qiuqiangkong/audioset_tagging_cnn"
"MTG/WGANSing" -> "seaniezhao/torch_npss"
"MTG/WGANSing" -> "mathigatti/midi2voice"
"MTG/WGANSing" -> "SoonbeomChoi/BEGANSing"
"MTG/WGANSing" -> "r9y9/nnsvs" ["e"=1]
"MTG/WGANSing" -> "SJTMusicTeam/SVS_system"
"MTG/WGANSing" -> "zizyzhang/DNN-Based-Singing-Voice-Synthesis"
"MTG/WGANSing" -> "yanggeng1995/GAN-TTS"
"MTG/WGANSing" -> "NVIDIA/mellotron"
"MTG/WGANSing" -> "ivanvovk/DurIAN"
"zyzisyz/mfa_conformer" -> "wenet-e2e/wespeaker"
"zyzisyz/mfa_conformer" -> "clovaai/aasist" ["e"=1]
"zyzisyz/mfa_conformer" -> "wngh1187/RawNeXt"
"yaoyao-liu/minimal-light" -> "yaoyao-liu/yaoyao-liu.github.io"
"FeiGeChuanShu/GFPGAN-ncnn" -> "FeiGeChuanShu/ncnn_Android_PP-TinyPose" ["e"=1]
"ALIZE-Speaker-Recognition/LIA_RAL" -> "ALIZE-Speaker-Recognition/alize-core"
"ALIZE-Speaker-Recognition/LIA_RAL" -> "ALIZE-Speaker-Recognition/android-alize"
"ALIZE-Speaker-Recognition/LIA_RAL" -> "ibillxia/VoicePrintReco"
"KunZhou9646/emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0" -> "KunZhou9646/Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT"
"KunZhou9646/emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0" -> "KunZhou9646/controllable_evc_code"
"KunZhou9646/emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0" -> "CZ26/CycleTransGAN-EVC"
"KunZhou9646/emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0" -> "KunZhou9646/seq2seq-EVC"
"cmusphinx/sphinxtrain" -> "cmusphinx/sphinxbase"
"ebadawy/voice_conversion" -> "RussellSB/tt-vae-gan"
"mohammadpz/CTC-Connectionist-Temporal-Classification" -> "sherjilozair/ctc"
"mohammadpz/CTC-Connectionist-Temporal-Classification" -> "skaae/Lasagne-CTC"
"mohammadpz/CTC-Connectionist-Temporal-Classification" -> "amaas/stanford-ctc"
"yajiemiao/eesen" -> "lingochamp/kaldi-ctc"
"yajiemiao/eesen" -> "srvk/eesen"
"yajiemiao/eesen" -> "vrenkens/tfkaldi"
"yajiemiao/eesen" -> "yajiemiao/kaldipdnn"
"yajiemiao/eesen" -> "rizar/attention-lvcsr"
"yajiemiao/eesen" -> "YiwenShaoStephen/pychain"
"yajiemiao/eesen" -> "awni/transducer"
"yajiemiao/eesen" -> "vesis84/kaldi-io-for-python"
"yajiemiao/eesen" -> "yajiemiao/pdnn"
"yajiemiao/eesen" -> "HawkAaron/warp-transducer"
"yajiemiao/eesen" -> "amaas/stanford-ctc"
"yajiemiao/eesen" -> "xingchensong/speech-recognition-papers"
"yajiemiao/eesen" -> "thu-spmi/CAT"
"yajiemiao/eesen" -> "mindorii/kws" ["e"=1]
"yajiemiao/eesen" -> "jzlianglu/pykaldi2"
"primaryobjects/voice-gender" -> "SuperKogito/Voice-based-gender-recognition"
"prosodylab/Prosodylab-Aligner" -> "pettarin/forced-alignment-tools"
"prosodylab/Prosodylab-Aligner" -> "JoFrhwld/FAVE"
"prosodylab/Prosodylab-Aligner" -> "mozilla/DSAlign"
"prosodylab/Prosodylab-Aligner" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"prosodylab/Prosodylab-Aligner" -> "lowerquality/gentle"
"prosodylab/Prosodylab-Aligner" -> "jaekookang/p2fa_py3"
"prosodylab/Prosodylab-Aligner" -> "dopefishh/praatalign"
"prosodylab/Prosodylab-Aligner" -> "nassosoassos/sail_align"
"prosodylab/Prosodylab-Aligner" -> "open-speech/speech-aligner"
"prosodylab/Prosodylab-Aligner" -> "tbright17/kaldi-dnn-ali-gop"
"prosodylab/Prosodylab-Aligner" -> "YannickJadoul/Parselmouth"
"prosodylab/Prosodylab-Aligner" -> "r9y9/pysptk"
"prosodylab/Prosodylab-Aligner" -> "prosodylab/prosodylab.dictionaries"
"prosodylab/Prosodylab-Aligner" -> "ksw0306/ClariNet"
"CUNY-CL/wikipron" -> "as-ideas/DeepPhonemizer"
"WindQAQ/listen-attend-and-spell" -> "thomasschmied/Speech_Recognition_with_Tensorflow"
"WindQAQ/listen-attend-and-spell" -> "sciforce/phones-las"
"BogiHsu/Tacotron2-PyTorch" -> "ttaoREtw/Tacotron-pytorch"
"BogiHsu/Tacotron2-PyTorch" -> "cyhuang-tw/AdaIN-VC"
"BogiHsu/Tacotron2-PyTorch" -> "BogiHsu/WG-WaveNet"
"BogiHsu/Tacotron2-PyTorch" -> "keonlee9420/Comprehensive-Transformer-TTS"
"BogiHsu/Tacotron2-PyTorch" -> "andi611/ZeroSpeech-TTS-without-T"
"BogiHsu/Tacotron2-PyTorch" -> "bfs18/tacotron2"
"JeremyCCHsu/vqvae-speech" -> "Kyubyong/vq-vae"
"JeremyCCHsu/vqvae-speech" -> "swasun/VQ-VAE-Speech"
"JeremyCCHsu/vqvae-speech" -> "ASzot/vq-vae-audio"
"JeremyCCHsu/vqvae-speech" -> "JeremyCCHsu/vae-npvc"
"xcmyz/FastVocoder" -> "LEEYOONHYUNG/BVAE-TTS"
"xcmyz/FastVocoder" -> "xcmyz/ConvTasNet4BasisMelGAN"
"xcmyz/FastVocoder" -> "thuhcsi/VAENAR-TTS"
"ttsunion/Deep-Expression" -> "syang1993/FFTNet"
"philipperemy/tensorflow-ctc-speech-recognition" -> "igormq/ctc_tensorflow_example"
"philipperemy/tensorflow-ctc-speech-recognition" -> "hirofumi0810/tensorflow_end2end_speech_recognition"
"M4Singer/M4Singer" -> "SJTMusicTeam/Muskits"
"M4Singer/M4Singer" -> "WelkinYang/Learn2Sing2.0"
"SungFeng-Huang/Meta-TTS" -> "keonlee9420/StyleSpeech"
"SungFeng-Huang/Meta-TTS" -> "KevinMIN95/StyleSpeech"
"haoheliu/Subband-Music-Separation" -> "ws-choi/ISMIR2020_U_Nets_SVS"
"hhguo/EA-SVC" -> "SongRongLee/mir-svc"
"hhguo/EA-SVC" -> "liusongxiang/ppg-vc"
"keonlee9420/VAENAR-TTS" -> "thuhcsi/VAENAR-TTS"
"ws-choi/ISMIR2020_U_Nets_SVS" -> "haoheliu/Subband-Music-Separation"
"Anwarvic/Deep-Learning-Specialization-2017--Coursera" -> "Anwarvic/Stanford_CS224n--NLP-with-Deep-Learning"
"Anwarvic/Deep-Learning-Specialization-2017--Coursera" -> "Anwarvic/MovieTweets--Search-Engine"
"mindslab-ai/wavegrad2" -> "junjun3518/alias-free-torch"
"mindslab-ai/wavegrad2" -> "mindslab-ai/nuwave"
"mindslab-ai/wavegrad2" -> "keonlee9420/WaveGrad2"
"mindslab-ai/wavegrad2" -> "mindslab-ai/phaseaug"
"mindslab-ai/wavegrad2" -> "mindslab-ai/univnet"
"acids-ircam/ddsp_pytorch" -> "ben-hayes/neural-waveshaping-synthesis"
"acids-ircam/ddsp_pytorch" -> "magenta/midi-ddsp"
"acids-ircam/ddsp_pytorch" -> "caillonantoine/RAVE"
"acids-ircam/ddsp_pytorch" -> "sweetcocoa/ddsp-pytorch"
"acids-ircam/ddsp_pytorch" -> "csteinmetz1/auraloss"
"acids-ircam/ddsp_pytorch" -> "torchsynth/torchsynth"
"acids-ircam/ddsp_pytorch" -> "magenta/ddsp" ["e"=1]
"acids-ircam/ddsp_pytorch" -> "maxrmorrison/torchcrepe"
"acids-ircam/ddsp_pytorch" -> "YatingMusic/ddsp-singing-vocoders"
"acids-ircam/ddsp_pytorch" -> "magenta/ddsp-vst"
"acids-ircam/ddsp_pytorch" -> "adobe-research/DeepAFx-ST"
"acids-ircam/ddsp_pytorch" -> "fcaspe/ddx7"
"acids-ircam/ddsp_pytorch" -> "mir-dataset-loaders/mirdata" ["e"=1]
"acids-ircam/ddsp_pytorch" -> "acids-ircam/nn_tilde"
"acids-ircam/ddsp_pytorch" -> "jongwook/onsets-and-frames" ["e"=1]
"coqui-ai/TTS-papers" -> "tts-tutorial/survey"
"coqui-ai/TTS-papers" -> "wenet-e2e/speech-synthesis-paper"
"coqui-ai/TTS-papers" -> "rishikksh20/FastSpeech2"
"coqui-ai/TTS-papers" -> "xcmyz/speech-synthesis-paper"
"coqui-ai/TTS-papers" -> "coqui-ai/open-speech-corpora"
"coqui-ai/TTS-papers" -> "keonlee9420/Comprehensive-Transformer-TTS"
"coqui-ai/TTS-papers" -> "keonlee9420/Expressive-FastSpeech2"
"coqui-ai/TTS-papers" -> "coqui-ai/TTS-recipes"
"coqui-ai/TTS-papers" -> "LeoniusChen/Attentions-in-Tacotron"
"coqui-ai/TTS-papers" -> "keonlee9420/DailyTalk"
"coqui-ai/TTS-papers" -> "sp-nitech/diffsptk"
"coqui-ai/TTS-papers" -> "rhasspy/gruut"
"coqui-ai/TTS-papers" -> "ming024/FastSpeech2"
"facebookresearch/vocoder-benchmark" -> "rishikksh20/iSTFTNet-pytorch"
"facebookresearch/vocoder-benchmark" -> "tencent-ailab/bddm"
"magenta/midi-ddsp" -> "acids-ircam/ddsp_pytorch"
"magenta/midi-ddsp" -> "YatingMusic/ddsp-singing-vocoders"
"magenta/midi-ddsp" -> "ben-hayes/neural-waveshaping-synthesis"
"magenta/midi-ddsp" -> "jeffreyjohnens/MetaMIDIDataset" ["e"=1]
"magenta/midi-ddsp" -> "magenta/ddsp-vst"
"magenta/midi-ddsp" -> "sweetcocoa/ddsp-pytorch"
"magenta/midi-ddsp" -> "torchsynth/torchsynth"
"magenta/midi-ddsp" -> "music-x-lab/POP909-Dataset" ["e"=1]
"magenta/midi-ddsp" -> "fcaspe/ddx7"
"magenta/midi-ddsp" -> "Spijkervet/torchaudio-augmentations" ["e"=1]
"magenta/midi-ddsp" -> "maxrmorrison/torchcrepe"
"magenta/midi-ddsp" -> "YatingMusic/compound-word-transformer" ["e"=1]
"mathigatti/midi2voice" -> "MTG/WGANSing"
"mathigatti/midi2voice" -> "seaniezhao/torch_npss"
"mathigatti/midi2voice" -> "gabolsgabs/DALI" ["e"=1]
"mathigatti/midi2voice" -> "vanstorm9/AI-Vocaloid-Kit-V2"
"tencent-ailab/bddm" -> "Rongjiehuang/FastDiff"
"tencent-ailab/bddm" -> "ncsoft/avocodo"
"tencent-ailab/bddm" -> "mindslab-ai/phaseaug"
"tencent-ailab/bddm" -> "facebookresearch/vocoder-benchmark"
"tencent-ailab/bddm" -> "SungFeng-Huang/Meta-TTS"
"MycroftAI/mimic1" -> "festvox/flite"
"MycroftAI/mimic1" -> "MycroftAI/mimic2"
"sigsep/sigsep-mus-db" -> "sigsep/sigsep-mus-eval"
"lazydevyo/SpleetGUI" -> "boy1dr/SpleeterGui"
"r4victor/syncabook" -> "r4victor/afaligner"
"r4victor/syncabook" -> "r4victor/synclibrivox"
"Kaljurand/K6nele" -> "jcsilva/docker-kaldi-gstreamer-server"
"Kaljurand/K6nele" -> "Kaljurand/dictate.js"
"Kaljurand/K6nele" -> "Kaljurand/K6nele-service"
"Kaljurand/K6nele" -> "ccoreilly/LocalSTT"
"Kaljurand/K6nele" -> "alumae/kaldi-gstreamer-server"
"Kaljurand/K6nele" -> "alumae/gst-kaldi-nnet2-online"
"Kaljurand/K6nele" -> "truongdo/kaldi-gstreamer-android-client"
"Kaljurand/K6nele" -> "Kaljurand/speechutils" ["e"=1]
"festvox/festival" -> "festvox/festvox"
"festvox/festival" -> "festvox/flite"
"festvox/festival" -> "festvox/speech_tools"
"festvox/festival" -> "Idlak/idlak"
"mostafaelaraby/wavegan-pytorch" -> "mazzzystar/WaveGAN-pytorch"
"denfed/leaf-audio-pytorch" -> "SarthakYadav/leaf-pytorch"
"denfed/leaf-audio-pytorch" -> "google-research/leaf-audio"
"tencent-ailab/3m-asr" -> "burchim/EfficientConformer"
"tencent-ailab/3m-asr" -> "jctian98/e2e_lfmmi"
"yl4579/AuxiliaryASR" -> "yl4579/PitchExtractor"
"Alexander-H-Liu/NPC" -> "iamyuanchung/VQ-APC"
"aimybox/aimybox-android-assistant" -> "aimybox/aimybox-android-sdk"
"liangstein/Chinese-speech-to-text" -> "CynthiaSuwi/ASR-for-Chinese-Pipeline"
"funcwj/ge2e-speaker-verification" -> "Suhee05/Text-Independent-Speaker-Verification"
"funcwj/ge2e-speaker-verification" -> "cvqluu/GE2E-Loss"
"funcwj/ge2e-speaker-verification" -> "Dannynis/xvector_pytorch"
"alumae/kaldi-offline-transcriber" -> "jcsilva/docker-kaldi-gstreamer-server"
"alumae/kaldi-offline-transcriber" -> "alumae/gst-kaldi-nnet2-online"
"alumae/kaldi-offline-transcriber" -> "alumae/kaldi-gstreamer-server"
"alumae/kaldi-offline-transcriber" -> "gooofy/py-kaldi-asr"
"alumae/kaldi-offline-transcriber" -> "dialogflow/asr-server"
"alumae/kaldi-offline-transcriber" -> "yajiemiao/kaldipdnn"
"alumae/kaldi-offline-transcriber" -> "srvk/eesen-transcriber"
"alumae/kaldi-offline-transcriber" -> "Kaljurand/dictate.js"
"alumae/kaldi-offline-transcriber" -> "alphacep/kaldi-websocket-python"
"speech-io/chinese_text_normalization" -> "open-speech/cn-text-normalizer"
"pfnet-research/meta-tasnet" -> "ws-choi/Conditioned-Source-Separation-LaSAFT"
"pfnet-research/meta-tasnet" -> "sigsep/norbert"
"KunZhou9646/Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT" -> "KunZhou9646/controllable_evc_code"
"KunZhou9646/Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT" -> "KunZhou9646/emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0"
"KunZhou9646/Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT" -> "KunZhou9646/seq2seq-EVC"
"KunZhou9646/Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT" -> "KunZhou9646/Emovox"
"mounalab/LSTM-RNN-VAD" -> "Cocoxili/VAD"
"alumae/gst-kaldi-nnet2-online" -> "jcsilva/docker-kaldi-gstreamer-server"
"alumae/gst-kaldi-nnet2-online" -> "alumae/kaldi-gstreamer-server"
"alumae/gst-kaldi-nnet2-online" -> "Kaljurand/dictate.js"
"alumae/gst-kaldi-nnet2-online" -> "gooofy/py-kaldi-asr"
"alumae/gst-kaldi-nnet2-online" -> "dialogflow/asr-server"
"alumae/gst-kaldi-nnet2-online" -> "alumae/kaldi-offline-transcriber"
"alumae/gst-kaldi-nnet2-online" -> "opendcd/opendcd"
"alumae/gst-kaldi-nnet2-online" -> "yajiemiao/kaldipdnn"
"alumae/gst-kaldi-nnet2-online" -> "UFAL-DSG/pykaldi"
"alumae/gst-kaldi-nnet2-online" -> "gooofy/zamia-speech"
"alumae/gst-kaldi-nnet2-online" -> "jpuigcerver/kaldi-decoders"
"alumae/gst-kaldi-nnet2-online" -> "Kaljurand/K6nele"
"alumae/gst-kaldi-nnet2-online" -> "XiaoMi/kaldi-onnx"
"alumae/gst-kaldi-nnet2-online" -> "YiwenShaoStephen/pychain"
"KunZhou9646/Emovox" -> "CZ26/CycleTransGAN-EVC"
"KunZhou9646/Emovox" -> "KunZhou9646/seq2seq-EVC"
"KunZhou9646/Mixed_Emotions" -> "KunZhou9646/Emovox"
"ubisoft/ubisoft-laforge-daft-exprt" -> "keonlee9420/Daft-Exprt"
"voicesauce/opensauce-python" -> "kirbyj/praatsauce"
"RicherMans/Datadriven-GPVAD" -> "RicherMans/GPV"
"Rongjiehuang/Multi-Singer" -> "SJTMusicTeam/Muskits"
"Rongjiehuang/Multi-Singer" -> "M4Singer/M4Singer"
"Rongjiehuang/Multi-Singer" -> "neosapience/mlp-singer"
"Deepest-Project/AlignTTS" -> "LEEYOONHYUNG/BVAE-TTS"
"hbuschme/TextGridTools" -> "kirbyj/praatsauce"
"keonlee9420/Parallel-Tacotron2" -> "keonlee9420/StyleSpeech"
"keonlee9420/Parallel-Tacotron2" -> "keonlee9420/Comprehensive-Transformer-TTS"
"keonlee9420/Parallel-Tacotron2" -> "ivanvovk/WaveGrad"
"keonlee9420/Parallel-Tacotron2" -> "mindslab-ai/univnet"
"keonlee9420/Parallel-Tacotron2" -> "rishikksh20/VocGAN"
"keonlee9420/Parallel-Tacotron2" -> "NVIDIA/BigVGAN"
"keonlee9420/Parallel-Tacotron2" -> "ivanvovk/DurIAN"
"keonlee9420/Parallel-Tacotron2" -> "xcmyz/speech-synthesis-paper"
"keonlee9420/Parallel-Tacotron2" -> "keonlee9420/DiffGAN-TTS"
"neonbjb/ocotillo" -> "neonbjb/DL-Art-School"
"cvqluu/simple_diarizer" -> "juanmc2005/rttm-viewer"
"yajiemiao/kaldipdnn" -> "yajiemiao/pdnn"
"yajiemiao/kaldipdnn" -> "yajiemiao/eesen"
"yajiemiao/kaldipdnn" -> "gooofy/py-kaldi-simple"
"Arkueid/Live2DMascot" -> "Paraworks/vits_with_chatgpt-gpt3"
"liuxubo717/LASS" -> "liuxubo717/sound_generation"
"liuxubo717/LASS" -> "liuxubo717/cl4ac"
"liuxubo717/LASS" -> "liuxubo717/SimPFs_Spectral"
"liuxubo717/LASS" -> "liuxubo717/V-ACT"
"liuxubo717/LASS" -> "liuxubo717/LASS-demopage"
"liuxubo717/SimPFs_Spectral" -> "liuxubo717/V-ACT"
"liuxubo717/SimPFs_Spectral" -> "liuxubo717/cl4ac"
"liuxubo717/SimPFs_Spectral" -> "liuxubo717/LASS-demopage"
"liuxubo717/SimPFs_Spectral" -> "liuxubo717/sound_generation"
"liuxubo717/SimPFs_Spectral" -> "liuxubo717/LASS"
"liuxubo717/cl4ac" -> "liuxubo717/SimPFs_Spectral"
"liuxubo717/cl4ac" -> "liuxubo717/V-ACT"
"liuxubo717/cl4ac" -> "liuxubo717/sound_generation"
"liuxubo717/cl4ac" -> "liuxubo717/LASS-demopage"
"liuxubo717/cl4ac" -> "liuxubo717/LASS"
"liuxubo717/sound_generation" -> "liuxubo717/cl4ac"
"liuxubo717/sound_generation" -> "liuxubo717/SimPFs_Spectral"
"liuxubo717/sound_generation" -> "liuxubo717/V-ACT"
"liuxubo717/sound_generation" -> "liuxubo717/LASS"
"liuxubo717/sound_generation" -> "liuxubo717/LASS-demopage"
"bfs18/tacotron2" -> "Yeongtae/tacotron2"
"yanggeng1995/FB-MelGAN" -> "r9y9/dnnsvs"
"yanggeng1995/FB-MelGAN" -> "yanggeng1995/EETS"
"Jackiexiao/zhtts" -> "thuhcsi/Crystal"
"Jackiexiao/zhtts" -> "lturing/tacotronv2_wavernn_chinese"
"Jackiexiao/zhtts" -> "LingLing-Speech/mandarin-tts-frontend-python"
"bigpon/vcc20_baseline_cyclevae" -> "k2kobayashi/crank"
"bigpon/vcc20_baseline_cyclevae" -> "SamuelBroughton/StarGAN-Voice-Conversion-2"
"bigpon/vcc20_baseline_cyclevae" -> "patrickltobing/cyclevae-vc"
"bigpon/vcc20_baseline_cyclevae" -> "bshall/ZeroSpeech"
"bigpon/vcc20_baseline_cyclevae" -> "tarepan/VoiceConversionLab"
"bigpon/vcc20_baseline_cyclevae" -> "jxzhanggg/nonparaSeq2seqVC_code"
"jpuigcerver/Laia" -> "cwig/start_follow_read"
"jpuigcerver/Laia" -> "jpuigcerver/PyLaia" ["e"=1]
"igormq/asr-study" -> "igormq/aes-lac-2018"
"igormq/asr-study" -> "mlrobsmt/KerasDeepSpeech"
"gooofy/py-kaldi-asr" -> "gooofy/zamia-speech"
"gooofy/py-kaldi-asr" -> "alumae/gst-kaldi-nnet2-online"
"gooofy/py-kaldi-asr" -> "jzlianglu/pykaldi2"
"gooofy/py-kaldi-asr" -> "dspavankumar/keras-kaldi"
"gooofy/py-kaldi-asr" -> "XiaoMi/kaldi-onnx"
"liuxubo717/LASS-demopage" -> "liuxubo717/V-ACT"
"liuxubo717/LASS-demopage" -> "liuxubo717/SimPFs_Spectral"
"liuxubo717/LASS-demopage" -> "liuxubo717/cl4ac"
"liuxubo717/V-ACT" -> "liuxubo717/SimPFs_Spectral"
"liuxubo717/V-ACT" -> "liuxubo717/cl4ac"
"liuxubo717/V-ACT" -> "liuxubo717/LASS-demopage"
"liuxubo717/V-ACT" -> "liuxubo717/sound_generation"
"liuxubo717/V-ACT" -> "liuxubo717/LASS"
"dominoar/QchatPlugins" -> "dominoar/QWaifuGPT3"
"Zeta36/tensorflow-image-wavenet" -> "Zeta36/tensorflow-tex-wavenet"
"idiap/pkwrap" -> "YiwenShaoStephen/pychain_example"
"idiap/pkwrap" -> "idiap/apam"
"riverphoenix/tacotron2" -> "selap91/Tacotron2"
"riverphoenix/tacotron2" -> "zuoxiang95/tacotron-1"
"Wendison/VQMIVC" -> "mindslab-ai/assem-vc"
"Wendison/VQMIVC" -> "liusongxiang/ppg-vc"
"Wendison/VQMIVC" -> "thuhcsi/VAENAR-TTS"
"Wendison/VQMIVC" -> "keonlee9420/StyleSpeech"
"Wendison/VQMIVC" -> "facebookresearch/speech-resynthesis"
"Wendison/VQMIVC" -> "bshall/VectorQuantizedCPC"
"Wendison/VQMIVC" -> "jxzhanggg/nonparaSeq2seqVC_code"
"Wendison/VQMIVC" -> "k2kobayashi/crank"
"Wendison/VQMIVC" -> "auspicious3000/SpeechSplit"
"Wendison/VQMIVC" -> "xcmyz/speech-synthesis-paper"
"Wendison/VQMIVC" -> "auspicious3000/autovc"
"Wendison/VQMIVC" -> "yistLin/FragmentVC"
"Wendison/VQMIVC" -> "mindslab-ai/cotatron"
"Wendison/VQMIVC" -> "cyhuang-tw/AdaIN-VC"
"Wendison/VQMIVC" -> "yl4579/StarGANv2-VC"
"google/REAPER" -> "r9y9/pyreaper"
"google/REAPER" -> "mmorise/World"
"google/REAPER" -> "CSTR-Edinburgh/magphase"
"google/REAPER" -> "r9y9/pysptk"
"google/REAPER" -> "dgaspari/pyrapt"
"google/REAPER" -> "YannickJadoul/Parselmouth"
"google/REAPER" -> "ronggong/pypYIN"
"google/REAPER" -> "mozilla/LPCNet"
"google/REAPER" -> "google/sparrowhawk"
"google/REAPER" -> "JeremyCCHsu/Python-Wrapper-for-World-Vocoder"
"google/REAPER" -> "HidekiKawahara/legacy_STRAIGHT"
"google/REAPER" -> "gillesdegottex/pulsemodel"
"google/REAPER" -> "marl/crepe" ["e"=1]
"google/REAPER" -> "Helsinki-NLP/prosody"
"google/REAPER" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"keonlee9420/Comprehensive-E2E-TTS" -> "choiHkk/CVAEJETS"
"keonlee9420/Comprehensive-E2E-TTS" -> "imdanboy/jets"
"keonlee9420/Comprehensive-E2E-TTS" -> "Labmem-Zhouyx/CDFSE_FastSpeech2"
"HawkAaron/RNN-Transducer" -> "awni/transducer"
"HawkAaron/RNN-Transducer" -> "HawkAaron/warp-transducer"
"HawkAaron/RNN-Transducer" -> "HawkAaron/E2E-ASR"
"HawkAaron/RNN-Transducer" -> "ZhengkunTian/rnn-transducer"
"HawkAaron/RNN-Transducer" -> "sequence-labeling/rnn-transducer"
"HawkAaron/RNN-Transducer" -> "1ytic/warp-rnnt"
"HawkAaron/RNN-Transducer" -> "noahchalifour/rnnt-speech-recognition"
"b04901014/UUVC" -> "MelissaChen15/control-vc"
"pritishyuvraj/Voice-Conversion-GAN" -> "TaiChunYen/Pytorch-CycleGAN-VC2"
"scelesticsiva/speaker_recognition_GMM_UBM" -> "dominoanty/SpeakerRecognition"
"scelesticsiva/speaker_recognition_GMM_UBM" -> "fedderrico/ubm_map_diarization"
"xiangxyq/xiangxyq_kaldi" -> "xiangxyq/minimize-chain-decoder"
"rendchevi/nix-tts" -> "polvanrijn/VoiceMe"
"rendchevi/nix-tts" -> "keonlee9420/Comprehensive-E2E-TTS"
"rendchevi/nix-tts" -> "lucidrains/natural-speech-pytorch"
"rendchevi/nix-tts" -> "keonlee9420/Comprehensive-Transformer-TTS"
"rendchevi/nix-tts" -> "MasayaKawamura/MB-iSTFT-VITS"
"rendchevi/nix-tts" -> "NVIDIA/radtts"
"rendchevi/nix-tts" -> "tencent-ailab/bddm"
"rendchevi/nix-tts" -> "dunky11/voicesmith"
"wangleiai/dVectorSpeakerRecognition" -> "rajathkmp/speaker-verification"
"wangleiai/dVectorSpeakerRecognition" -> "zengchang94622/Speaker_Verification_Tencent"
"kdavis-mozilla/vad.js" -> "Jam3/voice-activity-detection"
"joonson/voxceleb_unsupervised" -> "a-nagrani/VoxSRC2020"
"keonlee9420/DiffSinger" -> "keonlee9420/DiffGAN-TTS"
"keonlee9420/DiffSinger" -> "thuhcsi/VAENAR-TTS"
"keonlee9420/DiffSinger" -> "YatingMusic/ddsp-singing-vocoders"
"wenet-e2e/opencpop" -> "Rongjiehuang/Multi-Singer"
"wenet-e2e/opencpop" -> "M4Singer/M4Singer"
"wenet-e2e/opencpop" -> "hhguo/EA-SVC"
"wenet-e2e/opencpop" -> "CODEJIN/HiFiSinger"
"HuangZiliAndy/RPNSD" -> "nttcslab-sp/EEND-vector-clustering"
"HuangZiliAndy/RPNSD" -> "nryant/dscore"
"tango4j/Auto-Tuning-Spectral-Clustering" -> "desh2608/dover-lap"
"tango4j/Auto-Tuning-Spectral-Clustering" -> "nryant/dscore"
"cyhuang-tw/AdaIN-VC" -> "howard1337/S2VC"
"cyhuang-tw/AdaIN-VC" -> "cyhuang-tw/attack-vc"
"cyhuang-tw/AdaIN-VC" -> "cyhuang-tw/AutoVC"
"Zeqiang-Lai/Prosody_Prediction" -> "npujcong/Chinese_PSP"
"rishikksh20/gmvae_tacotron" -> "yanggeng1995/vae_tacotron"
"xushengyuan/Fastsinging" -> "xushengyuan/VocalnetOpenDataset"
"JeremyCCHsu/vae-npvc" -> "Kyubyong/cross_vc"
"JeremyCCHsu/vae-npvc" -> "ryokamoi/ppg_vc"
"JeremyCCHsu/vae-npvc" -> "jxzhanggg/nonparaSeq2seqVC_code"
"JeremyCCHsu/vae-npvc" -> "jjery2243542/voice_conversion"
"JeremyCCHsu/vae-npvc" -> "JeremyCCHsu/vc-vawgan"
"JeremyCCHsu/vae-npvc" -> "unilight/cdvae-vc"
"JeremyCCHsu/vae-npvc" -> "k2kobayashi/crank"
"JeremyCCHsu/vae-npvc" -> "JeremyCCHsu/vqvae-speech"
"JeremyCCHsu/vae-npvc" -> "guanlongzhao/fac-via-ppg"
"KunZhou9646/controllable_evc_code" -> "KunZhou9646/seq2seq-EVC"
"KunZhou9646/controllable_evc_code" -> "KunZhou9646/Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT"
"howard1337/S2VC" -> "cyhuang-tw/AdaIN-VC"
"howard1337/S2VC" -> "yistLin/universal-vocoder"
"howard1337/S2VC" -> "grtzsohalf/buy_vs_rent_and_invest"
"audio-captioning/audio-captioning-papers" -> "audio-captioning/dcase-2020-baseline"
"audio-captioning/audio-captioning-papers" -> "audio-captioning/audio-captioning-resources"
"audio-captioning/audio-captioning-papers" -> "lukewys/dcase_2020_T6"
"audio-captioning/audio-captioning-papers" -> "XinhaoMei/DCASE2021_task6_v2"
"yl4579/StyleTTS-VC" -> "yl4579/StyleTTS"
"chomeyama/HN-UnifiedSourceFilterGAN" -> "chomeyama/SiFiGAN"
"fatchord/FFTNet" -> "azraelkuan/FFTNet"
"yajiemiao/pdnn" -> "yajiemiao/kaldipdnn"
"yajiemiao/pdnn" -> "yajiemiao/eesen"
"yajiemiao/pdnn" -> "vitruvianscience/OpenDeep" ["e"=1]
"maxrmorrison/torchcrepe" -> "interactiveaudiolab/penn"
"maxrmorrison/torchcrepe" -> "descriptinc/cargan"
"maxrmorrison/torchcrepe" -> "chomeyama/SiFiGAN"
"maxrmorrison/torchcrepe" -> "YatingMusic/ddsp-singing-vocoders"
"maxrmorrison/torchcrepe" -> "sp-nitech/diffsptk"
"maxrmorrison/torchcrepe" -> "marl/crepe" ["e"=1]
"maxrmorrison/torchcrepe" -> "sweetcocoa/ddsp-pytorch"
"maxrmorrison/torchcrepe" -> "ivanvovk/WaveGrad"
"yistLin/universal-vocoder" -> "yistLin/human-evaluation"
"hhguo/MSMC-TTS" -> "liusongxiang/Large-Audio-Models"
"hhguo/MSMC-TTS" -> "mindslab-ai/phaseaug"
"hhguo/MSMC-TTS" -> "thuhcsi/VAENAR-TTS"
"keonlee9420/StyleSpeech" -> "KevinMIN95/StyleSpeech"
"keonlee9420/StyleSpeech" -> "keonlee9420/STYLER"
"keonlee9420/StyleSpeech" -> "SungFeng-Huang/Meta-TTS"
"keonlee9420/StyleSpeech" -> "thuhcsi/VAENAR-TTS"
"keonlee9420/StyleSpeech" -> "keonlee9420/Parallel-Tacotron2"
"keonlee9420/StyleSpeech" -> "keonlee9420/Cross-Speaker-Emotion-Transfer"
"keonlee9420/StyleSpeech" -> "keonlee9420/PortaSpeech"
"keonlee9420/StyleSpeech" -> "keonlee9420/Comprehensive-Transformer-TTS"
"keonlee9420/StyleSpeech" -> "keonlee9420/VAENAR-TTS"
"keonlee9420/StyleSpeech" -> "keonlee9420/Expressive-FastSpeech2"
"ncsoft/avocodo" -> "mindslab-ai/phaseaug"
"rishikksh20/Fre-GAN-pytorch" -> "rishikksh20/UnivNet-pytorch"
"rishikksh20/Fre-GAN-pytorch" -> "keonlee9420/VAENAR-TTS"
"rishikksh20/iSTFTNet-pytorch" -> "rishikksh20/HiFiplusplus-pytorch"
"ksw0306/WaveVAE" -> "npuichigo/waveglow"
"ksw0306/WaveVAE" -> "jaywalnut310/waveglow-vqvae"
"kensun0/Parallel-Wavenet" -> "zhf459/P_wavenet_vocoder"
"kensun0/Parallel-Wavenet" -> "bfs18/nsynth_wavenet"
"kensun0/Parallel-Wavenet" -> "andabi/parallel-wavenet-vocoder"
"sherjilozair/ctc" -> "mcf06/theano_ctc"
"sherjilozair/ctc" -> "mohammadpz/CTC-Connectionist-Temporal-Classification"
"Unisound/SampleRNN" -> "richardassar/SampleRNN_torch"
"Unisound/SampleRNN" -> "soroushmehr/sampleRNN_ICLR2017"
"awslabs/speech-representations" -> "rosinality/imputer-pytorch" ["e"=1]
"tts-tutorial/interspeech2022" -> "mindslab-ai/phaseaug"
"tts-tutorial/interspeech2022" -> "tts-tutorial/icassp2022"
"tts-tutorial/interspeech2022" -> "keonlee9420/DailyTalk"
"tts-tutorial/interspeech2022" -> "yangdongchao/Text-to-sound-Synthesis"
"cmusphinx/pocketsphinx-ios-demo" -> "cmusphinx/cmudict-tools"
"Fyfe93/RAVE-audition" -> "caillonantoine/RAVE"
"XinhaoMei/WavCaps" -> "audio-captioning/audio-captioning-papers"
"XinhaoMei/WavCaps" -> "XinhaoMei/DCASE2021_task6_v2"
"mozilla/FFTNet" -> "syang1993/FFTNet"
"placebokkk/e6870" -> "kaituoxu/E6870"
"RicherMans/AudioCaption" -> "lukewys/dcase_2020_T6"
"CynthiaSuwi/ASR-for-Chinese-Pipeline" -> "liangstein/Chinese-speech-to-text"
"CynthiaSuwi/ASR-for-Chinese-Pipeline" -> "EliasCai/speech_recognition_ctc"
"yl4579/PitchExtractor" -> "yl4579/AuxiliaryASR"
"open-speech/cn-text-normalizer" -> "speech-io/chinese_text_normalization"
"open-speech/cn-text-normalizer" -> "Joee1995/chn_text_norm"
"younggeun-kim/NCSR" -> "younggeun-kim/Styleformer"
"voithru/wav2vec2_finetune" -> "voithru/asr-text_classification-pipeline"
"voithru/wav2vec2_finetune" -> "voithru/voice-activity-detection"
"GitYCC/g2pW" -> "kakaobrain/g2pM"
"pyannote/pyannote-database" -> "pyannote/pyannote-core"
"KimythAnly/AGAIN-VC" -> "ericwudayi/SkipVQVC"
"geneing/WaveRNN-Pytorch" -> "G-Wang/WaveRNN-Pytorch"
"geneing/WaveRNN-Pytorch" -> "bshall/UniversalVocoding"
"geneing/WaveRNN-Pytorch" -> "dipjyoti92/SC-WaveRNN"
"geneing/WaveRNN-Pytorch" -> "h-meru/Tacotron-WaveRNN"
"geneing/WaveRNN-Pytorch" -> "mkotha/WaveRNN"
"farisalasmary/wav2vec2-kenlm" -> "patrickvonplaten/Wav2Vec2_PyCTCDecode"
"danpovey/k2" -> "pzelasko/lhotse"
"danpovey/k2" -> "k2-fsa/snowfall"
"danpovey/k2" -> "YiwenShaoStephen/pychain"
"danpovey/k2" -> "jzlianglu/pykaldi2"
"danpovey/k2" -> "athena-team/athena-decoder"
"AlexHarker/HISSTools_Library" -> "AlexHarker/HISSTools_Granular"
"rishikksh20/Avocodo-pytorch" -> "ncsoft/avocodo"
"XinhaoMei/DCASE2021_task6_v2" -> "audio-captioning/audio-captioning-resources"
"rongcloud/demo-web-sdk" -> "rongcloud/demo-server-php"
"rongcloud/demo-web-sdk" -> "rongcloud/rongcloud-web-im-sdk"
"emotiontts/emotiontts_open_db" -> "hash2430/pitchtron"
"emotiontts/emotiontts_open_db" -> "SoonbeomChoi/BEGANSing"
"shelling203/SpecAugment" -> "zcaceres/spec_augment"
"shelling203/SpecAugment" -> "Kyubyong/specAugment"
"shelling203/SpecAugment" -> "HawkAaron/warp-transducer"
"shelling203/SpecAugment" -> "YiwenShaoStephen/pychain"
"Labbeti/aac-datasets" -> "haoheliu/diffres-python"
"xiangxyq/minimize-chain-decoder" -> "xiangxyq/xiangxyq_kaldi"
"SoonbeomChoi/BEGANSing" -> "neosapience/mlp-singer"
"patrickltobing/cyclevae-vc-neuralvoco" -> "k2kobayashi/crank"
"tinoucas/spleeter-tflite-convert" -> "jinay1991/spleeter"
"azraelkuan/FFTNet" -> "fatchord/FFTNet"
"hccho2/Tacotron-Wavenet-Vocoder" -> "hccho2/hccho2.github.io"
"hccho2/Tacotron-Wavenet-Vocoder" -> "hccho2/Tacotron2-Wavenet-Korean-TTS"
"syang1993/FFTNet" -> "mozilla/FFTNet"
"csukuangfj/optimized_transducer" -> "iamjanvijay/rnnt_decoder_cuda"
"csukuangfj/optimized_transducer" -> "csukuangfj/transducer-loss-benchmarking"
"csukuangfj/optimized_transducer" -> "danpovey/fast_rnnt"
"rwsproat/text-normalization-data" -> "shauryr/google_text_normalization"
"Yeongtae/tacotron2" -> "Yeongtae/waveglow"
"voithru/asr-text_classification-pipeline" -> "voithru/wav2vec2_finetune"
"cdjkim/audiocaps" -> "XinhaoMei/ACT"
"cdjkim/audiocaps" -> "XinhaoMei/DCASE2021_task6_v2"
"audio-captioning/audio-captioning-resources" -> "XinhaoMei/DCASE2021_task6_v2"
"VVasanth/Spleeter_Unofficial_TF20_MobileApp" -> "jinay1991/spleeter"
"jinay1991/spleeter" -> "VVasanth/Spleeter_Unofficial_TF20_MobileApp"
"jinay1991/spleeter" -> "tinoucas/spleeter-tflite-convert"
"PaddlePaddle/PaddleSpeech" ["l"="0.45,39.969"]
"wenet-e2e/wenet" ["l"="0.45,39.916"]
"espnet/espnet" ["l"="0.517,39.96"]
"speechbrain/speechbrain" ["l"="0.467,39.899"]
"coqui-ai/TTS" ["l"="0.482,39.991"]
"nl8590687/ASRT_SpeechRecognition" ["l"="0.652,39.885"]
"jaywalnut310/vits" ["l"="0.165,40.18"]
"TensorSpeech/TensorFlowTTS" ["l"="0.495,40.027"]
"PaddlePaddle/PaddleNLP" ["l"="31.988,30.294"]
"NVIDIA/NeMo" ["l"="0.485,39.938"]
"babysor/MockingBird" ["l"="33.357,33.441"]
"yeyupiaoling/PPASR" ["l"="-0.196,39.355"]
"MoonInTheRiver/DiffSinger" ["l"="0.205,40.139"]
"ming024/FastSpeech2" ["l"="0.411,40.034"]
"PaddlePaddle/PaddleGAN" ["l"="34.015,35.787"]
"mozilla/TTS" ["l"="0.556,40.026"]
"kaldi-asr/kaldi" ["l"="0.612,39.935"]
"mravanelli/pytorch-kaldi" ["l"="0.561,39.869"]
"mozilla/DeepSpeech" ["l"="0.673,39.983"]
"facebookresearch/wav2letter" ["l"="0.644,39.912"]
"pytorch/fairseq" ["l"="29.978,32.515"]
"pykaldi/pykaldi" ["l"="0.566,39.849"]
"zzw922cn/awesome-speech-recognition-speech-synthesis-papers" ["l"="0.558,39.908"]
"syhw/wer_are_we" ["l"="0.589,39.874"]
"cmusphinx/pocketsphinx" ["l"="0.744,39.958"]
"jameslyons/python_speech_features" ["l"="0.537,39.871"]
"Uberi/speech_recognition" ["l"="0.687,39.953"]
"hgneng/ekho" ["l"="0.495,40.101"]
"junzew/HanTTS" ["l"="0.471,40.116"]
"Jackiexiao/MTTS" ["l"="0.483,40.065"]
"begeekmyfriend/tacotron" ["l"="0.513,40.123"]
"KuangDD/zhrtvc" ["l"="0.423,40.073"]
"thuhcsi/Crystal" ["l"="0.462,40.043"]
"begeekmyfriend/Tacotron-2" ["l"="0.487,40.15"]
"espeak-ng/espeak-ng" ["l"="0.558,40.05"]
"ranchlai/mandarin-tts" ["l"="0.395,40.042"]
"JasonWei512/Tacotron-2-Chinese" ["l"="0.455,40.105"]
"mmorise/World" ["l"="0.481,40.126"]
"CSTR-Edinburgh/merlin" ["l"="0.532,40.119"]
"Kyubyong/tacotron" ["l"="0.589,40.119"]
"xcmyz/FastSpeech" ["l"="0.455,40.071"]
"keithito/tacotron" ["l"="0.566,40.091"]
"alphacep/vosk-api" ["l"="0.665,39.933"]
"alphacep/vosk-server" ["l"="0.693,39.875"]
"coqui-ai/STT" ["l"="0.538,39.917"]
"alphacep/vosk-android-demo" ["l"="0.786,39.912"]
"snakers4/silero-models" ["l"="0.505,39.916"]
"alphacep/vosk" ["l"="0.754,39.885"]
"ideasman42/nerd-dictation" ["l"="0.838,39.951"]
"pyannote/pyannote-audio" ["l"="0.435,39.837"]
"tyiannak/pyAudioAnalysis" ["l"="0.506,39.888"]
"librosa/librosa" ["l"="0.421,39.948"]
"jiaaro/pydub" ["l"="0.552,39.938"]
"worldveil/dejavu" ["l"="0.587,38.518"]
"wiseman/py-webrtcvad" ["l"="0.495,39.811"]
"aubio/aubio" ["l"="1.659,38.544"]
"MTG/essentia" ["l"="1.7,38.528"]
"wq2012/awesome-diarization" ["l"="0.407,39.803"]
"pytorch/audio" ["l"="0.402,39.884"]
"google/uis-rnn" ["l"="0.433,39.779"]
"tyiannak/paura" ["l"="0.73,39.906"]
"suno-ai/bark" ["l"="0.235,39.992"]
"lucidrains/audiolm-pytorch" ["l"="0.188,39.973"]
"facebookresearch/encodec" ["l"="0.22,39.966"]
"NVIDIA/BigVGAN" ["l"="0.245,40.02"]
"microsoft/NeuralSpeech" ["l"="0.3,39.997"]
"lifeiteng/vall-e" ["l"="0.201,40.039"]
"lucidrains/naturalspeech2-pytorch" ["l"="0.193,40.028"]
"haoheliu/AudioLDM" ["l"="0.166,39.969"]
"microsoft/SpeechT5" ["l"="0.229,40.024"]
"NATSpeech/NATSpeech" ["l"="-27.325,19.395"]
"archinetai/audio-diffusion-pytorch" ["l"="0.145,39.962"]
"liusongxiang/Large-Audio-Models" ["l"="0.212,39.994"]
"archinetai/audio-ai-timeline" ["l"="0.137,39.951"]
"jik876/hifi-gan" ["l"="0.378,40.04"]
"huawei-noah/Speech-Backbones" ["l"="0.255,40.047"]
"lturing/tacotronv2_wavernn_chinese" ["l"="0.418,40.056"]
"KuangDD/zhvoice" ["l"="0.451,40.029"]
"syang1993/gst-tacotron" ["l"="0.455,40.117"]
"kakaobrain/g2pM" ["l"="0.461,39.989"]
"ivanvovk/DurIAN" ["l"="0.354,40.08"]
"speechio/chinese_text_normalization" ["l"="0.475,39.916"]
"xcmyz/speech-synthesis-paper" ["l"="0.38,40.052"]
"jackaduma/CycleGAN-VC2" ["l"="0.343,40.151"]
"Jackiexiao/zhtts" ["l"="0.402,40.055"]
"MycroftAI/mimic2" ["l"="0.548,40.128"]
"MycroftAI/mimic-recording-studio" ["l"="0.426,40.186"]
"MycroftAI/mimic1" ["l"="0.605,40.132"]
"MycroftAI/selene-backend" ["l"="2.849,39.685"]
"google/voice-builder" ["l"="0.572,40.183"]
"MycroftAI/mimic" ["l"="2.838,39.749"]
"MycroftAI/mycroft-precise" ["l"="2.675,39.751"]
"Kyubyong/dc_tts" ["l"="0.541,40.14"]
"Rayhane-mamah/Tacotron-2" ["l"="0.542,40.081"]
"r9y9/deepvoice3_pytorch" ["l"="0.554,40.113"]
"bootphon/phonemizer" ["l"="0.463,40.017"]
"thorstenMueller/deep-learning-german-tts" ["l"="0.498,40.115"]
"MycroftAI/adapt" ["l"="2.821,39.754"]
"NVIDIA/flowtron" ["l"="0.403,40.071"]
"neonbjb/tortoise-tts" ["l"="27.165,30.936"]
"CorentinJ/Real-Time-Voice-Cloning" ["l"="33.425,33.375"]
"openai/whisper" ["l"="27.192,30.814"]
"NVIDIA/tacotron2" ["l"="0.487,40.082"]
"ggerganov/whisper.cpp" ["l"="27.134,31.004"]
"Baidu-AIP/speech-demo" ["l"="0.696,39.487"]
"Baidu-AIP/speech-tts-cors" ["l"="0.72,39.453"]
"Baidu-AIP/java-sdk" ["l"="0.788,39.295"]
"Baidu-AIP/python-sdk" ["l"="0.76,39.349"]
"Baidu-AIP/speech-vad-demo" ["l"="0.536,39.733"]
"Baidu-AIP/nodejs-sdk" ["l"="0.748,39.375"]
"ZhengkunTian/OpenTransformer" ["l"="0.558,39.788"]
"Baidu-AIP/speech_realtime_api" ["l"="0.695,39.455"]
"Renovamen/Speech-and-Text" ["l"="0.668,39.671"]
"IceKyrin/sovits_guide" ["l"="0.033,40.216"]
"IceKyrin/sovits_f0_infer" ["l"="-0.024,40.204"]
"AlexandaJerry/whisper-vits-japanese" ["l"="0.022,40.212"]
"CjangCjengh/vits" ["l"="0.027,40.231"]
"Francis-Komizu/Sovits" ["l"="0.085,40.17"]
"innnky/emotional-vits" ["l"="0.055,40.213"]
"NaruseMioShirakana/MoeSS" ["l"="0.042,40.233"]
"openvpi/audio-slicer" ["l"="0.041,40.207"]
"luoyily/MoeTTS" ["l"="0.026,40.243"]
"zhaohui8969/VST_NetProcess-" ["l"="0.022,40.224"]
"innnky/so-vits-svc" ["l"="0.082,40.248"]
"audeering/w2v2-how-to" ["l"="0.05,40.182"]
"Francis-Komizu/StellaVoiceChanger" ["l"="-0.056,40.215"]
"fishaudio/fish-diffusion" ["l"="0.069,40.188"]
"bshall/hubert" ["l"="0.106,40.162"]
"Aloento/CraftVits" ["l"="-0.033,40.215"]
"JOETtheIV/VITS-Paimon" ["l"="0.009,40.212"]
"w4123/GenshinVoice" ["l"="0.031,40.2"]
"AlexandaJerry/vits-mandarin-biaobei" ["l"="-0.072,40.23"]
"w4123/vits" ["l"="0.014,40.201"]
"yxlllc/DDSP-SVC" ["l"="0.045,40.223"]
"TheKOG/Gal-Voice-Bot" ["l"="-0.01,40.229"]
"Plachtaa/VITS-fast-fine-tuning" ["l"="0.061,40.233"]
"MuBai-He/ChatWaifu-marai" ["l"="-0.072,40.291"]
"cjyaddone/ChatWaifuL2D" ["l"="-0.024,40.269"]
"cjyaddone/ChatWaifu" ["l"="0.006,40.264"]
"RockChinQ/qcg-installer" ["l"="26.478,30.853"]
"dominoar/QChatPlugins" ["l"="-0.172,40.305"]
"dominoar/QchatPlugins" ["l"="-0.123,40.298"]
"Stardust-minus/vits" ["l"="-0.026,40.184"]
"HuanLinMaster/Genshin-VitsWeb" ["l"="-0.041,40.18"]
"svc-develop-team/so-vits-svc" ["l"="0.12,40.254"]
"prophesier/diff-svc" ["l"="0.106,40.219"]
"CjangCjengh/MoeGoe" ["l"="0.045,40.254"]
"openvpi/DiffSinger" ["l"="0.082,40.223"]
"Anjok07/ultimatevocalremovergui" ["l"="0.152,40.218"]
"yuyuyzl/EasyVtuber" ["l"="0.025,40.299"]
"liujing04/Retrieval-based-Voice-Conversion-WebUI" ["l"="0.073,40.269"]
"kan-bayashi/ParallelWaveGAN" ["l"="0.435,40.065"]
"jaywalnut310/glow-tts" ["l"="0.379,40.063"]
"CjangCjengh/tacotron2-japanese" ["l"="-0.014,40.243"]
"CjangCjengh/TTSModels" ["l"="0.004,40.249"]
"CjangCjengh/MoeGoe_GUI" ["l"="0.009,40.237"]
"Francis-Komizu/VITS" ["l"="-0.008,40.215"]
"Paraworks/vits_with_chatgpt-gpt3" ["l"="-0.008,40.257"]
"ctrlcvs/xiaoyao-cvs-plugin" ["l"="-34.442,-17.832"]
"Yiyuiii/nonebot-plugin-moegoe" ["l"="-32.413,-15.616"]
"jerryuhoo/VTuberTalk" ["l"="-0.006,40.201"]
"wac81/vits_chinese" ["l"="-0.057,40.182"]
"deezer/spleeter" ["l"="0.228,40.224"]
"facebookresearch/demucs" ["l"="0.165,40.078"]
"iperov/DeepFaceLab" ["l"="33.409,33.299"]
"openai/jukebox" ["l"="0.235,40.155"]
"boy1dr/SpleeterGui" ["l"="0.189,40.296"]
"tensorflow/magenta" ["l"="33.66,32.394"]
"jantic/DeOldify" ["l"="33.571,33.101"]
"ExistentialAudio/BlackHole" ["l"="-47.176,-1.204"]
"deepfakes/faceswap" ["l"="33.508,33.375"]
"ytdl-org/youtube-dl" ["l"="-27.463,-16.959"]
"yt-dlp/yt-dlp" ["l"="-27.571,-17.073"]
"DemisEom/SpecAugment" ["l"="0.378,39.835"]
"zcaceres/spec_augment" ["l"="0.473,39.81"]
"facebookresearch/WavAugment" ["l"="0.383,39.87"]
"asteroid-team/torch-audiomentations" ["l"="0.287,39.881"]
"iver56/audiomentations" ["l"="0.307,39.857"]
"hirofumi0810/neural_sp" ["l"="0.546,39.816"]
"s3prl/s3prl" ["l"="0.402,39.901"]
"KimJeongSun/SpecAugment_numpy_scipy" ["l"="0.327,39.822"]
"kaituoxu/Speech-Transformer" ["l"="0.581,39.82"]
"kaituoxu/Listen-Attend-Spell" ["l"="0.566,39.766"]
"aliutkus/speechmetrics" ["l"="2.471,39.185"]
"google-research/leaf-audio" ["l"="0.268,39.863"]
"mravanelli/SincNet" ["l"="0.429,39.793"]
"qiuqiangkong/torchlibrosa" ["l"="0.236,39.869"]
"facebookresearch/denoiser" ["l"="2.442,39.185"]
"qiuqiangkong/audioset_tagging_cnn" ["l"="0.224,39.837"]
"Kaljurand/dictate.js" ["l"="0.748,39.857"]
"jcsilva/docker-kaldi-gstreamer-server" ["l"="0.72,39.846"]
"alumae/gst-kaldi-nnet2-online" ["l"="0.743,39.836"]
"alumae/kaldi-gstreamer-server" ["l"="0.683,39.859"]
"Kaljurand/K6nele" ["l"="0.771,39.857"]
"dialogflow/asr-server" ["l"="0.756,39.844"]
"r9y9/wavenet_vocoder" ["l"="0.538,40.101"]
"fatchord/WaveRNN" ["l"="0.514,40.078"]
"NVIDIA/waveglow" ["l"="0.513,40.096"]
"mozilla/LPCNet" ["l"="0.475,40.101"]
"descriptinc/melgan-neurips" ["l"="0.443,40.088"]
"RicherMans/GPV" ["l"="0.477,39.583"]
"RicherMans/Datadriven-GPVAD" ["l"="0.479,39.551"]
"YoavRamon/awesome-kaldi" ["l"="0.592,39.838"]
"alumae/kaldi-offline-transcriber" ["l"="0.759,39.83"]
"srvk/eesen" ["l"="0.624,39.83"]
"XiaoMi/kaldi-onnx" ["l"="0.594,39.811"]
"gooofy/py-kaldi-asr" ["l"="0.692,39.821"]
"cmusphinx/g2p-seq2seq" ["l"="0.58,39.944"]
"gooofy/zamia-speech" ["l"="0.656,39.852"]
"k2-fsa/k2" ["l"="0.525,39.829"]
"cvqluu/Factorized-TDNN" ["l"="0.355,39.706"]
"cvqluu/TDNN" ["l"="0.318,39.696"]
"manojpamk/pytorch_xvectors" ["l"="0.358,39.753"]
"yuyq96/D-TDNN" ["l"="0.312,39.662"]
"lawlict/ECAPA-TDNN" ["l"="0.331,39.752"]
"idiap/pkwrap" ["l"="0.498,39.709"]
"denfed/leaf-audio-pytorch" ["l"="0.249,39.832"]
"SarthakYadav/leaf-pytorch" ["l"="0.241,39.819"]
"KinWaiCheuk/nnAudio" ["l"="0.247,39.888"]
"csteinmetz1/auraloss" ["l"="0.17,39.929"]
"YuanGongND/ast" ["l"="0.24,39.851"]
"pranaymanocha/PerceptualAudio" ["l"="2.436,39.127"]
"facebookresearch/CPC_audio" ["l"="0.422,39.876"]
"adefossez/julius" ["l"="0.225,39.915"]
"FrancoisGrondin/BIRD" ["l"="2.52,39.323"]
"RetroCirce/Zero_Shot_Audio_Source_Separation" ["l"="0.168,39.844"]
"Spijkervet/torchaudio-augmentations" ["l"="1.726,38.431"]
"ibab/tensorflow-wavenet" ["l"="0.624,40.098"]
"tomlepaine/fast-wavenet" ["l"="0.605,40.152"]
"buriburisuri/speech-to-text-wavenet" ["l"="0.681,40.05"]
"basveeling/wavenet" ["l"="0.629,40.156"]
"andabi/deep-voice-conversion" ["l"="0.631,40.125"]
"pannous/tensorflow-speech-recognition" ["l"="0.679,39.898"]
"openai/pixel-cnn" ["l"="33.772,32.54"]
"carpedm20/DCGAN-tensorflow" ["l"="33.729,32.615"]
"iiscleap/NeuralPlda" ["l"="0.303,39.729"]
"cvqluu/dropclass_speaker" ["l"="0.292,39.787"]
"jefflai108/pytorch-kaldi-neural-speaker-embeddings" ["l"="0.338,39.848"]
"funcwj/ge2e-speaker-verification" ["l"="0.344,39.721"]
"nii-yamagishilab/multi-speaker-tacotron" ["l"="0.383,40.072"]
"mycrazycracy/tf-kaldi-speaker" ["l"="0.34,39.734"]
"qiuqiangkong/audioset_classification" ["l"="1.454,39.185"]
"kkoutini/PaSST" ["l"="0.188,39.842"]
"qiuqiangkong/panns_inference" ["l"="0.197,39.83"]
"lRomul/argus-freesound" ["l"="1.521,39.269"]
"yinkalario/General-Purpose-Sound-Recognition-Demo" ["l"="0.116,39.796"]
"karolpiczak/ESC-50" ["l"="0.212,39.819"]
"qiuqiangkong/panns_transfer_to_gtzan" ["l"="0.185,39.796"]
"RetroCirce/HTS-Audio-Transformer" ["l"="0.168,39.83"]
"MaigoAkisame/cmu-thesis" ["l"="1.453,39.133"]
"YuanGongND/ssast" ["l"="0.186,39.854"]
"sigsep/open-unmix-pytorch" ["l"="0.061,40.018"]
"f90/Wave-U-Net" ["l"="2.576,39.22"]
"sigsep/sigsep-mus-db" ["l"="-0.033,40.008"]
"marl/crepe" ["l"="1.758,38.491"]
"sigsep/sigsep-mus-eval" ["l"="-0.053,40.008"]
"sigsep/norbert" ["l"="-0.044,40.028"]
"jordipons/musicnn" ["l"="1.748,38.436"]
"francesclluis/source-separation-wavenet" ["l"="2.656,39.235"]
"mir-dataset-loaders/mirdata" ["l"="1.741,38.459"]
"mpariente/asteroid" ["l"="2.519,39.186"]
"pfnet-research/meta-tasnet" ["l"="-0.025,40.039"]
"asteroid-team/asteroid" ["l"="2.469,39.165"]
"magenta/ddsp" ["l"="1.775,38.523"]
"zzw922cn/Automatic_Speech_Recognition" ["l"="0.625,39.892"]
"MontrealCorpusTools/Montreal-Forced-Aligner" ["l"="0.473,40.032"]
"philipperemy/deep-speaker" ["l"="0.414,39.749"]
"SeanNaren/deepspeech.pytorch" ["l"="0.616,39.872"]
"KuangDD/aukit" ["l"="0.433,40.027"]
"KuangDD/phkit" ["l"="0.447,40.008"]
"open-speech/speech-aligner" ["l"="0.506,39.947"]
"mindslab-ai/cotatron" ["l"="0.346,40.09"]
"Helsinki-NLP/prosody" ["l"="0.47,40.07"]
"Tomiinek/Multilingual_Text_to_Speech" ["l"="0.364,40.059"]
"speech-io/chinese_text_normalization" ["l"="0.616,40.068"]
"JingShing/NovelAI-installation-tutorial" ["l"="-0.133,40.334"]
"JingShing/NovelAI-4chan-lowvram-ver" ["l"="-0.148,40.322"]
"JingShing/AI-Drawing-Spell-Generator" ["l"="-0.155,40.337"]
"JingShing/novelai-colab-ver" ["l"="-0.095,40.302"]
"JingShing/AI-image-tag-extractor" ["l"="-0.163,40.35"]
"lucidrains/musiclm-pytorch" ["l"="0.134,39.988"]
"LAION-AI/CLAP" ["l"="0.148,39.937"]
"acids-ircam/RAVE" ["l"="0.049,39.931"]
"teticio/audio-diffusion" ["l"="0.116,39.952"]
"samim23/polymath" ["l"="0.086,39.958"]
"csteinmetz1/ai-audio-startups" ["l"="0.169,39.907"]
"marcoppasini/musika" ["l"="0.085,39.946"]
"Harmonai-org/sample-generator" ["l"="0.094,39.931"]
"riffusion/riffusion-app" ["l"="0.105,39.942"]
"riffusion/riffusion" ["l"="0.11,39.966"]
"riffusion/riffusion-inference" ["l"="0.006,39.882"]
"enlyth/sd-webui-riffusion" ["l"="0.053,39.91"]
"MubertAI/Mubert-Text-to-Music" ["l"="0.07,39.978"]
"Kahsolt/stable-diffusion-webui-prompt-travel" ["l"="34.564,29.307"]
"nateraw/stable-diffusion-videos" ["l"="34.357,29.12"]
"salu133445/musegan" ["l"="1.835,38.593"]
"zero01101/openOutpaint-webUI-extension" ["l"="34.532,29.343"]
"sharonzhou/long_stable_diffusion" ["l"="34.349,29.144"]
"JingShing/ImageAI-colab-ver" ["l"="-0.195,40.349"]
"JingShing/Encryptor-Decryptor" ["l"="-0.23,40.369"]
"JingShing/Sorry-NovelAI" ["l"="-0.214,40.367"]
"JingShing/ImgaeAI-installation-tutorial" ["l"="-0.231,40.353"]
"GunwooHan/EasyVtuber" ["l"="-0.021,40.383"]
"pkhungurn/talking-head-anime-2-demo" ["l"="-0.013,40.361"]
"xfgryujk/blivechat" ["l"="-33.588,-18.125"]
"itorr/sakana" ["l"="-32.663,-15.363"]
"pkhungurn/talking-head-anime-demo" ["l"="-0.042,40.352"]
"xianfei/SysMocap" ["l"="35.846,35.058"]
"Tsuk1ko/bilibili-live-chat" ["l"="-33.609,-18.108"]
"DoodleBears/blivechat" ["l"="-33.539,-18.104"]
"jtkim-kaist/VAD" ["l"="0.478,39.736"]
"snakers4/silero-vad" ["l"="0.453,39.817"]
"marsbroshok/VAD-python" ["l"="0.467,39.716"]
"HarryVolek/PyTorch_Speaker_Verification" ["l"="0.392,39.759"]
"microsoft/DNS-Challenge" ["l"="2.458,39.193"]
"hcmlab/vadnet" ["l"="0.487,39.721"]
"tsurumeso/vocal-remover" ["l"="0.121,40.122"]
"atelier-anchor/smiley-sans" ["l"="-27.29,-17.623"]
"the1812/Bilibili-Evolved" ["l"="-27.243,-17.659"]
"xinntao/Real-ESRGAN" ["l"="-29.622,-18.829"]
"AaronFeng753/Waifu2x-Extension-GUI" ["l"="-29.651,-18.876"]
"jpush/jpush-api-java-client" ["l"="0.883,39.146"]
"qiniu/java-sdk" ["l"="0.854,39.069"]
"aliyun/aliyun-oss-java-sdk" ["l"="9.872,21.281"]
"good-life/PushTalk" ["l"="48.023,0.69"]
"jpush/jmessage-api-java-client" ["l"="0.898,39.126"]
"jpush/jpush-phonegap-plugin" ["l"="32.856,-31.187"]
"easemob/emchat-server-examples" ["l"="1.005,39.077"]
"PingPlusPlus/pingpp-java" ["l"="12.995,-11.348"]
"kuielab/mdx-net-submission" ["l"="-0.004,40.115"]
"JeffreyCA/spleeter-web" ["l"="0.162,40.281"]
"kuielab/mdx-net" ["l"="0.033,40.089"]
"haoheliu/voicefixer" ["l"="2.446,39.259"]
"MasayaKawamura/MB-iSTFT-VITS" ["l"="0.156,40.111"]
"mindslab-ai/nuwave" ["l"="0.145,40.071"]
"mindslab-ai/voicefilter" ["l"="2.54,39.162"]
"Rongjiehuang/ProDiff" ["l"="0.186,40.043"]
"Kyubyong/deepvoice3" ["l"="0.553,40.158"]
"soobinseo/Transformer-TTS" ["l"="0.434,40.1"]
"Kyubyong/speaker_adapted_tts" ["l"="0.621,40.228"]
"israelg99/deepvoice" ["l"="0.592,40.222"]
"riverphoenix/tacotron2" ["l"="0.64,40.197"]
"ksw0306/ClariNet" ["l"="0.465,40.161"]
"azraelkuan/parallel_wavenet_vocoder" ["l"="0.485,40.194"]
"sotelo/parrot" ["l"="0.549,40.179"]
"facebookresearch/loop" ["l"="0.532,40.164"]
"tiberiu44/TTS-Cube" ["l"="0.501,40.198"]
"leimao/Voice-Converter-CycleGAN" ["l"="0.311,40.189"]
"jackaduma/CycleGAN-VC3" ["l"="0.299,40.172"]
"ebadawy/voice_conversion" ["l"="0.296,40.206"]
"leimao/Voice_Converter_CycleGAN" ["l"="0.378,40.183"]
"liusongxiang/StarGAN-Voice-Conversion" ["l"="0.382,40.158"]
"hujinsen/StarGAN-Voice-Conversion" ["l"="0.366,40.175"]
"r9y9/gantts" ["l"="0.437,40.148"]
"jjery2243542/voice_conversion" ["l"="0.359,40.196"]
"auspicious3000/autovc" ["l"="0.362,40.125"]
"jjery2243542/adaptive_voice_conversion" ["l"="0.359,40.14"]
"mazzzystar/randomCNN-voice-transfer" ["l"="0.375,40.199"]
"k2kobayashi/sprocket" ["l"="0.402,40.167"]
"pritishyuvraj/Voice-Conversion-GAN" ["l"="0.38,40.241"]
"JeremyCCHsu/vae-npvc" ["l"="0.356,40.222"]
"hujinsen/pytorch-StarGAN-VC" ["l"="0.354,40.181"]
"TaiChunYen/Pytorch-CycleGAN-VC2" ["l"="0.367,40.226"]
"bshall/ZeroSpeech" ["l"="0.324,40.146"]
"SamuelBroughton/StarGAN-Voice-Conversion-2" ["l"="0.335,40.182"]
"jxzhanggg/nonparaSeq2seqVC_code" ["l"="0.342,40.14"]
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" ["l"="0.413,40.121"]
"Turing-Project/AntiFraudChatBot" ["l"="0.099,40.303"]
"Shawn-Inspur/Yuan-1.0" ["l"="0.078,40.355"]
"bigbrother666sh/shezhangbujianle" ["l"="0.061,40.37"]
"wechaty/wechaty" ["l"="26.544,30.846"]
"nonebot/nonebot2" ["l"="-32.415,-15.357"]
"EssayKillerBrain/EssayKiller_V2" ["l"="31.897,30.372"]
"wechaty/python-wechaty" ["l"="15.616,6.95"]
"THUDM/ChatGLM-6B" ["l"="27.085,31"]
"fuergaosi233/wechat-chatgpt" ["l"="26.646,30.946"]
"bynil/sov2ex" ["l"="0.885,38.622"]
"sciooga/v2ex-plus" ["l"="0.873,38.654"]
"Kenshin/sov2ex" ["l"="0.891,38.599"]
"pndurette/gTTS" ["l"="0.689,40.03"]
"bambocher/pocketsphinx-python" ["l"="0.779,39.961"]
"google/live-transcribe-speech-engine" ["l"="0.746,39.812"]
"android/animation" ["l"="51.02,-4.982"]
"GoogleCloudPlatform/android-docs-samples" ["l"="-5.713,-0.767"]
"biemster/asr" ["l"="0.903,39.765"]
"googleapis/java-speech" ["l"="0.829,39.799"]
"libai3/masr" ["l"="0.675,39.828"]
"MidCamp/live-captioning" ["l"="0.837,39.783"]
"ctripcorp/C-OCR" ["l"="6.553,17.981"]
"mozilla/androidspeech" ["l"="0.818,39.852"]
"TensorSpeech/TensorFlowASR" ["l"="0.525,39.844"]
"google/oboe" ["l"="50.889,1.827"]
"noahchalifour/rnnt-speech-recognition" ["l"="0.586,39.788"]
"hollowstrawberry/kohya-colab" ["l"="-0.052,40.548"]
"liasece/sd-webui-train-tools" ["l"="-0.034,40.518"]
"StarStringStudio/so-vits-svc" ["l"="-0.136,40.21"]
"StarStringStudio/diff-svc" ["l"="-0.161,40.215"]
"lmnt-com/wavegrad" ["l"="0.339,40.062"]
"ivanvovk/WaveGrad" ["l"="0.331,40.044"]
"lmnt-com/diffwave" ["l"="0.284,40.029"]
"janvainer/speedyspeech" ["l"="0.347,40.073"]
"rishikksh20/VocGAN" ["l"="0.318,40.058"]
"yanggeng1995/FB-MelGAN" ["l"="0.459,40.087"]
"tianrengao/SqueezeWave" ["l"="0.388,40.094"]
"k2kobayashi/crank" ["l"="0.323,40.131"]
"microsoft/muzic" ["l"="0.157,40.019"]
"Natooz/MidiTok" ["l"="1.859,38.539"]
"magenta/magenta" ["l"="1.891,38.581"]
"bytedance/music_source_separation" ["l"="0.166,39.997"]
"salu133445/muspy" ["l"="1.837,38.541"]
"music-x-lab/POP909-Dataset" ["l"="1.833,38.552"]
"SJTMusicTeam/Muskits" ["l"="0.12,40.06"]
"pkhungurn/talking-head-anime-3-demo" ["l"="0.007,40.387"]
"pkhungurn/talking-head-anime" ["l"="-0.107,40.397"]
"yiranran/Audio-driven-TalkingFace-HeadPose" ["l"="35.009,31.638"]
"TianxingWu/OpenVHead" ["l"="35.443,36.001"]
"pkhungurn/talking-head-anime-2" ["l"="-0.065,40.375"]
"nagadomi/lbpcascade_animeface" ["l"="34.193,30.264"]
"mchong6/GANsNRoses" ["l"="33.29,33.019"]
"albertpumarola/GANimation" ["l"="33.606,32.846"]
"YadiraF/DECA" ["l"="34.841,31.694"]
"kwea123/VTuber_Unity" ["l"="35.461,36.004"]
"peterljq/OpenMMD" ["l"="35.848,35.183"]
"RimoChan/Vtuber_Tutorial" ["l"="-32.713,-15.311"]
"lhotse-speech/lhotse" ["l"="0.497,39.828"]
"LCAV/pyroomacoustics" ["l"="2.412,39.201"]
"weirdseed/Vits-Android-ncnn" ["l"="-0.076,40.267"]
"weirdseed/vits-ncnn-convert-tool" ["l"="-0.099,40.277"]
"Voine/ChatWaifu_Mobile" ["l"="-0.057,40.279"]
"EdVince/diffusers-ncnn" ["l"="31.568,36.29"]
"FeiGeChuanShu/CodeFormer-ncnn" ["l"="31.542,36.304"]
"dueros/bot-sdk-java" ["l"="1.189,40.375"]
"penggle/kaptcha" ["l"="7.195,18.13"]
"happyfish100/fastdfs-client-java" ["l"="10.016,21.299"]
"nobody132/masr" ["l"="0.667,39.842"]
"xxbb1234021/speech_recognition" ["l"="0.68,39.799"]
"yeyupiaoling/MASR" ["l"="-0.178,39.371"]
"yeyupiaoling/PaddlePaddle-DeepSpeech" ["l"="-0.226,39.359"]
"binzhouchn/masr" ["l"="-25.747,18.962"]
"Z-yq/TensorflowASR" ["l"="0.574,39.789"]
"zw76859420/ASR_Theory" ["l"="0.635,39.811"]
"audier/DeepSpeechRecognition" ["l"="0.648,39.833"]
"xdcesc/my_ch_speech_recognition" ["l"="0.74,39.792"]
"sailist/ASRFrame" ["l"="0.693,39.807"]
"PaddlePaddle/DeepSpeech" ["l"="0.641,39.868"]
"MrNothing/AI-Blocks" ["l"="0.755,40.138"]
"oarriaga/face_classification" ["l"="0.798,40.133"]
"astorfi/speechpy" ["l"="0.458,39.798"]
"astorfi/TensorFlow-World-Resources" ["l"="23.385,31.205"]
"astorfi/3D-convolutional-speaker-recognition" ["l"="0.415,39.718"]
"JRMeyer/open-speech-corpora" ["l"="0.53,39.899"]
"r9y9/pysptk" ["l"="0.443,40.112"]
"ppwwyyxx/speaker-recognition" ["l"="0.441,39.699"]
"freewym/espresso" ["l"="0.541,39.841"]
"qqueing/DeepSpeaker-pytorch" ["l"="0.388,39.72"]
"a-nagrani/VGGVox" ["l"="0.387,39.731"]
"mindorii/kws" ["l"="2.652,39.717"]
"awni/speech" ["l"="0.611,39.812"]
"brightmart/nlp_chinese_corpus" ["l"="32.018,30.231"]
"shibing624/pycorrector" ["l"="32.063,30.276"]
"Zulko/moviepy" ["l"="22.971,4.37"]
"kkroening/ffmpeg-python" ["l"="23.144,4.278"]
"xinjli/allosaurus" ["l"="0.419,39.968"]
"dmort27/epitran" ["l"="0.443,39.996"]
"xinjli/transphone" ["l"="0.411,39.978"]
"dmort27/allovera" ["l"="0.4,39.984"]
"dmort27/panphon" ["l"="0.4,39.969"]
"festvox/datasets-CMU_Wilderness" ["l"="0.459,39.95"]
"lumaku/ctc-segmentation" ["l"="0.512,39.799"]
"xinjli/ucla-phonetic-corpus" ["l"="0.378,39.938"]
"andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning" ["l"="0.453,39.866"]
"phoible/dev" ["l"="0.363,39.943"]
"yistLin/FragmentVC" ["l"="0.318,40.082"]
"lingjzhu/charsiu" ["l"="0.431,39.983"]
"auspicious3000/SpeechSplit" ["l"="0.363,40.098"]
"Shahabks/myprosody" ["l"="-1.167,39.894"]
"NVIDIA/mellotron" ["l"="0.43,40.084"]
"ksw0306/FloWaveNet" ["l"="0.47,40.147"]
"seungwonpark/melgan" ["l"="0.415,40.087"]
"NVIDIA/nv-wavenet" ["l"="0.513,40.136"]
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" ["l"="0.332,40.083"]
"keonlee9420/Parallel-Tacotron2" ["l"="0.301,40.055"]
"rishikksh20/FastSpeech2" ["l"="0.297,40.043"]
"keonlee9420/Expressive-FastSpeech2" ["l"="0.268,40.073"]
"keonlee9420/DiffGAN-TTS" ["l"="0.241,40.063"]
"Wendison/VQMIVC" ["l"="0.297,40.106"]
"explosion/spaCy" ["l"="29.888,32.736"]
"facebookresearch/fastText" ["l"="30.048,32.705"]
"tensorflow/models" ["l"="33.821,35.342"]
"oxford-cs-deepnlp-2017/lectures" ["l"="23.5,30.985"]
"openai/gpt-2" ["l"="30.15,32.456"]
"openai/gpt-3" ["l"="30.241,32.422"]
"lucidrains/DALLE-pytorch" ["l"="34.342,28.843"]
"openai/DALL-E" ["l"="34.346,28.804"]
"openai/CLIP" ["l"="34.392,35.886"]
"ybayle/awesome-deep-learning-music" ["l"="1.746,38.508"]
"openvpi/vocoders" ["l"="0.05,40.196"]
"xunmengshe/OpenUtau" ["l"="0.037,40.178"]
"openvpi/diff-svc" ["l"="0.03,40.189"]
"innnky/diff-svc" ["l"="0.014,40.19"]
"flutydeer/audio-slicer" ["l"="0.015,40.178"]
"PaddlePaddle/Parakeet" ["l"="0.426,40.043"]
"L0SG/WaveFlow" ["l"="0.404,40.111"]
"Kyubyong/g2p" ["l"="0.49,40.01"]
"Jungjee/RawNet" ["l"="0.37,39.744"]
"clovaai/aasist" ["l"="21.528,14.499"]
"clovaai/voxceleb_trainer" ["l"="0.376,39.788"]
"Snowdar/asv-subtools" ["l"="0.398,39.785"]
"sasv-challenge/SASVC2022_Baseline" ["l"="21.529,14.524"]
"joonson/voxceleb_unsupervised" ["l"="0.292,39.721"]
"asvspoof-challenge/2021" ["l"="21.54,14.486"]
"WeidiXie/VGG-Speaker-Recognition" ["l"="0.39,39.741"]
"VITA-Group/AutoSpeech" ["l"="0.352,39.741"]
"seongmin-kye/meta-SR" ["l"="0.33,39.719"]
"Janghyun1230/Speaker_Verification" ["l"="0.399,39.732"]
"SayaSS/vits-finetuning" ["l"="0.009,40.223"]
"PlayVoice/vits_chinese" ["l"="0.12,40.138"]
"wenet-e2e/wespeaker" ["l"="0.395,39.832"]
"BUTSpeechFIT/VBx" ["l"="0.335,39.773"]
"TaoRuijie/ECAPA-TDNN" ["l"="0.331,39.791"]
"hitachi-speech/EEND" ["l"="0.36,39.781"]
"vesis84/kaldi-io-for-python" ["l"="0.529,39.817"]
"zyzisyz/mfa_conformer" ["l"="0.326,39.803"]
"facebookresearch/AudioMAE" ["l"="0.19,39.907"]
"juanmc2005/StreamingSpeakerDiarization" ["l"="0.311,39.786"]
"resemble-ai/Resemblyzer" ["l"="0.417,39.999"]
"moscow-technologies/blockchain-voting_2021" ["l"="-11.274,-3.742"]
"haoheliu/Subband-Music-Separation" ["l"="0.034,40.031"]
"haoheliu/2021-ISMIR-MSS-Challenge-CWS-PResUNet" ["l"="0.047,40.049"]
"magenta/mt3" ["l"="1.84,38.502"]
"innnky/vits-japanese" ["l"="-0.049,40.229"]
"Kyomotoi/ATRI" ["l"="-32.401,-15.379"]
"nateshmbhat/pyttsx3" ["l"="0.629,40.009"]
"RapidWareTech/pyttsx" ["l"="0.74,40.042"]
"TaylorSMarks/playsound" ["l"="0.783,40.057"]
"kivy/plyer" ["l"="22.604,5.498"]
"as-ideas/TransformerTTS" ["l"="0.441,40.051"]
"ggeop/Python-ai-assistant" ["l"="2.925,39.858"]
"asweigart/pyautogui" ["l"="45.368,-0.945"]
"ssut/py-googletrans" ["l"="15.76,-36.334"]
"readbeyond/aeneas" ["l"="0.585,40.036"]
"marytts/marytts" ["l"="0.639,40.066"]
"desbma/GoogleSpeech" ["l"="0.805,40.072"]
"hungtruong/Google-Translate-TTS" ["l"="0.821,40.056"]
"wit-ai/pywit" ["l"="31.467,27.728"]
"davidsandberg/facenet" ["l"="34.008,35.319"]
"omar178/Emotion-recognition" ["l"="-0.667,40.336"]
"junyanz/iGAN" ["l"="33.656,32.568"]
"AKSHAYUBHAT/DeepVideoAnalytics" ["l"="0.805,40.167"]
"WuJie1010/Facial-Expression-Recognition.Pytorch" ["l"="-0.689,40.347"]
"yu4u/age-gender-estimation" ["l"="34.825,33.319"]
"PAIR-code/facets" ["l"="25.452,33.857"]
"dpressel/rude-carnie" ["l"="34.833,33.343"]
"lengstrom/fast-style-transfer" ["l"="33.565,32.515"]
"DmitryUlyanov/deep-image-prior" ["l"="33.591,32.663"]
"isseu/emotion-recognition-neural-networks" ["l"="-0.675,40.301"]
"yunjey/StarGAN" ["l"="33.625,32.692"]
"cmusphinx/sphinx4" ["l"="0.728,39.98"]
"cmusphinx/sphinxbase" ["l"="0.75,39.981"]
"cmusphinx/pocketsphinx-android-demo" ["l"="0.778,39.983"]
"cmusphinx/pocketsphinx-android" ["l"="0.768,39.998"]
"cmusphinx/sphinxtrain" ["l"="0.752,39.996"]
"cmusphinx/pocketsphinx-python" ["l"="0.798,39.978"]
"cmusphinx/cmudict" ["l"="0.611,39.993"]
"lkuza2/java-speech-api" ["l"="0.833,40.001"]
"syl22-00/pocketsphinx.js" ["l"="2.287,39.878"]
"julius-speech/julius" ["l"="0.696,39.925"]
"adefossez/mdx21_demucs" ["l"="-0.046,40.121"]
"spotify/basic-pitch" ["l"="0.094,39.987"]
"stemrollerapp/stemroller" ["l"="-0.021,40.095"]
"jimbozhang/speechocean762" ["l"="0.846,39.828"]
"JazminVidal/gop-dnn-epadb" ["l"="0.843,39.814"]
"34j/so-vits-svc-fork" ["l"="0.101,40.236"]
"w-okada/voice-changer" ["l"="0.075,40.294"]
"PlayVoice/so-vits-svc-5.0" ["l"="0.051,40.281"]
"Winfredy/SadTalker" ["l"="35.107,31.58"]
"enhuiz/vall-e" ["l"="0.212,40.011"]
"NVIDIA/DeepLearningExamples" ["l"="34.095,35.627"]
"NVIDIA/Megatron-LM" ["l"="27.533,31.04"]
"microsoft/unilm" ["l"="34.387,35.927"]
"philipperemy/timit" ["l"="0.561,39.628"]
"Faur/TIMIT" ["l"="0.574,39.593"]
"rhdunn/espeak" ["l"="0.664,40.086"]
"numediart/MBROLA" ["l"="0.692,40.104"]
"festvox/flite" ["l"="0.598,40.084"]
"facebookresearch/flashlight" ["l"="0.585,39.858"]
"arrayfire/arrayfire" ["l"="27.87,35.336"]
"arrayfire/arrayfire-ml" ["l"="0.656,39.768"]
"facebookresearch/gtn" ["l"="0.548,39.78"]
"facebookresearch/libri-light" ["l"="0.475,39.842"]
"facebookresearch/nevergrad" ["l"="25.543,33.689"]
"NVIDIA/OpenSeq2Seq" ["l"="0.578,39.921"]
"YiwenShaoStephen/pychain" ["l"="0.573,39.797"]
"kpu/kenlm" ["l"="0.625,39.852"]
"prabhuomkar/pytorch-cpp" ["l"="32.02,35.822"]
"parlance/ctcdecode" ["l"="0.615,39.786"]
"moses-smt/mosesdecoder" ["l"="29.944,32.386"]
"rsennrich/subword-nmt" ["l"="29.949,32.431"]
"google/sentencepiece" ["l"="29.919,32.523"]
"clab/fast_align" ["l"="29.904,32.347"]
"CPJKU/madmom" ["l"="1.712,38.503"]
"faroit/awesome-python-scientific-audio" ["l"="1.689,38.501"]
"cuthbertLab/music21" ["l"="1.753,38.59"]
"keunwoochoi/kapre" ["l"="0.328,39.866"]
"craffel/mir_eval" ["l"="1.697,38.472"]
"r9y9/nnmnkwii" ["l"="0.454,40.142"]
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" ["l"="0.455,40.129"]
"foamliu/Tacotron2-Mandarin" ["l"="0.494,40.169"]
"JasonWei512/wavenet_vocoder" ["l"="0.475,40.169"]
"Joee1995/tacotron2-mandarin-griffin-lim" ["l"="0.522,40.197"]
"MachineLP/TensorFlowTTS_chinese" ["l"="0.512,40.112"]
"bbepoch/CuteChineseTTS" ["l"="0.45,40.167"]
"aidreamwin/TTS-Clone-Chinese" ["l"="0.403,40.098"]
"iwater/Real-Time-Voice-Cloning-Chinese" ["l"="0.445,40.158"]
"ArwenFeng/tacotron_mandarin" ["l"="0.544,40.232"]
"wqt2019/tacotron-2_melgan" ["l"="0.534,40.241"]
"wqt2019/tacotron-2_wavernn" ["l"="0.549,40.243"]
"awesome-archive/tacotron_cn" ["l"="0.53,40.228"]
"barronalex/Tacotron" ["l"="0.651,40.148"]
"zuoxiang95/tacotron-1" ["l"="0.592,40.176"]
"rishikksh20/vae_tacotron2" ["l"="0.407,40.13"]
"mozilla/voice-web" ["l"="0.605,39.908"]
"MycroftAI/mycroft-core" ["l"="2.809,39.791"]
"spotify/pedalboard" ["l"="0.208,39.932"]
"DBraun/DawDreamer" ["l"="1.82,37.779"]
"geemion/Khepri" ["l"="43.914,-26.96"]
"olilarkin/awesome-musicdsp" ["l"="1.622,37.806"]
"juce-framework/JUCE" ["l"="1.681,37.789"]
"soul-lang/SOUL" ["l"="1.643,37.806"]
"Alexander-H-Liu/End-to-end-ASR-Pytorch" ["l"="0.57,39.834"]
"gentaiscool/end2end-asr-pytorch" ["l"="0.57,39.777"]
"brentspell/hifi-gan-bwe" ["l"="0.168,40.049"]
"NVIDIA/radtts" ["l"="0.215,40.069"]
"rishikksh20/iSTFTNet-pytorch" ["l"="0.166,40.037"]
"Labmem-Zhouyx/CDFSE_FastSpeech2" ["l"="0.193,40.079"]
"tts-tutorial/survey" ["l"="0.334,40.035"]
"openspeech-team/openspeech" ["l"="0.493,39.794"]
"sooftware/conformer" ["l"="0.41,39.823"]
"sooftware/kospeech" ["l"="44.33,-15.141"]
"burchim/EfficientConformer" ["l"="0.493,39.771"]
"kssteven418/Squeezeformer" ["l"="0.495,39.759"]
"SpeechColab/GigaSpeech" ["l"="0.509,39.839"]
"upskyy/Squeezeformer" ["l"="0.493,39.746"]
"danpovey/fast_rnnt" ["l"="0.533,39.776"]
"tencent-ailab/pika" ["l"="0.544,39.801"]
"k2-fsa/icefall" ["l"="0.508,39.821"]
"sooftware/Speech-Recognition-Tutorial" ["l"="44.285,-15.174"]
"lucidrains/conformer" ["l"="0.434,39.812"]
"sooftware/KoSpeech" ["l"="44.37,-15.09"]
"sooftware/openspeech" ["l"="44.306,-15.164"]
"huyanxin/DeepComplexCRN" ["l"="2.493,39.212"]
"Wenzhe-Liu/awesome-speech-enhancement" ["l"="2.476,39.211"]
"ky1994/SpeechRecognition" ["l"="0.794,39.767"]
"zhvng/open-musiclm" ["l"="0.077,39.887"]
"LAION-AI/audio-dataset" ["l"="0.161,39.942"]
"jongwoojeff/DiscreteMathematics" ["l"="-0.5,40.058"]
"hongshin/DiscreteMath" ["l"="-0.462,40.05"]
"QosmoInc/neutone_sdk" ["l"="0.003,39.916"]
"acids-ircam/nn_tilde" ["l"="-0.021,39.924"]
"jatinchowdhury18/RTNeural" ["l"="1.841,37.85"]
"fcaspe/ddx7" ["l"="-0.005,39.927"]
"caillonantoine/RAVE" ["l"="0.013,39.946"]
"ben-hayes/neural-waveshaping-synthesis" ["l"="0.063,39.922"]
"Fyfe93/RAVE-audition" ["l"="-0.032,39.935"]
"moiseshorta/MelSpecVAE" ["l"="-0.046,39.918"]
"naotokui/RhythmVAE_M4L" ["l"="-0.088,39.851"]
"magenta/ddsp-vst" ["l"="0.03,39.954"]
"adobe-research/DeepAFx-ST" ["l"="0.042,39.943"]
"ninon-io/Impact-Synth-Hardware" ["l"="-0.061,39.893"]
"maxrmorrison/torchcrepe" ["l"="0.152,39.98"]
"torchsynth/torchsynth" ["l"="0.103,39.913"]
"facebookresearch/speech-resynthesis" ["l"="0.283,40.071"]
"DolbyLaboratories/neural-upsampling-artifacts-audio" ["l"="0.189,39.877"]
"yoyololicon/diffwave-sr" ["l"="0.204,39.882"]
"minzwon/sota-music-tagging-models" ["l"="1.717,38.414"]
"Spijkervet/CLMR" ["l"="1.734,38.414"]
"acids-ircam/ddsp_pytorch" ["l"="0.066,39.941"]
"csteinmetz1/micro-tcn" ["l"="0.065,39.897"]
"magenta/midi-ddsp" ["l"="0.066,39.955"]
"YatingMusic/ddsp-singing-vocoders" ["l"="0.138,40.02"]
"bshall/soft-vc" ["l"="0.153,40.132"]
"PlayVoice/VI-SVC" ["l"="0.095,40.14"]
"OlaWod/FreeVC" ["l"="0.168,40.124"]
"anonymous-pits/pits" ["l"="0.145,40.101"]
"polvanrijn/VoiceMe" ["l"="0.144,40.14"]
"Rongjiehuang/GenerSpeech" ["l"="0.186,40.052"]
"heatz123/naturalspeech" ["l"="0.17,40.099"]
"BuglyDevTeam/Bugly-Android-Demo" ["l"="1.047,38.884"]
"Meituan-Dianping/walle" ["l"="48.866,1.039"]
"umeng/MultiFunctionAndroidDemo" ["l"="1.092,38.88"]
"Jay-Goo/ProtectedApkResignerForWalle" ["l"="49.045,0.989"]
"aliyun/aliyun-oss-android-sdk" ["l"="1.001,38.907"]
"aliyun/alicloud-android-demo" ["l"="1.013,38.875"]
"keonlee9420/DailyTalk" ["l"="0.228,40.047"]
"yangdongchao/Text-to-sound-Synthesis" ["l"="0.176,40.01"]
"Edresson/YourTTS" ["l"="0.267,40.083"]
"hhguo/MSMC-TTS" ["l"="0.213,40.059"]
"microsoft/UniSpeech" ["l"="0.374,39.855"]
"keonlee9420/Comprehensive-Transformer-TTS" ["l"="0.261,40.057"]
"wenet-e2e/wetts" ["l"="0.256,40.004"]
"Rongjiehuang/FastDiff" ["l"="0.206,40.025"]
"VOICEVOX/voicevox" ["l"="0.121,40.438"]
"VOICEVOX/voicevox_engine" ["l"="0.114,40.407"]
"VOICEVOX/voicevox_core" ["l"="0.149,40.408"]
"isletennos/MMVC_Trainer" ["l"="0.12,40.368"]
"yuru7/HackGen" ["l"="-10.894,-23.858"]
"ePi5131/patch.aul" ["l"="-26.295,13.629"]
"jiro4989/ojosama" ["l"="-10.667,-23.869"]
"suzune25254649/bakusoku_aviutl_plugin" ["l"="-26.298,13.616"]
"yuru7/udev-gothic" ["l"="-10.903,-23.891"]
"misskey-dev/misskey" ["l"="-24.627,-19.585"]
"cocoa-mhlw/cocoa" ["l"="-10.835,-23.854"]
"googlefonts/morisawa-biz-ud-gothic" ["l"="-10.921,-23.872"]
"lilxyzw/lilToon" ["l"="-14.763,41.843"]
"hecomi/uLipSync" ["l"="-14.885,41.887"]
"vrm-c/UniVRM" ["l"="-14.827,41.805"]
"google/mozc-devices" ["l"="-10.867,-23.863"]
"TencentGameMate/chinese_speech_pretrain" ["l"="0.399,39.928"]
"athena-team/athena" ["l"="21.881,27.411"]
"Kitt-AI/snowboy" ["l"="2.639,39.88"]
"jasperproject/jasper-client" ["l"="2.73,39.886"]
"UFAL-DSG/alex" ["l"="0.936,39.812"]
"UFAL-DSG/pykaldi" ["l"="0.893,39.813"]
"UFAL-DSG/alex-asr" ["l"="0.945,39.798"]
"cuayahuitl/SimpleDS" ["l"="0.985,39.806"]
"bgshih/crnn" ["l"="29.448,34.127"]
"uber/horovod" ["l"="34.123,35.271"]
"eragonruan/text-detection-ctpn" ["l"="29.487,34.168"]
"OpenNMT/OpenNMT" ["l"="30.086,32.465"]
"bear63/sceneReco" ["l"="29.449,34.144"]
"argman/EAST" ["l"="29.488,34.116"]
"NVIDIA/pix2pixHD" ["l"="33.618,32.736"]
"tianzhi0549/CTPN" ["l"="29.433,34.144"]
"Zuntan03/CharFramework" ["l"="-0.013,40.485"]
"Zuntan03/LatentCoupleHelper" ["l"="34.523,29.341"]
"Zuntan03/LoraBlockWeightPlotHelper" ["l"="-0.036,40.501"]
"kslz/SoundLabel" ["l"="-0.206,40.311"]
"bigpon/vcc20_baseline_cyclevae" ["l"="0.338,40.171"]
"SamuelBroughton/StarGAN-Voice-Conversion" ["l"="0.326,40.213"]
"Oscarshu0719/pytorch-StarGAN-VC2" ["l"="0.336,40.206"]
"MingjieChen/LowResourceVC" ["l"="0.342,40.231"]
"dipjyoti92/StarGAN-Voice-Conversion-2" ["l"="0.318,40.217"]
"joansj/blow" ["l"="0.347,40.19"]
"ericwudayi/SkipVQVC" ["l"="0.324,40.163"]
"marcoppasini/MelGAN-VC" ["l"="0.335,40.194"]
"0x454447415244/HandwritingRecognitionSystem" ["l"="0.77,39.609"]
"sushant097/Handwritten-Line-Text-Recognition-using-Deep-Learning-with-Tensorflow" ["l"="0.802,39.585"]
"Breta01/handwriting-ocr" ["l"="0.8,39.569"]
"githubharald/SimpleHTR" ["l"="0.788,39.621"]
"awslabs/handwritten-text-recognition-for-apache-mxnet" ["l"="0.783,39.594"]
"arthurflor23/handwritten-text-recognition" ["l"="0.825,39.586"]
"cwig/start_follow_read" ["l"="0.76,39.59"]
"githubharald/WordDetector" ["l"="0.813,39.597"]
"frereit/TensorflowHandwritingRecognition" ["l"="0.821,39.571"]
"lquirosd/P2PaLA" ["l"="29.738,34.305"]
"lamhoangtung/LineHTR" ["l"="0.763,39.622"]
"dhlab-epfl/dhSegment" ["l"="29.717,34.299"]
"mittagessen/kraken" ["l"="29.632,34.344"]
"githubharald/CTCDecoder" ["l"="0.635,39.749"]
"githubharald/CTCWordBeamSearch" ["l"="0.664,39.712"]
"sjvasquez/handwriting-synthesis" ["l"="31.013,32.219"]
"google/speaker-id" ["l"="0.344,39.782"]
"wq2012/SpectralCluster" ["l"="0.36,39.768"]
"nryant/dscore" ["l"="0.332,39.764"]
"taylorlu/Speaker-Diarization" ["l"="0.371,39.758"]
"yistLin/dvector" ["l"="0.335,39.905"]
"Arkueid/Live2DMascot" ["l"="-0.037,40.285"]
"gstory0404/chatgpt_app" ["l"="-0.115,40.285"]
"BenAAndrew/Voice-Cloning-App" ["l"="0.307,40.07"]
"MachineEditor/MachineVideoEditor" ["l"="34.92,31.435"]
"SortAnon/ControllableTalkNet" ["l"="0.276,40.13"]
"neuralchen/SimSwap" ["l"="34.932,31.506"]
"dunky11/voicesmith" ["l"="0.211,40.083"]
"vlomme/Multi-Tacotron-Voice-Cloning" ["l"="0.354,40.109"]
"CMsmartvoice/One-Shot-Voice-Cloning" ["l"="0.227,40.078"]
"rotemtzaban/STIT" ["l"="33.249,33.045"]
"crouchred/speaker-recognition-py3" ["l"="0.391,39.671"]
"astorfi/3D-convolutional-speaker-recognition-pytorch" ["l"="0.371,39.709"]
"Walleclipse/Deep_Speaker-speaker_recognition_system" ["l"="0.386,39.702"]
"rajathkmp/speaker-verification" ["l"="0.382,39.687"]
"andabi/voice-vector" ["l"="0.404,39.691"]
"google/sparrowhawk" ["l"="0.582,40.158"]
"google/TextNormalizationCoveringGrammars" ["l"="0.639,40.213"]
"rwsproat/text-normalization-data" ["l"="0.676,40.246"]
"googlei18n/language-resources" ["l"="0.657,40.221"]
"danijel3/SparrowhawkTest" ["l"="0.609,40.185"]
"Atul-Anand-Jha/Speaker-Identification-Python" ["l"="0.388,39.647"]
"ALIZE-Speaker-Recognition/LIA_RAL" ["l"="0.415,39.532"]
"orchidas/Speaker-Recognition" ["l"="0.413,39.651"]
"amaurycrickx/recognito" ["l"="0.423,39.604"]
"Microsoft/Cognitive-SpeakerRecognition-Python" ["l"="0.433,39.631"]
"GauravWaghmare/Speaker-Identification" ["l"="0.382,39.66"]
"Jakobovski/free-spoken-digit-dataset" ["l"="0.512,39.772"]
"soerenab/AudioMNIST" ["l"="0.536,39.646"]
"karoldvl/ESC-50" ["l"="1.401,39.195"]
"jim-schwoebel/voice_datasets" ["l"="2.408,39.123"]
"castorini/honk" ["l"="2.635,39.708"]
"drethage/speech-denoising-wavenet" ["l"="2.533,39.242"]
"chrisdonahue/wavegan" ["l"="0.503,40.154"]
"mostafaelaraby/wavegan-pytorch" ["l"="0.574,40.271"]
"santi-pdp/segan" ["l"="2.516,39.235"]
"HGU-DLLAB/Korean-FastSpeech2-Pytorch" ["l"="0.152,40.053"]
"Kyubyong/g2pK" ["l"="44.322,-15.042"]
"emotiontts/emotiontts_open_db" ["l"="0.184,40.104"]
"WICWIU/WICWIU" ["l"="-0.132,40.043"]
"mmorise/ita-corpus" ["l"="0.141,40.362"]
"mmorise/rohan4600" ["l"="0.145,40.389"]
"isletennos/MMVC_Client" ["l"="0.114,40.355"]
"SeanNaren/warp-ctc" ["l"="29.504,34.082"]
"Mikubill/sd-webui-controlnet" ["l"="34.362,29.297"]
"alibaba/alpha" ["l"="49.135,1.118"]
"alibaba/Tangram-Android" ["l"="48.861,1.155"]
"Qihoo360/ArgusAPM" ["l"="49.102,1.078"]
"HujiangTechnology/gradle_plugin_android_aspectjx" ["l"="49.003,1.063"]
"zw76859420/ASR_Syllable" ["l"="0.675,39.769"]
"Pelhans/ZASR_tensorflow" ["l"="0.75,39.74"]
"tramphero/kaldi" ["l"="0.651,39.816"]
"daixiang789/tensorflow-examples" ["l"="0.723,39.796"]
"EliasCai/speech_recognition_ctc" ["l"="0.703,39.716"]
"hirofumi0810/tensorflow_end2end_speech_recognition" ["l"="0.671,39.809"]
"RHVoice/RHVoice" ["l"="0.633,39.972"]
"Stypox/dicio-android" ["l"="-28.536,-21.871"]
"sodapng/voice-over-translation" ["l"="-32.641,-21.252"]
"jcsteh/osara" ["l"="28.7,-28.877"]
"sharkboyto/nao" ["l"="0.692,40.002"]
"qw123wh/new-clock-fdroid" ["l"="-28.689,-22.008"]
"SergeyShk/Speech-to-Text-Russian" ["l"="-9.278,20.273"]
"tencent-ailab/bddm" ["l"="0.216,40.046"]
"Rongjiehuang/TranSpeech" ["l"="0.15,40.036"]
"yerfor/SyntaSpeech" ["l"="0.233,40.013"]
"WelkinYang/Learn2Sing2.0" ["l"="0.129,40.072"]
"rolczynski/Automatic-Speech-Recognition" ["l"="0.881,39.909"]
"30stomercury/Automatic-Speech-Recognition" ["l"="0.93,39.915"]
"Hiroshiba/realtime-yukarin" ["l"="0.276,40.263"]
"Hiroshiba/yukarin" ["l"="0.288,40.306"]
"Hiroshiba/become-yukarin" ["l"="0.267,40.297"]
"pstuvwx/Deep_VoiceChanger" ["l"="0.271,40.318"]
"liusongxiang/ppg-vc" ["l"="0.247,40.107"]
"santi-pdp/pase" ["l"="0.454,39.845"]
"koishijs/novelai-bot" ["l"="-32.518,-15.391"]
"DominikDoom/a1111-sd-webui-tagcomplete" ["l"="34.409,29.37"]
"sudoskys/StableDiffusionBook" ["l"="34.36,29.408"]
"wfjsw/danbooru-diffusion-prompt-builder" ["l"="34.37,29.431"]
"zcyzcy88/TagTable" ["l"="34.324,29.452"]
"acheong08/NovelAI-Colab" ["l"="-0.128,40.313"]
"KichangKim/DeepDanbooru" ["l"="34.326,29.347"]
"jimbozhang/kaldi-gop" ["l"="0.779,39.833"]
"alphacep/kaldi-websocket-python" ["l"="0.774,39.843"]
"jpuigcerver/kaldi-decoders" ["l"="0.795,39.838"]
"daanzu/kaldi-active-grammar" ["l"="-25.582,-15.375"]
"Vernacular-ai/kaldi-serve" ["l"="-25.619,-15.366"]
"SociallyIneptWeeb/LanguageLeapAI" ["l"="0.07,40.52"]
"0Xiaohei0/VoiceToJapanese" ["l"="0.074,40.545"]
"Koischizo/AI-Vtuber" ["l"="0.038,40.525"]
"ardha27/AI-Waifu-Vtuber" ["l"="0.037,40.504"]
"adi-panda/Kuebiko" ["l"="0.038,40.539"]
"carpedm20/multi-speaker-tacotron-tensorflow" ["l"="0.64,40.179"]
"GSByeon/multi-speaker-tacotron-tensorflow" ["l"="0.724,40.217"]
"hccho2/Tacotron2-Wavenet-Korean-TTS" ["l"="0.692,40.217"]
"hccho2/Tacotron-Wavenet-Vocoder-Korean" ["l"="0.695,40.232"]
"hccho2/Tacotron-Wavenet-Vocoder" ["l"="0.712,40.229"]
"kakao/khaiii" ["l"="44.513,-15.002"]
"sokcuri/multi-speaker-tacotron-tensorflow" ["l"="0.71,40.204"]
"insurgent92/CS231N_17_KOR_SUB" ["l"="44.622,-14.962"]
"devsisters/multi-speaker-tacotron-tensorflow" ["l"="44.697,-14.839"]
"lifefeel/SpeechSynthesis" ["l"="0.716,40.247"]
"hysts/anime-face-detector" ["l"="34.211,30.153"]
"yoyo-nb/Thin-Plate-Spline-Motion-Model" ["l"="35.077,31.551"]
"SerialLain3170/AwesomeAnimeResearch" ["l"="34.168,30.176"]
"yzhou359/MakeItTalk" ["l"="35.02,31.646"]
"Hangz-nju-cuhk/Talking-Face_PC-AVS" ["l"="35.039,31.616"]
"YuanxunLu/LiveSpeechPortraits" ["l"="35.056,31.611"]
"transpchan/Live3D-v2" ["l"="34.556,29.339"]
"LynnHo/EigenGAN-Tensorflow" ["l"="33.199,32.957"]
"snakers4/open_stt" ["l"="-9.247,20.291"]
"yandex/YaLM-100B" ["l"="-9.062,20.229"]
"sberbank-ai/ru-gpts" ["l"="-9.27,20.296"]
"janvarev/Irene-Voice-Assistant" ["l"="0.356,39.895"]
"easemob/web-im" ["l"="1.039,39.069"]
"easemob/webim" ["l"="1.073,39.063"]
"G-Wang/WaveRNN-Pytorch" ["l"="0.455,40.211"]
"geneing/WaveRNN-Pytorch" ["l"="0.437,40.173"]
"h-meru/Tacotron-WaveRNN" ["l"="0.486,40.21"]
"fatchord/FFTNet" ["l"="0.482,40.281"]
"mkotha/WaveRNN" ["l"="0.455,40.181"]
"dwango/UniVRM" ["l"="-14.902,41.844"]
"Hiroshiba/voicevox" ["l"="0.182,40.364"]
"Hiroshiba/voicevox_engine" ["l"="0.192,40.383"]
"sh-akira/VirtualMotionCapture" ["l"="-14.839,41.825"]
"omegasisters/homepage" ["l"="-10.794,-23.876"]
"ksasao/Gochiusearch" ["l"="-8.865,-23.256"]
"mmorise/kiritan_singing" ["l"="35.435,36.323"]
"XVI/AniLipSync" ["l"="-14.961,41.852"]
"BoragoCode/AttentionBasedProsodyPrediction" ["l"="0.505,40.042"]
"kan-bayashi/PytorchWaveNetVocoder" ["l"="0.477,40.158"]
"tuanad121/Python-WORLD" ["l"="0.458,40.232"]
"MoonInTheRiver/NeuralSVB" ["l"="0.162,40.061"]
"begeekmyfriend/tacotron2" ["l"="0.449,40.192"]
"baidu-research/warp-ctc" ["l"="27.086,33.924"]
"google/language-resources" ["l"="0.624,40.274"]
"festvox/festival" ["l"="0.667,40.137"]
"m-toman/tacorn" ["l"="0.517,40.267"]
"cywang97/StreamingTransformer" ["l"="0.54,39.81"]
"mobvoi/wenet" ["l"="0.541,39.826"]
"HawkAaron/warp-transducer" ["l"="0.586,39.799"]
"ZhengkunTian/rnn-transducer" ["l"="0.59,39.777"]
"xingchensong/speech-recognition-papers" ["l"="0.558,39.812"]
"synesthesiam/opentts" ["l"="2.747,39.677"]
"googlecreativelab/aiexperiments-giorgio-cam" ["l"="28.388,27.732"]
"alishdipani/Neural-Style-Transfer-Audio" ["l"="0.461,40.285"]
"Kyubyong/cross_vc" ["l"="0.358,40.264"]
"alokprasad/LPCTron" ["l"="0.485,40.178"]
"yanggeng1995/GAN-TTS" ["l"="0.375,40.085"]
"andabi/parallel-wavenet-vocoder" ["l"="0.465,40.197"]
"KinglittleQ/GST-Tacotron" ["l"="0.383,40.108"]
"Kyubyong/expressive_tacotron" ["l"="0.501,40.184"]
"npuichigo/waveglow" ["l"="0.477,40.201"]
"bfs18/nsynth_wavenet" ["l"="0.474,40.212"]
"mozilla/FFTNet" ["l"="0.521,40.253"]
"syang1993/FFTNet" ["l"="0.509,40.238"]
"alibaba/Alibaba-MIT-Speech" ["l"="0.609,39.83"]
"hmartiro/riffusion-app" ["l"="0.079,39.912"]
"Hiroshiba/voicevox_core" ["l"="0.189,40.407"]
"Sharad24/Neural-Voice-Cloning-with-Few-Samples" ["l"="0.418,40.204"]
"IEEE-NITK/Neural-Voice-Cloning" ["l"="0.407,40.19"]
"m-bain/whisperX" ["l"="27.019,31.35"]
"KR-HappyFace/KoDALLE" ["l"="-0.065,40.426"]
"hihellohowareyou/RESREF_Chatbot_data_for_Korean" ["l"="-0.082,40.444"]
"amsehili/auditok" ["l"="0.466,39.763"]
"ina-foss/inaSpeechSegmenter" ["l"="0.433,39.753"]
"eesungkim/Voice_Activity_Detector" ["l"="0.469,39.689"]
"filippogiruzzi/voice_activity_detection" ["l"="0.471,39.701"]
"linhdvu14/vggvox-speaker-identification" ["l"="0.372,39.693"]
"swshon/voxceleb-ivector" ["l"="0.359,39.619"]
"hsn-zeinali/x-vector-kaldi-tf" ["l"="0.399,39.715"]
"hbredin/TristouNet" ["l"="0.363,39.681"]
"lingochamp/kaldi-ctc" ["l"="0.647,39.799"]
"aishell-foundation/DaCiDian" ["l"="0.557,39.857"]
"pyannote/pyannote-db-voxceleb" ["l"="0.404,39.638"]
"RicherMans/PLDA" ["l"="0.335,39.679"]
"idiap/kaldi-ivector" ["l"="0.367,39.636"]
"tiagofrepereira2012/ivector_example" ["l"="0.347,39.61"]
"jymsuper/SpeakerRecognition_tutorial" ["l"="0.36,39.722"]
"wangleiai/dVectorSpeakerRecognition" ["l"="0.36,39.663"]
"coqui-ai/open-speech-corpora" ["l"="0.359,39.975"]
"coqui-ai/TTS-papers" ["l"="0.299,40.028"]
"keonlee9420/StyleSpeech" ["l"="0.263,40.066"]
"as-ideas/DeepPhonemizer" ["l"="0.318,40.01"]
"Kyubyong/css10" ["l"="0.398,40.017"]
"facebookresearch/voxpopuli" ["l"="0.491,39.847"]
"wesbz/SoundStream" ["l"="0.243,39.935"]
"lucidrains/vector-quantize-pytorch" ["l"="0.243,39.958"]
"mindslab-ai/phaseaug" ["l"="0.182,40.031"]
"sh-lee-prml/BigVGAN" ["l"="0.035,39.988"]
"yl4579/StyleTTS" ["l"="0.156,40.095"]
"LEEYOONHYUNG/BVAE-TTS" ["l"="0.322,40.071"]
"thuhcsi/VAENAR-TTS" ["l"="0.285,40.081"]
"bshall/UniversalVocoding" ["l"="0.388,40.137"]
"mindslab-ai/wavegrad2" ["l"="0.202,40.074"]
"yl4579/StyleTTS-VC" ["l"="0.173,40.112"]
"keunwoochoi/music-auto_tagging-keras" ["l"="1.797,38.418"]
"seth814/Audio-Classification" ["l"="1.414,39.254"]
"keunwoochoi/dl4mir" ["l"="1.746,38.426"]
"marl/openl3" ["l"="1.681,38.422"]
"keunwoochoi/transfer_learning_music" ["l"="1.788,38.396"]
"keunwoochoi/torchaudio-contrib" ["l"="1.662,38.414"]
"Picsart-AI-Research/Text2Video-Zero" ["l"="34.53,29.158"]
"speechbrain/speechbrain.github.io" ["l"="0.461,39.783"]
"nttcslab-sp/kaldiio" ["l"="0.486,39.802"]
"fgnt/nara_wpe" ["l"="2.45,39.209"]
"funcwj/setk" ["l"="2.445,39.218"]
"lochenchou/MOSNet" ["l"="2.467,39.113"]
"ksanjeevan/crnn-audio-classification" ["l"="1.449,39.239"]
"audioset/ontology" ["l"="1.437,39.178"]
"karolpiczak/paper-2015-esc-dataset" ["l"="0.178,39.784"]
"mtobeiyf/audio-classification" ["l"="1.425,39.221"]
"uezo/ChatdollKit" ["l"="0.051,40.402"]
"ksasao/TTSController" ["l"="0.093,40.434"]
"ddPn08/rvc-webui" ["l"="0.074,40.336"]
"nanokina/ebyroid" ["l"="0.091,40.47"]
"mikoto2000/VoiceroidController2" ["l"="0.083,40.457"]
"Ebycow/hanako" ["l"="0.091,40.49"]
"k2-fsa/snowfall" ["l"="0.526,39.792"]
"k2-fsa/sherpa" ["l"="0.508,39.787"]
"thu-spmi/CAT" ["l"="0.56,39.801"]
"DavidDiazGuerra/gpuRIR" ["l"="2.483,39.191"]
"lowerquality/gentle" ["l"="0.53,39.996"]
"pettarin/forced-alignment-tools" ["l"="0.534,40.013"]
"prosodylab/Prosodylab-Aligner" ["l"="0.576,40.017"]
"mozilla/DSAlign" ["l"="0.588,39.963"]
"YannickJadoul/Parselmouth" ["l"="0.466,40.056"]
"AdolfVonKleist/Phonetisaurus" ["l"="0.549,39.965"]
"sudara/awesome-juce" ["l"="1.711,37.828"]
"Tracktion/pluginval" ["l"="1.71,37.852"]
"Chowdhury-DSP/BYOD" ["l"="1.788,37.807"]
"Signalsmith-Audio/signalsmith-stretch" ["l"="1.753,37.88"]
"acids-ircam/rave_vst" ["l"="-0.01,39.941"]
"PlayVoice/lora-svc" ["l"="0.027,40.268"]
"log1stics/voice-generator-webui" ["l"="-0.024,40.219"]
"svc-develop-team/diff-svc" ["l"="-0.028,40.228"]
"hongshin/OperatingSystem" ["l"="-0.4,40.045"]
"hongshin/interesting-articles-in-software-engineering" ["l"="-0.432,40.045"]
"hongshin/LearningC" ["l"="-0.432,40.055"]
"Emotional-Text-to-Speech/dl-for-emo-tts" ["l"="0.284,40.096"]
"numediart/EmoV-DB" ["l"="0.275,40.221"]
"HLTSingapore/Emotional-Speech-Data" ["l"="0.264,40.104"]
"hash2430/pitchtron" ["l"="0.24,40.128"]
"Emotional-Text-to-Speech/hmm-for-emo-tts" ["l"="0.263,40.145"]
"keonlee9420/STYLER" ["l"="0.295,40.082"]
"ddlBoJack/Speech-Resources" ["l"="0.352,39.918"]
"cnlinxi/book-text-to-speech" ["l"="0.343,39.994"]
"speechio/BigCiDian" ["l"="0.516,39.867"]
"chenkui164/FastASR" ["l"="0.442,39.852"]
"Moon0316/T2A" ["l"="0.322,39.919"]
"WelkinYang/GradTTS" ["l"="0.276,40.062"]
"KevinMIN95/StyleSpeech" ["l"="0.228,40.056"]
"as-ideas/ForwardTacotron" ["l"="0.398,40.082"]
"facebookresearch/textlesslib" ["l"="0.281,39.992"]
"facebookresearch/vocoder-benchmark" ["l"="0.268,40.037"]
"lucidrains/natural-speech-pytorch" ["l"="0.201,40.051"]
"kahne/SpeechTransProgress" ["l"="27.934,33.922"]
"xcmyz/FastVocoder" ["l"="0.347,40.022"]
"jinhan/tacotron2-vae" ["l"="0.378,40.129"]
"yanggeng1995/vae_tacotron" ["l"="0.392,40.147"]
"ide8/tacotron2" ["l"="0.356,40.165"]
"KimythAnly/AGAIN-VC" ["l"="0.305,40.149"]
"andi611/ZeroSpeech-TTS-without-T" ["l"="0.318,40.202"]
"keonlee9420/DiffSinger" ["l"="0.2,40.064"]
"keonlee9420/WaveGrad2" ["l"="0.259,40.115"]
"keonlee9420/PortaSpeech" ["l"="0.244,40.083"]
"keonlee9420/Comprehensive-E2E-TTS" ["l"="0.221,40.091"]
"keonlee9420/Cross-Speaker-Emotion-Transfer" ["l"="0.186,40.07"]
"choiHkk/CVAEJETS" ["l"="0.22,40.106"]
"microsoft/VQ-Diffusion" ["l"="34.448,28.972"]
"lucidrains/x-clip" ["l"="31.8,34.796"]
"MishaLaskin/vqvae" ["l"="24.964,32.524"]
"lucidrains/x-transformers" ["l"="27.599,30.992"]
"karpathy/deep-vector-quantization" ["l"="24.952,32.545"]
"google/REAPER" ["l"="0.521,40.147"]
"kuangdd/ttskit" ["l"="0.386,39.996"]
"atomicoo/FCH-TTS" ["l"="0.362,40.01"]
"rishikksh20/TFGAN" ["l"="0.314,40.029"]
"ga642381/FastSpeech2" ["l"="0.293,39.979"]
"sp-uhh/sgmse" ["l"="2.481,39.335"]
"jzi040941/PercepNet" ["l"="2.457,39.236"]
"seungheondoh/music-text-representation" ["l"="0.098,39.884"]
"qiniu/qshell" ["l"="0.829,38.845"]
"qiniu/QSunSync" ["l"="0.799,38.8"]
"qiniu/js-sdk" ["l"="0.825,38.926"]
"qiniu/php-sdk" ["l"="-27.035,-42.897"]
"qiniu/python-sdk" ["l"="16.312,5.187"]
"qiniu/kodo-browser" ["l"="0.843,38.815"]
"zgldh/qiniu-laravel-storage" ["l"="-27.023,-42.82"]
"widuu/qiniu_ueditor_1.4.3" ["l"="0.805,38.874"]
"fooleap/disqus-php-api" ["l"="-36.664,-15.001"]
"gyk001/hexo-qiniu-sync" ["l"="-36.323,-15.114"]
"qiniu/logkit" ["l"="-12.593,1.381"]
"Suxiaogang/WeiboPicBed" ["l"="0.846,38.73"]
"oott123/bpcs_uploader" ["l"="-39.636,-17.633"]
"mysql-inception/inception" ["l"="-13.045,3.281"]
"qiniu/nodejs-sdk" ["l"="0.847,38.886"]
"qiniu/qlang" ["l"="-12.705,1.541"]
"lazydevyo/SpleetGUI" ["l"="0.18,40.329"]
"thooore/SpleeterGUI" ["l"="0.208,40.341"]
"diracdeltas/spleeter4max" ["l"="2.984,37.367"]
"wxbool/video-srt-windows" ["l"="-27.382,-17.883"]
"LuckyHookin/edge-TTS-record" ["l"="-27.499,-18"]
"Justin62628/Squirrel-RIFE" ["l"="-29.707,-18.851"]
"azuwis/pianotrans" ["l"="1.9,38.494"]
"TianZerL/Anime4KCPP" ["l"="-29.713,-18.884"]
"xifangczy/cat-catch" ["l"="-27.334,-17.837"]
"X-Lucifer/AI-Lossless-Zoomer" ["l"="-27.532,-17.868"]
"vooidzero/B23Downloader" ["l"="-27.384,-17.837"]
"dream7180/foobox-cn" ["l"="-27.393,-17.643"]
"yantaisa11/Retrieval-based-Voice-Conversion-WebUI-JP-localization" ["l"="0.063,40.315"]
"iceychris/LibreASR" ["l"="0.554,39.832"]
"theblackcat102/Online-Speech-Recognition" ["l"="0.558,39.775"]
"midas-research/audino" ["l"="0.479,39.859"]
"Music-and-Culture-Technology-Lab/omnizart" ["l"="1.796,38.498"]
"spotify/basic-pitch-ts" ["l"="-0.012,39.986"]
"idebtor/nowic" ["l"="-0.453,40.034"]
"idebtor/JoyAI" ["l"="-0.484,40.033"]
"ccoreilly/LocalSTT" ["l"="0.831,39.885"]
"alphacep/vosk-android-service" ["l"="0.842,39.905"]
"just-ai/aimybox-android-sdk" ["l"="0.969,39.918"]
"fumiama/MoeGoe-Android" ["l"="-0.03,40.298"]
"CjangCjengh/krkrFgiEditor" ["l"="-0.058,40.249"]
"robin1001/xdecoder" ["l"="0.809,39.829"]
"Suhee05/Text-Independent-Speaker-Verification" ["l"="0.352,39.692"]
"nicklashansen/voice-activity-detection" ["l"="0.49,39.692"]
"voithru/voice-activity-detection" ["l"="0.487,39.627"]
"wangshub/python-vad" ["l"="0.479,39.678"]
"jymsuper/VAD_tutorial" ["l"="0.486,39.661"]
"zhenghuatan/rVAD" ["l"="0.455,39.596"]
"qiniu/android-sdk" ["l"="0.928,38.982"]
"vincentherrmann/pytorch-wavenet" ["l"="0.565,40.132"]
"dhpollack/fast-wavenet.pytorch" ["l"="0.669,40.171"]
"golbin/WaveNet" ["l"="0.66,40.195"]
"Dankrushen/Wavenet-PyTorch" ["l"="0.611,40.175"]
"ttizze/BabyDORA" ["l"="0.035,40.45"]
"unity3d-jp/AnimeToolbox" ["l"="-12.92,40.245"]
"fishslot/video_loopback_for_webui" ["l"="34.527,29.352"]
"TREE-Ind/Blender-GPT" ["l"="0.015,40.437"]
"AUTOMATIC1111/stable-diffusion-webui-rembg" ["l"="34.481,29.397"]
"mattyamonaca/layerdivider" ["l"="34.531,29.385"]
"vrchat-community/osc" ["l"="-14.711,41.868"]
"uezo/gpt3-contextual" ["l"="0.051,40.429"]
"tori29umai0123/VRM_AI" ["l"="0.035,40.43"]
"r4victor/syncabook" ["l"="0.756,40.083"]
"papoteur-mga/elograf" ["l"="0.894,39.958"]
"phil294/AHK_X11" ["l"="-8.759,-20.072"]
"cursorless-dev/cursorless" ["l"="-25.659,-15.503"]
"synesthesiam/voice2json" ["l"="2.696,39.736"]
"sstadick/hck" ["l"="-12.009,-18.399"]
"rvaiya/warpd" ["l"="-11.403,-20.652"]
"rhasspy/larynx" ["l"="2.742,39.699"]
"queer/boxxy" ["l"="34.37,-28.047"]
"abb128/LiveCaptions" ["l"="-36.299,3.491"]
"harporoeder/ebpfsnitch" ["l"="-11.593,1.586"]
"iwillwen/qiniu.js" ["l"="0.813,38.955"]
"gpake/qiniu-wxapp-sdk" ["l"="14.597,-8.917"]
"moxiecode/plupload" ["l"="11.119,-33.361"]
"tmallfe/tmallfe.github.io" ["l"="11.501,-7.081"]
"lsxiao/qiniu4js" ["l"="0.787,38.928"]
"JacksonTian/eventproxy" ["l"="11.417,-7.126"]
"lenage/react-qiniu" ["l"="0.861,38.925"]
"qiniu/nodejs-sdk.v6" ["l"="12.52,-11.387"]
"guo-yu/koa-guide" ["l"="11.359,-6.969"]
"ddPn08/rvc-webui-colab" ["l"="0.047,40.349"]
"ashen-sensored/sd_webui_gligen" ["l"="34.641,29.262"]
"teftef6220/Voice_Separation_and_Selection" ["l"="0.041,40.358"]
"Jam3/voice-activity-detection" ["l"="0.548,39.484"]
"kdavis-mozilla/vad.js" ["l"="0.552,39.457"]
"JoFrhwld/FAVE" ["l"="0.656,40.048"]
"mlml/autovot" ["l"="0.727,40.08"]
"meienberger/runtipi" ["l"="-32.628,-22.925"]
"mikeroyal/Photogrammetry-Guide" ["l"="30.205,42.799"]
"sensity-ai/dot" ["l"="34.205,28.834"]
"webrcade/webrcade" ["l"="-43.491,14.701"]
"jetpack-io/devbox" ["l"="34.54,-28.213"]
"serhack/pdf-diff" ["l"="24.255,3.727"]
"pedrozath/coltrane" ["l"="1.564,37.814"]
"erdewit/HiFiScan" ["l"="34.275,-28.109"]
"LuanRT/YouTube.js" ["l"="29.428,-27.326"]
"7thSamurai/steganography" ["l"="34.4,-28.244"]
"anufrievroman/calcure" ["l"="-12.177,-18.11"]
"mprimi/portable-secret" ["l"="34.452,-28.246"]
"adriancooney/puppeteer-heap-snapshot" ["l"="22.011,-28.676"]
"hmartiro/riffusion-inference" ["l"="0.12,39.926"]
"google-research/frame-interpolation" ["l"="35.154,31.508"]
"VoltaML/voltaML-fast-stable-diffusion" ["l"="34.461,29.255"]
"kakaobrain/karlo" ["l"="34.495,29.152"]
"lkwq007/stablediffusion-infinity" ["l"="34.324,29.15"]
"deforum-art/deforum-for-automatic1111-webui" ["l"="34.403,29.247"]
"devilismyfriend/StableTuner" ["l"="34.437,29.278"]
"ashawkey/stable-dreamfusion" ["l"="34.321,29.078"]
"magenta/music-spectrogram-diffusion" ["l"="0.075,39.932"]
"SHI-Labs/Versatile-Diffusion" ["l"="34.492,29.078"]
"archinetai/audio-diffusion-pytorch-trainer" ["l"="0.038,39.918"]
"archinetai/audio-data-pytorch" ["l"="-0.014,39.895"]
"coqui-ai/STT-examples" ["l"="0.524,39.934"]
"Kinyugo/msanii" ["l"="0.029,39.905"]
"mindslab-ai/nuwave2" ["l"="0.136,40.046"]
"corticph/prefix-beam-search" ["l"="0.635,39.726"]
"awni/transducer" ["l"="0.598,39.768"]
"HawkAaron/RNN-Transducer" ["l"="0.623,39.769"]
"kensho-technologies/pyctcdecode" ["l"="0.545,39.76"]
"HawkAaron/E2E-ASR" ["l"="0.607,39.758"]
"Sundy1219/ctc_beam_search_lm" ["l"="0.707,39.671"]
"m-hayabusa/VRChat-Exif-Writer" ["l"="0.094,40.396"]
"malaybaku/VMagicMirror" ["l"="-14.867,41.824"]
"letiantian/Pinyin2Hanzi" ["l"="0.72,40.02"]
"letiantian/ChineseTone" ["l"="0.631,39.99"]
"LiuRoy/Pinyin_Demo" ["l"="0.79,40.034"]
"crownpku/Somiao-Pinyin" ["l"="31.388,28.371"]
"liuhuanyong/Pinyin2Chinese" ["l"="0.817,40.034"]
"mozillazg/python-pinyin" ["l"="31.936,30.247"]
"wdimmy/Automatic-Corpus-Generation" ["l"="31.524,28.707"]
"mozillazg/pinyin-data" ["l"="0.564,39.984"]
"Kyubyong/neural_chinese_transliterator" ["l"="31.387,28.404"]
"bojone/word-discovery" ["l"="32.198,30.21"]
"taiqing/pinyin2hanzi" ["l"="31.366,28.382"]
"sunpinyin/sunpinyin" ["l"="-34.189,-18.88"]
"ccheng16/correction" ["l"="31.447,28.753"]
"AndreyGuzhov/AudioCLIP" ["l"="0.148,39.888"]
"descriptinc/lyrebird-wav2clip" ["l"="0.095,39.845"]
"microsoft/CLAP" ["l"="0.156,39.923"]
"ArrowLuo/CLIP4Clip" ["l"="31.728,33.779"]
"GeWu-Lab/awesome-audiovisual-learning" ["l"="26.502,-20.575"]
"v-iashin/SpecVQGAN" ["l"="0.14,39.92"]
"AndreyGuzhov/ESResNeXt-fbsp" ["l"="0.117,39.843"]
"facebookresearch/SLIP" ["l"="31.756,34.811"]
"baidu-research/deep-voice" ["l"="0.644,40.299"]
"soroushmehr/sampleRNN_ICLR2017" ["l"="0.593,40.202"]
"thuhcsi/Crystal.TTVS" ["l"="0.431,40.125"]
"Zeqiang-Lai/Prosody_Prediction" ["l"="0.494,40.055"]
"Liu-Feng-deeplearning/TTS-frontend" ["l"="0.505,40.066"]
"sony/ai-research-code" ["l"="0.113,40.107"]
"ws-choi/Conditioned-Source-Separation-LaSAFT" ["l"="0.003,40.079"]
"tky823/DNN-based_source_separation" ["l"="2.535,39.147"]
"nussl/nussl" ["l"="1.665,38.396"]
"bigbrother666sh/Awada" ["l"="0.026,40.411"]
"thu-coai/EVA" ["l"="32.172,30.483"]
"yangjianxin1/CPM" ["l"="32.139,30.452"]
"yangjianxin1/Firefly" ["l"="27.373,31.3"]
"esbatmop/MNBVC" ["l"="27.46,31.252"]
"Torsion-Audio/Scyclone" ["l"="-0.08,39.871"]
"XavierXiao/Dreambooth-Stable-Diffusion" ["l"="34.345,29.091"]
"rinongal/textual_inversion" ["l"="34.377,29.081"]
"ShivamShrirao/diffusers" ["l"="34.362,29.187"]
"chineseocr/darknet-ocr" ["l"="29.582,34.156"]
"chineseocr/table-ocr" ["l"="29.706,34.139"]
"Sierkinhane/crnn_chinese_characters_rec" ["l"="29.499,34.129"]
"WenmuZhou/PSENet.pytorch" ["l"="29.598,34.082"]
"mozilla/DeepSpeech-examples" ["l"="0.723,39.888"]
"AASHISHAG/deepspeech-german" ["l"="0.719,39.931"]
"MainRo/deepspeech-server" ["l"="0.889,39.875"]
"tarekeldeeb/DeepSpeech-Quran" ["l"="0.866,39.886"]
"AIWintermuteAI/DeepSpeech_RaspberryPi4_Hotword" ["l"="0.799,39.883"]
"DanBmh/deepspeech-german" ["l"="0.755,39.912"]
"harlanhong/awesome-talking-head-generation" ["l"="35.072,31.602"]
"mallorbc/whisper_mic" ["l"="26.95,31.559"]
"megvii-research/CoNR" ["l"="34.619,29.27"]
"SkyTNT/anime-segmentation" ["l"="34.535,29.373"]
"ShuhongChen/panic3d-anime-reconstruction" ["l"="33.588,43.27"]
"sudosilico/sample-diffusion" ["l"="-0.004,39.861"]
"zqevans/audio-diffusion" ["l"="0.03,39.879"]
"deforum/stable-diffusion" ["l"="34.342,29.194"]
"crowsonkb/k-diffusion" ["l"="34.459,29.027"]
"huseinzol05/malay-dataset" ["l"="-0.036,40.074"]
"huseinzol05/malaya" ["l"="0.018,40.073"]
"pannous/caffe-speech-recognition" ["l"="0.759,39.797"]
"llSourcell/tensorflow_speech_recognition_demo" ["l"="0.735,39.826"]
"baidu-research/ba-dls-deepspeech" ["l"="0.719,39.814"]
"chrisdonahue/sheetsage" ["l"="-0.026,39.876"]
"cheriell/PM2S" ["l"="-0.057,39.857"]
"interactiveaudiolab/penn" ["l"="0.093,39.902"]
"brentspell/torch-yin" ["l"="0.026,39.852"]
"hamiltron/py-simple-audio" ["l"="0.874,40.076"]
"spatialaudio/python-sounddevice" ["l"="1.559,38.584"]
"jleb/pyaudio" ["l"="1.352,38.54"]
"Kyubyong/g2pC" ["l"="0.508,39.979"]
"cmusphinx/pocketsphinx-ios-demo" ["l"="0.795,40.005"]
"galrom/ContinuesVoiceRecognition" ["l"="0.859,39.993"]
"vikramezhil/DroidSpeech" ["l"="52.82,-3.506"]
"cmusphinx/cmudict-tools" ["l"="0.75,40.007"]
"PlayVoice/so-vits-svc" ["l"="-0.045,40.199"]
"SUC-DriverOld/so-vits-svc-Chinese-Detaild-Documents" ["l"="-0.112,40.197"]
"kylebgorman/wikipron" ["l"="0.354,39.956"]
"pettarin/ipapy" ["l"="0.379,39.966"]
"lingjzhu/CharsiuG2P" ["l"="0.387,39.979"]
"open-dict-data/ipa-dict" ["l"="0.513,40.005"]
"igormq/ctc_tensorflow_example" ["l"="0.71,39.787"]
"jonrein/tensorflow_CTC_example" ["l"="0.766,39.751"]
"philipperemy/tensorflow-ctc-speech-recognition" ["l"="0.726,39.78"]
"synckey/tensorflow_lstm_ctc_ocr" ["l"="29.373,34.104"]
"amaas/stanford-ctc" ["l"="0.751,39.764"]
"igormq/asr-study" ["l"="0.817,39.765"]
"vrenkens/tfkaldi" ["l"="0.688,39.769"]
"WindQAQ/listen-attend-and-spell" ["l"="0.664,39.754"]
"yajiemiao/eesen" ["l"="0.669,39.784"]
"mphilli/English-to-IPA" ["l"="0.639,40.042"]
"aparrish/pronouncingpy" ["l"="0.724,40.065"]
"sweetcocoa/pop2piano" ["l"="0.017,39.934"]
"wenet-e2e/speech-synthesis-paper" ["l"="0.236,40.037"]
"guan-yuan/Awesome-Singing-Voice-Synthesis-and-Singing-Voice-Conversion" ["l"="0.125,40.084"]
"DigitalPhonetics/IMS-Toucan" ["l"="0.252,40.07"]
"yl4579/StarGANv2-VC" ["l"="0.233,40.113"]
"wenet-e2e/speech-recognition-papers" ["l"="0.328,39.936"]
"k2-fsa/sherpa-onnx" ["l"="0.477,39.781"]
"k2-fsa/sherpa-ncnn" ["l"="0.474,39.794"]
"csukuangfj/kaldifeat" ["l"="0.495,39.784"]
"k2-fsa/multi_quantization" ["l"="0.479,39.753"]
"wenet-e2e/wekws" ["l"="2.667,39.697"]
"RapidAI/RapidASR" ["l"="0.442,39.8"]
"FeiGeChuanShu/GFPGAN-ncnn" ["l"="0.451,39.736"]
"FeiGeChuanShu/ncnn_paddleocr" ["l"="31.562,36.259"]
"alibaba-damo-academy/FunASR" ["l"="0.471,39.829"]
"pnnx/pnnx" ["l"="31.552,36.291"]
"M4Singer/M4Singer" ["l"="0.112,40.088"]
"nnsvs/nnsvs" ["l"="0.091,40.091"]
"yl4579/PitchExtractor" ["l"="0.136,40.114"]
"chomeyama/SiFiGAN" ["l"="0.079,40.087"]
"SongRongLee/mir-svc" ["l"="0.141,40.125"]
"dhchoi99/NANSY" ["l"="0.129,40.098"]
"rishikksh20/Avocodo-pytorch" ["l"="0.186,40.091"]
"Rongjiehuang/Multi-Singer" ["l"="0.094,40.075"]
"hueitan/javascript-sdk-design" ["l"="0.742,38.895"]
"sarbbottam/write-an-open-source-js-lib" ["l"="0.718,38.864"]
"kuitos/import-html-entry" ["l"="13.947,-8.227"]
"seriousben/embeddable-react-widget" ["l"="26.929,16.287"]
"umijs/father" ["l"="13.963,-8.244"]
"kentcdodds/react-jest-workshop" ["l"="0.697,38.89"]
"mjavascript/mastering-modular-javascript" ["l"="22.605,-26.562"]
"kslz/sound_dataset_tools2" ["l"="-0.078,40.244"]
"rotten-work/vits-mandarin-windows" ["l"="-0.086,40.226"]
"ncsoft/avocodo" ["l"="0.176,40.06"]
"Xflick/EEND_PyTorch" ["l"="0.31,39.764"]
"desh2608/dover-lap" ["l"="0.308,39.773"]
"cvqluu/simple_diarizer" ["l"="0.243,39.743"]
"juanmc2005/rttm-viewer" ["l"="0.26,39.756"]
"nttcslab-sp/EEND-vector-clustering" ["l"="0.323,39.783"]
"dihardchallenge/dihard3_baseline" ["l"="0.252,39.779"]
"praat/praat" ["l"="0.512,40.053"]
"timmahrt/praatIO" ["l"="0.487,40.045"]
"kylebgorman/textgrid" ["l"="0.518,40.036"]
"covarep/covarep" ["l"="-1.061,39.954"]
"GANtastic3/MaskCycleGAN-VC" ["l"="0.28,40.144"]
"mindslab-ai/assem-vc" ["l"="0.241,40.093"]
"SungFeng-Huang/Meta-TTS" ["l"="0.244,40.052"]
"tencent-ailab/3m-asr" ["l"="0.524,39.784"]
"csukuangfj/optimized_transducer" ["l"="0.553,39.747"]
"jctian98/e2e_lfmmi" ["l"="0.51,39.806"]
"cvqluu/GE2E-Loss" ["l"="0.315,39.679"]
"patrickvonplaten/Wav2Vec2_PyCTCDecode" ["l"="0.558,39.709"]
"farisalasmary/wav2vec2-kenlm" ["l"="0.564,39.696"]
"jzlianglu/pykaldi2" ["l"="0.58,39.809"]
"prosegrinder/python-cmudict" ["l"="0.714,40.041"]
"aparrish/pycorpora" ["l"="-4.625,-44.437"]
"aparrish/rwet" ["l"="15.267,-30.756"]
"aparrish/gutenberg-poetry-corpus" ["l"="15.225,-30.795"]
"alphacep/vosk-asterisk" ["l"="0.78,39.877"]
"alphacep/kaldi-android-demo" ["l"="0.722,39.833"]
"unispeech/unimrcp" ["l"="-27.959,11.046"]
"danpovey/k2" ["l"="0.598,39.789"]
"usernaamee/keras-wavenet" ["l"="0.687,40.186"]
"Zeta36/tensorflow-tex-wavenet" ["l"="0.62,40.199"]
"PrajitR/fast-pixel-cnn" ["l"="33.775,32.482"]
"umeng/UMAndroidSdkDemo" ["l"="1.089,38.916"]
"MobClub/ShareSDK-for-Android" ["l"="1.123,38.854"]
"umeng/MultiFunctionAndroidMavenDemo-master" ["l"="1.092,38.859"]
"lukhy/masr" ["l"="0.715,39.731"]
"tzyll/kaldi" ["l"="0.775,39.682"]
"maysrp/webdir" ["l"="-39.407,-17.824"]
"trytofix/hexo_weibo_image" ["l"="0.818,38.722"]
"dbbbit/ninja-search" ["l"="0.822,38.696"]
"Suxiaogang/doubanXbaidu" ["l"="0.871,38.717"]
"consatan/weibo_image_uploader" ["l"="-34.455,-13.185"]
"yujiandong/simpleforum" ["l"="-24.602,-39.455"]
"ilaria-manco/multimodal-ml-music" ["l"="0.066,39.867"]
"ilaria-manco/muscall" ["l"="0.011,39.827"]
"jeffreyjohnens/MetaMIDIDataset" ["l"="1.84,38.514"]
"annahung31/EMOPIA" ["l"="1.855,38.503"]
"1ytic/warp-rnnt" ["l"="0.578,39.771"]
"sooftware/RNN-Transducer" ["l"="0.628,39.71"]
"oov/aviutl_psdtoolkit" ["l"="-26.302,13.597"]
"r9y9/ttslearn" ["l"="25.497,31.829"]
"google/budoux" ["l"="-10.814,-23.788"]
"amate/InputPipePlugin" ["l"="-26.293,13.606"]
"googlefonts/morisawa-biz-ud-mincho" ["l"="-10.923,-23.89"]
"auspicious3000/AutoPST" ["l"="0.285,40.119"]
"zw76859420/ASR_WORD" ["l"="0.699,39.746"]
"zw76859420/ASRT_SpeechRecognition" ["l"="0.734,39.713"]
"oubowu/PinnedSectionItemDecoration" ["l"="48.545,1.408"]
"abelyao/qiniu-file-for-typecho" ["l"="0.779,38.868"]
"widuu/utf8_qiniu_ueditor" ["l"="0.786,38.85"]
"zcxu-eric/AVA-AVD" ["l"="0.259,39.732"]
"rakeshvar/rnn_ctc" ["l"="0.809,39.746"]
"mohammadpz/CTC-Connectionist-Temporal-Classification" ["l"="0.799,39.732"]
"rizar/attention-lvcsr" ["l"="0.696,39.78"]
"skaae/Lasagne-CTC" ["l"="0.784,39.726"]
"zxie/nn" ["l"="0.784,39.739"]
"shawntan/theano-ctc" ["l"="0.823,39.738"]
"sherjilozair/ctc" ["l"="0.818,39.721"]
"thomasmesnard/CTC-LSTM" ["l"="0.798,39.718"]
"hirofumi0810/asr_preprocessing" ["l"="0.734,39.766"]
"vrenkens/nabu" ["l"="0.7,39.763"]
"rwth-i6/returnn" ["l"="0.609,39.774"]
"Kyubyong/tacotron_asr" ["l"="0.72,40.001"]
"thomasschmied/Speech_Recognition_with_Tensorflow" ["l"="0.627,39.796"]
"yufan-aslp/AliMeeting" ["l"="0.292,39.774"]
"pyannote/pyannote-metrics" ["l"="0.304,39.74"]
"Dannynis/xvector_pytorch" ["l"="0.324,39.708"]
"bjfu-ai-institute/speaker-recognition-papers" ["l"="0.348,39.673"]
"Aurora11111/speaker-recognition-pytorch" ["l"="0.327,39.655"]
"mdangschat/ctc-asr" ["l"="0.653,39.734"]
"shelling203/SpecAugment" ["l"="0.517,39.754"]
"Minami-Yuduru/-ChatGPT_VITS" ["l"="-0.04,40.259"]
"Baidu-AIP/php-sdk" ["l"="0.751,39.319"]
"Baidu-AIP/QuickStart" ["l"="0.784,39.33"]
"FlorianKrey/DNC" ["l"="0.322,39.746"]
"tango4j/Auto-Tuning-Spectral-Clustering" ["l"="0.312,39.755"]
"Jamiroquai88/VBDiarization" ["l"="0.323,39.733"]
"sealtalk/sealtalk-server" ["l"="1.155,39.023"]
"sealtalk/sealtalk-web" ["l"="1.198,39.036"]
"rongcloud/server-sdk-java" ["l"="1.191,39.005"]
"sealtalk/sealtalk-desktop" ["l"="1.174,39.037"]
"sealtalk/sealtalk-ios" ["l"="1.194,38.968"]
"sealtalk/sealtalk-android" ["l"="48.494,1.38"]
"ddlBoJack/Awesome-Speech-Pretraining" ["l"="0.342,39.884"]
"hit-thusz-RookieCJ/FullSubNet-plus" ["l"="2.484,39.274"]
"double22a/speech_dataset" ["l"="0.41,39.868"]
"Ryuk17/SpeechAlgorithms" ["l"="2.44,39.237"]
"yamachu/VoicevoxEngineSharp" ["l"="0.174,40.43"]
"thooore/SpleeterCore" ["l"="0.223,40.366"]
"innnky/MB-iSTFT-VITS" ["l"="-0.058,40.263"]
"AliceNavigator/AI-Vtuber-chatglm" ["l"="-0.048,40.385"]
"MelissaChen15/control-vc" ["l"="0.177,40.153"]
"b04901014/UUVC" ["l"="0.142,40.168"]
"aalto-speech/speaker-diarization" ["l"="0.334,39.697"]
"aalto-speech/AaltoASR" ["l"="0.3,39.63"]
"yinruiqing/change_detection" ["l"="0.265,39.673"]
"egonina/pycasp" ["l"="0.281,39.639"]
"kensun0/Parallel-Wavenet" ["l"="0.475,40.23"]
"zhf459/P_wavenet_vocoder" ["l"="0.473,40.244"]
"A-Jacobson/tacotron2" ["l"="0.503,40.253"]
"azraelkuan/tensorflow_wavenet_vocoder" ["l"="0.491,40.246"]
"nii-yamagishilab/TSNetVocoder" ["l"="0.491,40.232"]
"azraelkuan/FFTNet" ["l"="0.491,40.267"]
"ksw0306/WaveVAE" ["l"="0.441,40.209"]
"bstriner/keras-adversarial" ["l"="33.895,32.642"]
"matthiasplappert/keras-rl" ["l"="24.894,35.902"]
"jacobgil/keras-dcgan" ["l"="33.797,32.619"]
"coreylynch/async-rl" ["l"="24.867,35.873"]
"farizrahman4u/seq2seq" ["l"="31.273,31.598"]
"ryokamoi/ppg_vc" ["l"="0.343,40.243"]
"dipjyoti92/SC-WaveRNN" ["l"="0.336,40.13"]
"KunZhou9646/Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT" ["l"="0.308,40.241"]
"KunZhou9646/seq2seq-EVC" ["l"="0.299,40.233"]
"patrickltobing/cyclevae-vc-neuralvoco" ["l"="0.308,40.162"]
"hrbigelow/ae-wavenet" ["l"="0.427,40.238"]
"nii-yamagishilab/Extended_VQVAE" ["l"="0.457,40.252"]
"asuni/wavelet_prosody_toolkit" ["l"="0.422,40.108"]
"r9y9/nnmnkwii_gallery" ["l"="0.47,40.182"]
"r9y9/nnsvs" ["l"="35.406,36.29"]
"r9y9/tacotron_pytorch" ["l"="0.425,40.168"]
"guanlongzhao/fac-via-ppg" ["l"="0.301,40.222"]
"google/tacotron" ["l"="0.499,40.13"]
"swasun/VQ-VAE-Speech" ["l"="0.395,40.217"]
"DongyaoZhu/VQ-VAE-WaveNet" ["l"="0.411,40.264"]
"JeremyCCHsu/vqvae-speech" ["l"="0.384,40.269"]
"yjlolo/vae-audio" ["l"="0.414,40.31"]
"bshall/VectorQuantizedCPC" ["l"="0.32,40.099"]
"ritheshkumar95/pytorch-vqvae" ["l"="24.999,32.568"]
"jaywalnut310/waveglow-vqvae" ["l"="0.421,40.253"]
"rishikksh20/gmvae_tacotron" ["l"="0.402,40.154"]
"maozhiqiang/wavernn" ["l"="0.47,40.272"]
"neonbjb/ocotillo" ["l"="0.451,40.31"]
"thorstenMueller/Thorsten-Voice" ["l"="0.434,40.276"]
"haoheliu/audioldm_eval" ["l"="0.13,39.893"]
"haoheliu/diffres-python" ["l"="0.107,39.859"]
"gudgud96/frechet-audio-distance" ["l"="0.16,39.878"]
"Labbeti/aac-datasets" ["l"="0.124,39.858"]
"clovaai/deep-text-recognition-benchmark" ["l"="29.587,34.136"]
"githubharald/WordSegmentation" ["l"="0.839,39.604"]
"githubharald/DeslantImg" ["l"="0.854,39.589"]
"solivr/tf-crnn" ["l"="29.417,34.067"]
"Grzego/handwriting-generation" ["l"="30.973,32.2"]
"githubharald/WordDetectorNN" ["l"="0.818,39.607"]
"wenet-e2e/WenetSpeech" ["l"="0.465,39.854"]
"SpeechColab/Leaderboard" ["l"="0.487,39.839"]
"wenet-e2e/wenet-kws" ["l"="2.66,39.637"]
"harshavkumar/word_segmentation" ["l"="0.846,39.578"]
"qiniudemo/qiniu-lab-android" ["l"="0.934,38.959"]
"YuanGongND/psla" ["l"="0.182,39.826"]
"nttcslab/msm-mae" ["l"="0.153,39.859"]
"AlanBaade/MAE-AST-Public" ["l"="0.168,39.866"]
"YuanGongND/cav-mae" ["l"="0.138,39.826"]
"nttcslab/byol-a" ["l"="0.154,39.793"]
"xyzlf/ShareSDK" ["l"="1.143,38.834"]
"dpirch/libfvad" ["l"="0.514,39.699"]
"cpuimage/WebRTC_VAD" ["l"="2.269,39.272"]
"jitsi/jitsi-webrtc-vad-wrapper" ["l"="0.537,39.616"]
"cotinyang/MRCP-Plugin-Demo" ["l"="-27.915,11.048"]
"voixen/voixen-vad" ["l"="0.538,39.555"]
"shiweixingcn/vad" ["l"="0.508,39.642"]
"shichaog/WebRTC-audio-processing" ["l"="2.357,39.268"]
"dtx525942103/vits-singing-voice-synthesis" ["l"="-0.072,40.194"]
"zhangyongmao/VISinger2" ["l"="0.075,40.066"]
"hhguo/EA-SVC" ["l"="0.19,40.114"]
"rongcloud/websdk-demo" ["l"="1.217,39.053"]
"rongcloud/rongcloud-web-im-sdk-v2" ["l"="1.237,39.044"]
"UEhQZXI/vits_chinese" ["l"="-0.141,40.243"]
"PlayVoice/VI-Speaker" ["l"="0.046,40.15"]
"fishaudio/audio-preprocess" ["l"="0.029,40.168"]
"yxlllc/pc-ddsp" ["l"="0.009,40.164"]
"yeyupiaoling/VoiceprintRecognition-Tensorflow" ["l"="0.204,39.751"]
"yeyupiaoling/Kersa-Speaker-Recognition" ["l"="0.172,39.715"]
"yeyupiaoling/VoiceprintRecognition-Pytorch" ["l"="0.267,39.769"]
"SunYanCN/Voiceprint-Recognition" ["l"="0.152,39.725"]
"Kevinnan-teen/Speaker-Recognition" ["l"="0.22,39.74"]
"fighting41love/zhvoice" ["l"="0.267,39.824"]
"LCF2764/speaker-feature-extractor" ["l"="0.172,39.73"]
"yeyupiaoling/AudioClassification-Tensorflow" ["l"="0.148,39.706"]
"SiddGururani/Pytorch-TDNN" ["l"="0.295,39.674"]
"kefirski/pytorch_TDNN" ["l"="0.289,39.653"]
"jonasvdd/TDNN" ["l"="0.279,39.661"]
"jtkim-kaist/Speech-enhancement" ["l"="2.489,39.262"]
"anicolson/DeepXi" ["l"="2.485,39.22"]
"microsoft/AEC-Challenge" ["l"="2.424,39.234"]
"nanahou/Awesome-Speech-Enhancement" ["l"="2.475,39.198"]
"Kurisu-Preston/AI-aqua-vc" ["l"="-0.034,40.238"]
"cpuimage/WebRTC_NS" ["l"="2.286,39.27"]
"xiangxyq/xiangxyq_kaldi" ["l"="0.621,39.755"]
"xiongyihui/python-webrtc-audio-processing" ["l"="2.337,39.28"]
"athena-team/athena-signal" ["l"="2.406,39.235"]
"XzaiCloud/AI-Vtuber" ["l"="-0.124,40.548"]
"XzaiCloud/AI-Vtuber-Kun" ["l"="-0.127,40.563"]
"cdfmlr/muvtuber" ["l"="-0.103,40.533"]
"Diamondfan/CTC_pytorch" ["l"="0.636,39.77"]
"jinserk/pytorch-asr" ["l"="0.703,39.695"]
"Sundy1219/eesen-for-thchs30" ["l"="0.687,39.737"]
"tbright17/kaldi-dnn-ali-gop" ["l"="0.699,39.834"]
"chenjiasheng/mwer" ["l"="0.58,39.748"]
"MarkWuNLP/SemanticMask" ["l"="0.635,39.696"]
"summerlvsong/Aggregation-Cross-Entropy" ["l"="29.594,34.041"]
"rongcloud/demo-app-imlib-live-chatroom-android" ["l"="1.235,38.999"]
"rongcloud/server-sdk-nodejs" ["l"="1.214,38.99"]
"yongxuUSTC/sednn" ["l"="2.505,39.246"]
"haoxiangsnr/Wave-U-Net-for-Speech-Enhancement" ["l"="2.528,39.218"]
"iariav/End-to-End-VAD" ["l"="0.466,39.667"]
"mozillazg/phrase-pinyin-data" ["l"="0.565,39.957"]
"mozillazg/go-pinyin" ["l"="-12.827,1.471"]
"kfcd/chaizi" ["l"="31.526,28.815"]
"hjzin/PolyphoneDisambiguation" ["l"="0.585,39.984"]
"xinglie/pinyin" ["l"="10.062,18.525"]
"TaoRuijie/Loss-Gated-Learning" ["l"="0.244,39.755"]
"ranchlai/speaker-verification" ["l"="0.284,39.76"]
"nikvaessen/w2v2-speaker" ["l"="0.272,39.788"]
"zacharywhitley/awesome-ocr" ["l"="29.694,34.224"]
"Aniket025/Medical-Prescription-OCR" ["l"="0.818,39.52"]
"arthurflor23/text-segmentation" ["l"="0.845,39.56"]
"johnsmithm/handwritten-tf-1.0" ["l"="0.839,39.529"]
"ahmetozlu/signature_extractor" ["l"="0.964,39.446"]
"emedvedev/attention-ocr" ["l"="29.422,34.13"]
"doxakis/form-segmentation" ["l"="0.807,39.533"]
"rongcloud/rongcloud-web-im-widget" ["l"="1.226,39.026"]
"NervanaSystems/deepspeech" ["l"="0.774,39.773"]
"SeanNaren/deepspeech.torch" ["l"="0.776,39.787"]
"samsungsds-rnd/deepspeech.mxnet" ["l"="0.811,39.79"]
"dspavankumar/keras-kaldi" ["l"="0.719,39.763"]
"easemob/sdkexamples-android" ["l"="1.019,39.031"]
"easemob/sdkdemoapp3.0_android" ["l"="1.043,39.013"]
"rongcloud/demo-app-android" ["l"="1.005,39.009"]
"rongcloud/demo-ui-android" ["l"="0.993,39.029"]
"rwth-i6/returnn-experiments" ["l"="0.604,39.746"]
"rwth-i6/sisyphus" ["l"="0.621,39.728"]
"rwth-i6/rasr" ["l"="0.643,39.715"]
"jpuigcerver/Laia" ["l"="0.711,39.652"]
"shibing624/parrots" ["l"="0.604,39.732"]
"hpc203/virtual_try_on_use_deep_learning" ["l"="0.182,39.764"]
"yeyupiaoling/VoiceprintRecognition-PaddlePaddle" ["l"="-0.204,39.384"]
"yeyupiaoling/VoiceprintRecognition-Keras" ["l"="0.224,39.79"]
"yeyupiaoling/AudioClassification-Pytorch" ["l"="0.178,39.688"]
"sequitur-g2p/sequitur-g2p" ["l"="0.597,39.974"]
"HuangZiliAndy/RPNSD" ["l"="0.299,39.754"]
"DonkeyShot21/uis-rnn-sml" ["l"="0.315,39.723"]
"Mashiro009/wenet-online-decoder-onnx" ["l"="0.439,39.722"]
"Mashiro009/wenet-onnx" ["l"="0.441,39.674"]
"TeaPoly/CTC-OptimizedLoss" ["l"="0.56,39.737"]
"wenet-e2e/WeTextProcessing" ["l"="0.435,39.868"]
"by2101/OpenASR" ["l"="0.574,39.739"]
"yeyupiaoling/PunctuationModel" ["l"="-0.154,39.359"]
"huismiling/wenet_trt8" ["l"="34.747,35.66"]
"iamyuanchung/Autoregressive-Predictive-Coding" ["l"="0.407,39.847"]
"bootphon/zerospeech2021_baseline" ["l"="0.359,39.864"]
"awslabs/speech-representations" ["l"="0.418,39.855"]
"jefflai108/Contrastive-Predictive-Coding-PyTorch" ["l"="34.561,36.209"]
"pzelasko/lhotse" ["l"="0.59,39.754"]
"csukuangfj/kaldilm" ["l"="0.506,39.749"]
"kaituoxu/E6870" ["l"="0.528,39.588"]
"placebokkk/e6870" ["l"="0.517,39.581"]
"iamjanvijay/rnnt_decoder_cuda" ["l"="0.582,39.721"]
"opendcd/opendcd" ["l"="0.827,39.836"]
"philipperemy/speaker-change-detection" ["l"="0.235,39.643"]
"hedonistrh/SpChangeDetect" ["l"="0.216,39.624"]
"nttcslab-sp/torchain" ["l"="0.621,39.696"]
"YiwenShaoStephen/pychain_example" ["l"="0.566,39.724"]
"timmahrt/ProMo" ["l"="0.585,40.064"]
"jcvasquezc/DisVoice" ["l"="-1.136,39.906"]
"hbuschme/TextGridTools" ["l"="0.439,40.016"]
"voithru/wav2vec2_finetune" ["l"="0.489,39.602"]
"voithru/asr-text_classification-pipeline" ["l"="0.495,39.592"]
"cornerfarmer/ctc_segmentation" ["l"="0.529,39.763"]
"EFord36/normalise" ["l"="0.727,40.103"]
"MaxMax2016/so-vits-svc" ["l"="-0.164,40.197"]
"rendchevi/nix-tts" ["l"="0.2,40.091"]
"zzsza/Boostcamp-AI-Tech-Product-Serving" ["l"="44.527,-15.104"]
"shugen002/shader" ["l"="-0.033,40.409"]
"Boostcamp-JoHan4Park/Interview-Study" ["l"="-0.044,40.404"]
"tensorflow/lingvo" ["l"="29.947,32.455"]
"alphanemeless/VITS_TXT_to_Audio" ["l"="-0.008,40.182"]
"rhasspy/gruut" ["l"="0.355,40.032"]
"german-asr/megs" ["l"="0.513,40.169"]
"monatis/german-tts" ["l"="0.517,40.176"]
"erogol/TTS-papers" ["l"="0.418,40.139"]
"jkeylu/v2ex.ext" ["l"="-39.866,-17.15"]
"VitoVan/v2excellent.js" ["l"="0.841,38.642"]
"Feiox/useless-websites" ["l"="-39.429,-17.55"]
"kokdemo/v2ex.k" ["l"="0.91,38.647"]
"satorioh/google-enhancer" ["l"="0.858,38.631"]
"Zhiqing-Lee/js-printer" ["l"="-39.776,-17.175"]
"RElbers/info-nce-pytorch" ["l"="0.237,40.186"]
"rongcloud/server-sdk-php" ["l"="1.043,39.099"]
"rongcloud/demo-server-php" ["l"="1.075,39.106"]
"julius-speech/dictation-kit" ["l"="0.821,39.928"]
"julius-speech/segmentation-kit" ["l"="0.798,39.935"]
"flashlight/wav2letter" ["l"="0.526,39.802"]
"rongcloud/demo-app-imlib-live-chatroom-ios" ["l"="1.203,38.941"]
"rongcloud/callkit-ios" ["l"="1.227,38.954"]
"drfeinberg/PraatScripts" ["l"="-1.19,39.888"]
"uhh-lt/kaldi-tuda-de" ["l"="0.809,39.898"]
"ynop/audiomate" ["l"="0.648,39.948"]
"ynop/deepspeech-german" ["l"="0.802,39.919"]
"AASHISHAG/DeepSpeech-API" ["l"="0.778,39.933"]
"OpenBMB/BMInf" ["l"="32.162,30.553"]
"clue-ai/PromptCLUE" ["l"="27.422,31.299"]
"OpenBMB/CPM-Live" ["l"="32.164,30.623"]
"microsoft/Megatron-DeepSpeed" ["l"="27.621,31.141"]
"TsinghuaAI/CPM-1-Generate" ["l"="32.186,30.452"]
"liuxp0827/govpr" ["l"="0.399,39.423"]
"liuxp0827/Jvpr" ["l"="0.396,39.402"]
"ibillxia/VoicePrintReco" ["l"="0.411,39.467"]
"dake/openVP" ["l"="0.396,39.457"]
"XinhaoMei/WavCaps" ["l"="0.043,39.895"]
"Deepest-Project/MelNet" ["l"="0.296,40.135"]
"resemble-ai/MelNet" ["l"="0.275,40.2"]
"jaeyeun97/MelNet" ["l"="0.265,40.172"]
"YuvalBecker/MelNet" ["l"="0.277,40.169"]
"MlWoo/LPCNet" ["l"="0.502,40.22"]
"mailong25/self-supervised-speech-recognition" ["l"="0.534,39.748"]
"m3hrdadfi/soxan" ["l"="-0.894,39.987"]
"dangvansam98/demo_vietasr" ["l"="0.534,39.692"]
"eastonYi/wav2vec" ["l"="0.578,39.671"]
"khanld/ASR-Wav2vec-Finetune" ["l"="0.55,39.69"]
"jonatasgrosman/huggingsound" ["l"="0.541,39.709"]
"jonatasgrosman/wav2vec2-sprint" ["l"="0.542,39.679"]
"novoic/surfboard" ["l"="-1.098,39.917"]
"iankur/ContextNet" ["l"="0.561,39.822"]
"KunZhou9646/emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0" ["l"="0.318,40.232"]
"jackaduma/LAS_Mandarin_PyTorch" ["l"="0.584,39.709"]
"yinkalario/Sound-Event-Detection-AudioSet" ["l"="0.071,39.765"]
"microsoft/WavText5K" ["l"="0.087,39.768"]
"Kikyo-16/Sound_event_detection" ["l"="1.462,39.106"]
"uber/pyro" ["l"="25.461,33.65"]
"tensorflow/tfjs-core" ["l"="28.067,27.678"]
"deepmind/sonnet" ["l"="24.865,35.774"]
"espnet/interspeech2019-tutorial" ["l"="0.55,39.727"]
"jitsi/jiwer" ["l"="0.561,39.756"]
"belambert/asr-evaluation" ["l"="0.669,39.725"]
"zszyellow/WER-in-python" ["l"="0.655,39.692"]
"nanoporetech/fast-ctc-decode" ["l"="0.588,39.731"]
"athena-team/athena-decoder" ["l"="0.581,39.761"]
"zeroQiaoba/ivector-xvector" ["l"="0.343,39.71"]
"whiteeat/ai-vtuber-alpha" ["l"="-0.044,40.477"]
"huangfangyi/FanXin3.0" ["l"="50.041,-1.892"]
"liangstein/Chinese-speech-to-text" ["l"="0.693,39.633"]
"opensourceteams/google-sdk-speech-to-text" ["l"="0.695,39.604"]
"Zws-China/VoiceToWords" ["l"="0.671,39.635"]
"Z-yq/TensorflowTTS" ["l"="0.613,39.713"]
"akoepke/audio-retrieval-benchmark" ["l"="0.126,39.877"]
"qiniu/csharp-sdk" ["l"="0.775,38.773"]
"qlemaire22/speech-music-detection" ["l"="0.425,39.677"]
"CynthiaSuwi/ASR-for-Chinese-Pipeline" ["l"="0.681,39.661"]
"just-ai/aimybox-android-assistant" ["l"="1.11,39.921"]
"just-ai/jaicf-kotlin" ["l"="1.17,39.922"]
"Executedone/Chinese-FastSpeech2" ["l"="0.065,40.15"]
"b04901014/MQTTS" ["l"="0.141,40.083"]
"Zz-ww/VITS-BigVGAN-SpanPSP-Chinese" ["l"="0.018,40.139"]
"rhasspy/larynx2" ["l"="0.038,40.137"]
"dathudeptrai/TensorflowTTS" ["l"="0.391,40.122"]
"liusongxiang/efficient_tts" ["l"="0.338,40.12"]
"tulasiram58827/TTS_TFLite" ["l"="0.402,40.235"]
"zzw922cn/LPC_for_TTS" ["l"="0.395,40.199"]
"Zeta36/tensorflow-image-wavenet" ["l"="0.655,40.242"]
"alexram1313/text-to-speech-sample" ["l"="0.518,40.21"]
"revsic/torch-nansypp" ["l"="0.057,40.105"]
"mickvanhulst/tf_chatbot_lotr" ["l"="0.821,39.816"]
"node-modules/qn" ["l"="0.887,38.852"]
"lezasantaizi/from_video_get_ASR_traindata" ["l"="0.8,39.698"]
"plison/opendial" ["l"="1.009,39.802"]
"ALIZE-Speaker-Recognition/alize-core" ["l"="0.417,39.507"]
"ALIZE-Speaker-Recognition/android-alize" ["l"="0.4,39.501"]
"DmitryUlyanov/neural-style-audio-tf" ["l"="0.55,40.3"]
"rupeshs/neuralsongstyle" ["l"="0.564,40.341"]
"vadim-v-lebedev/audio_style_tranfer" ["l"="0.56,40.359"]
"DmitryUlyanov/neural-style-audio-torch" ["l"="0.594,40.359"]
"pkmital/time-domain-neural-audio-style-transfer" ["l"="0.584,40.382"]
"sumuzhao/CycleGAN-Music-Style-Transfer" ["l"="1.84,38.609"]
"frhrdr/generative_audio" ["l"="0.548,40.331"]
"RaviSoji/plda" ["l"="0.303,39.693"]
"candlewill/Tacotron-2" ["l"="0.734,40.186"]
"JRMeyer/multi-task-kaldi" ["l"="0.767,39.713"]
"qqueing/SR_with_kaldi" ["l"="0.368,39.651"]
"yinkalario/Two-Stage-Polyphonic-Sound-Event-Detection-and-Localization" ["l"="1.437,39.108"]
"ryanwongsa/kaggle-birdsong-recognition" ["l"="1.571,39.314"]
"v0lta/Listen-attend-and-spell" ["l"="0.719,39.746"]
"fernandodelacalle/ResNet-Kaldi-Tensorflow-ASR" ["l"="0.732,39.727"]
"felixfuyihui/AISHELL-4" ["l"="0.303,39.8"]
"CZ26/CycleTransGAN-EVC" ["l"="0.299,40.265"]
"KunZhou9646/Emovox" ["l"="0.285,40.25"]
"Oknolaz/vasisualy" ["l"="0.311,39.877"]
"timhok/IreneVA-hassio-script-trigger-plugin" ["l"="0.326,39.892"]
"EnjiRouz/Voice-Assistant-App" ["l"="0.305,39.904"]
"RajSolai/TextSnatcher" ["l"="-28.773,-24.027"]
"NyanNyanovich/nyan" ["l"="0.308,39.889"]
"0x7o/text2keywords" ["l"="0.291,39.898"]
"putnik/mycroft-russian" ["l"="0.327,39.883"]
"ericmurphyxyz/rofi-wifi-menu" ["l"="-9.219,-20.416"]
"easemob/easeui" ["l"="1.067,38.973"]
"easemob/kefu-android-demo" ["l"="1.092,38.961"]
"iwillwen/node-qiniu" ["l"="0.909,38.828"]
"scelesticsiva/speaker_recognition_GMM_UBM" ["l"="0.371,39.576"]
"genzen2103/Speaker-Recognition-System-using-GMM" ["l"="0.386,39.62"]
"Anwarvic/Speaker-Recognition" ["l"="0.318,39.589"]
"sweekarsud/Goodness-of-Pronunciation" ["l"="0.792,39.821"]
"danijel3/KaldiWebrtcServer" ["l"="0.865,39.828"]
"XierHacker/Model_Fusion_Based_Prosody_Prediction" ["l"="0.536,40.045"]
"houzhengzhang/speaker_recognition" ["l"="0.35,39.648"]
"danpovey/pocolm" ["l"="0.251,39.683"]
"opendmm/opendmm" ["l"="0.798,38.674"]
"magenta/symbolic-music-diffusion" ["l"="1.891,38.527"]
"dada-bots/dadaGP" ["l"="2.815,37.896"]
"dvruette/figaro" ["l"="1.902,38.537"]
"sony/DiffRoll" ["l"="0.006,39.899"]
"YatingMusic/MuseMorphose" ["l"="1.882,38.535"]
"YatingMusic/miditoolkit" ["l"="1.839,38.53"]
"mindslab-ai/univnet" ["l"="0.233,40.071"]
"oscarknagg/voicemap" ["l"="0.362,39.594"]
"abhijeet3922/Speaker-identification-using-GMMs" ["l"="0.384,39.602"]
"SuperKogito/Voice-based-speaker-identification" ["l"="0.332,39.534"]
"iamyuanchung/VQ-APC" ["l"="0.347,39.825"]
"Alexander-H-Liu/NPC" ["l"="0.352,39.835"]
"juanmc2005/torch-plda" ["l"="0.257,39.662"]
"vzxxbacq/PLDA" ["l"="0.267,39.655"]
"dmitriy-serdyuk/kaldi-python" ["l"="0.747,39.729"]
"mila-udem/blocks-examples" ["l"="27.097,34.194"]
"yajiemiao/kaldipdnn" ["l"="0.744,39.78"]
"RayeRen/acad-homepage.github.io" ["l"="-0.058,40.057"]
"RayeRen/rayeren.github.io" ["l"="-0.103,40.057"]
"yaoyao-liu/minimal-light" ["l"="-0.161,40.061"]
"glam-imperial/EmotionalConversionStarGAN" ["l"="0.285,40.277"]
"KunZhou9646/controllable_evc_code" ["l"="0.3,40.255"]
"KrishnaDN/speech-emotion-recognition-using-self-attention" ["l"="-0.948,40.022"]
"Jeeseung-Park/Styleformer" ["l"="-0.105,40.09"]
"mindslab-ai/pnlp-mixer" ["l"="0.039,40.076"]
"younggeun-kim/NCSR" ["l"="-0.156,40.096"]
"younggeun-kim/Styleformer" ["l"="-0.137,40.094"]
"AIFanatic/google-offline-speech-recognition" ["l"="0.953,39.748"]
"biemster/gasr" ["l"="0.972,39.741"]
"jitsi/asr-wer" ["l"="0.689,39.68"]
"biemster/gtts" ["l"="1.004,39.731"]
"robmsmt/ASR_Audio_Data_Links" ["l"="0.593,39.89"]
"csukuangfj/transducer-loss-benchmarking" ["l"="0.52,39.739"]
"sooftware/End-to-End-Speech-Recognition-Models" ["l"="0.598,39.714"]
"foamliu/Speech-Transformer" ["l"="0.627,39.656"]
"imdanboy/jets" ["l"="0.188,40.123"]
"hcy71o/TransferTTS" ["l"="0.098,40.127"]
"collabora/spear-tts-pytorch" ["l"="0.077,40.129"]
"HaoranMiao/streaming-attention" ["l"="0.582,39.695"]
"csteinmetz1/steerable-nafx" ["l"="-0.033,39.951"]
"ilaria-manco/word2wave" ["l"="-0.069,39.94"]
"galgreshler/Catch-A-Waveform" ["l"="-0.09,39.948"]
"rncm-prism/prism-samplernn" ["l"="0.263,40.128"]
"hyakuchiki/diffsynth" ["l"="0.022,39.914"]
"turpaultn/DESED" ["l"="1.485,39.092"]
"nsu-ai/russian_g2p" ["l"="-9.177,20.333"]
"snakers4/russian_stt_text_normalization" ["l"="-9.199,20.324"]
"ThomasDelteil/HandwrittenTextRecognition_MXNet" ["l"="0.778,39.542"]
"amzn/convolutional-handwriting-gan" ["l"="30.936,32.227"]
"TaoRuijie/AVCleanse" ["l"="0.212,39.728"]
"callee2006/HGUNeuralNetworks" ["l"="-0.161,40.038"]
"callee2006/2019-Winter-HGU-Machine-Learing-Camp" ["l"="-0.243,40.044"]
"dome272/Paella" ["l"="34.546,29.099"]
"chavinlo/riffusion-manipulation" ["l"="0.055,39.884"]
"acids-ircam/creative_ml" ["l"="-0.088,39.917"]
"liuxubo717/sound_generation" ["l"="0.094,39.821"]
"jonatasgrosman/asrecognition" ["l"="0.573,39.648"]
"oliverguhr/wav2vec2-live" ["l"="0.561,39.675"]
"huggingface/speechbox" ["l"="27.679,31.154"]
"ccoreilly/wav2vec2-service" ["l"="0.542,39.664"]
"Edresson/Wav2Vec-Wrapper" ["l"="0.56,39.655"]
"BridgetteSong/ExpressiveTacotron" ["l"="0.103,40.042"]
"sarulab-speech/jtubespeech" ["l"="0.514,39.719"]
"xingchensong/Speech-Transformer-tf2.0" ["l"="0.648,39.6"]
"CUNY-CL/wikipron" ["l"="0.364,39.998"]
"Edresson/SC-GlowTTS" ["l"="0.279,39.955"]
"NVIDIA/NeMo-text-processing" ["l"="0.297,39.957"]
"bshall/acoustic-model" ["l"="0.12,40.17"]
"ubisoft/ubisoft-laforge-daft-exprt" ["l"="0.035,40.06"]
"quadrismegistus/prosodic" ["l"="0.653,40.267"]
"quadrismegistus/poesy" ["l"="0.692,40.296"]
"quadrismegistus/litlab-poetry" ["l"="0.679,40.288"]
"hyperreality/Poetry-Tools" ["l"="0.7,40.318"]
"manexagirrezabal/zeuscansion" ["l"="0.675,40.3"]
"facebookresearch/BinauralSpeechSynthesis" ["l"="0.315,39.976"]
"nii-yamagishilab/project-NN-Pytorch-scripts" ["l"="0.299,40.091"]
"Yablon/auorange" ["l"="0.422,40.02"]
"google-research-datasets/cvss" ["l"="27.916,33.905"]
"facebookresearch/covost" ["l"="27.912,33.929"]
"edufonseca/uclser20" ["l"="0.109,39.752"]
"andyweiqiu/asr-ios-local" ["l"="0.689,39.724"]
"andyweiqiu/SpeechRecognition" ["l"="0.731,39.692"]
"gnbaron/signature-recognition" ["l"="0.984,39.423"]
"EnzoSeason/signature_detection" ["l"="0.984,39.456"]
"amaljoseph/Signature-Verification_System_using_YOLOv5-and-CycleGAN" ["l"="1,39.445"]
"luizgh/sigver_wiwd" ["l"="1.009,39.406"]
"luizgh/sigver" ["l"="1.011,39.42"]
"watersink/ocrsegment" ["l"="29.788,34.322"]
"Aftaab99/OfflineSignatureVerification" ["l"="0.992,39.405"]
"yangdongchao/text-to-sound-synthesis-demo" ["l"="0.101,40.015"]
"yeyupiaoling/AudioClassification-PaddlePaddle" ["l"="0.133,39.648"]
"HighCWu/denoising-diffusion-paddle" ["l"="0.113,39.63"]
"mcvkhaos/GOP-LSTM" ["l"="0.774,39.817"]
"zzw922cn/awesome-speech-recognition-papers" ["l"="0.871,39.727"]
"patyork/python-deep-speech" ["l"="0.803,39.777"]
"guanlongzhao/ppg-gmm" ["l"="0.312,40.27"]
"cjerry1243/TransferLearning-CLVC" ["l"="0.31,40.282"]
"facebookresearch/gtn_applications" ["l"="0.597,39.657"]
"huyhoang17/CRNN_CTC_English_Handwriting_Recognition" ["l"="0.822,39.548"]
"sushant097/Devnagari-Handwritten-Word-Recongition-with-Deep-Learning" ["l"="0.805,39.551"]
"giovanniguidi/Seq-2-Seq-OCR" ["l"="0.826,39.558"]
"VVasanth/Spleeter_Unofficial_TF20_MobileApp" ["l"="0.204,40.358"]
"gvne/spleeterpp" ["l"="3.063,37.328"]
"ws-choi/ISMIR2020_U_Nets_SVS" ["l"="0.004,40.047"]
"chomeyama/HN-UnifiedSourceFilterGAN" ["l"="0.046,40.095"]
"laboroai/LaboroTVSpeech" ["l"="0.516,39.657"]
"festvox/festvox" ["l"="0.688,40.131"]
"festvox/speech_tools" ["l"="0.671,40.119"]
"happyalu/Flite-TTS-Engine-for-Android" ["l"="0.71,40.148"]
"Idlak/idlak" ["l"="0.687,40.119"]
"Francis-Komizu/VITS-Bilingual" ["l"="-0.073,40.211"]
"ben-hayes/neural-field-synth" ["l"="0.022,39.894"]
"gtn-org/gtn" ["l"="0.616,39.597"]
"rishikksh20/HiFiplusplus-pytorch" ["l"="0.086,40.033"]
"haoheliu/ssr_eval" ["l"="0.082,40.05"]
"andi611/Mockingjay-Speech-Representation" ["l"="0.364,39.813"]
"Anwarvic/Stanford_CS224n--NLP-with-Deep-Learning" ["l"="0.299,39.564"]
"Anwarvic/Deep-Learning-Nanodegree-Udacity-2019" ["l"="0.308,39.555"]
"Anwarvic/Deep-Learning-Specialization-2017--Coursera" ["l"="0.293,39.549"]
"vvestman/pytorch-ivectors" ["l"="0.277,39.558"]
"junjun3518/alias-free-torch" ["l"="0.149,40.063"]
"yl4579/AuxiliaryASR" ["l"="0.101,40.116"]
"yl4579/PL-BERT" ["l"="0.085,40.117"]
"xiangxyq/minimize-chain-decoder" ["l"="0.655,39.779"]
"numediart/MBROLA-voices" ["l"="0.727,40.122"]
"numediart/MBROLATOR" ["l"="0.739,40.114"]
"yoyololicon/music-demixing-challenge-ismir-2021-entry" ["l"="-0.065,40.119"]
"seaniezhao/torch_npss" ["l"="0.185,39.992"]
"MTG/WGANSing" ["l"="0.259,40.026"]
"xushengyuan/Fastsinging" ["l"="0.114,39.899"]
"flashlight/flashlight" ["l"="24.098,30.397"]
"vb000/Waveformer" ["l"="2.484,39.31"]
"joonson/voxconverse" ["l"="0.285,39.745"]
"begeekmyfriend/WaveRNN" ["l"="0.443,40.231"]
"GitYCC/g2pW" ["l"="0.396,39.96"]
"thuhcsi/FlatTN" ["l"="0.387,39.948"]
"open-speech/cn-text-normalizer" ["l"="0.613,40.049"]
"KrishnaDN/x-vector-pytorch" ["l"="0.308,39.713"]
"andreasjansson/fmsynth" ["l"="0.046,39.867"]
"acids-ircam/flow_synthesizer" ["l"="1.859,37.787"]
"yistLin/universal-vocoder" ["l"="0.335,40.013"]
"howard1337/S2VC" ["l"="0.3,40.014"]
"cyhuang-tw/AdaIN-VC" ["l"="0.288,40.05"]
"tzuhsien/Voice-conversion-evaluation" ["l"="0.314,40.046"]
"gooofy/kaldi-adapt-lm" ["l"="0.794,39.857"]
"open-dsl-dict/ipa-dict-dsl" ["l"="0.532,40.028"]
"JoseLlarena/Britfone" ["l"="0.598,40.015"]
"menelik3/cmudict-ipa" ["l"="0.58,40.001"]
"uiuc-sst/g2ps" ["l"="0.483,39.967"]
"albertaparicio/tfg-voice-conversion" ["l"="0.362,40.357"]
"shamidreza/dnnmapper" ["l"="0.366,40.393"]
"BogiHsu/Voice-Conversion" ["l"="0.362,40.246"]
"wnhsu/FactorizedHierarchicalVAE" ["l"="0.363,40.288"]
"vanstorm9/AI-Vocaloid-Kit-V2" ["l"="0.099,39.869"]
"vanstorm9/AI-vocaloid-kit" ["l"="0.05,39.833"]
"johndpope/Singing-Voice-Conversion-with-conditional-VAW-GAN" ["l"="0.109,40.149"]
"caltechlibrary/handprint" ["l"="0.893,39.56"]
"him4318/Transformer-ocr" ["l"="0.87,39.551"]
"vloison/Handwritten_Text_Recognition" ["l"="0.871,39.58"]
"arthurflor23/spelling-correction" ["l"="0.862,39.568"]
"phonexiaresearch/VBx-training-recipe" ["l"="0.283,39.735"]
"BUTSpeechFIT/EEND" ["l"="0.267,39.744"]
"BUTSpeechFIT/speakerbeam" ["l"="2.519,39.056"]
"HRNPH/AIwaifu" ["l"="0.011,40.523"]
"AlexHarker/FrameLib" ["l"="-0.127,39.914"]
"victor-shepardson/rave-supercollider" ["l"="-0.07,39.91"]
"maxfrenzel/SampleVAE" ["l"="-0.21,39.793"]
"maxfrenzel/SpectrogramVAE" ["l"="-0.163,39.814"]
"sp-nitech/SPTK" ["l"="0.004,40.008"]
"sp-nitech/diffsptk" ["l"="0.117,40.007"]
"audio-captioning/dcase-2020-baseline" ["l"="-0.136,39.866"]
"audio-captioning/audio-captioning-papers" ["l"="-0.104,39.869"]
"npuichigo/voicenet" ["l"="0.503,40.276"]
"HaiFengZeng/clari_wavenet_vocoder" ["l"="0.479,40.259"]
"So-Fann/VISinger" ["l"="0.054,40.07"]
"xingmegshuo/zhrtvc" ["l"="0.404,40.143"]
"mycrazycracy/speaker-embedding-with-phonetic-information" ["l"="0.281,39.695"]
"mycrazycracy/Backends-for-SRE19" ["l"="0.289,39.705"]
"Deepest-Project/AlignTTS" ["l"="0.336,40.099"]
"jinhan/tacotron2-gst" ["l"="0.364,40.156"]
"fschmid56/EfficientAT" ["l"="0.154,39.811"]
"DCASE-REPO/DESED_task" ["l"="1.493,39.07"]
"soham97/awesome-sound_event_detection" ["l"="0.14,39.808"]
"yistLin/human-evaluation" ["l"="0.322,39.951"]
"Tarteel-io/tarteel-ml" ["l"="0.917,39.887"]
"jadevaibhav/Signature-verification-using-deep-learning" ["l"="1.036,39.415"]
"phreeza/keras-GAN" ["l"="33.979,32.637"]
"huyouare/WaveNet-Theano" ["l"="0.755,40.221"]
"kastnerkyle/multi-speaker-tacotron-tensorflow" ["l"="0.767,40.246"]
"yanggeng1995/Multi-band-WaveRNN" ["l"="0.324,40.113"]
"entn-at/DurIAN-1" ["l"="0.332,40.108"]
"nwpuaslp/ASR_Course" ["l"="0.516,39.626"]
"tzyll/goparrot" ["l"="0.857,39.81"]
"mounalab/LSTM-RNN-VAD" ["l"="0.469,39.654"]
"xiyihong/webRTC-" ["l"="0.467,39.629"]
"oatsu-gh/ENUNU" ["l"="35.371,36.279"]
"shiyuzh2007/ASR" ["l"="0.639,39.788"]
"aaron-xichen/cnn-lstm-ctc" ["l"="0.834,39.725"]
"mwv/vad" ["l"="0.45,39.648"]
"vBaiCai/python-pesq" ["l"="2.512,39.212"]
"KunZhou9646/Mixed_Emotions" ["l"="0.26,40.194"]
"SuperKogito/SER-datasets" ["l"="-0.929,40.006"]
"icattlecoder/qiniu-csharp-sdk" ["l"="0.76,38.757"]
"CODEJIN/HiFiSinger" ["l"="0.065,40.082"]
"SJTMusicTeam/SVS_system" ["l"="0.13,40.035"]
"neosapience/mlp-singer" ["l"="0.094,40.062"]
"aimybox/aimybox-android-assistant" ["l"="1.213,39.922"]
"just-ai/panda" ["l"="1.191,39.937"]
"just-ai/jaicf-jaicp-caila-template" ["l"="1.192,39.916"]
"just-ai/jaicf-template" ["l"="1.194,39.926"]
"just-ai/alice-jaicf-template" ["l"="1.186,39.909"]
"rishikksh20/NU-Wave-pytorch" ["l"="0.062,40.058"]
"Riroaki/Chinese-Rhythm-Predictor" ["l"="0.529,40.178"]
"auspicious3000/contentvec" ["l"="0.202,40.112"]
"biggytruck/SpeechSplit2" ["l"="0.154,40.156"]
"tyiannak/basic_audio_analysis" ["l"="0.784,39.897"]
"Adirockzz95/Piwho" ["l"="0.414,39.563"]
"orctom/vad4j" ["l"="0.549,39.583"]
"CODEJIN/PWGAN_for_HiFiSinger" ["l"="0.021,40.094"]
"AzizCode92/Listen-Attend-and-Spell-Pytorch" ["l"="0.601,39.696"]
"ZhengkunTian/Speech-Tranformer-Pytorch" ["l"="0.615,39.677"]
"mbinkowski/DeepSpeechDistances" ["l"="0.395,40.182"]
"AlexHarker/HISSTools_Library" ["l"="-0.184,39.908"]
"tts-tutorial/interspeech2022" ["l"="0.122,40.024"]
"ashwan1/django-deepspeech-server" ["l"="0.946,39.872"]
"savoirfairelinux/num2words" ["l"="0.479,39.879"]
"jfilter/clean-text" ["l"="29.556,32.554"]
"PiotrTa/Huawei-Challenge-Speaker-Identification" ["l"="0.343,39.56"]
"belambert/edit-distance" ["l"="0.732,39.671"]
"dominoanty/SpeakerRecognition" ["l"="0.367,39.554"]
"SoonbeomChoi/BEGANSing" ["l"="0.136,40.059"]
"pyannote/pyannote-core" ["l"="0.263,39.716"]
"pyannote/pyannote-database" ["l"="0.244,39.711"]
"thuhcsi/NeuFA" ["l"="0.371,39.955"]
"WX-Wei/HarmoF0" ["l"="-0.045,39.983"]
"WuYiming6526/HARD" ["l"="-0.113,39.856"]
"rodrigodzf/NeuralResonatorVST" ["l"="-0.132,39.849"]
"sooftware/deepspeech2" ["l"="44.195,-15.261"]
"LeoniusChen/Attentions-in-Tacotron" ["l"="0.31,40.112"]
"thuhcsi/tacotron" ["l"="0.285,40.155"]
"bshall/Tacotron" ["l"="0.308,40.126"]
"ttaoREtw/Tacotron-pytorch" ["l"="0.344,40.215"]
"r9y9/Tacotron-2" ["l"="0.425,40.222"]
"ASzot/vq-vae-audio" ["l"="0.396,40.328"]
"pelillian/deep-learning-kd-diagnosis" ["l"="0.399,40.351"]
"deepgram/kur" ["l"="0.865,39.771"]
"jakebian/quiver" ["l"="30.96,31.56"]
"openai/cleverhans" ["l"="27.204,34.491"]
"srvk/eesen-transcriber" ["l"="0.849,39.798"]
"merantix/picasso" ["l"="34.169,35.106"]
"zhenghuatan/rVADfast" ["l"="0.451,39.563"]
"iamjanvijay/rnnt" ["l"="0.596,39.684"]
"uhh-lt/kaldi-model-server" ["l"="0.862,39.901"]
"liuxubo717/SimPFs_Spectral" ["l"="0.09,39.804"]
"frednam93/FDY-SED" ["l"="1.496,39.051"]
"wq2012/SimpleDER" ["l"="0.256,39.7"]
"himajin2045/voice-conversion" ["l"="0.33,40.24"]
"TensorSpeech/TensorflowTTS" ["l"="0.367,40.075"]
"sounakdey/SigNet" ["l"="1.001,39.388"]
"vjayd/Signature-verification-using-Siamese-CNN" ["l"="0.98,39.388"]
"netrapathak/Offline-Signature-Verification" ["l"="1.016,39.39"]
"huseinzol05/malaya-speech" ["l"="0.116,40.072"]
"onejiin/CycleGAN-VC2" ["l"="0.384,40.284"]
"ariacat3366/pytorch-StarGAN-VC2-implementation" ["l"="0.375,40.259"]
"wnhsu/ScalableFHVAE" ["l"="0.354,40.332"]
"wnhsu/SpeechVAE" ["l"="0.372,40.337"]
"yjlolo/gmvae-synth" ["l"="0.422,40.346"]
"keonlee9420/VAENAR-TTS" ["l"="0.212,40.099"]
"alphacep/kaldi-android" ["l"="0.79,39.809"]
"jcsilva/docker-kaldi-android" ["l"="0.806,39.808"]
"diontimmer/sample-diffusion-gui" ["l"="-0.037,39.834"]
"sudosilico/sample-diffusion-app" ["l"="-0.03,39.844"]
"chuachinhon/wav2vec2_transformers" ["l"="0.588,39.63"]
"SuperKogito/Voice-based-gender-recognition" ["l"="0.288,39.45"]
"x4nth055/gender-recognition-by-voice" ["l"="0.275,39.426"]
"PeihaoChen/regnet" ["l"="0.112,39.878"]
"Snirpo/node-vad" ["l"="0.545,39.524"]
"ponlponl123/-Prototype-AIVTuber" ["l"="0.029,40.566"]
"deepsound-project/samplernn-pytorch" ["l"="0.55,40.258"]
"richardassar/SampleRNN_torch" ["l"="0.589,40.255"]
"deepsound-project/pggan-pytorch" ["l"="0.585,40.322"]
"Unisound/SampleRNN" ["l"="0.605,40.249"]
"gcunhase/samplernn-pytorch" ["l"="0.568,40.287"]
"Cortexelus/dadabots_sampleRNN" ["l"="0.586,40.305"]
"luizgh/adversarial_signatures" ["l"="1.027,39.422"]
"EB324/signature_verification" ["l"="1.031,39.405"]
"ZVK/sampleRNN_ICLR2017" ["l"="0.631,40.252"]
"kundan2510/pixelCNN" ["l"="33.837,32.463"]
"callee2006/MachineLearning" ["l"="-0.274,40.043"]
"mlcommons/peoples-speech" ["l"="0.66,40.009"]
"sithu31296/audio-tagging" ["l"="0.116,39.781"]
"fgnt/pb_sed" ["l"="1.516,39.063"]
"naotokui/M4L-CreativeGAN-Rhythm" ["l"="-0.111,39.825"]
"naotokui/VAE_Rhythm_Generator" ["l"="-0.107,39.835"]
"naotokui/CreativeGAN-Rhythm" ["l"="-0.123,39.833"]
"v7b1/sigmund_64bit-version" ["l"="2.925,37.509"]
"anonymousiclr2018/Style-Transfer-for-Musical-Audio" ["l"="0.597,40.412"]
"Kaljurand/K6nele-service" ["l"="0.831,39.868"]
"ddlBoJack/MT4SSL" ["l"="0.287,39.915"]
"srvk/lm_build" ["l"="0.869,39.851"]
"PlayVoice/VI-SVS" ["l"="0,40.062"]
"jeongmincha/Online-Signature-Verification" ["l"="1.035,39.395"]
"guilhermefloriani/signature-recognition" ["l"="1.037,39.381"]
"sounakdey/Lenet_nicicon" ["l"="1.005,39.371"]
"Samir55/Image2Lines" ["l"="0.874,39.531"]
"yzyouzhang/AIR-ASVspoof" ["l"="21.532,14.47"]
"csteinmetz1/pymixconsole" ["l"="2.03,37.879"]
"mathigatti/midi2voice" ["l"="0.192,39.955"]
"zizyzhang/DNN-Based-Singing-Voice-Synthesis" ["l"="0.259,39.992"]
"wngh1187/RawNeXt" ["l"="0.281,39.799"]
"yaoyao-liu/yaoyao-liu.github.io" ["l"="-0.195,40.062"]
"FeiGeChuanShu/ncnn_Android_PP-TinyPose" ["l"="31.582,36.231"]
"RussellSB/tt-vae-gan" ["l"="0.312,40.258"]
"yajiemiao/pdnn" ["l"="0.736,39.75"]
"primaryobjects/voice-gender" ["l"="0.265,39.4"]
"jaekookang/p2fa_py3" ["l"="0.643,40.029"]
"dopefishh/praatalign" ["l"="0.623,40.034"]
"nassosoassos/sail_align" ["l"="0.659,40.03"]
"prosodylab/prosodylab.dictionaries" ["l"="0.615,40.027"]
"sciforce/phones-las" ["l"="0.714,39.709"]
"BogiHsu/Tacotron2-PyTorch" ["l"="0.28,40.182"]
"BogiHsu/WG-WaveNet" ["l"="0.267,40.243"]
"bfs18/tacotron2" ["l"="0.253,40.276"]
"Kyubyong/vq-vae" ["l"="0.384,40.314"]
"xcmyz/ConvTasNet4BasisMelGAN" ["l"="0.326,39.994"]
"ttsunion/Deep-Expression" ["l"="0.517,40.306"]
"Anwarvic/MovieTweets--Search-Engine" ["l"="0.281,39.53"]
"sweetcocoa/ddsp-pytorch" ["l"="0.042,39.968"]
"jongwook/onsets-and-frames" ["l"="1.795,38.473"]
"coqui-ai/TTS-recipes" ["l"="0.268,39.978"]
"YatingMusic/compound-word-transformer" ["l"="1.874,38.543"]
"gabolsgabs/DALI" ["l"="1.811,38.486"]
"r4victor/afaligner" ["l"="0.818,40.095"]
"r4victor/synclibrivox" ["l"="0.794,40.093"]
"truongdo/kaldi-gstreamer-android-client" ["l"="0.817,39.866"]
"Kaljurand/speechutils" ["l"="52.817,-3.572"]
"mazzzystar/WaveGAN-pytorch" ["l"="0.612,40.316"]
"aimybox/aimybox-android-sdk" ["l"="1.235,39.922"]
"Cocoxili/VAD" ["l"="0.463,39.614"]
"keonlee9420/Daft-Exprt" ["l"="-0.029,40.058"]
"voicesauce/opensauce-python" ["l"="0.331,39.958"]
"kirbyj/praatsauce" ["l"="0.372,39.984"]
"neonbjb/DL-Art-School" ["l"="0.463,40.357"]
"gooofy/py-kaldi-simple" ["l"="0.784,39.757"]
"liuxubo717/LASS" ["l"="0.062,39.798"]
"liuxubo717/cl4ac" ["l"="0.077,39.801"]
"liuxubo717/V-ACT" ["l"="0.07,39.807"]
"liuxubo717/LASS-demopage" ["l"="0.075,39.793"]
"Yeongtae/tacotron2" ["l"="0.251,40.332"]
"r9y9/dnnsvs" ["l"="0.566,40.074"]
"yanggeng1995/EETS" ["l"="0.53,40.066"]
"LingLing-Speech/mandarin-tts-frontend-python" ["l"="0.392,40.103"]
"patrickltobing/cyclevae-vc" ["l"="0.33,40.224"]
"tarepan/VoiceConversionLab" ["l"="0.335,40.254"]
"jpuigcerver/PyLaia" ["l"="29.707,34.334"]
"igormq/aes-lac-2018" ["l"="0.855,39.746"]
"mlrobsmt/KerasDeepSpeech" ["l"="0.874,39.747"]
"dominoar/QWaifuGPT3" ["l"="-0.148,40.3"]
"idiap/apam" ["l"="0.505,39.673"]
"selap91/Tacotron2" ["l"="0.675,40.227"]
"r9y9/pyreaper" ["l"="0.566,40.208"]
"CSTR-Edinburgh/magphase" ["l"="0.565,40.221"]
"dgaspari/pyrapt" ["l"="0.557,40.199"]
"ronggong/pypYIN" ["l"="0.548,40.219"]
"HidekiKawahara/legacy_STRAIGHT" ["l"="0.573,40.239"]
"gillesdegottex/pulsemodel" ["l"="0.544,40.204"]
"sequence-labeling/rnn-transducer" ["l"="0.668,39.739"]
"fedderrico/ubm_map_diarization" ["l"="0.362,39.541"]
"zengchang94622/Speaker_Verification_Tencent" ["l"="0.336,39.629"]
"a-nagrani/VoxSRC2020" ["l"="0.232,39.688"]
"wenet-e2e/opencpop" ["l"="0.081,40.104"]
"cyhuang-tw/attack-vc" ["l"="0.281,40.013"]
"cyhuang-tw/AutoVC" ["l"="0.271,40.015"]
"npujcong/Chinese_PSP" ["l"="0.532,40.056"]
"xushengyuan/VocalnetOpenDataset" ["l"="0.083,39.862"]
"JeremyCCHsu/vc-vawgan" ["l"="0.346,40.273"]
"unilight/cdvae-vc" ["l"="0.343,40.288"]
"grtzsohalf/buy_vs_rent_and_invest" ["l"="0.281,39.976"]
"audio-captioning/audio-captioning-resources" ["l"="-0.12,39.879"]
"lukewys/dcase_2020_T6" ["l"="-0.171,39.847"]
"XinhaoMei/DCASE2021_task6_v2" ["l"="-0.09,39.88"]
"vitruvianscience/OpenDeep" ["l"="27.08,34.204"]
"descriptinc/cargan" ["l"="0.124,39.908"]
"rishikksh20/Fre-GAN-pytorch" ["l"="0.058,40.123"]
"rishikksh20/UnivNet-pytorch" ["l"="-0.016,40.133"]
"mcf06/theano_ctc" ["l"="0.854,39.699"]
"rosinality/imputer-pytorch" ["l"="27.986,33.892"]
"tts-tutorial/icassp2022" ["l"="0.031,40.018"]
"RicherMans/AudioCaption" ["l"="-0.216,39.835"]
"Joee1995/chn_text_norm" ["l"="0.692,40.08"]
"AlexHarker/HISSTools_Granular" ["l"="-0.216,39.903"]
"rongcloud/demo-web-sdk" ["l"="1.104,39.111"]
"rongcloud/rongcloud-web-im-sdk" ["l"="1.126,39.113"]
"Kyubyong/specAugment" ["l"="0.52,39.682"]
"tinoucas/spleeter-tflite-convert" ["l"="0.228,40.414"]
"jinay1991/spleeter" ["l"="0.22,40.393"]
"hccho2/hccho2.github.io" ["l"="0.743,40.252"]
"shauryr/google_text_normalization" ["l"="0.718,40.278"]
"Yeongtae/waveglow" ["l"="0.256,40.355"]
"cdjkim/audiocaps" ["l"="-0.165,39.87"]
"XinhaoMei/ACT" ["l"="-0.2,39.864"]
}