digraph G {
"deepmind/learning-to-learn" -> "deepmind/lab"
"deepmind/learning-to-learn" -> "deepmind/sonnet"
"deepmind/learning-to-learn" -> "deepmind/dnc"
"deepmind/learning-to-learn" -> "openai/universe"
"deepmind/learning-to-learn" -> "carpedm20/deep-rl-tensorflow"
"deepmind/learning-to-learn" -> "openai/pixel-cnn" ["e"=1]
"deepmind/learning-to-learn" -> "tensorflow/fold"
"deepmind/learning-to-learn" -> "blei-lab/edward" ["e"=1]
"deepmind/learning-to-learn" -> "dennybritz/deeplearning-papernotes" ["e"=1]
"deepmind/learning-to-learn" -> "devsisters/DQN-tensorflow"
"deepmind/learning-to-learn" -> "openai/universe-starter-agent"
"deepmind/learning-to-learn" -> "songrotek/Deep-Learning-Papers-Reading-Roadmap" ["e"=1]
"deepmind/learning-to-learn" -> "BinRoot/TensorFlow-Book" ["e"=1]
"deepmind/learning-to-learn" -> "tflearn/tflearn" ["e"=1]
"deepmind/learning-to-learn" -> "google/seq2seq" ["e"=1]
"ChenglongChen/pytorch-DRL" -> "mohammadasghari/dqn-multi-agent-rl"
"ChenglongChen/pytorch-DRL" -> "Derekabc/MARL_CAVs"
"ChenglongChen/pytorch-DRL" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"ChenglongChen/pytorch-DRL" -> "marlbenchmark/on-policy"
"ChenglongChen/pytorch-DRL" -> "cts198859/deeprl_network"
"ChenglongChen/pytorch-DRL" -> "sisl/MADRL"
"ChenglongChen/pytorch-DRL" -> "xuehy/pytorch-maddpg"
"ChenglongChen/pytorch-DRL" -> "tinyzqh/light_mappo"
"ChenglongChen/pytorch-DRL" -> "starry-sky6688/MADDPG"
"ChenglongChen/pytorch-DRL" -> "Lizhi-sjtu/MARL-code-pytorch"
"Lizhi-sjtu/MARL-code-pytorch" -> "tinyzqh/light_mappo"
"Lizhi-sjtu/MARL-code-pytorch" -> "marlbenchmark/off-policy"
"Lizhi-sjtu/MARL-code-pytorch" -> "starry-sky6688/MADDPG"
"Lizhi-sjtu/MARL-code-pytorch" -> "starry-sky6688/MARL-Algorithms"
"Lizhi-sjtu/MARL-code-pytorch" -> "sanmuyang/multi-agent-PPO-on-SMAC"
"Lizhi-sjtu/MARL-code-pytorch" -> "Lizhi-sjtu/DRL-code-pytorch"
"TianhongDai/distributed-ppo" -> "alexis-jacq/Pytorch-DPPO"
"ericyangyu/PPO-for-Beginners" -> "nikhilbarhate99/PPO-PyTorch"
"ericyangyu/PPO-for-Beginners" -> "mila-iqia/spr"
"ericyangyu/PPO-for-Beginners" -> "quantumiracle/Popular-RL-Algorithms"
"ericyangyu/PPO-for-Beginners" -> "lmzintgraf/varibad"
"ericyangyu/PPO-for-Beginners" -> "MrSyee/pg-is-all-you-need"
"ericyangyu/PPO-for-Beginners" -> "RITCHIEHuang/DeepRL_Algorithms"
"ericyangyu/PPO-for-Beginners" -> "AntoineTheb/RNN-RL"
"ericyangyu/PPO-for-Beginners" -> "marlbenchmark/on-policy"
"ericyangyu/PPO-for-Beginners" -> "dongminlee94/deep_rl"
"ericyangyu/PPO-for-Beginners" -> "vwxyzjn/ppo-implementation-details"
"ericyangyu/PPO-for-Beginners" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"ericyangyu/PPO-for-Beginners" -> "Lizhi-sjtu/MARL-code-pytorch"
"ericyangyu/PPO-for-Beginners" -> "clvrai/awesome-rl-envs"
"ericyangyu/PPO-for-Beginners" -> "ChenglongChen/pytorch-DRL"
"ericyangyu/PPO-for-Beginners" -> "XinJingHao/PPO-Continuous-Pytorch"
"nikhilbarhate99/PPO-PyTorch" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"nikhilbarhate99/PPO-PyTorch" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"nikhilbarhate99/PPO-PyTorch" -> "pranz24/pytorch-soft-actor-critic"
"nikhilbarhate99/PPO-PyTorch" -> "ericyangyu/PPO-for-Beginners"
"nikhilbarhate99/PPO-PyTorch" -> "sfujim/TD3"
"nikhilbarhate99/PPO-PyTorch" -> "marlbenchmark/on-policy"
"nikhilbarhate99/PPO-PyTorch" -> "seungeunrho/minimalRL"
"nikhilbarhate99/PPO-PyTorch" -> "vwxyzjn/ppo-implementation-details"
"nikhilbarhate99/PPO-PyTorch" -> "higgsfield/RL-Adventure-2"
"nikhilbarhate99/PPO-PyTorch" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"nikhilbarhate99/PPO-PyTorch" -> "starry-sky6688/StarCraft"
"nikhilbarhate99/PPO-PyTorch" -> "Khrylx/PyTorch-RL"
"nikhilbarhate99/PPO-PyTorch" -> "zhangchuheng123/Reinforcement-Implementation"
"nikhilbarhate99/PPO-PyTorch" -> "oxwhirl/pymarl"
"nikhilbarhate99/PPO-PyTorch" -> "benelot/pybullet-gym"
"philtabor/Youtube-Code-Repository" -> "philtabor/Deep-Q-Learning-Paper-To-Code"
"philtabor/Youtube-Code-Repository" -> "philtabor/Actor-Critic-Methods-Paper-To-Code"
"philtabor/Youtube-Code-Repository" -> "pythonlessons/Reinforcement_Learning"
"philtabor/Youtube-Code-Repository" -> "philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients"
"philtabor/Youtube-Code-Repository" -> "sfujim/TD3"
"philtabor/Youtube-Code-Repository" -> "rail-berkeley/softlearning"
"philtabor/Youtube-Code-Repository" -> "nikhilbarhate99/PPO-PyTorch"
"philtabor/Youtube-Code-Repository" -> "marload/DeepRL-TensorFlow2"
"philtabor/Youtube-Code-Repository" -> "philtabor/Reinforcement-Learning-In-Motion"
"philtabor/Youtube-Code-Repository" -> "ericyangyu/PPO-for-Beginners"
"philtabor/Youtube-Code-Repository" -> "pranz24/pytorch-soft-actor-critic"
"philtabor/Youtube-Code-Repository" -> "marlbenchmark/on-policy"
"philtabor/Youtube-Code-Repository" -> "vwxyzjn/ppo-implementation-details"
"philtabor/Youtube-Code-Repository" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition"
"philtabor/Youtube-Code-Repository" -> "oxwhirl/smac"
"tinyzqh/light_mappo" -> "marlbenchmark/on-policy"
"tinyzqh/light_mappo" -> "Lizhi-sjtu/MARL-code-pytorch"
"tinyzqh/light_mappo" -> "marlbenchmark/off-policy"
"tinyzqh/light_mappo" -> "starry-sky6688/MADDPG"
"tinyzqh/light_mappo" -> "cyanrain7/TRPO-in-MARL"
"tinyzqh/light_mappo" -> "chauncygu/Multi-Agent-Constrained-Policy-Optimisation"
"tinyzqh/light_mappo" -> "uoe-agents/epymarl"
"tinyzqh/light_mappo" -> "starry-sky6688/MARL-Algorithms"
"tinyzqh/light_mappo" -> "Replicable-MARL/MARLlib"
"tinyzqh/light_mappo" -> "TimeBreaker/MARL-papers-with-code"
"tinyzqh/light_mappo" -> "shariqiqbal2810/maddpg-pytorch"
"tinyzqh/light_mappo" -> "Lizhi-sjtu/DRL-code-pytorch"
"vwxyzjn/ppo-implementation-details" -> "Lizhi-sjtu/DRL-code-pytorch"
"vwxyzjn/ppo-implementation-details" -> "nikhilbarhate99/PPO-PyTorch"
"vwxyzjn/ppo-implementation-details" -> "marlbenchmark/on-policy"
"vwxyzjn/ppo-implementation-details" -> "vwxyzjn/cleanrl"
"vwxyzjn/ppo-implementation-details" -> "vwxyzjn/invalid-action-masking"
"vwxyzjn/ppo-implementation-details" -> "wangcongrobot/awesome-isaac-gym"
"vwxyzjn/ppo-implementation-details" -> "Stable-Baselines-Team/stable-baselines3-contrib"
"distillpub/template" -> "deepmind/bsuite"
"distillpub/template" -> "distillpub/post--example"
"distillpub/template" -> "deepmind/dm-haiku" ["e"=1]
"distillpub/template" -> "deepmind/jraph" ["e"=1]
"distillpub/template" -> "arxiv-vanity/arxiv-vanity" ["e"=1]
"distillpub/template" -> "tensorflow/lucid" ["e"=1]
"distillpub/template" -> "maximecb/gym-minigrid"
"distillpub/template" -> "idyll-lang/idyll" ["e"=1]
"distillpub/template" -> "ramanans1/plan2explore"
"distillpub/template" -> "maximecb/gym-miniworld"
"distillpub/template" -> "google/jaxopt" ["e"=1]
"distillpub/template" -> "ermongroup/cs228-notes" ["e"=1]
"distillpub/template" -> "worldmodels/worldmodels.github.io"
"dennybritz/reinforcement-learning" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"dennybritz/reinforcement-learning" -> "aikorea/awesome-rl"
"dennybritz/reinforcement-learning" -> "openai/baselines"
"dennybritz/reinforcement-learning" -> "openai/gym"
"dennybritz/reinforcement-learning" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"dennybritz/reinforcement-learning" -> "openai/spinningup"
"dennybritz/reinforcement-learning" -> "google/dopamine"
"dennybritz/reinforcement-learning" -> "terryum/awesome-deep-learning-papers" ["e"=1]
"dennybritz/reinforcement-learning" -> "udacity/deep-reinforcement-learning"
"dennybritz/reinforcement-learning" -> "aymericdamien/TensorFlow-Examples" ["e"=1]
"dennybritz/reinforcement-learning" -> "yandexdataschool/Practical_RL"
"dennybritz/reinforcement-learning" -> "yunjey/pytorch-tutorial" ["e"=1]
"dennybritz/reinforcement-learning" -> "songrotek/Deep-Learning-Papers-Reading-Roadmap" ["e"=1]
"dennybritz/reinforcement-learning" -> "pytorch/examples" ["e"=1]
"dennybritz/reinforcement-learning" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"openai/spinningup" -> "openai/baselines"
"openai/spinningup" -> "hill-a/stable-baselines"
"openai/spinningup" -> "DLR-RM/stable-baselines3"
"openai/spinningup" -> "thu-ml/tianshou"
"openai/spinningup" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"openai/spinningup" -> "openai/gym"
"openai/spinningup" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"openai/spinningup" -> "dennybritz/reinforcement-learning"
"openai/spinningup" -> "google/dopamine"
"openai/spinningup" -> "openai/mujoco-py"
"openai/spinningup" -> "rlworkgroup/garage"
"openai/spinningup" -> "astooke/rlpyt"
"openai/spinningup" -> "rll/rllab"
"openai/spinningup" -> "deepmind/trfl"
"openai/spinningup" -> "ray-project/ray" ["e"=1]
"Farama-Foundation/Gymnasium" -> "vwxyzjn/cleanrl"
"Farama-Foundation/Gymnasium" -> "DLR-RM/stable-baselines3"
"Farama-Foundation/Gymnasium" -> "Farama-Foundation/PettingZoo"
"Farama-Foundation/Gymnasium" -> "DLR-RM/rl-baselines3-zoo"
"Farama-Foundation/Gymnasium" -> "pytorch/rl"
"Farama-Foundation/Gymnasium" -> "sail-sg/envpool"
"Farama-Foundation/Gymnasium" -> "deepmind/mujoco"
"Farama-Foundation/Gymnasium" -> "google/brax"
"Farama-Foundation/Gymnasium" -> "openai/spinningup"
"Farama-Foundation/Gymnasium" -> "tinkoff-ai/CORL"
"Farama-Foundation/Gymnasium" -> "thu-ml/tianshou"
"Farama-Foundation/Gymnasium" -> "NVIDIA-Omniverse/IsaacGymEnvs"
"Farama-Foundation/Gymnasium" -> "deepmind/dm_control"
"Farama-Foundation/Gymnasium" -> "openai/gym"
"Farama-Foundation/Gymnasium" -> "google-research/rliable"
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "openai/baselines"
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "dennybritz/reinforcement-learning"
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "openai/gym"
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "openai/spinningup"
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "thu-ml/tianshou"
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "aikorea/awesome-rl"
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "princewen/tensorflow_practice" ["e"=1]
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "rlcode/reinforcement-learning"
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "MorvanZhou/Tensorflow-Tutorial" ["e"=1]
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "hill-a/stable-baselines"
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "NeuronDance/DeepRL"
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "datawhalechina/easy-rl"
"NVIDIA-Omniverse/IsaacGymEnvs" -> "wangcongrobot/awesome-isaac-gym"
"NVIDIA-Omniverse/IsaacGymEnvs" -> "Denys88/rl_games"
"NVIDIA-Omniverse/IsaacGymEnvs" -> "NVIDIA-Omniverse/OmniIsaacGymEnvs"
"NVIDIA-Omniverse/IsaacGymEnvs" -> "leggedrobotics/legged_gym" ["e"=1]
"NVIDIA-Omniverse/IsaacGymEnvs" -> "nv-tlabs/ASE" ["e"=1]
"NVIDIA-Omniverse/IsaacGymEnvs" -> "PKU-MARL/DexterousHands"
"NVIDIA-Omniverse/IsaacGymEnvs" -> "google/brax"
"NVIDIA-Omniverse/IsaacGymEnvs" -> "deepmind/mujoco_menagerie"
"NVIDIA-Omniverse/IsaacGymEnvs" -> "xbpeng/DeepMimic" ["e"=1]
"NVIDIA-Omniverse/IsaacGymEnvs" -> "Toni-SM/skrl"
"NVIDIA-Omniverse/IsaacGymEnvs" -> "sail-sg/envpool"
"NVIDIA-Omniverse/IsaacGymEnvs" -> "leggedrobotics/rsl_rl" ["e"=1]
"NVIDIA-Omniverse/IsaacGymEnvs" -> "NVIDIA-Omniverse/Orbit"
"NVIDIA-Omniverse/IsaacGymEnvs" -> "stepjam/RLBench"
"NVIDIA-Omniverse/IsaacGymEnvs" -> "ARISE-Initiative/robosuite"
"google-research/football" -> "oxwhirl/pymarl"
"google-research/football" -> "oxwhirl/smac"
"google-research/football" -> "openai/multiagent-particle-envs"
"google-research/football" -> "deepmind/open_spiel"
"google-research/football" -> "marlbenchmark/on-policy"
"google-research/football" -> "LantaoYu/MARL-Papers"
"google-research/football" -> "openai/maddpg"
"google-research/football" -> "hill-a/stable-baselines"
"google-research/football" -> "openai/multi-agent-emergence-environments"
"google-research/football" -> "astooke/rlpyt"
"google-research/football" -> "deepmind/acme"
"google-research/football" -> "starry-sky6688/StarCraft"
"google-research/football" -> "deepmind/dm_control"
"google-research/football" -> "openai/spinningup"
"google-research/football" -> "BazkieBumpercar/GameplayFootball"
"aikorea/awesome-rl" -> "dennybritz/reinforcement-learning"
"aikorea/awesome-rl" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"aikorea/awesome-rl" -> "keon/awesome-nlp" ["e"=1]
"aikorea/awesome-rl" -> "src-d/awesome-machine-learning-on-source-code" ["e"=1]
"aikorea/awesome-rl" -> "openai/gym"
"aikorea/awesome-rl" -> "openai/baselines"
"aikorea/awesome-rl" -> "jbhuang0604/awesome-computer-vision" ["e"=1]
"aikorea/awesome-rl" -> "yandexdataschool/Practical_RL"
"aikorea/awesome-rl" -> "GoogleTrends/data" ["e"=1]
"aikorea/awesome-rl" -> "igrigorik/decisiontree" ["e"=1]
"aikorea/awesome-rl" -> "scikit-learn-contrib/lightning" ["e"=1]
"aikorea/awesome-rl" -> "ChristosChristofidis/awesome-deep-learning" ["e"=1]
"aikorea/awesome-rl" -> "nlintz/TensorFlow-Tutorials" ["e"=1]
"aikorea/awesome-rl" -> "jtoy/awesome-tensorflow" ["e"=1]
"aikorea/awesome-rl" -> "kjw0612/awesome-deep-vision" ["e"=1]
"PaddlePaddle/PARL" -> "thu-ml/tianshou"
"PaddlePaddle/PARL" -> "NeuronDance/DeepRL"
"PaddlePaddle/PARL" -> "datawhalechina/easy-rl"
"PaddlePaddle/PARL" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"PaddlePaddle/PARL" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"PaddlePaddle/PARL" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"PaddlePaddle/PARL" -> "zhoubolei/introRL"
"PaddlePaddle/PARL" -> "openai/multiagent-particle-envs"
"PaddlePaddle/PARL" -> "sfujim/TD3"
"PaddlePaddle/PARL" -> "wangshusen/DRL"
"PaddlePaddle/PARL" -> "PaddlePaddle/PGL" ["e"=1]
"PaddlePaddle/PARL" -> "openai/spinningup"
"PaddlePaddle/PARL" -> "openai/baselines"
"PaddlePaddle/PARL" -> "oxwhirl/pymarl"
"PaddlePaddle/PARL" -> "LantaoYu/MARL-Papers"
"thu-ml/tianshou" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"thu-ml/tianshou" -> "DLR-RM/stable-baselines3"
"thu-ml/tianshou" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"thu-ml/tianshou" -> "openai/spinningup"
"thu-ml/tianshou" -> "vwxyzjn/cleanrl"
"thu-ml/tianshou" -> "NeuronDance/DeepRL"
"thu-ml/tianshou" -> "openai/baselines"
"thu-ml/tianshou" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"thu-ml/tianshou" -> "hill-a/stable-baselines"
"thu-ml/tianshou" -> "astooke/rlpyt"
"thu-ml/tianshou" -> "LantaoYu/MARL-Papers"
"thu-ml/tianshou" -> "AI4Finance-Foundation/ElegantRL"
"thu-ml/tianshou" -> "PaddlePaddle/PARL"
"thu-ml/tianshou" -> "oxwhirl/pymarl"
"thu-ml/tianshou" -> "ShangtongZhang/DeepRL"
"yanpanlau/Keras-FlappyBird" -> "yanpanlau/DDPG-Keras-Torcs"
"yanpanlau/Keras-FlappyBird" -> "matthiasplappert/keras-rl"
"yanpanlau/Keras-FlappyBird" -> "coreylynch/async-rl"
"yanpanlau/Keras-FlappyBird" -> "keon/deep-q-learning"
"yanpanlau/Keras-FlappyBird" -> "farizrahman4u/qlearning4k"
"openai/gym" -> "openai/baselines"
"openai/gym" -> "dennybritz/reinforcement-learning"
"openai/gym" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"openai/gym" -> "aikorea/awesome-rl"
"openai/gym" -> "openai/spinningup"
"openai/gym" -> "pytorch/pytorch" ["e"=1]
"openai/gym" -> "openai/universe"
"openai/gym" -> "tensorflow/models" ["e"=1]
"openai/gym" -> "scikit-learn/scikit-learn" ["e"=1]
"openai/gym" -> "ray-project/ray" ["e"=1]
"openai/gym" -> "fchollet/keras" ["e"=1]
"openai/gym" -> "google/dopamine"
"openai/gym" -> "tensorflow/tensorflow" ["e"=1]
"openai/gym" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"openai/gym" -> "Unity-Technologies/ml-agents" ["e"=1]
"YeWR/EfficientZero" -> "werner-duvaud/muzero-general"
"YeWR/EfficientZero" -> "koulanurag/muzero-pytorch"
"YeWR/EfficientZero" -> "eloialonso/iris"
"YeWR/EfficientZero" -> "sail-sg/envpool"
"YeWR/EfficientZero" -> "rll-research/url_benchmark"
"YeWR/EfficientZero" -> "danijar/dreamerv2"
"YeWR/EfficientZero" -> "danijar/dreamerv3"
"YeWR/EfficientZero" -> "google-research/rliable"
"YeWR/EfficientZero" -> "kzl/decision-transformer"
"YeWR/EfficientZero" -> "MishaLaskin/curl"
"YeWR/EfficientZero" -> "facebookresearch/mbrl-lib"
"YeWR/EfficientZero" -> "danijar/crafter"
"YeWR/EfficientZero" -> "jurgisp/pydreamer"
"YeWR/EfficientZero" -> "deepmind/mctx" ["e"=1]
"YeWR/EfficientZero" -> "rail-berkeley/d4rl"
"cpnota/autonomous-learning-library" -> "PettingZoo-Team/PettingZoo" ["e"=1]
"cpnota/autonomous-learning-library" -> "astooke/rlpyt"
"cpnota/autonomous-learning-library" -> "pfnet/pfrl"
"cpnota/autonomous-learning-library" -> "MushroomRL/mushroom-rl"
"cpnota/autonomous-learning-library" -> "kengz/awesome-deep-rl"
"cpnota/autonomous-learning-library" -> "JannerM/mbpo"
"cpnota/autonomous-learning-library" -> "facebookresearch/mbrl-lib"
"cpnota/autonomous-learning-library" -> "vwxyzjn/cleanrl"
"cpnota/autonomous-learning-library" -> "clvrai/awesome-rl-envs"
"cpnota/autonomous-learning-library" -> "learnables/cherry"
"cpnota/autonomous-learning-library" -> "rlworkgroup/garage"
"cpnota/autonomous-learning-library" -> "quantumiracle/SOTA-RL-Algorithms"
"cpnota/autonomous-learning-library" -> "fabiopardo/tonic"
"cpnota/autonomous-learning-library" -> "iffiX/machin"
"cpnota/autonomous-learning-library" -> "maximecb/gym-minigrid"
"deepmind/bsuite" -> "deepmind/spriteworld"
"deepmind/bsuite" -> "deepmind/rlax" ["e"=1]
"deepmind/bsuite" -> "deepmind/open_spiel"
"deepmind/bsuite" -> "astooke/rlpyt"
"deepmind/bsuite" -> "maximecb/gym-minigrid"
"deepmind/bsuite" -> "deepmind/acme"
"deepmind/bsuite" -> "deepmind/dm_control"
"deepmind/bsuite" -> "deepmind/trfl"
"deepmind/bsuite" -> "rlworkgroup/garage"
"deepmind/bsuite" -> "vitchyr/rlkit"
"deepmind/bsuite" -> "google-research/planet"
"deepmind/bsuite" -> "rail-berkeley/softlearning"
"deepmind/bsuite" -> "deepmind/reverb"
"deepmind/bsuite" -> "google-research/rliable"
"deepmind/bsuite" -> "hill-a/stable-baselines"
"deepmind/trfl" -> "google/dopamine"
"deepmind/trfl" -> "facebookresearch/Horizon"
"deepmind/trfl" -> "deepmind/graph_nets" ["e"=1]
"deepmind/trfl" -> "tensorflow/agents"
"deepmind/trfl" -> "deepmind/bsuite"
"deepmind/trfl" -> "deepmind/scalable_agent"
"deepmind/trfl" -> "deepmind/dm_control"
"deepmind/trfl" -> "openai/baselines"
"deepmind/trfl" -> "NervanaSystems/coach"
"deepmind/trfl" -> "openai/spinningup"
"deepmind/trfl" -> "astooke/rlpyt"
"deepmind/trfl" -> "hill-a/stable-baselines"
"deepmind/trfl" -> "tensorflow/adanet" ["e"=1]
"deepmind/trfl" -> "rll/rllab"
"deepmind/trfl" -> "reinforceio/tensorforce"
"germain-hug/Deep-RL-Keras" -> "yanpanlau/DDPG-Keras-Torcs"
"germain-hug/Deep-RL-Keras" -> "keras-rl/keras-rl"
"germain-hug/Deep-RL-Keras" -> "keiohta/tf2rl"
"germain-hug/Deep-RL-Keras" -> "pythonlessons/Reinforcement_Learning"
"germain-hug/Deep-RL-Keras" -> "keon/deep-q-learning"
"germain-hug/Deep-RL-Keras" -> "xiaochus/Deep-Reinforcement-Learning-Practice"
"germain-hug/Deep-RL-Keras" -> "wau/keras-rl2"
"germain-hug/Deep-RL-Keras" -> "tensorforce/tensorforce"
"germain-hug/Deep-RL-Keras" -> "cyoon1729/deep-Q-networks"
"germain-hug/Deep-RL-Keras" -> "TianhongDai/reinforcement-learning-algorithms"
"germain-hug/Deep-RL-Keras" -> "anita-hu/TF2-RL"
"germain-hug/Deep-RL-Keras" -> "floodsung/DDPG"
"germain-hug/Deep-RL-Keras" -> "sisl/MADRL"
"germain-hug/Deep-RL-Keras" -> "marload/DeepRL-TensorFlow2"
"germain-hug/Deep-RL-Keras" -> "miroblog/deep_rl_trader" ["e"=1]
"google/dopamine" -> "deepmind/trfl"
"google/dopamine" -> "openai/baselines"
"google/dopamine" -> "dennybritz/reinforcement-learning"
"google/dopamine" -> "openai/spinningup"
"google/dopamine" -> "hill-a/stable-baselines"
"google/dopamine" -> "tensorflow/agents"
"google/dopamine" -> "openai/gym"
"google/dopamine" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"google/dopamine" -> "aikorea/awesome-rl"
"google/dopamine" -> "ray-project/ray" ["e"=1]
"google/dopamine" -> "deepmind/lab"
"google/dopamine" -> "deepmind/sonnet"
"google/dopamine" -> "keras-rl/keras-rl"
"google/dopamine" -> "deepmind/dm_control"
"google/dopamine" -> "deepmind/acme"
"vwxyzjn/cleanrl" -> "tinkoff-ai/CORL"
"vwxyzjn/cleanrl" -> "DLR-RM/stable-baselines3"
"vwxyzjn/cleanrl" -> "Farama-Foundation/Gymnasium"
"vwxyzjn/cleanrl" -> "DLR-RM/rl-baselines3-zoo"
"vwxyzjn/cleanrl" -> "thu-ml/tianshou"
"vwxyzjn/cleanrl" -> "sail-sg/envpool"
"vwxyzjn/cleanrl" -> "deepmind/acme"
"vwxyzjn/cleanrl" -> "Farama-Foundation/PettingZoo"
"vwxyzjn/cleanrl" -> "rlworkgroup/garage"
"vwxyzjn/cleanrl" -> "google/brax"
"vwxyzjn/cleanrl" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"vwxyzjn/cleanrl" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"vwxyzjn/cleanrl" -> "maximecb/gym-minigrid"
"vwxyzjn/cleanrl" -> "seungeunrho/minimalRL"
"vwxyzjn/cleanrl" -> "hill-a/stable-baselines"
"openai/baselines" -> "openai/gym"
"openai/baselines" -> "openai/spinningup"
"openai/baselines" -> "dennybritz/reinforcement-learning"
"openai/baselines" -> "hill-a/stable-baselines"
"openai/baselines" -> "google/dopamine"
"openai/baselines" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"openai/baselines" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"openai/baselines" -> "DLR-RM/stable-baselines3"
"openai/baselines" -> "ray-project/ray" ["e"=1]
"openai/baselines" -> "aikorea/awesome-rl"
"openai/baselines" -> "rll/rllab"
"openai/baselines" -> "thu-ml/tianshou"
"openai/baselines" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"openai/baselines" -> "ShangtongZhang/DeepRL"
"openai/baselines" -> "deepmind/dm_control"
"rlworkgroup/garage" -> "rll/rllab"
"rlworkgroup/garage" -> "rlworkgroup/metaworld"
"rlworkgroup/garage" -> "vitchyr/rlkit"
"rlworkgroup/garage" -> "astooke/rlpyt"
"rlworkgroup/garage" -> "rail-berkeley/softlearning"
"rlworkgroup/garage" -> "hill-a/stable-baselines"
"rlworkgroup/garage" -> "deepmind/dm_control"
"rlworkgroup/garage" -> "openai/spinningup"
"rlworkgroup/garage" -> "vwxyzjn/cleanrl"
"rlworkgroup/garage" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"rlworkgroup/garage" -> "DLR-RM/stable-baselines3"
"rlworkgroup/garage" -> "maximecb/gym-minigrid"
"rlworkgroup/garage" -> "deepmind/bsuite"
"rlworkgroup/garage" -> "sfujim/TD3"
"rlworkgroup/garage" -> "araffin/rl-baselines-zoo"
"NeuronDance/DeepRL" -> "thu-ml/tianshou"
"NeuronDance/DeepRL" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"NeuronDance/DeepRL" -> "wwxFromTju/awesome-reinforcement-learning-zh"
"NeuronDance/DeepRL" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"NeuronDance/DeepRL" -> "zhoubolei/introRL"
"NeuronDance/DeepRL" -> "oxwhirl/pymarl"
"NeuronDance/DeepRL" -> "PaddlePaddle/PARL"
"NeuronDance/DeepRL" -> "zhangchuheng123/Reinforcement-Implementation"
"NeuronDance/DeepRL" -> "LantaoYu/MARL-Papers"
"NeuronDance/DeepRL" -> "starry-sky6688/StarCraft"
"NeuronDance/DeepRL" -> "datawhalechina/easy-rl"
"NeuronDance/DeepRL" -> "wangshusen/DRL"
"NeuronDance/DeepRL" -> "ShangtongZhang/DeepRL"
"NeuronDance/DeepRL" -> "tigerneil/awesome-deep-rl"
"NeuronDance/DeepRL" -> "openai/spinningup"
"spragunr/deep_q_rl" -> "kristjankorjus/Replicating-DeepMind"
"spragunr/deep_q_rl" -> "tambetm/simple_dqn"
"spragunr/deep_q_rl" -> "kuz/DeepMind-Atari-Deep-Q-Learner"
"spragunr/deep_q_rl" -> "nivwusquorum/tensorflow-deepq"
"spragunr/deep_q_rl" -> "mgbellemare/Arcade-Learning-Environment"
"spragunr/deep_q_rl" -> "muupan/deep-reinforcement-learning-papers"
"spragunr/deep_q_rl" -> "asrivat1/DeepLearningVideoGames"
"spragunr/deep_q_rl" -> "muupan/dqn-in-the-caffe"
"spragunr/deep_q_rl" -> "rllab/rllab"
"spragunr/deep_q_rl" -> "coreylynch/async-rl"
"spragunr/deep_q_rl" -> "muupan/async-rl"
"spragunr/deep_q_rl" -> "shawntan/neural-turing-machines" ["e"=1]
"spragunr/deep_q_rl" -> "miyosuda/async_deep_reinforce"
"spragunr/deep_q_rl" -> "devsisters/DQN-tensorflow"
"spragunr/deep_q_rl" -> "matthiasplappert/keras-rl"
"facebookresearch/ELF" -> "pytorch/ELF" ["e"=1]
"facebookresearch/ELF" -> "TorchCraft/TorchCraft" ["e"=1]
"facebookresearch/ELF" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"facebookresearch/ELF" -> "reinforceio/tensorforce"
"facebookresearch/ELF" -> "rll/rllab"
"facebookresearch/ELF" -> "williamFalcon/DeepRLHacks"
"facebookresearch/ELF" -> "deepmind/dm_control"
"facebookresearch/ELF" -> "openai/universe-starter-agent"
"facebookresearch/ELF" -> "facebookresearch/darkforestGo" ["e"=1]
"facebookresearch/ELF" -> "openai/roboschool"
"facebookresearch/ELF" -> "pathak22/noreward-rl"
"facebookresearch/ELF" -> "geek-ai/MAgent"
"facebookresearch/ELF" -> "deepmind/lab"
"facebookresearch/ELF" -> "deepmind/dnc"
"facebookresearch/ELF" -> "deepmind/trfl"
"datawhalechina/easy-rl" -> "wangshusen/DRL"
"datawhalechina/easy-rl" -> "zhoubolei/introRL"
"datawhalechina/easy-rl" -> "thu-ml/tianshou"
"datawhalechina/easy-rl" -> "boyu-ai/Hands-on-RL"
"datawhalechina/easy-rl" -> "NeuronDance/DeepRL"
"datawhalechina/easy-rl" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"datawhalechina/easy-rl" -> "PaddlePaddle/PARL"
"datawhalechina/easy-rl" -> "cuhkrlcourse/RLexample"
"datawhalechina/easy-rl" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"datawhalechina/easy-rl" -> "DLR-RM/stable-baselines3"
"datawhalechina/easy-rl" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"datawhalechina/easy-rl" -> "AI4Finance-Foundation/ElegantRL"
"datawhalechina/easy-rl" -> "mli/paper-reading" ["e"=1]
"datawhalechina/easy-rl" -> "wangshusen/DeepLearning"
"datawhalechina/easy-rl" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"deepmind/sonnet" -> "deepmind/learning-to-learn"
"deepmind/sonnet" -> "deepmind/dnc"
"deepmind/sonnet" -> "deepmind/lab"
"deepmind/sonnet" -> "tensorflow/tensor2tensor" ["e"=1]
"deepmind/sonnet" -> "tensorflow/magenta" ["e"=1]
"deepmind/sonnet" -> "fchollet/keras" ["e"=1]
"deepmind/sonnet" -> "PAIR-code/facets" ["e"=1]
"deepmind/sonnet" -> "openai/baselines"
"deepmind/sonnet" -> "uber/horovod" ["e"=1]
"deepmind/sonnet" -> "google/dopamine"
"deepmind/sonnet" -> "deepmind/pysc2" ["e"=1]
"deepmind/sonnet" -> "facebookresearch/fastText" ["e"=1]
"deepmind/sonnet" -> "uber/pyro" ["e"=1]
"deepmind/sonnet" -> "facebookresearch/visdom" ["e"=1]
"deepmind/sonnet" -> "openai/universe"
"Ericonaldo/ILSwiss" -> "apexrl/Imitation-Learning-Paper-Lists"
"Ericonaldo/ILSwiss" -> "kristery/Awesome-Imitation-Learning"
"Farama-Foundation/Minigrid" -> "Farama-Foundation/Miniworld"
"Farama-Foundation/Minigrid" -> "lcswillems/rl-starter-files"
"Farama-Foundation/Minigrid" -> "Farama-Foundation/Gymnasium-Robotics"
"Lizhi-sjtu/DRL-code-pytorch" -> "Lizhi-sjtu/MARL-code-pytorch"
"Lizhi-sjtu/DRL-code-pytorch" -> "kaixindelele/DRLib"
"Lizhi-sjtu/DRL-code-pytorch" -> "tinyzqh/light_mappo"
"Lizhi-sjtu/DRL-code-pytorch" -> "vwxyzjn/ppo-implementation-details"
"Lizhi-sjtu/DRL-code-pytorch" -> "mengwanglalala/RL-algorithms"
"Lizhi-sjtu/DRL-code-pytorch" -> "marlbenchmark/on-policy"
"Lizhi-sjtu/DRL-code-pytorch" -> "XinJingHao/PPO-Continuous-Pytorch"
"Lizhi-sjtu/DRL-code-pytorch" -> "XinJingHao/RL-Algorithms-by-Pytorch"
"Lizhi-sjtu/DRL-code-pytorch" -> "starry-sky6688/MADDPG"
"Lizhi-sjtu/DRL-code-pytorch" -> "nikhilbarhate99/PPO-PyTorch"
"Lizhi-sjtu/DRL-code-pytorch" -> "AI4Finance-Foundation/ElegantRL"
"Lizhi-sjtu/DRL-code-pytorch" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"Lizhi-sjtu/DRL-code-pytorch" -> "ChenglongChen/pytorch-DRL"
"Lizhi-sjtu/DRL-code-pytorch" -> "zhangchuheng123/Reinforcement-Implementation"
"Lizhi-sjtu/DRL-code-pytorch" -> "marlbenchmark/off-policy"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "dennybritz/reinforcement-learning"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "aikorea/awesome-rl"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "openai/baselines"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "openai/gym"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "ShangtongZhang/DeepRL"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "openai/spinningup"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "udacity/deep-reinforcement-learning"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "google/dopamine"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "yandexdataschool/Practical_RL"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "rlcode/reinforcement-learning"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "thu-ml/tianshou"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "ctgk/PRML" ["e"=1]
"ShangtongZhang/reinforcement-learning-an-introduction" -> "LantaoYu/MARL-Papers"
"louisnino/RLcode" -> "zhangchuheng123/Reinforcement-Implementation"
"louisnino/RLcode" -> "tensorlayer/TensorLayer"
"louisnino/RLcode" -> "anita-hu/TF2-RL"
"louisnino/RLcode" -> "kaixindelele/DRLib"
"louisnino/RLcode" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"louisnino/RLcode" -> "StepNeverStop/RLs"
"louisnino/RLcode" -> "sfujim/TD3"
"louisnino/RLcode" -> "wangshusen/DRL"
"louisnino/RLcode" -> "qqiang00/Reinforce"
"louisnino/RLcode" -> "NovemberChopin/RL_Tutorial"
"louisnino/RLcode" -> "mengwanglalala/RL-algorithms"
"louisnino/RLcode" -> "luozachary/drl-rec" ["e"=1]
"louisnino/RLcode" -> "Yonv1943/ElegantRL"
"louisnino/RLcode" -> "AI4Finance-Foundation/ElegantRL"
"louisnino/RLcode" -> "Lizhi-sjtu/DRL-code-pytorch"
"openai/random-network-distillation" -> "jcwleo/random-network-distillation-pytorch"
"openai/random-network-distillation" -> "openai/large-scale-curiosity"
"openai/random-network-distillation" -> "pathak22/noreward-rl"
"openai/random-network-distillation" -> "haarnoja/sac"
"openai/random-network-distillation" -> "deepmind/scalable_agent"
"openai/random-network-distillation" -> "uber-research/go-explore"
"openai/random-network-distillation" -> "maximecb/gym-minigrid"
"openai/random-network-distillation" -> "junhyukoh/self-imitation-learning"
"openai/random-network-distillation" -> "openai/coinrun"
"openai/random-network-distillation" -> "rail-berkeley/softlearning"
"openai/random-network-distillation" -> "Kaixhin/Rainbow"
"openai/random-network-distillation" -> "vitchyr/rlkit"
"openai/random-network-distillation" -> "google-research/planet"
"openai/random-network-distillation" -> "uber-research/ape-x"
"openai/random-network-distillation" -> "sfujim/BCQ"
"rail-berkeley/rlkit" -> "takuseno/d3rlpy"
"rail-berkeley/rlkit" -> "aviralkumar2907/CQL"
"rail-berkeley/rlkit" -> "rail-berkeley/softlearning"
"rail-berkeley/rlkit" -> "facebookresearch/mbrl-lib"
"rail-berkeley/rlkit" -> "rail-berkeley/d4rl"
"rail-berkeley/rlkit" -> "denisyarats/pytorch_sac"
"rail-berkeley/rlkit" -> "pranz24/pytorch-soft-actor-critic"
"rail-berkeley/rlkit" -> "rlworkgroup/metaworld"
"rail-berkeley/rlkit" -> "quantumiracle/Popular-RL-Algorithms"
"rail-berkeley/rlkit" -> "DLR-RM/rl-baselines3-zoo"
"rail-berkeley/rlkit" -> "hanjuku-kaso/awesome-offline-rl"
"rail-berkeley/rlkit" -> "NVIDIA-Omniverse/IsaacGymEnvs"
"rail-berkeley/rlkit" -> "ikostrikov/jaxrl"
"rail-berkeley/rlkit" -> "haarnoja/sac"
"rail-berkeley/rlkit" -> "qgallouedec/panda-gym"
"tensorlayer/TensorLayer" -> "louisnino/RLcode"
"tensorlayer/TensorLayer" -> "tensorlayer/RLzoo" ["e"=1]
"tensorlayer/TensorLayer" -> "boyu-ai/Hands-on-RL"
"tensorlayer/TensorLayer" -> "tensorlayer/TensorLayerX" ["e"=1]
"tensorlayer/TensorLayer" -> "deep-reinforcement-learning-book/Chapter4-DQN"
"tensorlayer/TensorLayer" -> "AI4Finance-Foundation/ElegantRL"
"tensorlayer/TensorLayer" -> "tensorlayer/tensorlayer-chinese"
"tensorlayer/TensorLayer" -> "KISS1996/trexminer" ["e"=1]
"salesforce/ai-economist" -> "salesforce/warp-drive"
"salesforce/ai-economist" -> "deepmind/meltingpot"
"salesforce/ai-economist" -> "microsoft/EconML" ["e"=1]
"salesforce/ai-economist" -> "PettingZoo-Team/PettingZoo" ["e"=1]
"salesforce/ai-economist" -> "maximecb/gym-minigrid"
"salesforce/ai-economist" -> "Farama-Foundation/PettingZoo"
"salesforce/ai-economist" -> "instadeepai/Mava"
"salesforce/ai-economist" -> "eugenevinitsky/sequential_social_dilemma_games"
"salesforce/ai-economist" -> "projectmesa/mesa" ["e"=1]
"salesforce/ai-economist" -> "antontarasenko/awesome-economics" ["e"=1]
"salesforce/ai-economist" -> "hardmaru/slimevolleygym"
"salesforce/ai-economist" -> "KennethJudd/CompEcon2020" ["e"=1]
"salesforce/ai-economist" -> "geek-ai/MAgent"
"salesforce/ai-economist" -> "openai/procgen"
"salesforce/ai-economist" -> "oxwhirl/pymarl"
"starry-sky6688/MARL-Algorithms" -> "marlbenchmark/off-policy"
"starry-sky6688/MARL-Algorithms" -> "Lizhi-sjtu/MARL-code-pytorch"
"starry-sky6688/MARL-Algorithms" -> "marlbenchmark/on-policy"
"starry-sky6688/MARL-Algorithms" -> "starry-sky6688/MADDPG"
"starry-sky6688/MARL-Algorithms" -> "uoe-agents/epymarl"
"starry-sky6688/MARL-Algorithms" -> "Replicable-MARL/MARLlib"
"starry-sky6688/MARL-Algorithms" -> "oxwhirl/pymarl"
"starry-sky6688/MARL-Algorithms" -> "tinyzqh/light_mappo"
"starry-sky6688/MARL-Algorithms" -> "TimeBreaker/MARL-papers-with-code"
"starry-sky6688/MARL-Algorithms" -> "hijkzzz/pymarl2"
"starry-sky6688/MARL-Algorithms" -> "shariqiqbal2810/maddpg-pytorch"
"starry-sky6688/MARL-Algorithms" -> "oxwhirl/smac"
"starry-sky6688/MARL-Algorithms" -> "TimeBreaker/Multi-Agent-Reinforcement-Learning-papers"
"starry-sky6688/MARL-Algorithms" -> "PKU-MARL/Multi-Agent-Transformer"
"starry-sky6688/MARL-Algorithms" -> "oxwhirl/wqmix"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "Shmuma/ptan"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "udacity/deep-reinforcement-learning"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "NeuronDance/DeepRL"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "ShangtongZhang/DeepRL"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "simoninithomas/Deep_reinforcement_learning_Course"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "hill-a/stable-baselines"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "LantaoYu/MARL-Papers"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "openai/spinningup"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "PacktPublishing/Deep-Learning-with-Keras" ["e"=1]
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "higgsfield/RL-Adventure"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "andri27-ts/Reinforcement-Learning"
"deepmind/android_env" -> "HaveIBeenPwned/PwnedPasswordsCloudflareWorker"
"deepmind/android_env" -> "neuralmagic/sparseml" ["e"=1]
"deepmind/android_env" -> "clvrai/awesome-rl-envs"
"deepmind/android_env" -> "kzl/decision-transformer"
"deepmind/android_env" -> "google-research/rliable"
"deepmind/android_env" -> "rail-berkeley/d4rl"
"deepmind/android_env" -> "StarInitial/xpcheck"
"deepmind/android_env" -> "deepmind/acme"
"deepmind/android_env" -> "vxunderground/WinAPI-Tricks" ["e"=1]
"deepmind/android_env" -> "PettingZoo-Team/PettingZoo" ["e"=1]
"deepmind/android_env" -> "sail-sg/envpool"
"deepmind/android_env" -> "sorenisanerd/gotty" ["e"=1]
"deepmind/android_env" -> "YeWR/EfficientZero"
"deepmind/android_env" -> "google/brax"
"deepmind/android_env" -> "deepmind/reverb"
"deepmind/mujoco" -> "deepmind/dm_control"
"deepmind/mujoco" -> "openai/mujoco-py"
"deepmind/mujoco" -> "google/brax"
"deepmind/mujoco" -> "DLR-RM/stable-baselines3"
"deepmind/mujoco" -> "deepmind/mujoco_menagerie"
"deepmind/mujoco" -> "NVIDIA-Omniverse/IsaacGymEnvs"
"deepmind/mujoco" -> "deepmind/acme"
"deepmind/mujoco" -> "vwxyzjn/cleanrl"
"deepmind/mujoco" -> "ARISE-Initiative/robosuite"
"deepmind/mujoco" -> "stack-of-tasks/pinocchio" ["e"=1]
"deepmind/mujoco" -> "bulletphysics/bullet3" ["e"=1]
"deepmind/mujoco" -> "Farama-Foundation/Gymnasium"
"deepmind/mujoco" -> "RobotLocomotion/drake" ["e"=1]
"deepmind/mujoco" -> "thu-ml/tianshou"
"deepmind/mujoco" -> "rlworkgroup/metaworld"
"tonyzhaozh/aloha" -> "tonyzhaozh/act"
"tonyzhaozh/aloha" -> "haosulab/ManiSkill2"
"tonyzhaozh/aloha" -> "columbia-ai-robotics/diffusion_policy"
"tonyzhaozh/aloha" -> "facebookresearch/eai-vc"
"tonyzhaozh/aloha" -> "google-research/robopianist"
"tonyzhaozh/aloha" -> "eanswer/TactileSimulation" ["e"=1]
"tonyzhaozh/aloha" -> "columbia-ai-robotics/irp"
"Microsoft/malmo" -> "deepmind/lab"
"Microsoft/malmo" -> "openai/universe"
"Microsoft/malmo" -> "mgbellemare/Arcade-Learning-Environment"
"Microsoft/malmo" -> "openai/universe-starter-agent"
"Microsoft/malmo" -> "TorchCraft/TorchCraft" ["e"=1]
"Microsoft/malmo" -> "facebookresearch/ELF"
"Microsoft/malmo" -> "matthiasplappert/keras-rl"
"Microsoft/malmo" -> "facebookresearch/CommAI-env" ["e"=1]
"Microsoft/malmo" -> "tambetm/gym-minecraft"
"Microsoft/malmo" -> "openai/roboschool"
"Microsoft/malmo" -> "Microsoft/AirSim" ["e"=1]
"Microsoft/malmo" -> "devsisters/DQN-tensorflow"
"Microsoft/malmo" -> "Microsoft/CNTK" ["e"=1]
"Microsoft/malmo" -> "miyosuda/async_deep_reinforce"
"Microsoft/malmo" -> "coreylynch/async-rl"
"AcutronicRobotics/ros2learn" -> "AcutronicRobotics/gym-gazebo2"
"AcutronicRobotics/ros2learn" -> "AcutronicRobotics/moveit2"
"AcutronicRobotics/ros2learn" -> "AcutronicRobotics/MARA_threat_model"
"AcutronicRobotics/ros2learn" -> "AcutronicRobotics/MARA"
"AcutronicRobotics/ros2learn" -> "AcutronicRobotics/HRIM"
"tensorflow/fold" -> "stanfordnlp/treelstm" ["e"=1]
"tensorflow/fold" -> "clab/dynet" ["e"=1]
"tensorflow/fold" -> "deepmind/learning-to-learn"
"tensorflow/fold" -> "deepmind/sonnet"
"tensorflow/fold" -> "deepmind/dnc"
"tensorflow/fold" -> "google/seq2seq" ["e"=1]
"tensorflow/fold" -> "lanpa/tensorboard-pytorch" ["e"=1]
"tensorflow/fold" -> "LantaoYu/SeqGAN" ["e"=1]
"tensorflow/fold" -> "ppwwyyxx/tensorpack" ["e"=1]
"tensorflow/fold" -> "tensorflow/transform" ["e"=1]
"tensorflow/fold" -> "uber/horovod" ["e"=1]
"tensorflow/fold" -> "openai/cleverhans" ["e"=1]
"tensorflow/fold" -> "taolei87/sru" ["e"=1]
"tensorflow/fold" -> "openai/pixel-cnn" ["e"=1]
"tensorflow/fold" -> "tensorflow/skflow" ["e"=1]
"openai/multiagent-competition" -> "openai/multiagent-particle-envs"
"openai/multiagent-competition" -> "openai/robosumo"
"openai/multiagent-competition" -> "geek-ai/MAgent"
"openai/multiagent-competition" -> "openai/mlsh"
"openai/multiagent-competition" -> "openai/maddpg"
"openai/multiagent-competition" -> "openai/imitation"
"openai/multiagent-competition" -> "deepmind/scalable_agent"
"openai/multiagent-competition" -> "openai/multi-agent-emergence-environments"
"openai/multiagent-competition" -> "pathak22/noreward-rl"
"openai/multiagent-competition" -> "deepmind/pycolab"
"openai/multiagent-competition" -> "MultiAgentLearning/playground"
"openai/multiagent-competition" -> "openai/large-scale-curiosity"
"openai/multiagent-competition" -> "oxwhirl/smac"
"openai/multiagent-competition" -> "openai/roboschool"
"openai/multiagent-competition" -> "openai/coinrun"
"tensorlayer/tensorlayer" -> "tensorlayer/srgan" ["e"=1]
"tensorlayer/tensorlayer" -> "tensorlayer/RLzoo" ["e"=1]
"tensorlayer/tensorlayer" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"tensorlayer/tensorlayer" -> "google/dopamine"
"tensorlayer/tensorlayer" -> "NeuronDance/DeepRL"
"tensorlayer/tensorlayer" -> "tensorlayer/awesome-tensorlayer"
"tensorlayer/tensorlayer" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"tensorlayer/tensorlayer" -> "openai/baselines"
"tensorlayer/tensorlayer" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"tensorlayer/tensorlayer" -> "openai/spinningup"
"tensorlayer/tensorlayer" -> "hill-a/stable-baselines"
"tensorlayer/tensorlayer" -> "tensorpack/tensorpack" ["e"=1]
"tensorlayer/tensorlayer" -> "keras-rl/keras-rl"
"tensorlayer/tensorlayer" -> "tensorflow/agents"
"tensorlayer/tensorlayer" -> "dennybritz/reinforcement-learning"
"reinforceio/tensorforce" -> "matthiasplappert/keras-rl"
"reinforceio/tensorforce" -> "NervanaSystems/coach"
"reinforceio/tensorforce" -> "rll/rllab"
"reinforceio/tensorforce" -> "awjuliani/DeepRL-Agents"
"reinforceio/tensorforce" -> "rlcode/reinforcement-learning"
"reinforceio/tensorforce" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"reinforceio/tensorforce" -> "openai/universe-starter-agent"
"reinforceio/tensorforce" -> "williamFalcon/DeepRLHacks"
"reinforceio/tensorforce" -> "facebookresearch/ELF"
"reinforceio/tensorforce" -> "carpedm20/deep-rl-tensorflow"
"reinforceio/tensorforce" -> "joschu/modular_rl"
"reinforceio/tensorforce" -> "deepmind/trfl"
"reinforceio/tensorforce" -> "NVlabs/GA3C"
"reinforceio/tensorforce" -> "tensorflow/agents"
"reinforceio/tensorforce" -> "deepmind/dm_control"
"openai/multiagent-particle-envs" -> "openai/maddpg"
"openai/multiagent-particle-envs" -> "oxwhirl/pymarl"
"openai/multiagent-particle-envs" -> "geek-ai/MAgent"
"openai/multiagent-particle-envs" -> "oxwhirl/smac"
"openai/multiagent-particle-envs" -> "LantaoYu/MARL-Papers"
"openai/multiagent-particle-envs" -> "marlbenchmark/on-policy"
"openai/multiagent-particle-envs" -> "starry-sky6688/StarCraft"
"openai/multiagent-particle-envs" -> "xuehy/pytorch-maddpg"
"openai/multiagent-particle-envs" -> "shariqiqbal2810/MAAC"
"openai/multiagent-particle-envs" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"openai/multiagent-particle-envs" -> "sisl/MADRL"
"openai/multiagent-particle-envs" -> "shariqiqbal2810/maddpg-pytorch"
"openai/multiagent-particle-envs" -> "Farama-Foundation/PettingZoo"
"openai/multiagent-particle-envs" -> "openai/multi-agent-emergence-environments"
"openai/multiagent-particle-envs" -> "openai/multiagent-competition"
"NervanaSystems/coach" -> "reinforceio/tensorforce"
"NervanaSystems/coach" -> "hill-a/stable-baselines"
"NervanaSystems/coach" -> "rll/rllab"
"NervanaSystems/coach" -> "deepmind/trfl"
"NervanaSystems/coach" -> "vitchyr/rlkit"
"NervanaSystems/coach" -> "chainer/chainerrl"
"NervanaSystems/coach" -> "tensorflow/agents"
"NervanaSystems/coach" -> "rlworkgroup/garage"
"NervanaSystems/coach" -> "deepmind/dm_control"
"NervanaSystems/coach" -> "williamFalcon/DeepRLHacks"
"NervanaSystems/coach" -> "facebookresearch/Horizon"
"NervanaSystems/coach" -> "astooke/rlpyt"
"NervanaSystems/coach" -> "tensorforce/tensorforce"
"NervanaSystems/coach" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"NervanaSystems/coach" -> "kengz/SLM-Lab"
"muupan/async-rl" -> "miyosuda/async_deep_reinforce"
"muupan/async-rl" -> "coreylynch/async-rl"
"muupan/async-rl" -> "NVlabs/GA3C"
"muupan/async-rl" -> "joschu/modular_rl"
"muupan/async-rl" -> "rllab/rllab"
"muupan/async-rl" -> "muupan/deep-reinforcement-learning-papers"
"muupan/async-rl" -> "miyosuda/unreal"
"muupan/async-rl" -> "Kaixhin/Atari"
"muupan/async-rl" -> "Ardavans/DSR"
"muupan/async-rl" -> "Jabberwockyll/deep_rl_ale"
"muupan/async-rl" -> "openai/universe-starter-agent"
"muupan/async-rl" -> "Zeta36/Asynchronous-Methods-for-Deep-Reinforcement-Learning"
"muupan/async-rl" -> "traai/async-deep-rl"
"muupan/async-rl" -> "carpedm20/deep-rl-tensorflow"
"muupan/async-rl" -> "openai/rllab"
"wangshusen/DRL" -> "wangshusen/DeepLearning"
"wangshusen/DRL" -> "datawhalechina/easy-rl"
"wangshusen/DRL" -> "NeuronDance/DeepRL"
"wangshusen/DRL" -> "boyu-ai/Hands-on-RL"
"wangshusen/DRL" -> "thu-ml/tianshou"
"wangshusen/DRL" -> "zhoubolei/introRL"
"wangshusen/DRL" -> "DeepRLChinese/DeepRL-Chinese"
"wangshusen/DRL" -> "PaddlePaddle/PARL"
"wangshusen/DRL" -> "MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning"
"wangshusen/DRL" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"wangshusen/DRL" -> "wangshusen/RecommenderSystem" ["e"=1]
"wangshusen/DRL" -> "AI4Finance-Foundation/ElegantRL"
"wangshusen/DRL" -> "cuhkrlcourse/RLexample"
"wangshusen/DRL" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"wangshusen/DRL" -> "DLR-RM/stable-baselines3"
"wangshusen/DeepLearning" -> "wangshusen/DRL"
"wangshusen/DeepLearning" -> "datawhalechina/easy-rl"
"wangshusen/DeepLearning" -> "mli/paper-reading" ["e"=1]
"wangshusen/DeepLearning" -> "wangshusen/RecommenderSystem" ["e"=1]
"wangshusen/DeepLearning" -> "zhoubolei/introRL"
"wangshusen/DeepLearning" -> "NeuronDance/DeepRL"
"wangshusen/DeepLearning" -> "wmathor/nlp-tutorial" ["e"=1]
"wangshusen/DeepLearning" -> "dair-ai/ml-visuals" ["e"=1]
"wangshusen/DeepLearning" -> "wangshusen/AdvancedAlgorithms"
"wangshusen/DeepLearning" -> "thu-ml/tianshou"
"wangshusen/DeepLearning" -> "xmu-xiaoma666/External-Attention-pytorch" ["e"=1]
"wangshusen/DeepLearning" -> "TingsongYu/PyTorch_Tutorial" ["e"=1]
"wangshusen/DeepLearning" -> "tsyw/MachineLearningNotes" ["e"=1]
"wangshusen/DeepLearning" -> "DA-southampton/NLP_ability" ["e"=1]
"wangshusen/DeepLearning" -> "chaoyanghe/Awesome-Federated-Learning" ["e"=1]
"tensorflow/agents" -> "hill-a/stable-baselines"
"tensorflow/agents" -> "tensorforce/tensorforce"
"tensorflow/agents" -> "deepmind/trfl"
"tensorflow/agents" -> "google/dopamine"
"tensorflow/agents" -> "openai/baselines"
"tensorflow/agents" -> "deepmind/acme"
"tensorflow/agents" -> "keras-rl/keras-rl"
"tensorflow/agents" -> "NervanaSystems/coach"
"tensorflow/agents" -> "reinforceio/tensorforce"
"tensorflow/agents" -> "deepmind/dm_control"
"tensorflow/agents" -> "astooke/rlpyt"
"tensorflow/agents" -> "rll/rllab"
"tensorflow/agents" -> "rlworkgroup/garage"
"tensorflow/agents" -> "openai/spinningup"
"tensorflow/agents" -> "deepmind/bsuite"
"eugenevinitsky/sequential_social_dilemma_games" -> "deepmind/meltingpot"
"eugenevinitsky/sequential_social_dilemma_games" -> "minqi/learning-to-communicate-pytorch"
"eugenevinitsky/sequential_social_dilemma_games" -> "social-dilemma/multiagent"
"eugenevinitsky/sequential_social_dilemma_games" -> "koulanurag/ma-gym"
"eugenevinitsky/sequential_social_dilemma_games" -> "IC3Net/IC3Net"
"eugenevinitsky/sequential_social_dilemma_games" -> "oxwhirl/pymarl"
"eugenevinitsky/sequential_social_dilemma_games" -> "YuhangSong/Arena-Baselines"
"eugenevinitsky/sequential_social_dilemma_games" -> "alshedivat/lola"
"eugenevinitsky/sequential_social_dilemma_games" -> "ermongroup/MA-AIRL"
"eugenevinitsky/sequential_social_dilemma_games" -> "shariqiqbal2810/MAAC"
"eugenevinitsky/sequential_social_dilemma_games" -> "oxwhirl/smac"
"eugenevinitsky/sequential_social_dilemma_games" -> "PKU-AI-Edge/DGN"
"eugenevinitsky/sequential_social_dilemma_games" -> "schroederdewitt/multiagent_mujoco"
"eugenevinitsky/sequential_social_dilemma_games" -> "HumanCompatibleAI/human_aware_rl"
"eugenevinitsky/sequential_social_dilemma_games" -> "TonghanWang/ROMA"
"deepmind/hanabi-learning-environment" -> "facebookresearch/hanabi_SAD"
"deepmind/hanabi-learning-environment" -> "oxwhirl/smac"
"deepmind/hanabi-learning-environment" -> "oxwhirl/pymarl"
"deepmind/hanabi-learning-environment" -> "MultiAgentLearning/playground"
"deepmind/hanabi-learning-environment" -> "facebookresearch/Hanabi_SPARTA"
"deepmind/hanabi-learning-environment" -> "eugenevinitsky/sequential_social_dilemma_games"
"deepmind/hanabi-learning-environment" -> "deepmind/bsuite"
"deepmind/hanabi-learning-environment" -> "openai/multiagent-particle-envs"
"deepmind/hanabi-learning-environment" -> "maximecb/gym-minigrid"
"deepmind/hanabi-learning-environment" -> "openai/neural-mmo"
"deepmind/hanabi-learning-environment" -> "google-research/planet"
"deepmind/hanabi-learning-environment" -> "deepmind/scalable_agent"
"deepmind/hanabi-learning-environment" -> "deepmind/spriteworld"
"deepmind/hanabi-learning-environment" -> "vitchyr/multiworld"
"deepmind/hanabi-learning-environment" -> "Quuxplusone/Hanabi"
"Scitator/Run-Skeleton-Run" -> "fgvbrt/nips_rl" ["e"=1]
"Scitator/Run-Skeleton-Run" -> "AdamStelmaszczyk/learning2run"
"Scitator/Run-Skeleton-Run" -> "nnaisense/2017-learning-to-run"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "sudharsan13296/Hands-On-Reinforcement-Learning-With-Python"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "qqiang00/reinforce"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "PacktPublishing/Reinforcement-Learning-Algorithms-with-Python"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "ZhiqingXiao/rl-book"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "gxnk/reinforcement-learning-code"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "wwxFromTju/awesome-reinforcement-learning-zh"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "haarnoja/sac"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "NeuronDance/DeepRL"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "StepNeverStop/RLs"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "PacktPublishing/Python-Reinforcement-Learning-Projects"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "openai/maddpg"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "CoderWangcai/DRL_Path_Planning"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "TianhongDai/reinforcement-learning-algorithms"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "simoninithomas/Deep_reinforcement_learning_Course"
"zhoubolei/introRL" -> "cuhkrlcourse/RLexample"
"zhoubolei/introRL" -> "NeuronDance/DeepRL"
"zhoubolei/introRL" -> "wwxFromTju/awesome-reinforcement-learning-zh"
"zhoubolei/introRL" -> "datawhalechina/easy-rl"
"zhoubolei/introRL" -> "thu-ml/tianshou"
"zhoubolei/introRL" -> "cuhkrlcourse/DeepRL-Tutorials"
"zhoubolei/introRL" -> "datawhalechina/leedeeprl-notes"
"zhoubolei/introRL" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"zhoubolei/introRL" -> "sfujim/TD3"
"zhoubolei/introRL" -> "wangshusen/DRL"
"zhoubolei/introRL" -> "PaddlePaddle/PARL"
"zhoubolei/introRL" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"zhoubolei/introRL" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"zhoubolei/introRL" -> "openai/spinningup"
"zhoubolei/introRL" -> "ZhiqingXiao/rl-book"
"simoninithomas/Deep_reinforcement_learning_Course" -> "udacity/deep-reinforcement-learning"
"simoninithomas/Deep_reinforcement_learning_Course" -> "dennybritz/reinforcement-learning"
"simoninithomas/Deep_reinforcement_learning_Course" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"simoninithomas/Deep_reinforcement_learning_Course" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"simoninithomas/Deep_reinforcement_learning_Course" -> "openai/spinningup"
"simoninithomas/Deep_reinforcement_learning_Course" -> "andri27-ts/Reinforcement-Learning"
"simoninithomas/Deep_reinforcement_learning_Course" -> "openai/baselines"
"simoninithomas/Deep_reinforcement_learning_Course" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"simoninithomas/Deep_reinforcement_learning_Course" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On"
"simoninithomas/Deep_reinforcement_learning_Course" -> "keras-rl/keras-rl"
"simoninithomas/Deep_reinforcement_learning_Course" -> "yandexdataschool/Practical_RL"
"simoninithomas/Deep_reinforcement_learning_Course" -> "aikorea/awesome-rl"
"simoninithomas/Deep_reinforcement_learning_Course" -> "rlcode/reinforcement-learning"
"simoninithomas/Deep_reinforcement_learning_Course" -> "hill-a/stable-baselines"
"simoninithomas/Deep_reinforcement_learning_Course" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"openai/mujoco-py" -> "deepmind/dm_control"
"openai/mujoco-py" -> "openai/roboschool"
"openai/mujoco-py" -> "rll/rllab"
"openai/mujoco-py" -> "openai/spinningup"
"openai/mujoco-py" -> "openai/baselines"
"openai/mujoco-py" -> "deepmind/mujoco"
"openai/mujoco-py" -> "hill-a/stable-baselines"
"openai/mujoco-py" -> "rlworkgroup/garage"
"openai/mujoco-py" -> "openai/multiagent-particle-envs"
"openai/mujoco-py" -> "vitchyr/rlkit"
"openai/mujoco-py" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"openai/mujoco-py" -> "DLR-RM/stable-baselines3"
"openai/mujoco-py" -> "rail-berkeley/softlearning"
"openai/mujoco-py" -> "openai/retro"
"openai/mujoco-py" -> "haarnoja/sac"
"StepNeverStop/RLs" -> "zhangchuheng123/Reinforcement-Implementation"
"StepNeverStop/RLs" -> "RITCHIEHuang/DeepRL_Algorithms"
"StepNeverStop/RLs" -> "starry-sky6688/StarCraft"
"StepNeverStop/RLs" -> "Yonv1943/ElegantRL"
"StepNeverStop/RLs" -> "anita-hu/TF2-RL"
"StepNeverStop/RLs" -> "kaixindelele/DRLib"
"StepNeverStop/RLs" -> "Rafael1s/Deep-Reinforcement-Learning-Algorithms"
"StepNeverStop/RLs" -> "NeuronDance/DeepRL"
"StepNeverStop/RLs" -> "sjtu-marl/malib"
"StepNeverStop/RLs" -> "dongminlee94/deep_rl"
"StepNeverStop/RLs" -> "quantumiracle/SOTA-RL-Algorithms"
"StepNeverStop/RLs" -> "iffiX/machin"
"StepNeverStop/RLs" -> "starry-sky6688/MADDPG"
"StepNeverStop/RLs" -> "gxywy/rl-plotter"
"StepNeverStop/RLs" -> "keiohta/tf2rl"
"devsisters/DQN-tensorflow" -> "carpedm20/deep-rl-tensorflow"
"devsisters/DQN-tensorflow" -> "tambetm/simple_dqn"
"devsisters/DQN-tensorflow" -> "junhyukoh/deep-reinforcement-learning-papers"
"devsisters/DQN-tensorflow" -> "awjuliani/DeepRL-Agents"
"devsisters/DQN-tensorflow" -> "nivwusquorum/tensorflow-deepq"
"devsisters/DQN-tensorflow" -> "matthiasplappert/keras-rl"
"devsisters/DQN-tensorflow" -> "miyosuda/async_deep_reinforce"
"devsisters/DQN-tensorflow" -> "kuz/DeepMind-Atari-Deep-Q-Learner"
"devsisters/DQN-tensorflow" -> "coreylynch/async-rl"
"devsisters/DQN-tensorflow" -> "rlcode/reinforcement-learning"
"devsisters/DQN-tensorflow" -> "mgbellemare/Arcade-Learning-Environment"
"devsisters/DQN-tensorflow" -> "spragunr/deep_q_rl"
"devsisters/DQN-tensorflow" -> "keon/deep-q-learning"
"devsisters/DQN-tensorflow" -> "reinforceio/tensorforce"
"devsisters/DQN-tensorflow" -> "ppwwyyxx/tensorpack" ["e"=1]
"udacity/deep-reinforcement-learning" -> "ShangtongZhang/DeepRL"
"udacity/deep-reinforcement-learning" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"udacity/deep-reinforcement-learning" -> "simoninithomas/Deep_reinforcement_learning_Course"
"udacity/deep-reinforcement-learning" -> "dennybritz/reinforcement-learning"
"udacity/deep-reinforcement-learning" -> "udacity/deep-learning-v2-pytorch" ["e"=1]
"udacity/deep-reinforcement-learning" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"udacity/deep-reinforcement-learning" -> "aikorea/awesome-rl"
"udacity/deep-reinforcement-learning" -> "higgsfield/RL-Adventure-2"
"udacity/deep-reinforcement-learning" -> "openai/spinningup"
"udacity/deep-reinforcement-learning" -> "udacity/deep-learning" ["e"=1]
"udacity/deep-reinforcement-learning" -> "higgsfield/RL-Adventure"
"udacity/deep-reinforcement-learning" -> "openai/baselines"
"udacity/deep-reinforcement-learning" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On"
"udacity/deep-reinforcement-learning" -> "yandexdataschool/Practical_RL"
"udacity/deep-reinforcement-learning" -> "rlcode/reinforcement-learning"
"andyljones/reinforcement-learning-discord-wiki" -> "laszukdawid/ai-traineree"
"dongminlee94/deep_rl" -> "MrSyee/pg-is-all-you-need"
"dongminlee94/deep_rl" -> "quantumiracle/SOTA-RL-Algorithms"
"dongminlee94/deep_rl" -> "Rafael1s/Deep-Reinforcement-Learning-Algorithms"
"dongminlee94/deep_rl" -> "Khrylx/PyTorch-RL"
"dongminlee94/deep_rl" -> "TianhongDai/reinforcement-learning-algorithms"
"dongminlee94/deep_rl" -> "RchalYang/torchrl"
"dongminlee94/deep_rl" -> "RITCHIEHuang/DeepRL_Algorithms"
"dongminlee94/deep_rl" -> "BY571/Soft-Actor-Critic-and-Extensions"
"dongminlee94/deep_rl" -> "zhangchuheng123/Reinforcement-Implementation"
"dongminlee94/deep_rl" -> "qfettes/DeepRL-Tutorials"
"dongminlee94/deep_rl" -> "dongminlee94/Samsung-DRL-Code" ["e"=1]
"dongminlee94/deep_rl" -> "reinforcement-learning-kr/lets-do-irl"
"dongminlee94/deep_rl" -> "denisyarats/pytorch_sac"
"dongminlee94/deep_rl" -> "quantumiracle/Popular-RL-Algorithms"
"dongminlee94/deep_rl" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"facebookresearch/slbo" -> "roosephu/slbo"
"qfettes/DeepRL-Tutorials" -> "higgsfield/RL-Adventure"
"qfettes/DeepRL-Tutorials" -> "Kaixhin/Rainbow"
"qfettes/DeepRL-Tutorials" -> "astooke/rlpyt"
"qfettes/DeepRL-Tutorials" -> "ShangtongZhang/DeepRL"
"qfettes/DeepRL-Tutorials" -> "MrSyee/pg-is-all-you-need"
"qfettes/DeepRL-Tutorials" -> "dongminlee94/deep_rl"
"qfettes/DeepRL-Tutorials" -> "higgsfield/RL-Adventure-2"
"qfettes/DeepRL-Tutorials" -> "TianhongDai/reinforcement-learning-algorithms"
"qfettes/DeepRL-Tutorials" -> "Khrylx/PyTorch-RL"
"qfettes/DeepRL-Tutorials" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"qfettes/DeepRL-Tutorials" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"qfettes/DeepRL-Tutorials" -> "Curt-Park/rainbow-is-all-you-need"
"qfettes/DeepRL-Tutorials" -> "sfujim/TD3"
"qfettes/DeepRL-Tutorials" -> "seungeunrho/minimalRL"
"qfettes/DeepRL-Tutorials" -> "vitchyr/rlkit"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "thu-ml/tianshou"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "higgsfield/RL-Adventure"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "sfujim/TD3"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "nikhilbarhate99/PPO-PyTorch"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "ShangtongZhang/DeepRL"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "NeuronDance/DeepRL"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "higgsfield/RL-Adventure-2"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "seungeunrho/minimalRL"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "openai/multiagent-particle-envs"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "starry-sky6688/StarCraft"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "oxwhirl/pymarl"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "astooke/rlpyt"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "LantaoYu/MARL-Papers"
"Kautenja/gym-super-mario-bros" -> "Kautenja/nes-py"
"Kautenja/gym-super-mario-bros" -> "ppaquette/gym-super-mario"
"Kautenja/gym-super-mario-bros" -> "uvipen/Super-mario-bros-A3C-pytorch"
"Kautenja/gym-super-mario-bros" -> "uvipen/Super-mario-bros-PPO-pytorch"
"Kautenja/gym-super-mario-bros" -> "openai/random-network-distillation"
"Kautenja/gym-super-mario-bros" -> "openai/retro"
"Kautenja/gym-super-mario-bros" -> "jcwleo/mario_rl"
"Kautenja/gym-super-mario-bros" -> "mpSchrader/gym-sokoban"
"Kautenja/gym-super-mario-bros" -> "maximecb/gym-minigrid"
"Kautenja/gym-super-mario-bros" -> "mwydmuch/ViZDoom"
"Kautenja/gym-super-mario-bros" -> "pathak22/noreward-rl"
"Kautenja/gym-super-mario-bros" -> "minerllabs/minerl"
"Kautenja/gym-super-mario-bros" -> "kenjyoung/MinAtar"
"Kautenja/gym-super-mario-bros" -> "MrSyee/pg-is-all-you-need"
"Kautenja/gym-super-mario-bros" -> "openai/procgen"
"philtabor/Deep-Q-Learning-Paper-To-Code" -> "philtabor/Actor-Critic-Methods-Paper-To-Code"
"philtabor/Deep-Q-Learning-Paper-To-Code" -> "philtabor/Reinforcement-Learning-In-Motion"
"philtabor/Deep-Q-Learning-Paper-To-Code" -> "philtabor/Youtube-Code-Repository"
"reiniscimurs/DRL-robot-navigation" -> "reiniscimurs/GDAE"
"reiniscimurs/DRL-robot-navigation" -> "m5823779/motion-planner-reinforcement-learning"
"reiniscimurs/DRL-robot-navigation" -> "LeeKeyu/sarl_star"
"reiniscimurs/DRL-robot-navigation" -> "Crawford-fang/turtlebot3_DQN"
"applenob/rl_learn" -> "zhuliquan/reinforcement_learning_basic_book"
"applenob/rl_learn" -> "wwxFromTju/awesome-reinforcement-learning-zh"
"applenob/rl_learn" -> "apachecn/stanford-cs234-notes-zh"
"berkeleydeeprlcourse/homework" -> "rll/rllab"
"berkeleydeeprlcourse/homework" -> "vitchyr/rlkit"
"berkeleydeeprlcourse/homework" -> "rail-berkeley/softlearning"
"berkeleydeeprlcourse/homework" -> "williamFalcon/DeepRLHacks"
"berkeleydeeprlcourse/homework" -> "joschu/modular_rl"
"berkeleydeeprlcourse/homework" -> "ShangtongZhang/DeepRL"
"berkeleydeeprlcourse/homework" -> "astooke/rlpyt"
"berkeleydeeprlcourse/homework" -> "openai/mujoco-py"
"berkeleydeeprlcourse/homework" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"berkeleydeeprlcourse/homework" -> "junhyukoh/deep-reinforcement-learning-papers"
"berkeleydeeprlcourse/homework" -> "carpedm20/deep-rl-tensorflow"
"berkeleydeeprlcourse/homework" -> "higgsfield/RL-Adventure-2"
"berkeleydeeprlcourse/homework" -> "haarnoja/sac"
"berkeleydeeprlcourse/homework" -> "sfujim/TD3"
"berkeleydeeprlcourse/homework" -> "LantaoYu/MARL-Papers"
"minerllabs/competition_submission_template" -> "minerllabs/baselines"
"seungeunrho/minimalRL" -> "astooke/rlpyt"
"seungeunrho/minimalRL" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"seungeunrho/minimalRL" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"seungeunrho/minimalRL" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"seungeunrho/minimalRL" -> "higgsfield/RL-Adventure-2"
"seungeunrho/minimalRL" -> "vwxyzjn/cleanrl"
"seungeunrho/minimalRL" -> "higgsfield/RL-Adventure"
"seungeunrho/minimalRL" -> "hill-a/stable-baselines"
"seungeunrho/minimalRL" -> "Curt-Park/rainbow-is-all-you-need"
"seungeunrho/minimalRL" -> "ShangtongZhang/DeepRL"
"seungeunrho/minimalRL" -> "nikhilbarhate99/PPO-PyTorch"
"seungeunrho/minimalRL" -> "MrSyee/pg-is-all-you-need"
"seungeunrho/minimalRL" -> "sfujim/TD3"
"seungeunrho/minimalRL" -> "maximecb/gym-minigrid"
"seungeunrho/minimalRL" -> "LantaoYu/MARL-Papers"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "sudharsan13296/Deep-Reinforcement-Learning-With-Python"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "PacktPublishing/Hands-On-Reinforcement-Learning-with-Python"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "sudharsan13296/Hands-On-Meta-Learning-With-Python" ["e"=1]
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "TianhongDai/reinforcement-learning-algorithms"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "andri27-ts/Reinforcement-Learning"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "udacity/deep-reinforcement-learning"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "omerbsezer/Reinforcement_learning_tutorial_with_demo"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "vmayoral/basic_reinforcement_learning"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "rll/rllab"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "dalmia/David-Silver-Reinforcement-learning"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "qfettes/DeepRL-Tutorials"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "simoninithomas/Deep_reinforcement_learning_Course"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "sudharsan13296/Awesome-Meta-Learning" ["e"=1]
"glample/Arnold" -> "akolishchak/doom-net-pytorch"
"glample/Arnold" -> "mwydmuch/ViZDoom"
"glample/Arnold" -> "ruiminshen/yolo2-pytorch" ["e"=1]
"glample/Arnold" -> "Breakend/gym-extensions"
"glample/Arnold" -> "IntelVCL/DirectFuturePrediction"
"glample/Arnold" -> "NVlabs/GA3C"
"glample/Arnold" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"glample/Arnold" -> "ikostrikov/pytorch-a3c"
"glample/Arnold" -> "zuoxingdong/gym-maze"
"glample/Arnold" -> "itaicaspi/keras-dqn-doom"
"glample/Arnold" -> "iabem97/topanga" ["e"=1]
"glample/Arnold" -> "Breakend/DeepReinforcementLearningThatMatters"
"glample/Arnold" -> "joschu/modular_rl"
"glample/Arnold" -> "AppliedDataSciencePartners/WorldModels"
"glample/Arnold" -> "pathak22/noreward-rl"
"semitable/robotic-warehouse" -> "semitable/lb-foraging"
"semitable/robotic-warehouse" -> "uoe-agents/epymarl"
"semitable/robotic-warehouse" -> "uoe-agents/robotic-warehouse"
"omerbsezer/Generative_Models_Tutorial_with_Demo" -> "omerbsezer/Reinforcement_learning_tutorial_with_demo"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "thu-ml/tianshou"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "astooke/rlpyt"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "openai/spinningup"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "ShangtongZhang/DeepRL"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "DLR-RM/stable-baselines3"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "hill-a/stable-baselines"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "NeuronDance/DeepRL"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "openai/baselines"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "higgsfield/RL-Adventure"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "higgsfield/RL-Adventure-2"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "dennybritz/reinforcement-learning"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "seungeunrho/minimalRL"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "mimoralea/gdrl"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "kengz/SLM-Lab"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "qfettes/DeepRL-Tutorials"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "kengz/awesome-deep-rl"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "MrSyee/pg-is-all-you-need"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "geek-ai/MAgent"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "omerbsezer/Reinforcement_learning_tutorial_with_demo"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "maxpumperla/deep_learning_and_the_game_of_go" ["e"=1]
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "TianhongDai/reinforcement-learning-algorithms"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "andri27-ts/Reinforcement-Learning"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "udacity/deep-reinforcement-learning"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "philtabor/Youtube-Code-Repository"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "oxwhirl/pymarl"
"andri27-ts/Reinforcement-Learning" -> "simoninithomas/Deep_reinforcement_learning_Course"
"andri27-ts/Reinforcement-Learning" -> "udacity/deep-reinforcement-learning"
"andri27-ts/Reinforcement-Learning" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"andri27-ts/Reinforcement-Learning" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"andri27-ts/Reinforcement-Learning" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On"
"andri27-ts/Reinforcement-Learning" -> "rlcode/reinforcement-learning"
"andri27-ts/Reinforcement-Learning" -> "yandexdataschool/Practical_RL"
"andri27-ts/Reinforcement-Learning" -> "qfettes/DeepRL-Tutorials"
"andri27-ts/Reinforcement-Learning" -> "higgsfield/RL-Adventure"
"andri27-ts/Reinforcement-Learning" -> "andri27-ts/1-Year-MachineLearning-Journey"
"andri27-ts/Reinforcement-Learning" -> "sudharsan13296/Hands-On-Reinforcement-Learning-With-Python"
"andri27-ts/Reinforcement-Learning" -> "Rafael1s/Deep-Reinforcement-Learning-Algorithms"
"andri27-ts/Reinforcement-Learning" -> "NeuronDance/DeepRL"
"andri27-ts/Reinforcement-Learning" -> "higgsfield/RL-Adventure-2"
"andri27-ts/Reinforcement-Learning" -> "aikorea/awesome-rl"
"higgsfield/RL-Adventure" -> "higgsfield/RL-Adventure-2"
"higgsfield/RL-Adventure" -> "ShangtongZhang/DeepRL"
"higgsfield/RL-Adventure" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"higgsfield/RL-Adventure" -> "yandexdataschool/Practical_RL"
"higgsfield/RL-Adventure" -> "Kaixhin/Rainbow"
"higgsfield/RL-Adventure" -> "qfettes/DeepRL-Tutorials"
"higgsfield/RL-Adventure" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"higgsfield/RL-Adventure" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"higgsfield/RL-Adventure" -> "astooke/rlpyt"
"higgsfield/RL-Adventure" -> "hill-a/stable-baselines"
"higgsfield/RL-Adventure" -> "Curt-Park/rainbow-is-all-you-need"
"higgsfield/RL-Adventure" -> "vitchyr/rlkit"
"higgsfield/RL-Adventure" -> "seungeunrho/minimalRL"
"higgsfield/RL-Adventure" -> "openai/baselines"
"higgsfield/RL-Adventure" -> "udacity/deep-reinforcement-learning"
"openai/universe" -> "deepmind/lab"
"openai/universe" -> "openai/universe-starter-agent"
"openai/universe" -> "openai/gym"
"openai/universe" -> "deepmind/learning-to-learn"
"openai/universe" -> "openai/baselines"
"openai/universe" -> "deepmind/sonnet"
"openai/universe" -> "fchollet/keras" ["e"=1]
"openai/universe" -> "openai/retro"
"openai/universe" -> "songrotek/Deep-Learning-Papers-Reading-Roadmap" ["e"=1]
"openai/universe" -> "tensorflow/magenta" ["e"=1]
"openai/universe" -> "dennybritz/reinforcement-learning"
"openai/universe" -> "deepmind/pysc2" ["e"=1]
"openai/universe" -> "tflearn/tflearn" ["e"=1]
"openai/universe" -> "matthiasplappert/keras-rl"
"openai/universe" -> "oxford-cs-deepnlp-2017/lectures" ["e"=1]
"deepmind/lab" -> "openai/universe"
"deepmind/lab" -> "deepmind/learning-to-learn"
"deepmind/lab" -> "deepmind/sonnet"
"deepmind/lab" -> "deepmind/dm_control"
"deepmind/lab" -> "openai/baselines"
"deepmind/lab" -> "deepmind/pysc2" ["e"=1]
"deepmind/lab" -> "google/dopamine"
"deepmind/lab" -> "openai/gym"
"deepmind/lab" -> "deepmind/trfl"
"deepmind/lab" -> "deepmind/dnc"
"deepmind/lab" -> "dennybritz/reinforcement-learning"
"deepmind/lab" -> "aikorea/awesome-rl"
"deepmind/lab" -> "facebookresearch/ELF"
"deepmind/lab" -> "rll/rllab"
"deepmind/lab" -> "openai/roboschool"
"keras-rl/keras-rl" -> "tensorflow/agents"
"keras-rl/keras-rl" -> "hill-a/stable-baselines"
"keras-rl/keras-rl" -> "openai/baselines"
"keras-rl/keras-rl" -> "google/dopamine"
"keras-rl/keras-rl" -> "tensorforce/tensorforce"
"keras-rl/keras-rl" -> "dennybritz/reinforcement-learning"
"keras-rl/keras-rl" -> "rll/rllab"
"keras-rl/keras-rl" -> "deepmind/trfl"
"keras-rl/keras-rl" -> "udacity/deep-reinforcement-learning"
"keras-rl/keras-rl" -> "rlcode/reinforcement-learning"
"keras-rl/keras-rl" -> "aikorea/awesome-rl"
"keras-rl/keras-rl" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"keras-rl/keras-rl" -> "simoninithomas/Deep_reinforcement_learning_Course"
"keras-rl/keras-rl" -> "openai/spinningup"
"keras-rl/keras-rl" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"medipixel/rl_algorithms" -> "MrSyee/pg-is-all-you-need"
"medipixel/rl_algorithms" -> "opherlieber/rltime"
"medipixel/rl_algorithms" -> "cyoon1729/RLcycle"
"medipixel/rl_algorithms" -> "Curt-Park/rainbow-is-all-you-need"
"medipixel/rl_algorithms" -> "openai/random-network-distillation"
"medipixel/rl_algorithms" -> "Khrylx/PyTorch-RL"
"medipixel/rl_algorithms" -> "sfujim/BCQ"
"medipixel/rl_algorithms" -> "kakaoenterprise/JORLDY" ["e"=1]
"medipixel/rl_algorithms" -> "quantumiracle/SOTA-RL-Algorithms"
"medipixel/rl_algorithms" -> "unixpickle/anyrl-py" ["e"=1]
"medipixel/rl_algorithms" -> "reinforcement-learning-kr/pg_travel"
"medipixel/rl_algorithms" -> "astooke/rlpyt"
"medipixel/rl_algorithms" -> "rlgraph/rlgraph"
"medipixel/rl_algorithms" -> "kairproject/kair_algorithms_draft"
"medipixel/rl_algorithms" -> "nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch"
"rll/rllab" -> "rlworkgroup/garage"
"rll/rllab" -> "vitchyr/rlkit"
"rll/rllab" -> "deepmind/dm_control"
"rll/rllab" -> "haarnoja/sac"
"rll/rllab" -> "openai/baselines"
"rll/rllab" -> "rail-berkeley/softlearning"
"rll/rllab" -> "berkeleydeeprlcourse/homework"
"rll/rllab" -> "hill-a/stable-baselines"
"rll/rllab" -> "openai/mujoco-py"
"rll/rllab" -> "reinforceio/tensorforce"
"rll/rllab" -> "astooke/rlpyt"
"rll/rllab" -> "ShangtongZhang/DeepRL"
"rll/rllab" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"rll/rllab" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"rll/rllab" -> "higgsfield/RL-Adventure-2"
"ShangtongZhang/DeepRL" -> "udacity/deep-reinforcement-learning"
"ShangtongZhang/DeepRL" -> "higgsfield/RL-Adventure"
"ShangtongZhang/DeepRL" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"ShangtongZhang/DeepRL" -> "astooke/rlpyt"
"ShangtongZhang/DeepRL" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"ShangtongZhang/DeepRL" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"ShangtongZhang/DeepRL" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"ShangtongZhang/DeepRL" -> "higgsfield/RL-Adventure-2"
"ShangtongZhang/DeepRL" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"ShangtongZhang/DeepRL" -> "openai/baselines"
"ShangtongZhang/DeepRL" -> "vitchyr/rlkit"
"ShangtongZhang/DeepRL" -> "hill-a/stable-baselines"
"ShangtongZhang/DeepRL" -> "sfujim/TD3"
"ShangtongZhang/DeepRL" -> "rll/rllab"
"ShangtongZhang/DeepRL" -> "jingweiz/pytorch-rl"
"yenchenlin/DeepLearningFlappyBird" -> "lengstrom/fast-style-transfer" ["e"=1]
"yenchenlin/DeepLearningFlappyBird" -> "udacity/deep-learning" ["e"=1]
"yenchenlin/DeepLearningFlappyBird" -> "devsisters/DQN-tensorflow"
"yenchenlin/DeepLearningFlappyBird" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"yenchenlin/DeepLearningFlappyBird" -> "asrivat1/DeepLearningVideoGames"
"yenchenlin/DeepLearningFlappyBird" -> "dennybritz/reinforcement-learning"
"yenchenlin/DeepLearningFlappyBird" -> "Rochester-NRT/RocAlphaGo" ["e"=1]
"yenchenlin/DeepLearningFlappyBird" -> "sourabhv/FlapPyBird"
"yenchenlin/DeepLearningFlappyBird" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"yenchenlin/DeepLearningFlappyBird" -> "aikorea/awesome-rl"
"yenchenlin/DeepLearningFlappyBird" -> "junhyukoh/deep-reinforcement-learning-papers"
"yenchenlin/DeepLearningFlappyBird" -> "carpedm20/DCGAN-tensorflow" ["e"=1]
"yenchenlin/DeepLearningFlappyBird" -> "openai/baselines"
"yenchenlin/DeepLearningFlappyBird" -> "carpedm20/deep-rl-tensorflow"
"yenchenlin/DeepLearningFlappyBird" -> "openai/gym"
"DLR-RM/stable-baselines3" -> "DLR-RM/rl-baselines3-zoo"
"DLR-RM/stable-baselines3" -> "hill-a/stable-baselines"
"DLR-RM/stable-baselines3" -> "thu-ml/tianshou"
"DLR-RM/stable-baselines3" -> "vwxyzjn/cleanrl"
"DLR-RM/stable-baselines3" -> "openai/spinningup"
"DLR-RM/stable-baselines3" -> "openai/baselines"
"DLR-RM/stable-baselines3" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"DLR-RM/stable-baselines3" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"DLR-RM/stable-baselines3" -> "Farama-Foundation/Gymnasium"
"DLR-RM/stable-baselines3" -> "deepmind/acme"
"DLR-RM/stable-baselines3" -> "rlworkgroup/garage"
"DLR-RM/stable-baselines3" -> "astooke/rlpyt"
"DLR-RM/stable-baselines3" -> "deepmind/dm_control"
"DLR-RM/stable-baselines3" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"DLR-RM/stable-baselines3" -> "ray-project/ray" ["e"=1]
"Toni-SM/skrl" -> "NVIDIA-Omniverse/OmniIsaacGymEnvs"
"Toni-SM/skrl" -> "Denys88/rl_games"
"Toni-SM/skrl" -> "wangcongrobot/awesome-isaac-gym"
"Toni-SM/skrl" -> "cypypccpy/Isaac-ManipulaRL"
"Toni-SM/skrl" -> "iamlab-cmu/isaacgym-utils"
"Toni-SM/skrl" -> "NVIDIA-Omniverse/IsaacGymEnvs"
"Toni-SM/skrl" -> "PKU-MARL/DexterousHands"
"Toni-SM/skrl" -> "pairlab/leibnizgym"
"google-research/dreamer" -> "danijar/dreamer"
"google-research/dreamer" -> "danijar/dreamerv2"
"google-research/dreamer" -> "juliusfrost/dreamer-pytorch"
"google-research/dreamer" -> "ramanans1/plan2explore"
"google-research/dreamer" -> "google-research/planet"
"google-research/dreamer" -> "JannerM/mbpo"
"google-research/dreamer" -> "yusukeurakami/dreamer-pytorch"
"google-research/dreamer" -> "MishaLaskin/curl"
"google-research/dreamer" -> "MishaLaskin/rad"
"google-research/dreamer" -> "Kaixhin/PlaNet"
"google-research/dreamer" -> "kchua/handful-of-trials"
"google-research/dreamer" -> "ctallec/world-models"
"google-research/dreamer" -> "rail-berkeley/d4rl"
"google-research/dreamer" -> "rlworkgroup/metaworld"
"google-research/dreamer" -> "WilsonWangTHU/mbbl"
"rlworkgroup/metaworld" -> "stepjam/RLBench"
"rlworkgroup/metaworld" -> "katerakelly/oyster"
"rlworkgroup/metaworld" -> "rlworkgroup/garage"
"rlworkgroup/metaworld" -> "rail-berkeley/d4rl"
"rlworkgroup/metaworld" -> "facebookresearch/mbrl-lib"
"rlworkgroup/metaworld" -> "vitchyr/multiworld"
"rlworkgroup/metaworld" -> "MishaLaskin/curl"
"rlworkgroup/metaworld" -> "tristandeleu/pytorch-maml-rl" ["e"=1]
"rlworkgroup/metaworld" -> "JannerM/mbpo"
"rlworkgroup/metaworld" -> "ARISE-Initiative/robosuite"
"rlworkgroup/metaworld" -> "clvrai/awesome-rl-envs"
"rlworkgroup/metaworld" -> "vitchyr/rlkit"
"rlworkgroup/metaworld" -> "iclavera/learning_to_adapt"
"rlworkgroup/metaworld" -> "rail-berkeley/softlearning"
"rlworkgroup/metaworld" -> "danijar/dreamer"
"wangcongrobot/awesome-isaac-gym" -> "NVIDIA-Omniverse/IsaacGymEnvs"
"wangcongrobot/awesome-isaac-gym" -> "leggedrobotics/legged_gym" ["e"=1]
"wangcongrobot/awesome-isaac-gym" -> "NVIDIA-Omniverse/OmniIsaacGymEnvs"
"wangcongrobot/awesome-isaac-gym" -> "Denys88/rl_games"
"wangcongrobot/awesome-isaac-gym" -> "PKU-MARL/DexterousHands"
"wangcongrobot/awesome-isaac-gym" -> "Toni-SM/skrl"
"wangcongrobot/awesome-isaac-gym" -> "NVIDIA-Omniverse/Orbit"
"wangcongrobot/awesome-isaac-gym" -> "ir413/mvp"
"wangcongrobot/awesome-isaac-gym" -> "erwincoumans/motion_imitation" ["e"=1]
"wangcongrobot/awesome-isaac-gym" -> "google-research/ravens"
"wangcongrobot/awesome-isaac-gym" -> "leggedrobotics/rsl_rl" ["e"=1]
"wangcongrobot/awesome-isaac-gym" -> "Mehooz/vision4leg" ["e"=1]
"wangcongrobot/awesome-isaac-gym" -> "qgallouedec/panda-gym"
"wangcongrobot/awesome-isaac-gym" -> "nv-tlabs/ASE" ["e"=1]
"wangcongrobot/awesome-isaac-gym" -> "deepmind/mujoco_menagerie"
"werner-duvaud/muzero-general" -> "YeWR/EfficientZero"
"werner-duvaud/muzero-general" -> "koulanurag/muzero-pytorch"
"werner-duvaud/muzero-general" -> "suragnair/alpha-zero-general" ["e"=1]
"werner-duvaud/muzero-general" -> "deepmind/open_spiel"
"werner-duvaud/muzero-general" -> "johan-gras/MuZero"
"werner-duvaud/muzero-general" -> "deepmind/acme"
"werner-duvaud/muzero-general" -> "vwxyzjn/cleanrl"
"werner-duvaud/muzero-general" -> "danijar/dreamerv2"
"werner-duvaud/muzero-general" -> "deepmind/mctx" ["e"=1]
"werner-duvaud/muzero-general" -> "kaesve/muzero"
"werner-duvaud/muzero-general" -> "astooke/rlpyt"
"werner-duvaud/muzero-general" -> "kzl/decision-transformer"
"werner-duvaud/muzero-general" -> "DLR-RM/stable-baselines3"
"werner-duvaud/muzero-general" -> "eloialonso/iris"
"werner-duvaud/muzero-general" -> "hill-a/stable-baselines"
"robo-code/robocode" -> "robocode-dev/tank-royale"
"robo-code/robocode" -> "turkishviking/Python-Robocode"
"robo-code/robocode" -> "Voidious/Diamond"
"robo-code/robocode" -> "jkflying/literumble"
"robo-code/robocode" -> "stevenpjg/QlearningRobocodeNN"
"clvrai/awesome-rl-envs" -> "kengz/awesome-deep-rl"
"clvrai/awesome-rl-envs" -> "rail-berkeley/d4rl"
"clvrai/awesome-rl-envs" -> "stepjam/RLBench"
"clvrai/awesome-rl-envs" -> "rlworkgroup/metaworld"
"clvrai/awesome-rl-envs" -> "hanjuku-kaso/awesome-offline-rl"
"clvrai/awesome-rl-envs" -> "facebookresearch/mbrl-lib"
"clvrai/awesome-rl-envs" -> "MrSyee/pg-is-all-you-need"
"clvrai/awesome-rl-envs" -> "sail-sg/envpool"
"clvrai/awesome-rl-envs" -> "takuseno/d3rlpy"
"clvrai/awesome-rl-envs" -> "clvrai/furniture"
"clvrai/awesome-rl-envs" -> "google-research/rliable"
"clvrai/awesome-rl-envs" -> "ARISE-Initiative/robosuite"
"clvrai/awesome-rl-envs" -> "maximecb/gym-minigrid"
"clvrai/awesome-rl-envs" -> "AboudyKreidieh/h-baselines"
"clvrai/awesome-rl-envs" -> "astooke/rlpyt"
"hardmaru/slimevolleygym" -> "openai/procgen"
"hardmaru/slimevolleygym" -> "PettingZoo-Team/PettingZoo" ["e"=1]
"hardmaru/slimevolleygym" -> "maximecb/gym-minigrid"
"hardmaru/slimevolleygym" -> "hardmaru/estool" ["e"=1]
"hardmaru/slimevolleygym" -> "sail-sg/envpool"
"hardmaru/slimevolleygym" -> "deepmind/dqn_zoo"
"hardmaru/slimevolleygym" -> "vwxyzjn/cleanrl"
"hardmaru/slimevolleygym" -> "hardmaru/rlzoo"
"hardmaru/slimevolleygym" -> "eloialonso/iris"
"hardmaru/slimevolleygym" -> "koulanurag/ma-gym"
"hardmaru/slimevolleygym" -> "openai/robogym"
"hardmaru/slimevolleygym" -> "eleurent/highway-env" ["e"=1]
"hardmaru/slimevolleygym" -> "rasmusbergpalm/evostrat" ["e"=1]
"hardmaru/slimevolleygym" -> "seungjaeryanlee/awesome-rl-competitions"
"hardmaru/slimevolleygym" -> "MrSyee/pg-is-all-you-need"
"2019ChenGong/RL-Paper-notes" -> "hanjuku-kaso/awesome-offline-rl"
"2019ChenGong/RL-Paper-notes" -> "starry-sky6688/StarCraft"
"2019ChenGong/RL-Paper-notes" -> "MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning"
"2019ChenGong/RL-Paper-notes" -> "kaixindelele/DRLib"
"2019ChenGong/RL-Paper-notes" -> "sjtu-marl/malib"
"2019ChenGong/RL-Paper-notes" -> "rail-berkeley/d4rl"
"2019ChenGong/RL-Paper-notes" -> "sfujim/BCQ"
"2019ChenGong/RL-Paper-notes" -> "xionghuichen/RLAssistant"
"2019ChenGong/RL-Paper-notes" -> "marlbenchmark/on-policy"
"2019ChenGong/RL-Paper-notes" -> "wwxFromTju/awesome-reinforcement-learning-lib"
"2019ChenGong/RL-Paper-notes" -> "hijkzzz/pymarl2"
"2019ChenGong/RL-Paper-notes" -> "chauncygu/Safe-Reinforcement-Learning-Baselines"
"MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning" -> "boyu-ai/Hands-on-RL"
"MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning" -> "wangshusen/DRL"
"MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning" -> "marlbenchmark/on-policy"
"MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning" -> "2019ChenGong/RL-Paper-notes"
"MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning" -> "datawhalechina/easy-rl"
"MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning" -> "NeuronDance/DeepRL"
"MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning" -> "sail-sg/envpool"
"MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning" -> "zhoubolei/introRL"
"MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning" -> "huawei-noah/SMARTS" ["e"=1]
"MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning" -> "Replicable-MARL/MARLlib"
"MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning" -> "kaixindelele/DRLib"
"MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning" -> "Lizhi-sjtu/DRL-code-pytorch"
"MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning" -> "uoe-agents/epymarl"
"MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning" -> "sjtu-marl/malib"
"MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning" -> "chauncygu/Safe-Reinforcement-Learning-Baselines"
"Replicable-MARL/MARLlib" -> "uoe-agents/epymarl"
"Replicable-MARL/MARLlib" -> "elleryqueenhomels/AI_for_Atari" ["e"=1]
"Replicable-MARL/MARLlib" -> "elleryqueenhomels/google_sketcher" ["e"=1]
"Replicable-MARL/MARLlib" -> "TracyWang95/legal-prompts-for-gpt" ["e"=1]
"Replicable-MARL/MARLlib" -> "marlbenchmark/on-policy"
"Replicable-MARL/MARLlib" -> "marlbenchmark/off-policy"
"Replicable-MARL/MARLlib" -> "weiwensangsang/golang-internal" ["e"=1]
"Replicable-MARL/MARLlib" -> "harryzhangOG/Deep-RL-Notes" ["e"=1]
"Replicable-MARL/MARLlib" -> "KISS1996/trexminer" ["e"=1]
"Replicable-MARL/MARLlib" -> "Weasley-J/mydtt-plus-spring-boot-starter" ["e"=1]
"Replicable-MARL/MARLlib" -> "the-zion/matrix-core" ["e"=1]
"Replicable-MARL/MARLlib" -> "tuneflow/tuneflow-py" ["e"=1]
"Replicable-MARL/MARLlib" -> "FractonProtocol/FractonV1" ["e"=1]
"Replicable-MARL/MARLlib" -> "HackerBar-Sec/HackerBar" ["e"=1]
"Replicable-MARL/MARLlib" -> "hepingood/mpu6050" ["e"=1]
"huawei-noah/xingtian" -> "huawei-noah/SMARTS" ["e"=1]
"huawei-noah/xingtian" -> "tencent-ailab/TLeague"
"huawei-noah/xingtian" -> "sjtu-marl/malib"
"huawei-noah/xingtian" -> "tencent-ailab/tleague_projpage"
"huawei-noah/xingtian" -> "ying-wen/malib"
"huawei-noah/xingtian" -> "marlbenchmark/on-policy"
"huawei-noah/xingtian" -> "sail-sg/envpool"
"huawei-noah/xingtian" -> "StepNeverStop/RLs"
"huawei-noah/xingtian" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"huawei-noah/xingtian" -> "rail-berkeley/d4rl"
"huawei-noah/xingtian" -> "uoe-agents/epymarl"
"huawei-noah/xingtian" -> "starry-sky6688/StarCraft"
"kaixindelele/DRLib" -> "Lizhi-sjtu/DRL-code-pytorch"
"kaixindelele/DRLib" -> "zhangchuheng123/Reinforcement-Implementation"
"kaixindelele/DRLib" -> "gxywy/rl-plotter"
"kaixindelele/DRLib" -> "marlbenchmark/off-policy"
"kaixindelele/DRLib" -> "marlbenchmark/on-policy"
"kaixindelele/DRLib" -> "AI4Finance-LLC/ElegantRL" ["e"=1]
"kaixindelele/DRLib" -> "YangRui2015/Sparse-Reward-Algorithms"
"kaixindelele/DRLib" -> "Lizhi-sjtu/MARL-code-pytorch"
"kaixindelele/DRLib" -> "kaixindelele/RHER"
"kaixindelele/DRLib" -> "borninfreedom/kuka-reach-drl"
"kaixindelele/DRLib" -> "StepNeverStop/RLs"
"kaixindelele/DRLib" -> "louisnino/RLcode"
"kaixindelele/DRLib" -> "tinyzqh/light_mappo"
"kaixindelele/DRLib" -> "Yonv1943/ElegantRL"
"kaixindelele/DRLib" -> "Ericonaldo/ILSwiss"
"sfujim/TD3" -> "pranz24/pytorch-soft-actor-critic"
"sfujim/TD3" -> "haarnoja/sac"
"sfujim/TD3" -> "vitchyr/rlkit"
"sfujim/TD3" -> "rail-berkeley/softlearning"
"sfujim/TD3" -> "sfujim/BCQ"
"sfujim/TD3" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"sfujim/TD3" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"sfujim/TD3" -> "ShangtongZhang/DeepRL"
"sfujim/TD3" -> "higgsfield/RL-Adventure-2"
"sfujim/TD3" -> "astooke/rlpyt"
"sfujim/TD3" -> "hill-a/stable-baselines"
"sfujim/TD3" -> "rll/rllab"
"sfujim/TD3" -> "nikhilbarhate99/PPO-PyTorch"
"sfujim/TD3" -> "cuhkrlcourse/RLexample"
"sfujim/TD3" -> "rlworkgroup/garage"
"sudharsan13296/Deep-Reinforcement-Learning-With-Python" -> "sudharsan13296/Hands-On-Reinforcement-Learning-With-Python"
"sudharsan13296/Deep-Reinforcement-Learning-With-Python" -> "PacktPublishing/Deep-Reinforcement-Learning-with-Python"
"tencent-ailab/hok_env" -> "sjtu-marl/malib"
"tencent-ailab/hok_env" -> "sail-sg/envpool"
"tencent-ailab/hok_env" -> "oxwhirl/smac"
"tencent-ailab/hok_env" -> "marlbenchmark/on-policy"
"tencent-ailab/hok_env" -> "liuruoze/mini-AlphaStar" ["e"=1]
"tencent-ailab/hok_env" -> "Ericonaldo/ILSwiss"
"tencent-ailab/hok_env" -> "uoe-agents/epymarl"
"tencent-ailab/hok_env" -> "lich14/CDS"
"tencent-ailab/hok_env" -> "xionghuichen/RLAssistant"
"tencent-ailab/hok_env" -> "jidiai/ai_lib"
"tencent-ailab/hok_env" -> "tencent-ailab/TLeague"
"tencent-ailab/hok_env" -> "PKU-MARL/Multi-Agent-Transformer"
"tencent-ailab/hok_env" -> "hijkzzz/pymarl2"
"tencent-ailab/hok_env" -> "NVIDIA-Omniverse/IsaacGymEnvs"
"tencent-ailab/hok_env" -> "deepmind/alphastar" ["e"=1]
"Damcy/prioritized-experience-replay" -> "takoika/PrioritizedExperienceReplay"
"Damcy/prioritized-experience-replay" -> "jaara/AI-blog"
"itaicaspi/keras-dqn-doom" -> "fhennecker/deepdoom"
"sourabhv/FlapPyBird" -> "asrivat1/DeepLearningVideoGames"
"sourabhv/FlapPyBird" -> "yenchenlin/DeepLearningFlappyBird"
"sourabhv/FlapPyBird" -> "chncyhn/flappybird-qlearning-bot"
"sourabhv/FlapPyBird" -> "TimoWilken/flappy-bird-pygame"
"sourabhv/FlapPyBird" -> "kidscancode/pygame_tutorials" ["e"=1]
"sourabhv/FlapPyBird" -> "mx0c/super-mario-python" ["e"=1]
"sourabhv/FlapPyBird" -> "ntasfi/PyGame-Learning-Environment"
"sourabhv/FlapPyBird" -> "techwithtim/Pygame-Tutorials" ["e"=1]
"sourabhv/FlapPyBird" -> "justinmeister/Mario-Level-1" ["e"=1]
"sourabhv/FlapPyBird" -> "SarvagyaVaish/FlappyBirdRL"
"sourabhv/FlapPyBird" -> "Mekire/pygame-samples" ["e"=1]
"sourabhv/FlapPyBird" -> "justinmeister/The-Stolen-Crown-RPG" ["e"=1]
"sourabhv/FlapPyBird" -> "yanpanlau/Keras-FlappyBird"
"sourabhv/FlapPyBird" -> "floodsung/DRL-FlappyBird"
"sourabhv/FlapPyBird" -> "nikitasrivatsan/DeepLearningVideoGames"
"tonyzhaozh/act" -> "tonyzhaozh/aloha"
"openai/maddpg" -> "openai/multiagent-particle-envs"
"openai/maddpg" -> "xuehy/pytorch-maddpg"
"openai/maddpg" -> "oxwhirl/pymarl"
"openai/maddpg" -> "shariqiqbal2810/maddpg-pytorch"
"openai/maddpg" -> "shariqiqbal2810/MAAC"
"openai/maddpg" -> "LantaoYu/MARL-Papers"
"openai/maddpg" -> "geek-ai/MAgent"
"openai/maddpg" -> "starry-sky6688/StarCraft"
"openai/maddpg" -> "marlbenchmark/on-policy"
"openai/maddpg" -> "oxwhirl/smac"
"openai/maddpg" -> "sisl/MADRL"
"openai/maddpg" -> "starry-sky6688/MADDPG"
"openai/maddpg" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"openai/maddpg" -> "mlii/mfrl"
"openai/maddpg" -> "DKuan/MADDPG_torch"
"openai/procgen" -> "openai/train-procgen"
"openai/procgen" -> "maximecb/gym-minigrid"
"openai/procgen" -> "openai/phasic-policy-gradient"
"openai/procgen" -> "sail-sg/envpool"
"openai/procgen" -> "maximecb/gym-miniworld"
"openai/procgen" -> "rlworkgroup/metaworld"
"openai/procgen" -> "openai/gym3"
"openai/procgen" -> "openai/safety-gym"
"openai/procgen" -> "clvrai/awesome-rl-envs"
"openai/procgen" -> "openai/retro"
"openai/procgen" -> "deepmind/bsuite"
"openai/procgen" -> "astooke/rlpyt"
"openai/procgen" -> "rlworkgroup/garage"
"openai/procgen" -> "rail-berkeley/softlearning"
"openai/procgen" -> "hardmaru/slimevolleygym"
"DanielTakeshi/Paper_Notes" -> "DanielTakeshi/rl_algorithms"
"DanielTakeshi/Paper_Notes" -> "vitchyr/rlkit"
"DanielTakeshi/Paper_Notes" -> "rll/rllab"
"DanielTakeshi/Paper_Notes" -> "berkeleydeeprlcourse/homework"
"DanielTakeshi/Paper_Notes" -> "justinjfu/inverse_rl"
"DanielTakeshi/Paper_Notes" -> "openai/robosumo"
"DanielTakeshi/Paper_Notes" -> "joschu/modular_rl"
"DanielTakeshi/Paper_Notes" -> "rlworkgroup/metaworld"
"DanielTakeshi/Paper_Notes" -> "tristandeleu/pytorch-maml-rl" ["e"=1]
"DanielTakeshi/Paper_Notes" -> "dennybritz/deeplearning-papernotes" ["e"=1]
"DanielTakeshi/Paper_Notes" -> "rail-berkeley/softlearning"
"DanielTakeshi/Paper_Notes" -> "shaneshixiang/rllabplusplus"
"DanielTakeshi/Paper_Notes" -> "astooke/rlpyt"
"DanielTakeshi/Paper_Notes" -> "katerakelly/oyster"
"DanielTakeshi/Paper_Notes" -> "cbfinn/gps"
"deepmind/dm_control" -> "openai/mujoco-py"
"deepmind/dm_control" -> "rll/rllab"
"deepmind/dm_control" -> "vitchyr/rlkit"
"deepmind/dm_control" -> "rlworkgroup/garage"
"deepmind/dm_control" -> "deepmind/acme"
"deepmind/dm_control" -> "deepmind/lab"
"deepmind/dm_control" -> "openai/roboschool"
"deepmind/dm_control" -> "deepmind/mujoco"
"deepmind/dm_control" -> "openai/baselines"
"deepmind/dm_control" -> "deepmind/bsuite"
"deepmind/dm_control" -> "benelot/pybullet-gym"
"deepmind/dm_control" -> "hill-a/stable-baselines"
"deepmind/dm_control" -> "google/brax"
"deepmind/dm_control" -> "astooke/rlpyt"
"deepmind/dm_control" -> "maximecb/gym-minigrid"
"hzwer/NIPS2017-LearningToRun" -> "ctmakro/stanford-osrl"
"hzwer/NIPS2017-LearningToRun" -> "nnaisense/2017-learning-to-run"
"junhyukoh/deep-reinforcement-learning-papers" -> "muupan/deep-reinforcement-learning-papers"
"junhyukoh/deep-reinforcement-learning-papers" -> "carpedm20/deep-rl-tensorflow"
"junhyukoh/deep-reinforcement-learning-papers" -> "devsisters/DQN-tensorflow"
"junhyukoh/deep-reinforcement-learning-papers" -> "aikorea/awesome-rl"
"junhyukoh/deep-reinforcement-learning-papers" -> "matthiasplappert/keras-rl"
"junhyukoh/deep-reinforcement-learning-papers" -> "rlcode/reinforcement-learning"
"junhyukoh/deep-reinforcement-learning-papers" -> "williamFalcon/DeepRLHacks"
"junhyukoh/deep-reinforcement-learning-papers" -> "LantaoYu/MARL-Papers"
"junhyukoh/deep-reinforcement-learning-papers" -> "dennybritz/deeplearning-papernotes" ["e"=1]
"junhyukoh/deep-reinforcement-learning-papers" -> "awjuliani/DeepRL-Agents"
"junhyukoh/deep-reinforcement-learning-papers" -> "rll/rllab"
"junhyukoh/deep-reinforcement-learning-papers" -> "miyosuda/async_deep_reinforce"
"junhyukoh/deep-reinforcement-learning-papers" -> "andrewliao11/Deep-Reinforcement-Learning-Survey"
"junhyukoh/deep-reinforcement-learning-papers" -> "dennybritz/reinforcement-learning"
"junhyukoh/deep-reinforcement-learning-papers" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"Curt-Park/rainbow-is-all-you-need" -> "MrSyee/pg-is-all-you-need"
"Curt-Park/rainbow-is-all-you-need" -> "Kaixhin/Rainbow"
"Curt-Park/rainbow-is-all-you-need" -> "higgsfield/RL-Adventure"
"Curt-Park/rainbow-is-all-you-need" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"Curt-Park/rainbow-is-all-you-need" -> "seungeunrho/minimalRL"
"Curt-Park/rainbow-is-all-you-need" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"Curt-Park/rainbow-is-all-you-need" -> "NeuronDance/DeepRL"
"Curt-Park/rainbow-is-all-you-need" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"Curt-Park/rainbow-is-all-you-need" -> "medipixel/rl_algorithms"
"Curt-Park/rainbow-is-all-you-need" -> "qfettes/DeepRL-Tutorials"
"Curt-Park/rainbow-is-all-you-need" -> "vwxyzjn/cleanrl"
"Curt-Park/rainbow-is-all-you-need" -> "thu-ml/tianshou"
"Curt-Park/rainbow-is-all-you-need" -> "astooke/rlpyt"
"Curt-Park/rainbow-is-all-you-need" -> "higgsfield/RL-Adventure-2"
"Curt-Park/rainbow-is-all-you-need" -> "ShangtongZhang/DeepRL"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "NeuronDance/DeepRL"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "zhoubolei/introRL"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "applenob/rl_learn"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "LantaoYu/MARL-Papers"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "princewen/tensorflow_practice" ["e"=1]
"wwxFromTju/awesome-reinforcement-learning-zh" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "thu-ml/tianshou"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "openai/spinningup"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "oxwhirl/pymarl"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "tigerneil/awesome-deep-rl"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "cuhkrlcourse/RLexample"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "datawhalechina/easy-rl"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "datawhalechina/leedeeprl-notes"
"deepmind/dnc" -> "Mostafa-Samir/DNC-tensorflow" ["e"=1]
"deepmind/dnc" -> "deepmind/sonnet"
"deepmind/dnc" -> "deepmind/learning-to-learn"
"deepmind/dnc" -> "carpedm20/NTM-tensorflow" ["e"=1]
"deepmind/dnc" -> "deepmind/lab"
"deepmind/dnc" -> "facebook/MemNN" ["e"=1]
"deepmind/dnc" -> "ixaxaar/pytorch-dnc" ["e"=1]
"deepmind/dnc" -> "deepmind/rc-data" ["e"=1]
"deepmind/dnc" -> "devsisters/DQN-tensorflow"
"deepmind/dnc" -> "tensorflow/fold"
"deepmind/dnc" -> "openai/universe-starter-agent"
"deepmind/dnc" -> "facebookresearch/ELF"
"deepmind/dnc" -> "bioinf-jku/SNNs" ["e"=1]
"deepmind/dnc" -> "carpedm20/deep-rl-tensorflow"
"deepmind/dnc" -> "reinforceio/tensorforce"
"AmazingAng/deep-RL-elements" -> "gxywy/rl-plotter"
"AmazingAng/deep-RL-elements" -> "BY571/DQN-Atari-Agents"
"AmazingAng/deep-RL-elements" -> "jmichaux/dqn-pytorch"
"openai/imitation" -> "openai/vime"
"openai/imitation" -> "andrewliao11/gail-tf"
"openai/imitation" -> "joschu/modular_rl"
"openai/imitation" -> "YunzhuLi/InfoGAIL"
"openai/imitation" -> "openai/iaf" ["e"=1]
"openai/imitation" -> "openai/InfoGAN" ["e"=1]
"openai/imitation" -> "justinjfu/inverse_rl"
"openai/imitation" -> "TheAbhiKumar/tensorflow-value-iteration-networks"
"openai/imitation" -> "uidilr/gail_ppo_tf"
"openai/imitation" -> "MatthewJA/Inverse-Reinforcement-Learning"
"openai/imitation" -> "sisl/gail-driver"
"openai/imitation" -> "openai/rllab"
"openai/imitation" -> "junhyukoh/self-imitation-learning"
"openai/imitation" -> "kristery/Awesome-Imitation-Learning"
"openai/imitation" -> "openai/evolution-strategies-starter" ["e"=1]
"andrewliao11/gail-tf" -> "uidilr/gail_ppo_tf"
"andrewliao11/gail-tf" -> "openai/imitation"
"andrewliao11/gail-tf" -> "YunzhuLi/InfoGAIL"
"andrewliao11/gail-tf" -> "itaicaspi/mgail"
"andrewliao11/gail-tf" -> "justinjfu/inverse_rl"
"andrewliao11/gail-tf" -> "stormmax/irl-imitation"
"andrewliao11/gail-tf" -> "sisl/gail-driver"
"andrewliao11/gail-tf" -> "reinforcement-learning-kr/lets-do-irl"
"andrewliao11/gail-tf" -> "yrlu/irl-imitation"
"andrewliao11/gail-tf" -> "ahq1993/inverse_rl"
"andrewliao11/gail-tf" -> "jangirrishabh/toyCarIRL"
"andrewliao11/gail-tf" -> "hoangminhle/hierarchical_IL_RL"
"andrewliao11/gail-tf" -> "tianheyu927/mil"
"haarnoja/softqlearning" -> "rail-berkeley/softlearning"
"haarnoja/softqlearning" -> "haarnoja/sac"
"haarnoja/softqlearning" -> "shaneshixiang/rllabplusplus"
"haarnoja/softqlearning" -> "vitchyr/rlkit"
"haarnoja/softqlearning" -> "junhyukoh/value-prediction-network"
"haarnoja/softqlearning" -> "openai/vime"
"haarnoja/softqlearning" -> "justinjfu/inverse_rl"
"haarnoja/softqlearning" -> "junhyukoh/self-imitation-learning"
"haarnoja/softqlearning" -> "joschu/modular_rl"
"haarnoja/softqlearning" -> "rll/rllab"
"haarnoja/softqlearning" -> "vitchyr/multiworld"
"haarnoja/softqlearning" -> "sfujim/BCQ"
"haarnoja/softqlearning" -> "JannerM/mbpo"
"haarnoja/softqlearning" -> "hoangminhle/hierarchical_IL_RL"
"haarnoja/softqlearning" -> "cbfinn/maml_rl" ["e"=1]
"hoangminhle/hierarchical_IL_RL" -> "tianheyu927/mil"
"hoangminhle/hierarchical_IL_RL" -> "openai/mlsh"
"hoangminhle/hierarchical_IL_RL" -> "dmakian/feudal_networks"
"hoangminhle/hierarchical_IL_RL" -> "junhyukoh/self-imitation-learning"
"hoangminhle/hierarchical_IL_RL" -> "andrew-j-levy/Hierarchical-Actor-Critc-HAC-"
"hoangminhle/hierarchical_IL_RL" -> "EthanMacdonald/h-DQN"
"hoangminhle/hierarchical_IL_RL" -> "mrkulk/hierarchical-deep-RL"
"hoangminhle/hierarchical_IL_RL" -> "andrewliao11/gail-tf"
"hoangminhle/hierarchical_IL_RL" -> "skumar9876/Hierarchical-DQN"
"hoangminhle/hierarchical_IL_RL" -> "florensacc/snn4hrl"
"hoangminhle/hierarchical_IL_RL" -> "haarnoja/softqlearning"
"matthiasplappert/keras-rl" -> "coreylynch/async-rl"
"matthiasplappert/keras-rl" -> "reinforceio/tensorforce"
"matthiasplappert/keras-rl" -> "carpedm20/deep-rl-tensorflow"
"matthiasplappert/keras-rl" -> "awjuliani/DeepRL-Agents"
"matthiasplappert/keras-rl" -> "openai/rllab"
"matthiasplappert/keras-rl" -> "devsisters/DQN-tensorflow"
"matthiasplappert/keras-rl" -> "junhyukoh/deep-reinforcement-learning-papers"
"matthiasplappert/keras-rl" -> "farizrahman4u/seq2seq" ["e"=1]
"matthiasplappert/keras-rl" -> "rlcode/reinforcement-learning"
"matthiasplappert/keras-rl" -> "joschu/modular_rl"
"matthiasplappert/keras-rl" -> "miyosuda/async_deep_reinforce"
"matthiasplappert/keras-rl" -> "openai/universe-starter-agent"
"matthiasplappert/keras-rl" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"matthiasplappert/keras-rl" -> "nivwusquorum/tensorflow-deepq"
"matthiasplappert/keras-rl" -> "spragunr/deep_q_rl"
"ml-jku/baselines-rudder" -> "yadrimz/option-critic"
"ml-jku/baselines-rudder" -> "openai/large-scale-curiosity"
"ml-jku/baselines-rudder" -> "zuoxingdong/lagom"
"ml-jku/baselines-rudder" -> "jeanharb/option_critic"
"ml-jku/baselines-rudder" -> "deepmind/scalable_agent"
"ml-jku/baselines-rudder" -> "openai/random-network-distillation"
"anyscale/academy" -> "ray-project/tutorial"
"anyscale/academy" -> "sven1977/rllib_tutorials"
"anyscale/academy" -> "DerwenAI/rllib_tutorials"
"anyscale/academy" -> "DerwenAI/ray_tutorial"
"anyscale/academy" -> "rail-berkeley/d4rl"
"anyscale/academy" -> "NVIDIA-Merlin/publications" ["e"=1]
"anyscale/academy" -> "sjtu-marl/malib"
"anyscale/academy" -> "oxwhirl/smac"
"deepmind/meltingpot" -> "eugenevinitsky/sequential_social_dilemma_games"
"deepmind/meltingpot" -> "deepmind/lab2d"
"deepmind/meltingpot" -> "koulanurag/ma-gym"
"deepmind/meltingpot" -> "instadeepai/Mava"
"deepmind/meltingpot" -> "schroederdewitt/multiagent_mujoco"
"deepmind/meltingpot" -> "marlbenchmark/on-policy"
"deepmind/meltingpot" -> "PettingZoo-Team/PettingZoo" ["e"=1]
"deepmind/meltingpot" -> "salesforce/warp-drive"
"deepmind/meltingpot" -> "uoe-agents/epymarl"
"deepmind/meltingpot" -> "oxwhirl/smac"
"deepmind/meltingpot" -> "Replicable-MARL/MARLlib"
"deepmind/meltingpot" -> "RobertTLange/gymnax"
"deepmind/meltingpot" -> "kandouss/marlgrid"
"deepmind/meltingpot" -> "sail-sg/envpool"
"deepmind/meltingpot" -> "Farama-Foundation/MAgent2"
"deepmind/open_spiel" -> "deepmind/bsuite"
"deepmind/open_spiel" -> "deepmind/acme"
"deepmind/open_spiel" -> "astooke/rlpyt"
"deepmind/open_spiel" -> "datamllab/rlcard" ["e"=1]
"deepmind/open_spiel" -> "werner-duvaud/muzero-general"
"deepmind/open_spiel" -> "deepmind/dm_control"
"deepmind/open_spiel" -> "LantaoYu/MARL-Papers"
"deepmind/open_spiel" -> "hill-a/stable-baselines"
"deepmind/open_spiel" -> "oxwhirl/pymarl"
"deepmind/open_spiel" -> "deepmind/trfl"
"deepmind/open_spiel" -> "openai/multiagent-particle-envs"
"deepmind/open_spiel" -> "openai/spinningup"
"deepmind/open_spiel" -> "google/dopamine"
"deepmind/open_spiel" -> "maximecb/gym-minigrid"
"deepmind/open_spiel" -> "google-research/football"
"facebookresearch/ReAgent" -> "astooke/rlpyt"
"facebookresearch/ReAgent" -> "rlworkgroup/garage"
"facebookresearch/ReAgent" -> "deepmind/acme"
"facebookresearch/ReAgent" -> "tensorflow/agents"
"facebookresearch/ReAgent" -> "sfujim/TD3"
"facebookresearch/ReAgent" -> "facebookresearch/mbrl-lib"
"facebookresearch/ReAgent" -> "google-research/batch_rl"
"facebookresearch/ReAgent" -> "google-research/seed_rl"
"facebookresearch/ReAgent" -> "pfnet/pfrl"
"facebookresearch/ReAgent" -> "rail-berkeley/d4rl"
"facebookresearch/ReAgent" -> "kzl/decision-transformer"
"facebookresearch/ReAgent" -> "david-cortes/contextualbandits" ["e"=1]
"facebookresearch/ReAgent" -> "google/dopamine"
"facebookresearch/ReAgent" -> "hill-a/stable-baselines"
"facebookresearch/ReAgent" -> "deepmind/open_spiel"
"facebookresearch/mbrl-lib" -> "WilsonWangTHU/mbbl"
"facebookresearch/mbrl-lib" -> "rlworkgroup/metaworld"
"facebookresearch/mbrl-lib" -> "kchua/handful-of-trials"
"facebookresearch/mbrl-lib" -> "takuseno/d3rlpy"
"facebookresearch/mbrl-lib" -> "danijar/dreamerv2"
"facebookresearch/mbrl-lib" -> "danijar/dreamerv3"
"facebookresearch/mbrl-lib" -> "rll-research/url_benchmark"
"facebookresearch/mbrl-lib" -> "rail-berkeley/d4rl"
"facebookresearch/mbrl-lib" -> "opendilab/awesome-model-based-RL" ["e"=1]
"facebookresearch/mbrl-lib" -> "google/brax"
"facebookresearch/mbrl-lib" -> "rail-berkeley/rlkit"
"facebookresearch/mbrl-lib" -> "clvrai/awesome-rl-envs"
"facebookresearch/mbrl-lib" -> "sail-sg/envpool"
"facebookresearch/mbrl-lib" -> "danijar/dreamer"
"facebookresearch/mbrl-lib" -> "jannerm/mbpo"
"geek-ai/MAgent" -> "openai/multiagent-particle-envs"
"geek-ai/MAgent" -> "oxwhirl/pymarl"
"geek-ai/MAgent" -> "openai/maddpg"
"geek-ai/MAgent" -> "LantaoYu/MARL-Papers"
"geek-ai/MAgent" -> "oxwhirl/smac"
"geek-ai/MAgent" -> "sisl/MADRL"
"geek-ai/MAgent" -> "mlii/mfrl"
"geek-ai/MAgent" -> "openai/multiagent-competition"
"geek-ai/MAgent" -> "shariqiqbal2810/MAAC"
"geek-ai/MAgent" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"geek-ai/MAgent" -> "MultiAgentLearning/playground"
"geek-ai/MAgent" -> "starry-sky6688/StarCraft"
"geek-ai/MAgent" -> "rll/rllab"
"geek-ai/MAgent" -> "marlbenchmark/on-policy"
"geek-ai/MAgent" -> "PKU-AI-Edge/DGN"
"google-research/batch_rl" -> "aviralkumar2907/CQL"
"google-research/batch_rl" -> "sfujim/BCQ"
"google-research/batch_rl" -> "rail-berkeley/d4rl"
"google-research/batch_rl" -> "takuseno/d3rlpy"
"google-research/batch_rl" -> "rail-berkeley/d4rl_evaluations"
"google-research/batch_rl" -> "hanjuku-kaso/awesome-offline-rl"
"google-research/batch_rl" -> "JannerM/mbpo"
"google-research/batch_rl" -> "aviralkumar2907/BEAR"
"google-research/batch_rl" -> "sfujim/TD3_BC"
"google-research/batch_rl" -> "jannerm/trajectory-transformer"
"google-research/batch_rl" -> "tianheyu927/mopo"
"google-research/batch_rl" -> "facebookresearch/torchbeast"
"google-research/batch_rl" -> "kzl/decision-transformer"
"google-research/batch_rl" -> "rll-research/url_benchmark"
"google-research/batch_rl" -> "ikostrikov/implicit_q_learning"
"kaixin96/rl-generalization-paper" -> "rraileanu/idaac"
"kaixin96/rl-generalization-paper" -> "nicklashansen/dmcontrol-generalization-benchmark"
"kzl/decision-transformer" -> "jannerm/trajectory-transformer"
"kzl/decision-transformer" -> "rail-berkeley/d4rl"
"kzl/decision-transformer" -> "takuseno/d3rlpy"
"kzl/decision-transformer" -> "eloialonso/iris"
"kzl/decision-transformer" -> "YeWR/EfficientZero"
"kzl/decision-transformer" -> "marlbenchmark/on-policy"
"kzl/decision-transformer" -> "hanjuku-kaso/awesome-offline-rl"
"kzl/decision-transformer" -> "MishaLaskin/curl"
"kzl/decision-transformer" -> "vwxyzjn/cleanrl"
"kzl/decision-transformer" -> "opendilab/awesome-decision-transformer" ["e"=1]
"kzl/decision-transformer" -> "rlworkgroup/metaworld"
"kzl/decision-transformer" -> "facebookresearch/mbrl-lib"
"kzl/decision-transformer" -> "nikhilbarhate99/min-decision-transformer"
"kzl/decision-transformer" -> "astooke/rlpyt"
"kzl/decision-transformer" -> "sail-sg/envpool"
"mgbellemare/Arcade-Learning-Environment" -> "spragunr/deep_q_rl"
"mgbellemare/Arcade-Learning-Environment" -> "kuz/DeepMind-Atari-Deep-Q-Learner"
"mgbellemare/Arcade-Learning-Environment" -> "devsisters/DQN-tensorflow"
"mgbellemare/Arcade-Learning-Environment" -> "tambetm/simple_dqn"
"mgbellemare/Arcade-Learning-Environment" -> "openai/retro"
"mgbellemare/Arcade-Learning-Environment" -> "openai/atari-py"
"mgbellemare/Arcade-Learning-Environment" -> "deepmind/dm_control"
"mgbellemare/Arcade-Learning-Environment" -> "deepmind/open_spiel"
"mgbellemare/Arcade-Learning-Environment" -> "deepmind/lab"
"mgbellemare/Arcade-Learning-Environment" -> "miyosuda/async_deep_reinforce"
"mgbellemare/Arcade-Learning-Environment" -> "maximecb/gym-minigrid"
"mgbellemare/Arcade-Learning-Environment" -> "rll/rllab"
"mgbellemare/Arcade-Learning-Environment" -> "ntasfi/PyGame-Learning-Environment"
"mgbellemare/Arcade-Learning-Environment" -> "openai/universe-starter-agent"
"mgbellemare/Arcade-Learning-Environment" -> "Kaixhin/Rainbow"
"nicklashansen/dmcontrol-generalization-benchmark" -> "nicklashansen/svea-vit"
"nicklashansen/dmcontrol-generalization-benchmark" -> "nicklashansen/policy-adaptation-during-deployment"
"nicklashansen/dmcontrol-generalization-benchmark" -> "facebookresearch/deep_bisim4control"
"nicklashansen/dmcontrol-generalization-benchmark" -> "rraileanu/auto-drac"
"nicklashansen/dmcontrol-generalization-benchmark" -> "denisyarats/dmc2gym"
"nicklashansen/dmcontrol-generalization-benchmark" -> "denisyarats/drq"
"nicklashansen/dmcontrol-generalization-benchmark" -> "jangirrishabh/look-closer"
"sail-sg/envpool" -> "alex-petrenko/sample-factory"
"sail-sg/envpool" -> "RobertTLange/gymnax"
"sail-sg/envpool" -> "YeWR/EfficientZero"
"sail-sg/envpool" -> "Denys88/rl_games"
"sail-sg/envpool" -> "google/brax"
"sail-sg/envpool" -> "takuseno/d3rlpy"
"sail-sg/envpool" -> "vwxyzjn/cleanrl"
"sail-sg/envpool" -> "NVIDIA-Omniverse/IsaacGymEnvs"
"sail-sg/envpool" -> "google-research/rliable"
"sail-sg/envpool" -> "PKU-MARL/DexterousHands"
"sail-sg/envpool" -> "sjtu-marl/malib"
"sail-sg/envpool" -> "tinkoff-ai/CORL"
"sail-sg/envpool" -> "facebookresearch/mbrl-lib"
"sail-sg/envpool" -> "danijar/dreamerv3"
"sail-sg/envpool" -> "deepmind/mujoco_menagerie"
"salesforce/warp-drive" -> "deepmind/meltingpot"
"salesforce/warp-drive" -> "instadeepai/Mava"
"salesforce/warp-drive" -> "sail-sg/envpool"
"salesforce/warp-drive" -> "alex-petrenko/sample-factory"
"salesforce/warp-drive" -> "uoe-agents/epymarl"
"salesforce/warp-drive" -> "sjtu-marl/malib"
"salesforce/warp-drive" -> "RobertTLange/gymnax"
"takuseno/d3rlpy" -> "hanjuku-kaso/awesome-offline-rl"
"takuseno/d3rlpy" -> "rail-berkeley/d4rl"
"takuseno/d3rlpy" -> "aviralkumar2907/CQL"
"takuseno/d3rlpy" -> "tinkoff-ai/CORL"
"takuseno/d3rlpy" -> "google-research/batch_rl"
"takuseno/d3rlpy" -> "sfujim/BCQ"
"takuseno/d3rlpy" -> "rail-berkeley/rlkit"
"takuseno/d3rlpy" -> "sail-sg/envpool"
"takuseno/d3rlpy" -> "takuseno/minerva"
"takuseno/d3rlpy" -> "facebookresearch/mbrl-lib"
"takuseno/d3rlpy" -> "kzl/decision-transformer"
"takuseno/d3rlpy" -> "Farama-Foundation/D4RL"
"takuseno/d3rlpy" -> "google-research/rliable"
"takuseno/d3rlpy" -> "sfujim/TD3_BC"
"takuseno/d3rlpy" -> "clvrai/awesome-rl-envs"
"takuseno/minerva" -> "takuseno/d4rl-pybullet"
"twni2016/pomdp-baselines" -> "lmzintgraf/varibad"
"twni2016/pomdp-baselines" -> "ikostrikov/rlpd"
"twni2016/pomdp-baselines" -> "ikostrikov/jaxrl"
"twni2016/pomdp-baselines" -> "HorizonRobotics/alf"
"twni2016/pomdp-baselines" -> "araffin/sbx"
"astooke/rlpyt" -> "vitchyr/rlkit"
"astooke/rlpyt" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"astooke/rlpyt" -> "hill-a/stable-baselines"
"astooke/rlpyt" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"astooke/rlpyt" -> "rlworkgroup/garage"
"astooke/rlpyt" -> "ShangtongZhang/DeepRL"
"astooke/rlpyt" -> "maximecb/gym-minigrid"
"astooke/rlpyt" -> "deepmind/bsuite"
"astooke/rlpyt" -> "deepmind/acme"
"astooke/rlpyt" -> "rll/rllab"
"astooke/rlpyt" -> "rail-berkeley/softlearning"
"astooke/rlpyt" -> "seungeunrho/minimalRL"
"astooke/rlpyt" -> "thu-ml/tianshou"
"astooke/rlpyt" -> "sfujim/TD3"
"astooke/rlpyt" -> "Kaixhin/Rainbow"
"deepmind/acme" -> "deepmind/reverb"
"deepmind/acme" -> "deepmind/rlax" ["e"=1]
"deepmind/acme" -> "astooke/rlpyt"
"deepmind/acme" -> "deepmind/dm_control"
"deepmind/acme" -> "deepmind/dm-haiku" ["e"=1]
"deepmind/acme" -> "vwxyzjn/cleanrl"
"deepmind/acme" -> "deepmind/open_spiel"
"deepmind/acme" -> "hill-a/stable-baselines"
"deepmind/acme" -> "deepmind/bsuite"
"deepmind/acme" -> "DLR-RM/stable-baselines3"
"deepmind/acme" -> "tensorflow/agents"
"deepmind/acme" -> "google-research/seed_rl"
"deepmind/acme" -> "rlworkgroup/garage"
"deepmind/acme" -> "werner-duvaud/muzero-general"
"deepmind/acme" -> "maximecb/gym-minigrid"
"deepmind/reverb" -> "deepmind/acme"
"deepmind/reverb" -> "google-research/seed_rl"
"deepmind/reverb" -> "deepmind/launchpad" ["e"=1]
"deepmind/reverb" -> "deepmind/bsuite"
"deepmind/reverb" -> "deepmind/rlax" ["e"=1]
"deepmind/reverb" -> "facebookresearch/torchbeast"
"deepmind/reverb" -> "google-research/rliable"
"deepmind/reverb" -> "sail-sg/envpool"
"deepmind/reverb" -> "deepmind/dm_env" ["e"=1]
"deepmind/reverb" -> "deepmind/scalable_agent"
"deepmind/reverb" -> "deepmind/dqn_zoo"
"deepmind/reverb" -> "sjtu-marl/malib"
"deepmind/reverb" -> "tencent-ailab/tleague_projpage"
"deepmind/reverb" -> "rail-berkeley/softlearning"
"deepmind/reverb" -> "NVlabs/cule"
"deepmind/scalable_agent" -> "google-research/seed_rl"
"deepmind/scalable_agent" -> "facebookresearch/torchbeast"
"deepmind/scalable_agent" -> "openai/random-network-distillation"
"deepmind/scalable_agent" -> "deepmind/trfl"
"deepmind/scalable_agent" -> "uber-research/ape-x"
"deepmind/scalable_agent" -> "rail-berkeley/softlearning"
"deepmind/scalable_agent" -> "vitchyr/rlkit"
"deepmind/scalable_agent" -> "oxwhirl/smac"
"deepmind/scalable_agent" -> "NVlabs/GA3C"
"deepmind/scalable_agent" -> "junhyukoh/self-imitation-learning"
"deepmind/scalable_agent" -> "openai/coinrun"
"deepmind/scalable_agent" -> "pathak22/noreward-rl"
"deepmind/scalable_agent" -> "Kaixhin/Rainbow"
"deepmind/scalable_agent" -> "openai/large-scale-curiosity"
"deepmind/scalable_agent" -> "openai/multiagent-competition"
"dmakian/feudal_networks" -> "Nat-D/FeatureControlHRL"
"dmakian/feudal_networks" -> "yadrimz/option-critic"
"dmakian/feudal_networks" -> "jeanharb/option_critic"
"dmakian/feudal_networks" -> "junhyukoh/value-prediction-network"
"dmakian/feudal_networks" -> "florensacc/snn4hrl"
"dmakian/feudal_networks" -> "EthanMacdonald/h-DQN"
"google-research/seed_rl" -> "deepmind/scalable_agent"
"google-research/seed_rl" -> "facebookresearch/torchbeast"
"google-research/seed_rl" -> "deepmind/reverb"
"google-research/seed_rl" -> "deepmind/acme"
"google-research/seed_rl" -> "alex-petrenko/sample-factory"
"google-research/seed_rl" -> "MishaLaskin/curl"
"google-research/seed_rl" -> "sjtu-marl/malib"
"google-research/seed_rl" -> "uber-research/go-explore"
"google-research/seed_rl" -> "astooke/rlpyt"
"google-research/seed_rl" -> "tencent-ailab/tleague_projpage"
"google-research/seed_rl" -> "kzl/decision-transformer"
"google-research/seed_rl" -> "openai/random-network-distillation"
"google-research/seed_rl" -> "sail-sg/envpool"
"google-research/seed_rl" -> "PettingZoo-Team/PettingZoo" ["e"=1]
"google-research/seed_rl" -> "ray-project/rl-experiments"
"keiohta/tf2rl" -> "marload/DeepRL-TensorFlow2"
"keiohta/tf2rl" -> "anita-hu/TF2-RL"
"keiohta/tf2rl" -> "reinforcement-learning-kr/lets-do-irl"
"keiohta/tf2rl" -> "StepNeverStop/RLs"
"keiohta/tf2rl" -> "RITCHIEHuang/DeepRL_Algorithms"
"keiohta/tf2rl" -> "inoryy/tensorflow2-deep-reinforcement-learning"
"keiohta/tf2rl" -> "germain-hug/Deep-RL-Keras"
"keiohta/tf2rl" -> "rail-berkeley/d4rl"
"keiohta/tf2rl" -> "ymd-h/cpprb"
"keiohta/tf2rl" -> "tensorlayer/RLzoo" ["e"=1]
"keiohta/tf2rl" -> "WilsonWangTHU/mbbl"
"keiohta/tf2rl" -> "xuanlinli17/CS285_Fa19_Deep_Reinforcement_Learning"
"keiohta/tf2rl" -> "tensorforce/tensorforce"
"keiohta/tf2rl" -> "araffin/rl-baselines-zoo"
"keiohta/tf2rl" -> "yrlu/irl-imitation"
"openai/multi-agent-emergence-environments" -> "openai/mujoco-worldgen"
"openai/multi-agent-emergence-environments" -> "openai/multiagent-particle-envs"
"openai/multi-agent-emergence-environments" -> "oxwhirl/pymarl"
"openai/multi-agent-emergence-environments" -> "oxwhirl/smac"
"openai/multi-agent-emergence-environments" -> "openai/maddpg"
"openai/multi-agent-emergence-environments" -> "marlbenchmark/on-policy"
"openai/multi-agent-emergence-environments" -> "geek-ai/MAgent"
"openai/multi-agent-emergence-environments" -> "LantaoYu/MARL-Papers"
"openai/multi-agent-emergence-environments" -> "starry-sky6688/StarCraft"
"openai/multi-agent-emergence-environments" -> "openai/multiagent-competition"
"openai/multi-agent-emergence-environments" -> "maximecb/gym-minigrid"
"openai/multi-agent-emergence-environments" -> "PettingZoo-Team/PettingZoo" ["e"=1]
"openai/multi-agent-emergence-environments" -> "openai/mujoco-py"
"openai/multi-agent-emergence-environments" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"openai/multi-agent-emergence-environments" -> "deepmind/open_spiel"
"ray-project/tutorial" -> "anyscale/academy"
"ray-project/tutorial" -> "ray-project/rl-experiments"
"ray-project/tutorial" -> "google-research/seed_rl"
"ray-project/tutorial" -> "ray-project/xgboost_ray" ["e"=1]
"ray-project/tutorial" -> "sjtu-marl/malib"
"ray-project/tutorial" -> "deepmind/scalable_agent"
"ray-project/tutorial" -> "deepmind/bsuite"
"ray-project/tutorial" -> "oxwhirl/pymarl"
"ray-project/tutorial" -> "oxwhirl/smac"
"ray-project/tutorial" -> "deepmind/reverb"
"ray-project/tutorial" -> "shariqiqbal2810/MAAC"
"ray-project/tutorial" -> "araffin/rl-baselines-zoo"
"ray-project/tutorial" -> "ray-project/ray" ["e"=1]
"ray-project/tutorial" -> "haarnoja/softqlearning"
"ray-project/tutorial" -> "rll/rllab"
"tencent-ailab/TLeague" -> "tencent-ailab/tleague_projpage"
"tencent-ailab/TLeague" -> "JBLanier/pipeline-psro"
"tencent-ailab/tleague_projpage" -> "tencent-ailab/TLeague"
"uber-research/go-explore" -> "openai/random-network-distillation"
"uber-research/go-explore" -> "openai/large-scale-curiosity"
"uber-research/go-explore" -> "google-research/seed_rl"
"uber-research/go-explore" -> "junhyukoh/self-imitation-learning"
"uber-research/go-explore" -> "pathak22/noreward-rl"
"uber-research/go-explore" -> "uber-research/poet" ["e"=1]
"uber-research/go-explore" -> "maximecb/gym-minigrid"
"uber-research/go-explore" -> "deepmind/scalable_agent"
"uber-research/go-explore" -> "google-research/episodic-curiosity"
"uber-research/go-explore" -> "MishaLaskin/curl"
"uber-research/go-explore" -> "google-research/planet"
"uber-research/go-explore" -> "uber-research/deep-neuroevolution" ["e"=1]
"uber-research/go-explore" -> "rail-berkeley/softlearning"
"uber-research/go-explore" -> "junhyukoh/value-prediction-network"
"uber-research/go-explore" -> "openai/procgen"
"vitchyr/rlkit" -> "rail-berkeley/softlearning"
"vitchyr/rlkit" -> "haarnoja/sac"
"vitchyr/rlkit" -> "astooke/rlpyt"
"vitchyr/rlkit" -> "rlworkgroup/garage"
"vitchyr/rlkit" -> "sfujim/TD3"
"vitchyr/rlkit" -> "vitchyr/multiworld"
"vitchyr/rlkit" -> "rll/rllab"
"vitchyr/rlkit" -> "deepmind/dm_control"
"vitchyr/rlkit" -> "pranz24/pytorch-soft-actor-critic"
"vitchyr/rlkit" -> "rail-berkeley/d4rl"
"vitchyr/rlkit" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"vitchyr/rlkit" -> "maximecb/gym-minigrid"
"vitchyr/rlkit" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"vitchyr/rlkit" -> "hill-a/stable-baselines"
"vitchyr/rlkit" -> "ShangtongZhang/DeepRL"
"BYU-PCCL/holodeck" -> "BYU-PCCL/holodeck-engine"
"BYU-PCCL/holodeck" -> "deepdrive/deepdrive" ["e"=1]
"BYU-PCCL/holodeck" -> "zfw1226/gym-unrealcv" ["e"=1]
"BYU-PCCL/holodeck" -> "vitchyr/multiworld"
"BYU-PCCL/holodeck" -> "maximecb/gym-minigrid"
"BYU-PCCL/holodeck" -> "vitchyr/rlkit"
"BYU-PCCL/holodeck" -> "deepmind/bsuite"
"BYU-PCCL/holodeck" -> "MagNet-DL/magnet" ["e"=1]
"BYU-PCCL/holodeck" -> "locuslab/mpc.pytorch" ["e"=1]
"BYU-PCCL/holodeck" -> "uber-research/ape-x"
"BYU-PCCL/holodeck" -> "stanfordnmbl/osim-rl"
"BYU-PCCL/holodeck" -> "erlerobot/gym-gazebo"
"BYU-PCCL/holodeck" -> "deepmind/pycolab"
"chainer/chainerrl" -> "NervanaSystems/coach"
"chainer/chainerrl" -> "pfnet/pfrl"
"chainer/chainerrl" -> "chainer/chainer" ["e"=1]
"chainer/chainerrl" -> "rll/rllab"
"chainer/chainerrl" -> "muupan/async-rl"
"chainer/chainerrl" -> "Kaixhin/Rainbow"
"chainer/chainerrl" -> "hill-a/stable-baselines"
"chainer/chainerrl" -> "astooke/rlpyt"
"chainer/chainerrl" -> "chainer/chainermn" ["e"=1]
"chainer/chainerrl" -> "chainer/chainercv" ["e"=1]
"chainer/chainerrl" -> "reinforceio/tensorforce"
"chainer/chainerrl" -> "rlworkgroup/garage"
"chainer/chainerrl" -> "deepmind/bsuite"
"chainer/chainerrl" -> "ShangtongZhang/DeepRL"
"chainer/chainerrl" -> "openai/random-network-distillation"
"iffiX/machin" -> "starry-sky6688/MADDPG"
"iffiX/machin" -> "DKuan/MADDPG_torch"
"iffiX/machin" -> "RITCHIEHuang/DeepRL_Algorithms"
"iffiX/machin" -> "Kaixhin/imitation-learning"
"iffiX/machin" -> "StepNeverStop/RLs"
"iffiX/machin" -> "mlii/mfrl"
"iffiX/machin" -> "AboudyKreidieh/h-baselines"
"iffiX/machin" -> "marlbenchmark/on-policy"
"iffiX/machin" -> "dongminlee94/deep_rl"
"iffiX/machin" -> "sjtu-marl/malib"
"iffiX/machin" -> "clvrai/awesome-rl-envs"
"iffiX/machin" -> "uoe-agents/epymarl"
"iffiX/machin" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"iffiX/machin" -> "starry-sky6688/StarCraft"
"iffiX/machin" -> "BY571/Soft-Actor-Critic-and-Extensions"
"Kaixhin/Rainbow" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"Kaixhin/Rainbow" -> "higgsfield/RL-Adventure"
"Kaixhin/Rainbow" -> "Curt-Park/rainbow-is-all-you-need"
"Kaixhin/Rainbow" -> "vitchyr/rlkit"
"Kaixhin/Rainbow" -> "astooke/rlpyt"
"Kaixhin/Rainbow" -> "ikostrikov/pytorch-a3c"
"Kaixhin/Rainbow" -> "ShangtongZhang/DeepRL"
"Kaixhin/Rainbow" -> "qfettes/DeepRL-Tutorials"
"Kaixhin/Rainbow" -> "maximecb/gym-minigrid"
"Kaixhin/Rainbow" -> "sfujim/TD3"
"Kaixhin/Rainbow" -> "deepmind/scalable_agent"
"Kaixhin/Rainbow" -> "openai/random-network-distillation"
"Kaixhin/Rainbow" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"Kaixhin/Rainbow" -> "pathak22/noreward-rl"
"Kaixhin/Rainbow" -> "higgsfield/RL-Adventure-2"
"alexis-jacq/LOLA_DiCE" -> "alshedivat/lola"
"alshedivat/lola" -> "alexis-jacq/LOLA_DiCE"
"alshedivat/lola" -> "hhexiy/opponent"
"deepmind/mathematics_dataset" -> "hendrycks/math" ["e"=1]
"deepmind/mathematics_dataset" -> "fchollet/ARC"
"deepmind/mathematics_dataset" -> "openai/grade-school-math" ["e"=1]
"deepmind/mathematics_dataset" -> "deepmind/bsuite"
"deepmind/mathematics_dataset" -> "facebookresearch/SymbolicMathematics" ["e"=1]
"deepmind/mathematics_dataset" -> "facebookresearch/pythia" ["e"=1]
"deepmind/mathematics_dataset" -> "deepmind/AQuA" ["e"=1]
"deepmind/mathematics_dataset" -> "facebookresearch/XLM" ["e"=1]
"deepmind/mathematics_dataset" -> "deepmind/dm_control"
"deepmind/mathematics_dataset" -> "facebookresearch/nevergrad" ["e"=1]
"deepmind/mathematics_dataset" -> "deepmind/graph_nets" ["e"=1]
"deepmind/mathematics_dataset" -> "harvardnlp/pytorch-struct" ["e"=1]
"deepmind/mathematics_dataset" -> "kimiyoung/transformer-xl" ["e"=1]
"deepmind/mathematics_dataset" -> "facebookresearch/PyTorch-BigGraph" ["e"=1]
"deepmind/mathematics_dataset" -> "facebookresearch/pyrobot" ["e"=1]
"tensorforce/tensorforce" -> "tensorflow/agents"
"tensorforce/tensorforce" -> "hill-a/stable-baselines"
"tensorforce/tensorforce" -> "keras-rl/keras-rl"
"tensorforce/tensorforce" -> "NervanaSystems/coach"
"tensorforce/tensorforce" -> "IntelLabs/coach"
"tensorforce/tensorforce" -> "rlworkgroup/garage"
"tensorforce/tensorforce" -> "araffin/rl-baselines-zoo"
"tensorforce/tensorforce" -> "rll/rllab"
"tensorforce/tensorforce" -> "deepmind/acme"
"tensorforce/tensorforce" -> "deepmind/trfl"
"tensorforce/tensorforce" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"tensorforce/tensorforce" -> "astooke/rlpyt"
"tensorforce/tensorforce" -> "deepmind/open_spiel"
"tensorforce/tensorforce" -> "DLR-RM/stable-baselines3"
"tensorforce/tensorforce" -> "rail-berkeley/softlearning"
"HumanCompatibleAI/overcooked_ai" -> "HumanCompatibleAI/human_aware_rl"
"HumanCompatibleAI/overcooked_ai" -> "Stanford-ILIAD/PantheonRL"
"HumanCompatibleAI/overcooked_ai" -> "HumanCompatibleAI/overcooked-demo"
"HumanCompatibleAI/overcooked_ai" -> "rosewang2008/gym-cooking"
"HumanCompatibleAI/overcooked_ai" -> "schroederdewitt/multiagent_mujoco"
"HumanCompatibleAI/overcooked_ai" -> "oxwhirl/smac"
"HumanCompatibleAI/overcooked_ai" -> "marlbenchmark/on-policy"
"HumanCompatibleAI/overcooked_ai" -> "uoe-agents/epymarl"
"HumanCompatibleAI/overcooked_ai" -> "HumanCompatibleAI/imitation"
"HumanCompatibleAI/overcooked_ai" -> "oxwhirl/pymarl"
"HumanCompatibleAI/overcooked_ai" -> "rlworkgroup/metaworld"
"HumanCompatibleAI/overcooked_ai" -> "PettingZoo-Team/PettingZoo" ["e"=1]
"HumanCompatibleAI/overcooked_ai" -> "eugenevinitsky/sequential_social_dilemma_games"
"HumanCompatibleAI/overcooked_ai" -> "starry-sky6688/StarCraft"
"HumanCompatibleAI/overcooked_ai" -> "alex-petrenko/sample-factory"
"MineDojo/MineDojo" -> "openai/Video-Pre-Training"
"MineDojo/MineDojo" -> "MineDojo/MineCLIP"
"MineDojo/MineDojo" -> "minerllabs/minerl"
"MineDojo/MineDojo" -> "danijar/dreamerv3"
"MineDojo/MineDojo" -> "vimalabs/VIMA"
"MineDojo/MineDojo" -> "kzl/decision-transformer"
"MineDojo/MineDojo" -> "sail-sg/envpool"
"MineDojo/MineDojo" -> "eloialonso/iris"
"MineDojo/MineDojo" -> "YeWR/EfficientZero"
"MineDojo/MineDojo" -> "rlworkgroup/metaworld"
"MineDojo/MineDojo" -> "google/brax"
"MineDojo/MineDojo" -> "deepmind/mujoco_menagerie"
"MineDojo/MineDojo" -> "jannerm/diffuser"
"MineDojo/MineDojo" -> "GT-RIPL/Awesome-LLM-Robotics"
"MineDojo/MineDojo" -> "vwxyzjn/cleanrl"
"brynhayder/reinforcement_learning_an_introduction" -> "LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions"
"brynhayder/reinforcement_learning_an_introduction" -> "iamhectorotero/rlai-exercises"
"brynhayder/reinforcement_learning_an_introduction" -> "vojtamolda/reinforcement-learning-an-introduction"
"brynhayder/reinforcement_learning_an_introduction" -> "diegoalejogm/Reinforcement-Learning"
"brynhayder/reinforcement_learning_an_introduction" -> "mharbuz/rlbook-exercises"
"brynhayder/reinforcement_learning_an_introduction" -> "mtrazzi/rl-book-challenge" ["e"=1]
"brynhayder/reinforcement_learning_an_introduction" -> "JKCooper2/rlai-exercises"
"openai/retro" -> "openai/baselines"
"openai/retro" -> "openai/universe"
"openai/retro" -> "openai/procgen"
"openai/retro" -> "openai/roboschool"
"openai/retro" -> "openai/spinningup"
"openai/retro" -> "mgbellemare/Arcade-Learning-Environment"
"openai/retro" -> "openai/mujoco-py"
"openai/retro" -> "deepmind/dm_control"
"openai/retro" -> "rll/rllab"
"openai/retro" -> "google/dopamine"
"openai/retro" -> "deepmind/lab"
"openai/retro" -> "deepmind/open_spiel"
"openai/retro" -> "pathak22/noreward-rl"
"openai/retro" -> "mwydmuch/ViZDoom"
"openai/retro" -> "hill-a/stable-baselines"
"Skylark0924/Machine-Learning-is-ALL-You-Need" -> "Skylark0924/Reinforcement-Learning-in-Robotics"
"Skylark0924/Machine-Learning-is-ALL-You-Need" -> "zhangchuheng123/Reinforcement-Implementation"
"Skylark0924/Machine-Learning-is-ALL-You-Need" -> "kaixindelele/DRLib"
"Skylark0924/Machine-Learning-is-ALL-You-Need" -> "reinforcement-learning-kr/lets-do-irl"
"Skylark0924/Reinforcement-Learning-in-Robotics" -> "Skylark0924/Machine-Learning-is-ALL-You-Need"
"Skylark0924/Reinforcement-Learning-in-Robotics" -> "Skylark0924/Rofunc"
"boyu-ai/Hands-on-RL" -> "datawhalechina/easy-rl"
"boyu-ai/Hands-on-RL" -> "MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning"
"boyu-ai/Hands-on-RL" -> "wangshusen/DRL"
"boyu-ai/Hands-on-RL" -> "marlbenchmark/on-policy"
"boyu-ai/Hands-on-RL" -> "kaixindelele/DRLib"
"boyu-ai/Hands-on-RL" -> "jidiai/ai_lib"
"boyu-ai/Hands-on-RL" -> "openai/multiagent-particle-envs"
"boyu-ai/Hands-on-RL" -> "AI4Finance-Foundation/ElegantRL"
"boyu-ai/Hands-on-RL" -> "NeuronDance/DeepRL"
"boyu-ai/Hands-on-RL" -> "tensorlayer/TensorLayer"
"boyu-ai/Hands-on-RL" -> "tinyzqh/light_mappo"
"boyu-ai/Hands-on-RL" -> "cuhkrlcourse/RLexample"
"boyu-ai/Hands-on-RL" -> "shariqiqbal2810/MAAC"
"boyu-ai/Hands-on-RL" -> "starry-sky6688/MARL-Algorithms"
"boyu-ai/Hands-on-RL" -> "Lizhi-sjtu/DRL-code-pytorch"
"LantaoYu/MARL-Papers" -> "openai/multiagent-particle-envs"
"LantaoYu/MARL-Papers" -> "oxwhirl/pymarl"
"LantaoYu/MARL-Papers" -> "openai/maddpg"
"LantaoYu/MARL-Papers" -> "geek-ai/MAgent"
"LantaoYu/MARL-Papers" -> "oxwhirl/smac"
"LantaoYu/MARL-Papers" -> "starry-sky6688/StarCraft"
"LantaoYu/MARL-Papers" -> "tigerneil/awesome-deep-rl"
"LantaoYu/MARL-Papers" -> "openai/baselines"
"LantaoYu/MARL-Papers" -> "marlbenchmark/on-policy"
"LantaoYu/MARL-Papers" -> "thu-ml/tianshou"
"LantaoYu/MARL-Papers" -> "ShangtongZhang/DeepRL"
"LantaoYu/MARL-Papers" -> "sisl/MADRL"
"LantaoYu/MARL-Papers" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"LantaoYu/MARL-Papers" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"LantaoYu/MARL-Papers" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"iassael/learning-to-communicate" -> "minqi/learning-to-communicate-pytorch"
"iassael/learning-to-communicate" -> "facebookresearch/CommNet"
"iassael/learning-to-communicate" -> "sisl/MADRL"
"iassael/learning-to-communicate" -> "rhoowd/sched_net"
"iassael/learning-to-communicate" -> "IC3Net/IC3Net"
"iassael/learning-to-communicate" -> "xuehy/pytorch-maddpg"
"iassael/learning-to-communicate" -> "openai/multiagent-particle-envs"
"iassael/learning-to-communicate" -> "carpedm20/NAF-tensorflow"
"iassael/learning-to-communicate" -> "facebookarchive/CommNet"
"iassael/learning-to-communicate" -> "openai/maddpg"
"iassael/learning-to-communicate" -> "ludc/rltorch" ["e"=1]
"iassael/learning-to-communicate" -> "TakuyaHiraoka/Multi-Agent-Reinforcement-Learning-in-Stochastic-Games"
"iassael/learning-to-communicate" -> "wwxFromTju/deepmind_MAS_enviroment"
"iassael/learning-to-communicate" -> "facebook/MazeBase" ["e"=1]
"iassael/learning-to-communicate" -> "starry-sky6688/StarCraft"
"oxwhirl/smac" -> "oxwhirl/pymarl"
"oxwhirl/smac" -> "starry-sky6688/StarCraft"
"oxwhirl/smac" -> "marlbenchmark/on-policy"
"oxwhirl/smac" -> "openai/multiagent-particle-envs"
"oxwhirl/smac" -> "hijkzzz/pymarl2"
"oxwhirl/smac" -> "uoe-agents/epymarl"
"oxwhirl/smac" -> "openai/maddpg"
"oxwhirl/smac" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"oxwhirl/smac" -> "shariqiqbal2810/MAAC"
"oxwhirl/smac" -> "geek-ai/MAgent"
"oxwhirl/smac" -> "PettingZoo-Team/PettingZoo" ["e"=1]
"oxwhirl/smac" -> "LantaoYu/MARL-Papers"
"oxwhirl/smac" -> "Farama-Foundation/PettingZoo"
"oxwhirl/smac" -> "TonghanWang/ROMA"
"oxwhirl/smac" -> "shariqiqbal2810/maddpg-pytorch"
"srl-freiburg/pedsim_ros" -> "onlytailei/gym_ped_sim"
"srl-freiburg/pedsim_ros" -> "vita-epfl/CrowdNav"
"srl-freiburg/pedsim_ros" -> "mit-acl/cadrl_ros"
"srl-freiburg/pedsim_ros" -> "yuxiang-gao/PySocialForce"
"srl-freiburg/pedsim_ros" -> "sybrenstuvel/Python-RVO2"
"srl-freiburg/pedsim_ros" -> "spencer-project/spencer_people_tracking" ["e"=1]
"srl-freiburg/pedsim_ros" -> "svenkreiss/socialforce"
"srl-freiburg/pedsim_ros" -> "ml-lab-cuny/menge_ros"
"srl-freiburg/pedsim_ros" -> "marinaKollmitz/human_aware_navigation" ["e"=1]
"srl-freiburg/pedsim_ros" -> "ChanganVR/RelationalGraphLearning"
"srl-freiburg/pedsim_ros" -> "mfe7/cadrl_ros"
"srl-freiburg/pedsim_ros" -> "mit-acl/gym-collision-avoidance"
"srl-freiburg/pedsim_ros" -> "MengeCrowdSim/Menge"
"srl-freiburg/pedsim_ros" -> "RGring/drl_local_planner_ros_stable_baselines"
"srl-freiburg/pedsim_ros" -> "LeeKeyu/sarl_star"
"svenkreiss/socialforce" -> "yuxiang-gao/PySocialForce"
"svenkreiss/socialforce" -> "dslwz2008/SocialForceModel"
"svenkreiss/socialforce" -> "lc6chang/Social_Force_Model"
"svenkreiss/socialforce" -> "fawwazbmn/SocialForceModel"
"svenkreiss/socialforce" -> "srl-freiburg/pedsim_ros"
"yuxiang-gao/PySocialForce" -> "svenkreiss/socialforce"
"yuxiang-gao/PySocialForce" -> "CMU-TBD/SocNavBench"
"yandexdataschool/Practical_RL" -> "yandexdataschool/Practical_DL" ["e"=1]
"yandexdataschool/Practical_RL" -> "higgsfield/RL-Adventure"
"yandexdataschool/Practical_RL" -> "aikorea/awesome-rl"
"yandexdataschool/Practical_RL" -> "dennybritz/reinforcement-learning"
"yandexdataschool/Practical_RL" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"yandexdataschool/Practical_RL" -> "yandexdataschool/nlp_course" ["e"=1]
"yandexdataschool/Practical_RL" -> "higgsfield/RL-Adventure-2"
"yandexdataschool/Practical_RL" -> "esokolov/ml-course-hse" ["e"=1]
"yandexdataschool/Practical_RL" -> "rlcode/reinforcement-learning"
"yandexdataschool/Practical_RL" -> "udacity/deep-reinforcement-learning"
"yandexdataschool/Practical_RL" -> "openai/baselines"
"yandexdataschool/Practical_RL" -> "ShangtongZhang/DeepRL"
"yandexdataschool/Practical_RL" -> "google/dopamine"
"yandexdataschool/Practical_RL" -> "openai/spinningup"
"yandexdataschool/Practical_RL" -> "simoninithomas/Deep_reinforcement_learning_Course"
"vmayoral/basic_reinforcement_learning" -> "erlerobot/gym-gazebo"
"vmayoral/basic_reinforcement_learning" -> "rlcode/reinforcement-learning"
"vmayoral/basic_reinforcement_learning" -> "awjuliani/DeepRL-Agents"
"vmayoral/basic_reinforcement_learning" -> "mpatacchiola/dissecting-reinforcement-learning"
"vmayoral/basic_reinforcement_learning" -> "aikorea/awesome-rl"
"vmayoral/basic_reinforcement_learning" -> "junhyukoh/deep-reinforcement-learning-papers"
"vmayoral/basic_reinforcement_learning" -> "muupan/deep-reinforcement-learning-papers"
"vmayoral/basic_reinforcement_learning" -> "carpedm20/deep-rl-tensorflow"
"vmayoral/basic_reinforcement_learning" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"vmayoral/basic_reinforcement_learning" -> "joschu/modular_rl"
"vmayoral/basic_reinforcement_learning" -> "matthiasplappert/keras-rl"
"vmayoral/basic_reinforcement_learning" -> "berkeleydeeprlcourse/homework"
"vmayoral/basic_reinforcement_learning" -> "devsisters/DQN-tensorflow"
"vmayoral/basic_reinforcement_learning" -> "jingweiz/pytorch-rl"
"vmayoral/basic_reinforcement_learning" -> "sudharsan13296/Hands-On-Reinforcement-Learning-With-Python"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "muupan/deep-reinforcement-learning-papers"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "junhyukoh/deep-reinforcement-learning-papers"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "carpedm20/deep-rl-tensorflow"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "openai/rllab"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "algorithmdog/Reinforcement_Learning_Blog"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "williamFalcon/DeepRLHacks"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "reinforceio/tensorforce"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "TheAbhiKumar/tensorflow-value-iteration-networks"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "miyosuda/async_deep_reinforce"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "steveKapturowski/tensorflow-rl"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "shaneshixiang/rllabplusplus"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "yukezhu/tensorflow-reinforce"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "Kaixhin/NoisyNet-A3C"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "songrotek/Meta-Learning-Papers" ["e"=1]
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "openai/multiagent-competition"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "starry-sky6688/StarCraft"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "openai/multiagent-particle-envs"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "marlbenchmark/on-policy"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "oxwhirl/pymarl"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "sisl/MADRL"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "koulanurag/ma-gym"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "oxwhirl/smac"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "shariqiqbal2810/MAAC"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "xuehy/pytorch-maddpg"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "starry-sky6688/MADDPG"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "shariqiqbal2810/maddpg-pytorch"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "PKU-AI-Edge/DGN"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "marlbenchmark/off-policy"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "openai/maddpg"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "geek-ai/MAgent"
"jsg71/Deep-Hedging" -> "YuMan-Tam/deep-hedging"
"Yonv1943/ElegantRL" -> "zhangchuheng123/Reinforcement-Implementation"
"Yonv1943/ElegantRL" -> "StepNeverStop/RLs"
"Yonv1943/ElegantRL" -> "starry-sky6688/StarCraft"
"Yonv1943/ElegantRL" -> "kaixindelele/DRLib"
"Yonv1943/ElegantRL" -> "iffiX/machin"
"higgsfield/RL-Adventure-2" -> "higgsfield/RL-Adventure"
"higgsfield/RL-Adventure-2" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"higgsfield/RL-Adventure-2" -> "ShangtongZhang/DeepRL"
"higgsfield/RL-Adventure-2" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"higgsfield/RL-Adventure-2" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"higgsfield/RL-Adventure-2" -> "sfujim/TD3"
"higgsfield/RL-Adventure-2" -> "hill-a/stable-baselines"
"higgsfield/RL-Adventure-2" -> "rll/rllab"
"higgsfield/RL-Adventure-2" -> "vitchyr/rlkit"
"higgsfield/RL-Adventure-2" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"higgsfield/RL-Adventure-2" -> "seungeunrho/minimalRL"
"higgsfield/RL-Adventure-2" -> "openai/baselines"
"higgsfield/RL-Adventure-2" -> "astooke/rlpyt"
"higgsfield/RL-Adventure-2" -> "yandexdataschool/Practical_RL"
"higgsfield/RL-Adventure-2" -> "rlworkgroup/garage"
"leegao/readme2tex" -> "agurodriguez/github-texify"
"leegao/readme2tex" -> "tokestermw/tensorflow-shakespeare" ["e"=1]
"leegao/readme2tex" -> "leegao/float-hacks" ["e"=1]
"leegao/readme2tex" -> "coreylynch/async-rl"
"leegao/readme2tex" -> "carpedm20/awesome-torch" ["e"=1]
"leegao/readme2tex" -> "muupan/async-rl"
"carpedm20/deep-rl-tensorflow" -> "devsisters/DQN-tensorflow"
"carpedm20/deep-rl-tensorflow" -> "coreylynch/async-rl"
"carpedm20/deep-rl-tensorflow" -> "matthiasplappert/keras-rl"
"carpedm20/deep-rl-tensorflow" -> "junhyukoh/deep-reinforcement-learning-papers"
"carpedm20/deep-rl-tensorflow" -> "awjuliani/DeepRL-Agents"
"carpedm20/deep-rl-tensorflow" -> "tambetm/simple_dqn"
"carpedm20/deep-rl-tensorflow" -> "miyosuda/async_deep_reinforce"
"carpedm20/deep-rl-tensorflow" -> "muupan/async-rl"
"carpedm20/deep-rl-tensorflow" -> "nivwusquorum/tensorflow-deepq"
"carpedm20/deep-rl-tensorflow" -> "muupan/deep-reinforcement-learning-papers"
"carpedm20/deep-rl-tensorflow" -> "carpedm20/NTM-tensorflow" ["e"=1]
"carpedm20/deep-rl-tensorflow" -> "openai/rllab"
"carpedm20/deep-rl-tensorflow" -> "TheAbhiKumar/tensorflow-value-iteration-networks"
"carpedm20/deep-rl-tensorflow" -> "reinforceio/tensorforce"
"carpedm20/deep-rl-tensorflow" -> "openai/universe-starter-agent"
"tigerneil/awesome-deep-rl" -> "LantaoYu/MARL-Papers"
"tigerneil/awesome-deep-rl" -> "oxwhirl/pymarl"
"tigerneil/awesome-deep-rl" -> "starry-sky6688/StarCraft"
"tigerneil/awesome-deep-rl" -> "NeuronDance/DeepRL"
"tigerneil/awesome-deep-rl" -> "zhangchuheng123/Reinforcement-Implementation"
"tigerneil/awesome-deep-rl" -> "openai/multiagent-particle-envs"
"tigerneil/awesome-deep-rl" -> "oxwhirl/smac"
"tigerneil/awesome-deep-rl" -> "marlbenchmark/on-policy"
"tigerneil/awesome-deep-rl" -> "geek-ai/MAgent"
"tigerneil/awesome-deep-rl" -> "openai/maddpg"
"tigerneil/awesome-deep-rl" -> "ShangtongZhang/DeepRL"
"tigerneil/awesome-deep-rl" -> "shariqiqbal2810/MAAC"
"tigerneil/awesome-deep-rl" -> "aikorea/awesome-rl"
"tigerneil/awesome-deep-rl" -> "hijkzzz/pymarl2"
"tigerneil/awesome-deep-rl" -> "xuehy/pytorch-maddpg"
"microsoft/malmo" -> "minerllabs/minerl"
"microsoft/malmo" -> "tambetm/gym-minecraft"
"microsoft/malmo" -> "crowdAI/marLo"
"microsoft/malmo" -> "minerllabs/baselines"
"microsoft/malmo" -> "facebookresearch/craftassist"
"microsoft/malmo" -> "openai/multi-agent-emergence-environments"
"microsoft/malmo" -> "openai/Video-Pre-Training"
"microsoft/malmo" -> "MineDojo/MineDojo"
"microsoft/malmo" -> "facebookresearch/nle"
"microsoft/malmo" -> "mwydmuch/ViZDoom"
"microsoft/malmo" -> "google-research/dreamer"
"microsoft/malmo" -> "maximecb/gym-minigrid"
"microsoft/malmo" -> "openai/procgen"
"microsoft/malmo" -> "rlworkgroup/metaworld"
"microsoft/malmo" -> "MineDojo/MineCLIP"
"benelot/pybullet-gym" -> "erwincoumans/pybullet_robots"
"benelot/pybullet-gym" -> "stepjam/RLBench"
"benelot/pybullet-gym" -> "deepmind/dm_control"
"benelot/pybullet-gym" -> "maximecb/gym-minigrid"
"benelot/pybullet-gym" -> "rail-berkeley/softlearning"
"benelot/pybullet-gym" -> "vitchyr/rlkit"
"benelot/pybullet-gym" -> "araffin/rl-baselines-zoo"
"benelot/pybullet-gym" -> "google-research/ravens"
"benelot/pybullet-gym" -> "astooke/rlpyt"
"benelot/pybullet-gym" -> "araffin/robotics-rl-srl"
"benelot/pybullet-gym" -> "rlworkgroup/metaworld"
"benelot/pybullet-gym" -> "rlworkgroup/garage"
"benelot/pybullet-gym" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"benelot/pybullet-gym" -> "openai/roboschool"
"benelot/pybullet-gym" -> "WilsonWangTHU/mbbl"
"openai/mlsh" -> "hoangminhle/hierarchical_IL_RL"
"openai/mlsh" -> "openai/robosumo"
"openai/mlsh" -> "andrew-j-levy/Hierarchical-Actor-Critc-HAC-"
"openai/mlsh" -> "openai/multiagent-competition"
"openai/mlsh" -> "florensacc/snn4hrl"
"openai/mlsh" -> "openai/EPG"
"openai/mlsh" -> "jeanharb/option_critic"
"openai/mlsh" -> "haarnoja/sac"
"openai/mlsh" -> "pathak22/noreward-rl"
"openai/mlsh" -> "openai/vime"
"openai/mlsh" -> "openai/coinrun"
"openai/mlsh" -> "cbfinn/maml_rl" ["e"=1]
"openai/mlsh" -> "openai/large-scale-curiosity"
"openai/mlsh" -> "openai/imitation"
"openai/mlsh" -> "openai/evolution-strategies-starter" ["e"=1]
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "omerbsezer/Generative_Models_Tutorial_with_Demo"
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "Khrylx/PyTorch-RL"
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "qfettes/DeepRL-Tutorials"
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "TianhongDai/reinforcement-learning-algorithms"
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "omerbsezer/LSTM_RNN_Tutorials_with_Demo"
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "sudharsan13296/Hands-On-Reinforcement-Learning-With-Python"
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "dongminlee94/deep_rl"
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "MrSyee/pg-is-all-you-need"
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "quantumiracle/Popular-RL-Algorithms"
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "rail-berkeley/softlearning"
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "dalmia/David-Silver-Reinforcement-learning"
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "seungjaeryanlee/awesome-rl-competitions"
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "eleurent/rl-agents" ["e"=1]
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "eleurent/phd-bibliography" ["e"=1]
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "DeepReinforcementLearning/DeepReinforcementLearningInAction"
"omerbsezer/LSTM_RNN_Tutorials_with_Demo" -> "omerbsezer/Reinforcement_learning_tutorial_with_demo"
"omerbsezer/LSTM_RNN_Tutorials_with_Demo" -> "omerbsezer/Generative_Models_Tutorial_with_Demo"
"omerbsezer/LSTM_RNN_Tutorials_with_Demo" -> "jaungiers/LSTM-Neural-Network-for-Time-Series-Prediction" ["e"=1]
"omerbsezer/LSTM_RNN_Tutorials_with_Demo" -> "NourozR/Stock-Price-Prediction-LSTM" ["e"=1]
"omerbsezer/LSTM_RNN_Tutorials_with_Demo" -> "buomsoo-kim/Easy-deep-learning-with-Keras" ["e"=1]
"google-research/robotics_transformer" -> "lucidrains/robotic-transformer-pytorch"
"google-research/robotics_transformer" -> "peract/peract"
"google-research/robotics_transformer" -> "vimalabs/VIMA"
"google-research/robotics_transformer" -> "GT-RIPL/Awesome-LLM-Robotics"
"google-research/robotics_transformer" -> "NVIDIA-Omniverse/Orbit"
"google-research/robotics_transformer" -> "google-research/language-table"
"google-research/robotics_transformer" -> "google-research/ravens"
"google-research/robotics_transformer" -> "facebookresearch/r3m"
"google-research/robotics_transformer" -> "microsoft/PromptCraft-Robotics"
"google-research/robotics_transformer" -> "facebookresearch/eai-vc"
"google-research/robotics_transformer" -> "cliport/cliport"
"google-research/robotics_transformer" -> "haosulab/ManiSkill2"
"google-research/robotics_transformer" -> "stepjam/RLBench"
"google-research/robotics_transformer" -> "NVIDIA-Omniverse/IsaacGymEnvs"
"google-research/robotics_transformer" -> "vimalabs/VIMABench"
"lucidrains/robotic-transformer-pytorch" -> "google-research/robotics_transformer"
"lucidrains/robotic-transformer-pytorch" -> "ir413/mvp"
"lucidrains/robotic-transformer-pytorch" -> "peract/peract"
"lucidrains/robotic-transformer-pytorch" -> "facebookresearch/r3m"
"lucidrains/robotic-transformer-pytorch" -> "columbia-ai-robotics/diffusion_policy"
"lucidrains/robotic-transformer-pytorch" -> "stepjam/ARM"
"TianhongDai/reinforcement-learning-algorithms" -> "Khrylx/PyTorch-RL"
"TianhongDai/reinforcement-learning-algorithms" -> "dongminlee94/deep_rl"
"TianhongDai/reinforcement-learning-algorithms" -> "ikostrikov/pytorch-trpo"
"TianhongDai/reinforcement-learning-algorithms" -> "qfettes/DeepRL-Tutorials"
"TianhongDai/reinforcement-learning-algorithms" -> "TianhongDai/hindsight-experience-replay"
"TianhongDai/reinforcement-learning-algorithms" -> "Kchu/DeepRL_PyTorch"
"TianhongDai/reinforcement-learning-algorithms" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"TianhongDai/reinforcement-learning-algorithms" -> "quantumiracle/Popular-RL-Algorithms"
"TianhongDai/reinforcement-learning-algorithms" -> "RITCHIEHuang/DeepRL_Algorithms"
"TianhongDai/reinforcement-learning-algorithms" -> "cts198859/deeprl_network"
"TianhongDai/reinforcement-learning-algorithms" -> "Rafael1s/Deep-Reinforcement-Learning-Algorithms"
"TianhongDai/reinforcement-learning-algorithms" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"TianhongDai/reinforcement-learning-algorithms" -> "starry-sky6688/MADDPG"
"TianhongDai/reinforcement-learning-algorithms" -> "pranz24/pytorch-soft-actor-critic"
"TianhongDai/reinforcement-learning-algorithms" -> "ShangtongZhang/DeepRL"
"IntelLabs/coach" -> "tensorforce/tensorforce"
"IntelLabs/coach" -> "MushroomRL/mushroom-rl"
"IntelLabs/coach" -> "facebookresearch/ReAgent"
"IntelLabs/coach" -> "Stable-Baselines-Team/stable-baselines3-contrib"
"IntelLabs/coach" -> "rlworkgroup/garage"
"IntelLabs/coach" -> "Kaixhin/imitation-learning"
"IntelLabs/coach" -> "oxwhirl/pymarl"
"IntelLabs/coach" -> "instadeepai/Mava"
"IntelLabs/coach" -> "HumanCompatibleAI/imitation"
"IntelLabs/coach" -> "deepmind/acme"
"IntelLabs/coach" -> "cjy1992/gym-carla" ["e"=1]
"IntelLabs/coach" -> "takuseno/d3rlpy"
"IntelLabs/coach" -> "google-research/rlds"
"IntelLabs/coach" -> "MatthewJA/Inverse-Reinforcement-Learning"
"IntelLabs/coach" -> "marload/DeepRL-TensorFlow2"
"seungjaeryanlee/awesome-rl-competitions" -> "ugurkanates/awesome-real-world-rl"
"seungjaeryanlee/awesome-rl-competitions" -> "clvrai/awesome-rl-envs"
"seungjaeryanlee/awesome-rl-competitions" -> "MrSyee/pg-is-all-you-need"
"seungjaeryanlee/awesome-rl-competitions" -> "google-research/batch_rl"
"seungjaeryanlee/awesome-rl-competitions" -> "maximecb/gym-miniworld"
"mwydmuch/ViZDoom" -> "glample/Arnold"
"mwydmuch/ViZDoom" -> "pathak22/noreward-rl"
"mwydmuch/ViZDoom" -> "openai/retro"
"mwydmuch/ViZDoom" -> "maximecb/gym-minigrid"
"mwydmuch/ViZDoom" -> "shakenes/vizdoomgym"
"mwydmuch/ViZDoom" -> "NervanaSystems/coach"
"mwydmuch/ViZDoom" -> "openai/random-network-distillation"
"mwydmuch/ViZDoom" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"mwydmuch/ViZDoom" -> "openai/roboschool"
"mwydmuch/ViZDoom" -> "deepmind/lab"
"mwydmuch/ViZDoom" -> "ikostrikov/pytorch-a3c"
"mwydmuch/ViZDoom" -> "MultiAgentLearning/playground"
"mwydmuch/ViZDoom" -> "sail-sg/envpool"
"mwydmuch/ViZDoom" -> "miyosuda/unreal"
"mwydmuch/ViZDoom" -> "awjuliani/DeepRL-Agents"
"minerllabs/minerl" -> "openai/Video-Pre-Training"
"minerllabs/minerl" -> "minerllabs/baselines"
"minerllabs/minerl" -> "microsoft/malmo"
"minerllabs/minerl" -> "MineDojo/MineDojo"
"minerllabs/minerl" -> "tambetm/gym-minecraft"
"minerllabs/minerl" -> "minerllabs/competition_submission_template"
"minerllabs/minerl" -> "crowdAI/marLo"
"minerllabs/minerl" -> "danijar/crafter"
"minerllabs/minerl" -> "minerllabs/competition_submission_starter_template"
"minerllabs/minerl" -> "eloialonso/iris"
"minerllabs/minerl" -> "danijar/dreamerv3"
"minerllabs/minerl" -> "HumanCompatibleAI/overcooked_ai"
"minerllabs/minerl" -> "rlworkgroup/metaworld"
"minerllabs/minerl" -> "danijar/dreamer"
"minerllabs/minerl" -> "openai/procgen"
"alex-petrenko/sample-factory" -> "sail-sg/envpool"
"alex-petrenko/sample-factory" -> "google-research/seed_rl"
"alex-petrenko/sample-factory" -> "Denys88/rl_games"
"alex-petrenko/sample-factory" -> "RobertTLange/gymnax"
"alex-petrenko/sample-factory" -> "facebookresearch/torchbeast"
"alex-petrenko/sample-factory" -> "alex-petrenko/megaverse"
"alex-petrenko/sample-factory" -> "vwxyzjn/cleanrl"
"alex-petrenko/sample-factory" -> "tinkoff-ai/CORL"
"alex-petrenko/sample-factory" -> "deepmind/lab2d"
"alex-petrenko/sample-factory" -> "YeWR/EfficientZero"
"alex-petrenko/sample-factory" -> "salesforce/warp-drive"
"alex-petrenko/sample-factory" -> "eloialonso/iris"
"alex-petrenko/sample-factory" -> "instadeepai/jumanji"
"alex-petrenko/sample-factory" -> "google/brax"
"alex-petrenko/sample-factory" -> "araffin/sbx"
"eloialonso/iris" -> "YeWR/EfficientZero"
"eloialonso/iris" -> "danijar/dreamerv3"
"eloialonso/iris" -> "rll-research/url_benchmark"
"eloialonso/iris" -> "kzl/decision-transformer"
"eloialonso/iris" -> "jurgisp/pydreamer"
"eloialonso/iris" -> "danijar/dreamerv2"
"eloialonso/iris" -> "danijar/crafter"
"eloialonso/iris" -> "jannerm/trajectory-transformer"
"eloialonso/iris" -> "sail-sg/envpool"
"eloialonso/iris" -> "RajGhugare19/dreamerv2"
"eloialonso/iris" -> "facebookresearch/drqv2"
"eloialonso/iris" -> "openai/Video-Pre-Training"
"eloialonso/iris" -> "google-research/rliable"
"eloialonso/iris" -> "alex-petrenko/sample-factory"
"eloialonso/iris" -> "MishaLaskin/curl"
"MatthewJA/Inverse-Reinforcement-Learning" -> "yrlu/irl-imitation"
"MatthewJA/Inverse-Reinforcement-Learning" -> "reinforcement-learning-kr/lets-do-irl"
"MatthewJA/Inverse-Reinforcement-Learning" -> "jangirrishabh/toyCarIRL"
"MatthewJA/Inverse-Reinforcement-Learning" -> "justinjfu/inverse_rl"
"MatthewJA/Inverse-Reinforcement-Learning" -> "stormmax/irl-imitation"
"MatthewJA/Inverse-Reinforcement-Learning" -> "openai/imitation"
"MatthewJA/Inverse-Reinforcement-Learning" -> "HumanCompatibleAI/imitation"
"MatthewJA/Inverse-Reinforcement-Learning" -> "Khrylx/PyTorch-RL"
"MatthewJA/Inverse-Reinforcement-Learning" -> "ermongroup/MA-AIRL"
"MatthewJA/Inverse-Reinforcement-Learning" -> "andrewliao11/gail-tf"
"MatthewJA/Inverse-Reinforcement-Learning" -> "uidilr/gail_ppo_tf"
"MatthewJA/Inverse-Reinforcement-Learning" -> "sjchoi86/irl_rocks"
"MatthewJA/Inverse-Reinforcement-Learning" -> "cbfinn/gps"
"MatthewJA/Inverse-Reinforcement-Learning" -> "neka-nat/inv_rl"
"MatthewJA/Inverse-Reinforcement-Learning" -> "ahq1993/inverse_rl"
"maximecb/gym-minigrid" -> "lcswillems/rl-starter-files"
"maximecb/gym-minigrid" -> "maximecb/gym-miniworld"
"maximecb/gym-minigrid" -> "astooke/rlpyt"
"maximecb/gym-minigrid" -> "openai/procgen"
"maximecb/gym-minigrid" -> "vitchyr/rlkit"
"maximecb/gym-minigrid" -> "deepmind/pycolab"
"maximecb/gym-minigrid" -> "hill-a/stable-baselines"
"maximecb/gym-minigrid" -> "deepmind/bsuite"
"maximecb/gym-minigrid" -> "openai/multiagent-particle-envs"
"maximecb/gym-minigrid" -> "oxwhirl/pymarl"
"maximecb/gym-minigrid" -> "rlworkgroup/metaworld"
"maximecb/gym-minigrid" -> "deepmind/dm_control"
"maximecb/gym-minigrid" -> "facebookresearch/torchbeast"
"maximecb/gym-minigrid" -> "rlworkgroup/garage"
"maximecb/gym-minigrid" -> "araffin/rl-baselines-zoo"
"reinforcement-learning-kr/lets-do-irl" -> "yrlu/irl-imitation"
"reinforcement-learning-kr/lets-do-irl" -> "MatthewJA/Inverse-Reinforcement-Learning"
"reinforcement-learning-kr/lets-do-irl" -> "jangirrishabh/toyCarIRL"
"reinforcement-learning-kr/lets-do-irl" -> "Khrylx/PyTorch-RL"
"reinforcement-learning-kr/lets-do-irl" -> "kristery/Awesome-Imitation-Learning"
"reinforcement-learning-kr/lets-do-irl" -> "Kaixhin/imitation-learning"
"reinforcement-learning-kr/lets-do-irl" -> "HumanCompatibleAI/imitation"
"reinforcement-learning-kr/lets-do-irl" -> "andrewliao11/gail-tf"
"reinforcement-learning-kr/lets-do-irl" -> "justinjfu/inverse_rl"
"reinforcement-learning-kr/lets-do-irl" -> "ahq1993/inverse_rl"
"reinforcement-learning-kr/lets-do-irl" -> "ermongroup/MA-AIRL"
"reinforcement-learning-kr/lets-do-irl" -> "qzed/irl-maxent"
"reinforcement-learning-kr/lets-do-irl" -> "reinforcement-learning-kr/pg_travel"
"reinforcement-learning-kr/lets-do-irl" -> "MCZhi/Driving-IRL-NGSIM" ["e"=1]
"reinforcement-learning-kr/lets-do-irl" -> "Ericonaldo/ILSwiss"
"yrlu/irl-imitation" -> "MatthewJA/Inverse-Reinforcement-Learning"
"yrlu/irl-imitation" -> "reinforcement-learning-kr/lets-do-irl"
"yrlu/irl-imitation" -> "jangirrishabh/toyCarIRL"
"yrlu/irl-imitation" -> "ahq1993/inverse_rl"
"yrlu/irl-imitation" -> "qzed/irl-maxent"
"yrlu/irl-imitation" -> "justinjfu/inverse_rl"
"yrlu/irl-imitation" -> "ermongroup/MA-AIRL"
"yrlu/irl-imitation" -> "neka-nat/inv_rl"
"yrlu/irl-imitation" -> "andrewliao11/gail-tf"
"yrlu/irl-imitation" -> "Kaixhin/imitation-learning"
"yrlu/irl-imitation" -> "sjchoi86/irl_rocks"
"yrlu/irl-imitation" -> "yfzhang/vehicle-motion-forecasting"
"yrlu/irl-imitation" -> "nav74neet/gail_gym"
"yrlu/irl-imitation" -> "uidilr/gail_ppo_tf"
"yrlu/irl-imitation" -> "HumanCompatibleAI/imitation"
"fchollet/ARC" -> "ellisk42/ec" ["e"=1]
"fchollet/ARC" -> "deepmind/mathematics_dataset"
"fchollet/ARC" -> "tensorflow/lucid" ["e"=1]
"fchollet/ARC" -> "google-research/disentanglement_lib" ["e"=1]
"fchollet/ARC" -> "deepmind/dm-haiku" ["e"=1]
"fchollet/ARC" -> "deepmind/acme"
"fchollet/ARC" -> "deepmind/bsuite"
"fchollet/ARC" -> "google/BIG-bench" ["e"=1]
"fchollet/ARC" -> "arogozhnikov/einops" ["e"=1]
"fchollet/ARC" -> "tomgoldstein/loss-landscape" ["e"=1]
"fchollet/ARC" -> "maximecb/gym-minigrid"
"fchollet/ARC" -> "locuslab/SATNet" ["e"=1]
"fchollet/ARC" -> "deepmind/graph_nets" ["e"=1]
"fchollet/ARC" -> "deepmind/rlax" ["e"=1]
"fchollet/ARC" -> "google/trax" ["e"=1]
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "spragunr/deep_q_rl"
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "tambetm/simple_dqn"
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "mgbellemare/Arcade-Learning-Environment"
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "devsisters/DQN-tensorflow"
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "kristjankorjus/Replicating-DeepMind"
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "nivwusquorum/tensorflow-deepq"
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "miyosuda/async_deep_reinforce"
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "asrivat1/DeepLearningVideoGames"
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "carpedm20/deep-rl-tensorflow"
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "junhyukoh/deep-reinforcement-learning-papers"
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "matthiasplappert/keras-rl"
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "muupan/deep-reinforcement-learning-papers"
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "openai/universe-starter-agent"
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "facebook/MemNN" ["e"=1]
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "Kaixhin/Atari"
"joyiswu/UCL-Deep-learning-ans-Reinforcement-learning" -> "mikezhang95/ML_Assignment"
"joyiswu/UCL-Deep-learning-ans-Reinforcement-learning" -> "RylanSchaeffer/ucl-adv-dl-rl"
"openai/roboschool" -> "openai/mujoco-py"
"openai/roboschool" -> "deepmind/dm_control"
"openai/roboschool" -> "rll/rllab"
"openai/roboschool" -> "joschu/modular_rl"
"openai/roboschool" -> "openai/baselines"
"openai/roboschool" -> "benelot/pybullet-gym"
"openai/roboschool" -> "openai/retro"
"openai/roboschool" -> "openai/universe-starter-agent"
"openai/roboschool" -> "erlerobot/gym-gazebo"
"openai/roboschool" -> "NervanaSystems/coach"
"openai/roboschool" -> "openai/rllab"
"openai/roboschool" -> "reinforceio/tensorforce"
"openai/roboschool" -> "facebookresearch/ELF"
"openai/roboschool" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"openai/roboschool" -> "openai/evolution-strategies-starter" ["e"=1]
"rlcode/reinforcement-learning" -> "awjuliani/DeepRL-Agents"
"rlcode/reinforcement-learning" -> "aikorea/awesome-rl"
"rlcode/reinforcement-learning" -> "dennybritz/reinforcement-learning"
"rlcode/reinforcement-learning" -> "reinforceio/tensorforce"
"rlcode/reinforcement-learning" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"rlcode/reinforcement-learning" -> "matthiasplappert/keras-rl"
"rlcode/reinforcement-learning" -> "junhyukoh/deep-reinforcement-learning-papers"
"rlcode/reinforcement-learning" -> "yandexdataschool/Practical_RL"
"rlcode/reinforcement-learning" -> "carpedm20/deep-rl-tensorflow"
"rlcode/reinforcement-learning" -> "devsisters/DQN-tensorflow"
"rlcode/reinforcement-learning" -> "ShangtongZhang/DeepRL"
"rlcode/reinforcement-learning" -> "udacity/deep-reinforcement-learning"
"rlcode/reinforcement-learning" -> "openai/baselines"
"rlcode/reinforcement-learning" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"rlcode/reinforcement-learning" -> "rll/rllab"
"keon/deep-q-learning" -> "devsisters/DQN-tensorflow"
"keon/deep-q-learning" -> "rlcode/reinforcement-learning"
"keon/deep-q-learning" -> "keras-rl/keras-rl"
"keon/deep-q-learning" -> "awjuliani/DeepRL-Agents"
"keon/deep-q-learning" -> "tambetm/simple_dqn"
"keon/deep-q-learning" -> "germain-hug/Deep-RL-Keras"
"keon/deep-q-learning" -> "matthiasplappert/keras-rl"
"keon/deep-q-learning" -> "keon/policy-gradient"
"keon/deep-q-learning" -> "yanpanlau/DDPG-Keras-Torcs"
"keon/deep-q-learning" -> "carpedm20/deep-rl-tensorflow"
"keon/deep-q-learning" -> "kh-kim/stock_market_reinforcement_learning" ["e"=1]
"keon/deep-q-learning" -> "yanpanlau/Keras-FlappyBird"
"keon/deep-q-learning" -> "simoninithomas/Deep_reinforcement_learning_Course"
"keon/deep-q-learning" -> "coreylynch/async-rl"
"keon/deep-q-learning" -> "spragunr/deep_q_rl"
"openai/neural-mmo" -> "jsuarez5341/neural-mmo-client"
"openai/neural-mmo" -> "jsuarez5341/neural-mmo"
"openai/neural-mmo" -> "openai/multiagent-particle-envs"
"openai/neural-mmo" -> "oxwhirl/smac"
"openai/neural-mmo" -> "openai/multi-agent-emergence-environments"
"openai/neural-mmo" -> "MultiAgentLearning/playground"
"openai/neural-mmo" -> "oxwhirl/pymarl"
"openai/neural-mmo" -> "deepmind/hanabi-learning-environment"
"openai/neural-mmo" -> "eugenevinitsky/sequential_social_dilemma_games"
"openai/neural-mmo" -> "NeuralMMO/environment"
"openai/neural-mmo" -> "openai/maddpg"
"openai/neural-mmo" -> "geek-ai/MAgent"
"openai/neural-mmo" -> "openai/multiagent-competition"
"openai/neural-mmo" -> "openai/large-scale-curiosity"
"openai/neural-mmo" -> "openai/retro"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "dongminlee94/deep_rl"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "StepNeverStop/RLs"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "TianhongDai/reinforcement-learning-algorithms"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "quantumiracle/Popular-RL-Algorithms"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "MrSyee/pg-is-all-you-need"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "DKuan/MADDPG_torch"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "pythonlessons/Reinforcement_Learning"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "RITCHIEHuang/DeepRL_Algorithms"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "zhangchuheng123/Reinforcement-Implementation"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "XinJingHao/RL-Algorithms-by-Pytorch"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "nikhilbarhate99/PPO-PyTorch"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "anita-hu/TF2-RL"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "denisyarats/pytorch_sac"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "andri27-ts/Reinforcement-Learning"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "berkeleydeeprlcourse/homework_fall2020"
"HaveIBeenPwned/PwnedPasswordsAzureFunction" -> "HaveIBeenPwned/PwnedPasswordsCloudflareWorker"
"HaveIBeenPwned/PwnedPasswordsAzureFunction" -> "HaveIBeenPwned/3DModels"
"HaveIBeenPwned/PwnedPasswordsCloudflareWorker" -> "HaveIBeenPwned/PwnedPasswordsAzureFunction"
"HaveIBeenPwned/PwnedPasswordsCloudflareWorker" -> "deepmind/android_env"
"HaveIBeenPwned/PwnedPasswordsCloudflareWorker" -> "HaveIBeenPwned/3DModels"
"HaveIBeenPwned/PwnedPasswordsCloudflareWorker" -> "StarInitial/xpcheck"
"facebookresearch/nle" -> "facebookresearch/minihack"
"facebookresearch/nle" -> "maximecb/gym-minigrid"
"facebookresearch/nle" -> "facebookresearch/torchbeast"
"facebookresearch/nle" -> "openai/procgen"
"facebookresearch/nle" -> "facebookresearch/impact-driven-exploration"
"facebookresearch/nle" -> "danijar/crafter"
"facebookresearch/nle" -> "ngoodger/nle-language-wrapper"
"facebookresearch/nle" -> "uber-research/go-explore"
"facebookresearch/nle" -> "microsoft/TextWorld" ["e"=1]
"facebookresearch/nle" -> "Bam4d/Griddly"
"facebookresearch/nle" -> "PettingZoo-Team/PettingZoo" ["e"=1]
"facebookresearch/nle" -> "rlworkgroup/metaworld"
"facebookresearch/nle" -> "tambetm/gym-minecraft"
"facebookresearch/nle" -> "deepmind/bsuite"
"facebookresearch/nle" -> "hardmaru/slimevolleygym"
"RobertTLange/gymnax" -> "RobertTLange/evosax"
"RobertTLange/gymnax" -> "instadeepai/jumanji"
"RobertTLange/gymnax" -> "luchris429/purejaxrl"
"RobertTLange/gymnax" -> "araffin/sbx"
"RobertTLange/gymnax" -> "google-research/rliable"
"RobertTLange/gymnax" -> "sail-sg/envpool"
"RobertTLange/gymnax" -> "ikostrikov/jaxrl"
"RobertTLange/gymnax" -> "deepmind/rlax" ["e"=1]
"RobertTLange/gymnax" -> "deepmind/distrax" ["e"=1]
"RobertTLange/gymnax" -> "google/evojax"
"RobertTLange/gymnax" -> "google/brax"
"RobertTLange/gymnax" -> "danijar/dreamerv3"
"RobertTLange/gymnax" -> "kenjyoung/MinAtar"
"RobertTLange/gymnax" -> "RobertTLange/gymnax-blines"
"RobertTLange/gymnax" -> "twni2016/pomdp-baselines"
"lcswillems/torch-ac" -> "lcswillems/rl-starter-files"
"mila-iqia/atari-representation-learning" -> "MishaLaskin/curl"
"mila-iqia/atari-representation-learning" -> "tkipf/c-swm" ["e"=1]
"mila-iqia/atari-representation-learning" -> "mila-iqia/spr"
"mila-iqia/atari-representation-learning" -> "denisyarats/dmc2gym"
"mila-iqia/atari-representation-learning" -> "danijar/dreamer"
"mila-iqia/atari-representation-learning" -> "denisyarats/drq"
"mila-iqia/atari-representation-learning" -> "aravindsrinivas/curl_rainbow"
"mila-iqia/atari-representation-learning" -> "alexlee-gk/slac"
"google/evojax" -> "RobertTLange/evosax"
"google/evojax" -> "RobertTLange/gymnax"
"google/evojax" -> "adaptive-intelligent-robotics/QDax"
"google/evojax" -> "nnaisense/evotorch"
"google/evojax" -> "google/brax"
"google/evojax" -> "deepmind/chex" ["e"=1]
"google/evojax" -> "icaros-usc/pyribs"
"google/evojax" -> "deepmind/rlax" ["e"=1]
"google/evojax" -> "n2cholas/awesome-jax" ["e"=1]
"google/evojax" -> "google/CommonLoopUtils" ["e"=1]
"google/evojax" -> "instadeepai/jumanji"
"google/evojax" -> "google/jaxtyping" ["e"=1]
"google/evojax" -> "deepmind/optax" ["e"=1]
"google/evojax" -> "google/jaxopt" ["e"=1]
"google/evojax" -> "deepmind/dm-haiku" ["e"=1]
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "joyiswu/UCL-Deep-learning-ans-Reinforcement-learning"
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "RylanSchaeffer/ucl-adv-dl-rl"
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "Zhenye-Na/advanced-deep-learning-and-reinforcement-learning-deepmind"
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "dalmia/David-Silver-Reinforcement-learning"
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "berkeleydeeprlcourse/homework"
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "YidingYu/UCL-DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning"
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "mikezhang95/ML_Assignment"
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "NeuronDance/DeepRL"
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "simoninithomas/Deep_reinforcement_learning_Course"
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "wwxFromTju/awesome-reinforcement-learning-zh"
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "aamini/introtodeeplearning_labs" ["e"=1]
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "Zhenye-Na/reinforcement-learning-stanford"
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "udacity/deep-reinforcement-learning"
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "glouppe/info8010-deep-learning" ["e"=1]
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On"
"beyretb/AnimalAI-Olympics" -> "mdcrosby/animal-ai"
"beyretb/AnimalAI-Olympics" -> "Unity-Technologies/obstacle-tower-env"
"beyretb/AnimalAI-Olympics" -> "maximecb/gym-miniworld"
"beyretb/AnimalAI-Olympics" -> "openai/coinrun"
"beyretb/AnimalAI-Olympics" -> "seungjaeryanlee/awesome-rl-competitions"
"beyretb/AnimalAI-Olympics" -> "deepmind/bsuite"
"beyretb/AnimalAI-Olympics" -> "astooke/rlpyt"
"beyretb/AnimalAI-Olympics" -> "maximecb/gym-minigrid"
"beyretb/AnimalAI-Olympics" -> "MultiAgentLearning/playground"
"beyretb/AnimalAI-Olympics" -> "google-research/dreamer"
"beyretb/AnimalAI-Olympics" -> "benelot/pybullet-gym"
"beyretb/AnimalAI-Olympics" -> "stepjam/RLBench"
"beyretb/AnimalAI-Olympics" -> "uber-research/go-explore"
"beyretb/AnimalAI-Olympics" -> "kandouss/marlgrid"
"beyretb/AnimalAI-Olympics" -> "stanfordnmbl/osim-rl"
"facebookresearch/rl" -> "google-research/rliable"
"facebookresearch/rl" -> "facebookresearch/mbrl-lib"
"facebookresearch/rl" -> "ikostrikov/jaxrl"
"facebookresearch/rl" -> "sail-sg/envpool"
"facebookresearch/rl" -> "facebookresearch/rlmeta"
"facebookresearch/rl" -> "clvrai/awesome-rl-envs"
"facebookresearch/rl" -> "vwxyzjn/cleanrl"
"facebookresearch/rl" -> "facebookresearch/moolib"
"facebookresearch/rl" -> "facebookresearch/torchbeast"
"facebookresearch/rl" -> "rll-research/url_benchmark"
"facebookresearch/rl" -> "danijar/dreamerv3"
"facebookresearch/rl" -> "YeWR/EfficientZero"
"facebookresearch/rl" -> "deepmind/meltingpot"
"facebookresearch/rl" -> "jurgisp/memory-maze"
"facebookresearch/rl" -> "lucidrains/robotic-transformer-pytorch"
"RobertTLange/evosax" -> "google/evojax"
"RobertTLange/evosax" -> "RobertTLange/gymnax"
"RobertTLange/evosax" -> "adaptive-intelligent-robotics/QDax"
"RobertTLange/evosax" -> "henry-prior/jax-rl"
"RobertTLange/evosax" -> "instadeepai/jumanji"
"RobertTLange/evosax" -> "nnaisense/evotorch"
"RobertTLange/evosax" -> "luchris429/purejaxrl"
"RobertTLange/evosax" -> "deepmind/distrax" ["e"=1]
"google/brain-tokyo-workshop" -> "weightagnostic/weightagnostic.github.io"
"google/brain-tokyo-workshop" -> "uber-research/PyTorch-NEAT" ["e"=1]
"google/brain-tokyo-workshop" -> "danijar/dreamerv2"
"google/brain-tokyo-workshop" -> "maximecb/gym-minigrid"
"google/brain-tokyo-workshop" -> "hardmaru/estool" ["e"=1]
"google/brain-tokyo-workshop" -> "deepmind/bsuite"
"google/brain-tokyo-workshop" -> "google/evojax"
"google/brain-tokyo-workshop" -> "deepmind/scalable_agent"
"google/brain-tokyo-workshop" -> "astooke/rlpyt"
"google/brain-tokyo-workshop" -> "CodeReclaimers/neat-python" ["e"=1]
"google/brain-tokyo-workshop" -> "deepmind/rlax" ["e"=1]
"google/brain-tokyo-workshop" -> "openai/procgen"
"google/brain-tokyo-workshop" -> "google-research/planet"
"google/brain-tokyo-workshop" -> "uber-research/deep-neuroevolution" ["e"=1]
"google/brain-tokyo-workshop" -> "deepmind/open_spiel"
"nnaisense/evotorch" -> "RobertTLange/evosax"
"nnaisense/evotorch" -> "google/evojax"
"nnaisense/evotorch" -> "google/brax"
"nnaisense/evotorch" -> "RobertTLange/gymnax"
"nnaisense/evotorch" -> "facebookresearch/theseus" ["e"=1]
"nnaisense/evotorch" -> "google-research/rliable"
"nnaisense/evotorch" -> "samuela/git-re-basin"
"nnaisense/evotorch" -> "EvolutionGym/evogym"
"nnaisense/evotorch" -> "sail-sg/envpool"
"nnaisense/evotorch" -> "google/learned_optimization" ["e"=1]
"nnaisense/evotorch" -> "eloialonso/iris"
"nnaisense/evotorch" -> "clvrai/awesome-rl-envs"
"nnaisense/evotorch" -> "deepmind/mujoco_menagerie"
"nnaisense/evotorch" -> "pytorch/rl"
"nnaisense/evotorch" -> "vwxyzjn/cleanrl"
"EthanMacdonald/h-DQN" -> "mrkulk/hierarchical-deep-RL"
"EthanMacdonald/h-DQN" -> "Ardavans/DSR"
"EthanMacdonald/h-DQN" -> "skumar9876/Hierarchical-DQN"
"EthanMacdonald/h-DQN" -> "Nat-D/FeatureControlHRL"
"IntelVCL/DirectFuturePrediction" -> "akolishchak/doom-net-pytorch"
"Kaixhin/ACER" -> "alexis-jacq/Pytorch-DPPO"
"Kaixhin/ACER" -> "dchetelat/acer"
"Kaixhin/ACER" -> "ikostrikov/pytorch-trpo"
"Kaixhin/ACER" -> "jingweiz/pytorch-rl"
"Kaixhin/ACER" -> "mjacar/pytorch-trpo"
"MG2033/A2C" -> "pemami4911/deep-rl"
"NVlabs/GA3C" -> "miyosuda/async_deep_reinforce"
"NVlabs/GA3C" -> "muupan/async-rl"
"NVlabs/GA3C" -> "miyosuda/unreal"
"NVlabs/GA3C" -> "ikostrikov/pytorch-a3c"
"NVlabs/GA3C" -> "dgriff777/rl_a3c_pytorch"
"NVlabs/GA3C" -> "coreylynch/async-rl"
"NVlabs/GA3C" -> "openai/universe-starter-agent"
"NVlabs/GA3C" -> "steveKapturowski/tensorflow-rl"
"NVlabs/GA3C" -> "deepmind/scalable_agent"
"NVlabs/GA3C" -> "pathak22/noreward-rl"
"NVlabs/GA3C" -> "reinforceio/tensorforce"
"NVlabs/GA3C" -> "awjuliani/Meta-RL"
"NVlabs/GA3C" -> "TheAbhiKumar/tensorflow-value-iteration-networks"
"NVlabs/GA3C" -> "Alfredvc/paac"
"NVlabs/GA3C" -> "carpedm20/deep-rl-tensorflow"
"aleju/mario-ai" -> "ehrenbrav/DeepQNetwork"
"aleju/mario-ai" -> "Kautenja/gym-super-mario-bros"
"aleju/mario-ai" -> "ppaquette/gym-super-mario"
"aleju/mario-ai" -> "songrotek/DRL-FlappyBird"
"aleju/mario-ai" -> "rameshvarun/NeuralKart" ["e"=1]
"aleju/mario-ai" -> "xbpeng/DeepTerrainRL"
"aleju/mario-ai" -> "aleju/self-driving-truck" ["e"=1]
"aleju/mario-ai" -> "kuz/DeepMind-Atari-Deep-Q-Learner"
"aleju/mario-ai" -> "tambetm/simple_dqn"
"aleju/mario-ai" -> "coreylynch/async-rl"
"aleju/mario-ai" -> "miyosuda/async_deep_reinforce"
"aleju/mario-ai" -> "navneet-nmk/pytorch-rl"
"aleju/mario-ai" -> "asrivat1/DeepLearningVideoGames"
"aleju/mario-ai" -> "muupan/async-rl"
"aleju/mario-ai" -> "mgbellemare/Arcade-Learning-Environment"
"avivt/VIN" -> "TheAbhiKumar/tensorflow-value-iteration-networks"
"avivt/VIN" -> "kentsommer/pytorch-value-iteration-networks"
"avivt/VIN" -> "zuoxingdong/VIN_PyTorch_Visdom"
"avivt/VIN" -> "zhongwen/predictron"
"avivt/VIN" -> "zuoxingdong/VIN_TensorFlow"
"avivt/VIN" -> "miyosuda/async_deep_reinforce"
"avivt/VIN" -> "openai/vime"
"avivt/VIN" -> "shaneshixiang/rllabplusplus"
"avivt/VIN" -> "songrotek/DDPG"
"avivt/VIN" -> "junhyukoh/value-prediction-network"
"avivt/VIN" -> "YunzhuLi/InfoGAIL"
"avivt/VIN" -> "openai/rllab"
"avivt/VIN" -> "joschu/modular_rl"
"awjuliani/Meta-RL" -> "miyosuda/unreal"
"awjuliani/Meta-RL" -> "katerakelly/oyster"
"awjuliani/Meta-RL" -> "achao2013/Learning-To-Reinforcement-Learn" ["e"=1]
"awjuliani/Meta-RL" -> "tristandeleu/pytorch-maml-rl" ["e"=1]
"awjuliani/Meta-RL" -> "miyosuda/async_deep_reinforce"
"awjuliani/Meta-RL" -> "mtrazzi/two-step-task"
"awjuliani/Meta-RL" -> "cbfinn/maml_rl" ["e"=1]
"awjuliani/Meta-RL" -> "NVlabs/GA3C"
"awjuliani/Meta-RL" -> "awjuliani/DeepRL-Agents"
"awjuliani/Meta-RL" -> "EthanMacdonald/h-DQN"
"awjuliani/Meta-RL" -> "jonasrothfuss/ProMP"
"awjuliani/Meta-RL" -> "TheAbhiKumar/tensorflow-value-iteration-networks"
"awjuliani/Meta-RL" -> "iclavera/learning_to_adapt"
"awjuliani/Meta-RL" -> "pathak22/noreward-rl"
"awjuliani/Meta-RL" -> "mwufi/meta-rl-bandits"
"deepmind/ai-safety-gridworlds" -> "deepmind/pycolab"
"deepmind/ai-safety-gridworlds" -> "openai/safety-gym"
"deepmind/ai-safety-gridworlds" -> "maximecb/gym-minigrid"
"deepmind/ai-safety-gridworlds" -> "openai/safety-starter-agents"
"deepmind/ai-safety-gridworlds" -> "befelix/safe_learning"
"deepmind/ai-safety-gridworlds" -> "deepmind/bsuite"
"deepmind/ai-safety-gridworlds" -> "kenjyoung/MinAtar"
"deepmind/ai-safety-gridworlds" -> "deepmind/scalable_agent"
"deepmind/ai-safety-gridworlds" -> "quanvuong/handful-of-trials-pytorch"
"deepmind/ai-safety-gridworlds" -> "openai/coinrun"
"deepmind/ai-safety-gridworlds" -> "eleurent/rl-agents" ["e"=1]
"deepmind/ai-safety-gridworlds" -> "mpSchrader/gym-sokoban"
"deepmind/ai-safety-gridworlds" -> "openai/random-network-distillation"
"deepmind/ai-safety-gridworlds" -> "mila-iqia/atari-representation-learning"
"deepmind/ai-safety-gridworlds" -> "rlworkgroup/metaworld"
"deepmind/dqn" -> "ikostrikov/pytorch-a3c"
"deepmind/dqn" -> "tambetm/simple_dqn"
"deepmind/dqn" -> "deepmind/dnc"
"deepmind/dqn" -> "deepmind/pycolab"
"deepmind/dqn" -> "devsisters/DQN-tensorflow"
"deepmind/dqn" -> "deepmind/scalable_agent"
"deepmind/dqn" -> "mgbellemare/Arcade-Learning-Environment"
"deepmind/dqn" -> "transedward/pytorch-dqn"
"deepmind/dqn" -> "deepmind/dqn_zoo"
"deepmind/dqn" -> "ikostrikov/pytorch-trpo"
"deepmind/dqn" -> "deepmind/xitari"
"deepmind/dqn" -> "miyosuda/unreal"
"deepmind/dqn" -> "gliese581gg/DQN_tensorflow"
"deepmind/dqn" -> "Kaixhin/Rainbow"
"deepmind/dqn" -> "openai/atari-py"
"deepmind/pycolab" -> "deepmind/ai-safety-gridworlds"
"deepmind/pycolab" -> "maximecb/gym-minigrid"
"deepmind/pycolab" -> "deepmind/scalable_agent"
"deepmind/pycolab" -> "deepmind/bsuite"
"deepmind/pycolab" -> "openai/multiagent-competition"
"deepmind/pycolab" -> "junhyukoh/value-prediction-network"
"deepmind/pycolab" -> "mpSchrader/gym-sokoban"
"deepmind/pycolab" -> "benelot/pybullet-gym"
"deepmind/pycolab" -> "williamFalcon/DeepRLHacks"
"deepmind/pycolab" -> "miyosuda/unreal"
"deepmind/pycolab" -> "deepmind/rlax" ["e"=1]
"deepmind/pycolab" -> "deepmind/dm_control"
"deepmind/pycolab" -> "kenjyoung/MinAtar"
"deepmind/pycolab" -> "geek-ai/MAgent"
"deepmind/pycolab" -> "facebookresearch/ELF"
"greydanus/visualize_atari" -> "nikaashpuri/sarfa-saliency"
"greydanus/visualize_atari" -> "greydanus/baby-a3c"
"jingweiz/pytorch-rl" -> "ikostrikov/pytorch-a3c"
"jingweiz/pytorch-rl" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"jingweiz/pytorch-rl" -> "dgriff777/rl_a3c_pytorch"
"jingweiz/pytorch-rl" -> "ShangtongZhang/DeepRL"
"jingweiz/pytorch-rl" -> "transedward/pytorch-dqn"
"jingweiz/pytorch-rl" -> "Khrylx/PyTorch-RL"
"jingweiz/pytorch-rl" -> "jingweiz/pytorch-dnc" ["e"=1]
"jingweiz/pytorch-rl" -> "Kaixhin/ACER"
"jingweiz/pytorch-rl" -> "williamFalcon/DeepRLHacks"
"jingweiz/pytorch-rl" -> "zuoxingdong/VIN_PyTorch_Visdom"
"jingweiz/pytorch-rl" -> "onlytailei/A3C-PyTorch"
"jingweiz/pytorch-rl" -> "ghliu/pytorch-ddpg"
"jingweiz/pytorch-rl" -> "carpedm20/deep-rl-tensorflow"
"jingweiz/pytorch-rl" -> "lanpa/tensorboard-pytorch" ["e"=1]
"jingweiz/pytorch-rl" -> "NervanaSystems/coach"
"kengz/openai_lab" -> "steveKapturowski/tensorflow-rl"
"navneet-nmk/pytorch-rl" -> "Khrylx/PyTorch-RL"
"navneet-nmk/pytorch-rl" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"navneet-nmk/pytorch-rl" -> "jingweiz/pytorch-rl"
"navneet-nmk/pytorch-rl" -> "TianhongDai/hindsight-experience-replay"
"navneet-nmk/pytorch-rl" -> "vitchyr/rlkit"
"navneet-nmk/pytorch-rl" -> "vitchyr/multiworld"
"navneet-nmk/pytorch-rl" -> "higgsfield/Imagination-Augmented-Agents"
"navneet-nmk/pytorch-rl" -> "rail-berkeley/softlearning"
"navneet-nmk/pytorch-rl" -> "openai/coinrun"
"navneet-nmk/pytorch-rl" -> "zuoxingdong/lagom"
"navneet-nmk/pytorch-rl" -> "MillionIntegrals/vel"
"navneet-nmk/pytorch-rl" -> "andrew-j-levy/Hierarchical-Actor-Critc-HAC-"
"navneet-nmk/pytorch-rl" -> "araffin/learning-to-drive-in-5-minutes" ["e"=1]
"navneet-nmk/pytorch-rl" -> "araffin/robotics-rl-srl"
"navneet-nmk/pytorch-rl" -> "tristandeleu/pytorch-maml-rl" ["e"=1]
"skumar9876/Hierarchical-DQN" -> "mrkulk/hierarchical-deep-RL"
"skumar9876/Hierarchical-DQN" -> "fedingo/Hierarchical-DQN"
"skumar9876/Hierarchical-DQN" -> "EthanMacdonald/h-DQN"
"yandexdataschool/AgentNet" -> "yandexdataschool/YSDA_deeplearning17" ["e"=1]
"yandexdataschool/AgentNet" -> "miyosuda/async_deep_reinforce"
"yandexdataschool/AgentNet" -> "yandexdataschool/MLatImperial2017"
"tensorlayer/awesome-tensorlayer" -> "tensorlayer/tensorlayer"
"tensorlayer/awesome-tensorlayer" -> "tensorlayer/chinese-book" ["e"=1]
"PKU-MARL/Safe-Policy-Optimization" -> "PKU-MARL/omnisafe" ["e"=1]
"PKU-MARL/Safe-Policy-Optimization" -> "chauncygu/Safe-Reinforcement-Learning-Baselines"
"PKU-MARL/Safe-Policy-Optimization" -> "openai/safety-starter-agents"
"PKU-MARL/Safe-Policy-Optimization" -> "chauncygu/Multi-Agent-Constrained-Policy-Optimisation"
"PKU-MARL/Safe-Policy-Optimization" -> "PKU-MARL/safety-gymnasium"
"PKU-MARL/Safe-Policy-Optimization" -> "openai/safety-gym"
"PKU-MARL/Safe-Policy-Optimization" -> "PKU-MARL/DexterousHands"
"jannerm/diffuser" -> "anuragajay/decision-diffuser"
"jannerm/diffuser" -> "columbia-ai-robotics/diffusion_policy"
"jannerm/diffuser" -> "jannerm/trajectory-transformer"
"jannerm/diffuser" -> "rail-berkeley/d4rl"
"jannerm/diffuser" -> "peract/peract"
"jannerm/diffuser" -> "ikostrikov/implicit_q_learning"
"jannerm/diffuser" -> "ZhengyaoJiang/latentplan"
"jannerm/diffuser" -> "cliport/cliport"
"jannerm/diffuser" -> "facebookresearch/drqv2"
"jannerm/diffuser" -> "Farama-Foundation/D4RL"
"jannerm/diffuser" -> "conglu1997/v-d4rl"
"jannerm/diffuser" -> "vimalabs/VIMA"
"jannerm/diffuser" -> "google-research/ibc"
"jannerm/diffuser" -> "rll-research/url_benchmark"
"jannerm/diffuser" -> "hanjuku-kaso/awesome-offline-rl"
"cyoon1729/deep-Q-networks" -> "cyoon1729/Policy-Gradient-Methods"
"cyoon1729/deep-Q-networks" -> "cyoon1729/Reinforcement-learning"
"cyoon1729/deep-Q-networks" -> "BY571/DQN-Atari-Agents"
"cyoon1729/deep-Q-networks" -> "dxyang/DQN_pytorch"
"joschu/modular_rl" -> "openai/rllab"
"joschu/modular_rl" -> "ikostrikov/pytorch-trpo"
"joschu/modular_rl" -> "pat-coady/trpo"
"joschu/modular_rl" -> "miyosuda/async_deep_reinforce"
"joschu/modular_rl" -> "openai/imitation"
"joschu/modular_rl" -> "shaneshixiang/rllabplusplus"
"joschu/modular_rl" -> "wojzaremba/trpo"
"joschu/modular_rl" -> "muupan/async-rl"
"joschu/modular_rl" -> "openai/vime"
"joschu/modular_rl" -> "TheAbhiKumar/tensorflow-value-iteration-networks"
"joschu/modular_rl" -> "rllab/rllab"
"joschu/modular_rl" -> "coreylynch/async-rl"
"joschu/modular_rl" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"joschu/modular_rl" -> "cbfinn/gps"
"joschu/modular_rl" -> "Breakend/DeepReinforcementLearningThatMatters"
"jsuarez5341/neural-mmo" -> "openai/neural-mmo"
"jsuarez5341/neural-mmo" -> "jsuarez5341/neural-mmo-client"
"jsuarez5341/neural-mmo-client" -> "openai/neural-mmo"
"jsuarez5341/neural-mmo-client" -> "jsuarez5341/neural-mmo"
"koulanurag/muzero-pytorch" -> "johan-gras/MuZero"
"koulanurag/muzero-pytorch" -> "YeWR/EfficientZero"
"koulanurag/muzero-pytorch" -> "Zeta36/muzero"
"koulanurag/muzero-pytorch" -> "werner-duvaud/muzero-general"
"koulanurag/muzero-pytorch" -> "kaesve/muzero"
"koulanurag/muzero-pytorch" -> "denisyarats/pytorch_sac"
"koulanurag/muzero-pytorch" -> "Xingyu-Lin/mbpo_pytorch"
"mimoralea/gdrl" -> "DeepReinforcementLearning/DeepReinforcementLearningInAction"
"mimoralea/gdrl" -> "mimoralea/applied-reinforcement-learning"
"mimoralea/gdrl" -> "udacity/deep-reinforcement-learning"
"mimoralea/gdrl" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition"
"mimoralea/gdrl" -> "ShangtongZhang/DeepRL"
"mimoralea/gdrl" -> "JuliaReinforcementLearning/ReinforcementLearningAnIntroduction.jl" ["e"=1]
"mimoralea/gdrl" -> "TikhonJelvis/RL-book" ["e"=1]
"mimoralea/gdrl" -> "kengz/awesome-deep-rl"
"mimoralea/gdrl" -> "rlcode/per"
"mimoralea/gdrl" -> "Shmuma/ptan"
"mimoralea/gdrl" -> "PacktPublishing/Mastering-Reinforcement-Learning-with-Python"
"mimoralea/gdrl" -> "higgsfield/RL-Adventure"
"mimoralea/gdrl" -> "JuliaReinforcementLearning/ReinforcementLearning.jl" ["e"=1]
"mimoralea/gdrl" -> "ucaiado/QLearning_Trading" ["e"=1]
"mimoralea/gdrl" -> "LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions"
"karpathy/reinforcejs" -> "karpathy/recurrentjs"
"karpathy/reinforcejs" -> "karpathy/convnetjs" ["e"=1]
"karpathy/reinforcejs" -> "spragunr/deep_q_rl"
"karpathy/reinforcejs" -> "joschu/modular_rl"
"karpathy/reinforcejs" -> "coreylynch/async-rl"
"karpathy/reinforcejs" -> "nivwusquorum/tensorflow-deepq"
"karpathy/reinforcejs" -> "mgbellemare/Arcade-Learning-Environment"
"karpathy/reinforcejs" -> "rllab/rllab"
"karpathy/reinforcejs" -> "janhuenermann/neurojs" ["e"=1]
"karpathy/reinforcejs" -> "tambetm/simple_dqn"
"karpathy/reinforcejs" -> "junhyukoh/deep-reinforcement-learning-papers"
"karpathy/reinforcejs" -> "SirTificate/gekko-neuralnet" ["e"=1]
"karpathy/reinforcejs" -> "karpathy/svmjs" ["e"=1]
"karpathy/reinforcejs" -> "rlpy/rlpy" ["e"=1]
"karpathy/reinforcejs" -> "devsisters/DQN-tensorflow"
"asrivat1/DeepLearningVideoGames" -> "nivwusquorum/tensorflow-deepq"
"asrivat1/DeepLearningVideoGames" -> "spragunr/deep_q_rl"
"asrivat1/DeepLearningVideoGames" -> "yenchenlin1994/DeepLearningFlappyBird"
"asrivat1/DeepLearningVideoGames" -> "tambetm/simple_dqn"
"asrivat1/DeepLearningVideoGames" -> "sourabhv/FlapPyBird"
"asrivat1/DeepLearningVideoGames" -> "coreylynch/async-rl"
"asrivat1/DeepLearningVideoGames" -> "devsisters/DQN-tensorflow"
"asrivat1/DeepLearningVideoGames" -> "kuz/DeepMind-Atari-Deep-Q-Learner"
"asrivat1/DeepLearningVideoGames" -> "harvitronix/reinforcement-learning-car"
"asrivat1/DeepLearningVideoGames" -> "yenchenlin/DeepLearningFlappyBird"
"asrivat1/DeepLearningVideoGames" -> "gliese581gg/DQN_tensorflow"
"asrivat1/DeepLearningVideoGames" -> "carpedm20/deep-rl-tensorflow"
"asrivat1/DeepLearningVideoGames" -> "muupan/async-rl"
"asrivat1/DeepLearningVideoGames" -> "miyosuda/async_deep_reinforce"
"asrivat1/DeepLearningVideoGames" -> "matthiasplappert/keras-rl"
"deepmind/xitari" -> "deepmind/alewrap"
"deepmind/xitari" -> "deepmind/plplot-ffi"
"BarisYazici/deep-rl-grasping" -> "mahyaret/kuka_rl"
"BarisYazici/deep-rl-grasping" -> "AndrejOrsula/drl_grasping"
"BarisYazici/deep-rl-grasping" -> "borninfreedom/kuka-reach-drl"
"BarisYazici/deep-rl-grasping" -> "google-research/ravens"
"BarisYazici/deep-rl-grasping" -> "qgallouedec/panda-gym"
"BarisYazici/deep-rl-grasping" -> "hsp-iit/pybullet-robot-envs"
"BarisYazici/deep-rl-grasping" -> "IanYangChina/pybullet_multigoal_gym"
"BarisYazici/deep-rl-grasping" -> "eleramp/pybullet-object-models"
"ghliu/pytorch-ddpg" -> "ikostrikov/pytorch-ddpg-naf"
"ghliu/pytorch-ddpg" -> "ikostrikov/pytorch-a3c"
"ghliu/pytorch-ddpg" -> "ikostrikov/pytorch-trpo"
"ghliu/pytorch-ddpg" -> "pranz24/pytorch-soft-actor-critic"
"ghliu/pytorch-ddpg" -> "vy007vikas/PyTorch-ActorCriticRL"
"ghliu/pytorch-ddpg" -> "xuehy/pytorch-maddpg"
"ghliu/pytorch-ddpg" -> "floodsung/DDPG"
"ghliu/pytorch-ddpg" -> "jingweiz/pytorch-rl"
"ghliu/pytorch-ddpg" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"ghliu/pytorch-ddpg" -> "Khrylx/PyTorch-RL"
"ghliu/pytorch-ddpg" -> "dgriff777/rl_a3c_pytorch"
"ghliu/pytorch-ddpg" -> "ChenglongChen/pytorch-madrl"
"ghliu/pytorch-ddpg" -> "sfujim/TD3"
"ghliu/pytorch-ddpg" -> "Kaixhin/ACER"
"ghliu/pytorch-ddpg" -> "jimkon/Deep-Reinforcement-Learning-in-Large-Discrete-Action-Spaces"
"pranz24/pytorch-soft-actor-critic" -> "sfujim/TD3"
"pranz24/pytorch-soft-actor-critic" -> "haarnoja/sac"
"pranz24/pytorch-soft-actor-critic" -> "rail-berkeley/softlearning"
"pranz24/pytorch-soft-actor-critic" -> "denisyarats/pytorch_sac"
"pranz24/pytorch-soft-actor-critic" -> "vitchyr/rlkit"
"pranz24/pytorch-soft-actor-critic" -> "nikhilbarhate99/PPO-PyTorch"
"pranz24/pytorch-soft-actor-critic" -> "quantumiracle/Popular-RL-Algorithms"
"pranz24/pytorch-soft-actor-critic" -> "Khrylx/PyTorch-RL"
"pranz24/pytorch-soft-actor-critic" -> "BY571/Soft-Actor-Critic-and-Extensions"
"pranz24/pytorch-soft-actor-critic" -> "rail-berkeley/rlkit"
"pranz24/pytorch-soft-actor-critic" -> "sfujim/BCQ"
"pranz24/pytorch-soft-actor-critic" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"pranz24/pytorch-soft-actor-critic" -> "ikostrikov/pytorch-trpo"
"pranz24/pytorch-soft-actor-critic" -> "rail-berkeley/d4rl"
"pranz24/pytorch-soft-actor-critic" -> "katerakelly/oyster"
"rlchina/RLCN" -> "jidiai/ai_lib"
"rlchina/RLCN" -> "sjtu-marl/malib"
"shariqiqbal2810/MAAC" -> "shariqiqbal2810/maddpg-pytorch"
"shariqiqbal2810/MAAC" -> "starry-sky6688/StarCraft"
"shariqiqbal2810/MAAC" -> "oxwhirl/pymarl"
"shariqiqbal2810/MAAC" -> "openai/maddpg"
"shariqiqbal2810/MAAC" -> "openai/multiagent-particle-envs"
"shariqiqbal2810/MAAC" -> "marlbenchmark/on-policy"
"shariqiqbal2810/MAAC" -> "xuehy/pytorch-maddpg"
"shariqiqbal2810/MAAC" -> "PKU-AI-Edge/DGN"
"shariqiqbal2810/MAAC" -> "mlii/mfrl"
"shariqiqbal2810/MAAC" -> "oxwhirl/smac"
"shariqiqbal2810/MAAC" -> "minqi/learning-to-communicate-pytorch"
"shariqiqbal2810/MAAC" -> "sisl/MADRL"
"shariqiqbal2810/MAAC" -> "starry-sky6688/MADDPG"
"shariqiqbal2810/MAAC" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"shariqiqbal2810/MAAC" -> "IC3Net/IC3Net"
"thesouther/MARL" -> "manaski/MARL"
"thesouther/MARL" -> "marlbenchmark/off-policy"
"thesouther/MARL" -> "starry-sky6688/StarCraft"
"thesouther/MARL" -> "verystrongjoe/qmix"
"befelix/safe_learning" -> "befelix/SafeOpt"
"befelix/safe_learning" -> "befelix/lyapunov-learning"
"befelix/safe_learning" -> "YaChienChang/Neural-Lyapunov-Control"
"befelix/safe_learning" -> "rcheng805/RL-CBF"
"befelix/safe_learning" -> "hari-sikchi/safeRL"
"befelix/safe_learning" -> "befelix/safe-exploration"
"befelix/safe_learning" -> "AliBaheri/Safe-Reinforcement-Learning"
"befelix/safe_learning" -> "befelix/Safe-RL-Benchmark"
"befelix/safe_learning" -> "utiasDSL/safe-control-gym"
"befelix/safe_learning" -> "openai/safety-gym"
"befelix/safe_learning" -> "befelix/SafeMDP"
"befelix/safe_learning" -> "jachiam/cpo"
"befelix/safe_learning" -> "openai/safety-starter-agents"
"Unity-Technologies/obstacle-tower-env" -> "Unity-Technologies/obstacle-tower-challenge"
"Unity-Technologies/obstacle-tower-env" -> "unixpickle/obs-tower2"
"Unity-Technologies/obstacle-tower-env" -> "Unity-Technologies/obstacle-tower-source"
"Unity-Technologies/obstacle-tower-env" -> "openai/random-network-distillation"
"Unity-Technologies/obstacle-tower-env" -> "maximecb/gym-miniworld"
"Unity-Technologies/obstacle-tower-env" -> "beyretb/AnimalAI-Olympics"
"Unity-Technologies/obstacle-tower-env" -> "maximecb/gym-minigrid"
"Unity-Technologies/obstacle-tower-env" -> "junhyukoh/self-imitation-learning"
"Unity-Technologies/obstacle-tower-env" -> "uber-research/ape-x"
"Unity-Technologies/obstacle-tower-env" -> "openai/procgen"
"Unity-Technologies/obstacle-tower-env" -> "google-research/planet"
"Unity-Technologies/obstacle-tower-env" -> "openai/coinrun"
"Unity-Technologies/obstacle-tower-env" -> "deepmind/scalable_agent"
"Unity-Technologies/obstacle-tower-env" -> "Unity-Technologies/marathon-envs" ["e"=1]
"Unity-Technologies/obstacle-tower-env" -> "unixpickle/anyrl-py" ["e"=1]
"hill-a/stable-baselines" -> "araffin/rl-baselines-zoo"
"hill-a/stable-baselines" -> "DLR-RM/stable-baselines3"
"hill-a/stable-baselines" -> "openai/baselines"
"hill-a/stable-baselines" -> "openai/spinningup"
"hill-a/stable-baselines" -> "astooke/rlpyt"
"hill-a/stable-baselines" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"hill-a/stable-baselines" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"hill-a/stable-baselines" -> "rlworkgroup/garage"
"hill-a/stable-baselines" -> "tensorflow/agents"
"hill-a/stable-baselines" -> "vitchyr/rlkit"
"hill-a/stable-baselines" -> "rll/rllab"
"hill-a/stable-baselines" -> "ShangtongZhang/DeepRL"
"hill-a/stable-baselines" -> "maximecb/gym-minigrid"
"hill-a/stable-baselines" -> "thu-ml/tianshou"
"hill-a/stable-baselines" -> "google/dopamine"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "nikhilbarhate99/PPO-PyTorch"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "ShangtongZhang/DeepRL"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "ikostrikov/pytorch-a3c"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "astooke/rlpyt"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "sfujim/TD3"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "hill-a/stable-baselines"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "Khrylx/PyTorch-RL"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "higgsfield/RL-Adventure-2"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "DLR-RM/stable-baselines3"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "vitchyr/rlkit"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "higgsfield/RL-Adventure"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "thu-ml/tianshou"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "oxwhirl/pymarl"
"quantumiracle/Popular-RL-Algorithms" -> "pranz24/pytorch-soft-actor-critic"
"quantumiracle/Popular-RL-Algorithms" -> "rail-berkeley/rlkit"
"quantumiracle/Popular-RL-Algorithms" -> "clvrai/awesome-rl-envs"
"quantumiracle/Popular-RL-Algorithms" -> "starry-sky6688/MADDPG"
"quantumiracle/Popular-RL-Algorithms" -> "RITCHIEHuang/DeepRL_Algorithms"
"quantumiracle/Popular-RL-Algorithms" -> "dongminlee94/deep_rl"
"quantumiracle/Popular-RL-Algorithms" -> "sfujim/TD3"
"quantumiracle/Popular-RL-Algorithms" -> "TianhongDai/reinforcement-learning-algorithms"
"quantumiracle/Popular-RL-Algorithms" -> "kengz/awesome-deep-rl"
"quantumiracle/Popular-RL-Algorithms" -> "BY571/Soft-Actor-Critic-and-Extensions"
"quantumiracle/Popular-RL-Algorithms" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"quantumiracle/Popular-RL-Algorithms" -> "vwxyzjn/cleanrl"
"quantumiracle/Popular-RL-Algorithms" -> "MrSyee/pg-is-all-you-need"
"quantumiracle/Popular-RL-Algorithms" -> "StepNeverStop/RLs"
"quantumiracle/Popular-RL-Algorithms" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"oxwhirl/pymarl" -> "oxwhirl/smac"
"oxwhirl/pymarl" -> "starry-sky6688/StarCraft"
"oxwhirl/pymarl" -> "openai/multiagent-particle-envs"
"oxwhirl/pymarl" -> "marlbenchmark/on-policy"
"oxwhirl/pymarl" -> "LantaoYu/MARL-Papers"
"oxwhirl/pymarl" -> "openai/maddpg"
"oxwhirl/pymarl" -> "hijkzzz/pymarl2"
"oxwhirl/pymarl" -> "shariqiqbal2810/MAAC"
"oxwhirl/pymarl" -> "uoe-agents/epymarl"
"oxwhirl/pymarl" -> "geek-ai/MAgent"
"oxwhirl/pymarl" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"oxwhirl/pymarl" -> "Farama-Foundation/PettingZoo"
"oxwhirl/pymarl" -> "PettingZoo-Team/PettingZoo" ["e"=1]
"oxwhirl/pymarl" -> "shariqiqbal2810/maddpg-pytorch"
"oxwhirl/pymarl" -> "sisl/MADRL"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "Shmuma/ptan"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "DeepReinforcementLearning/DeepReinforcementLearningInAction"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "MrSyee/pg-is-all-you-need"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "mimoralea/gdrl"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "kengz/SLM-Lab"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "Curt-Park/rainbow-is-all-you-need"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "vwxyzjn/cleanrl"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "zhangchuheng123/Reinforcement-Implementation"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "pfnet/pfrl"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "marlbenchmark/on-policy"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "dongminlee94/deep_rl"
"lweitkamp/option-critic-pytorch" -> "jeanharb/option_critic"
"lweitkamp/option-critic-pytorch" -> "alversafa/option-critic-arch"
"lweitkamp/option-critic-pytorch" -> "mklissa/PPOC"
"lweitkamp/option-critic-pytorch" -> "veronicachelu/temporal_abstraction"
"ugo-nama-kun/gym_torcs" -> "yanpanlau/DDPG-Keras-Torcs"
"ugo-nama-kun/gym_torcs" -> "YurongYou/rlTORCS"
"ugo-nama-kun/gym_torcs" -> "abhisheknaik96/MultiAgentTORCS"
"ugo-nama-kun/gym_torcs" -> "jastfkjg/DDPG_Torcs_PyTorch"
"ugo-nama-kun/gym_torcs" -> "lanquarden/pyScrcClient"
"ugo-nama-kun/gym_torcs" -> "openai/imitation"
"ugo-nama-kun/gym_torcs" -> "zsdonghao/Imitation-Learning-Dagger-Torcs"
"ugo-nama-kun/gym_torcs" -> "giuse/vtorcs"
"ugo-nama-kun/gym_torcs" -> "YunzhuLi/InfoGAIL"
"ugo-nama-kun/gym_torcs" -> "dosssman/GymTorcs"
"ugo-nama-kun/gym_torcs" -> "shaneshixiang/rllabplusplus"
"ugo-nama-kun/gym_torcs" -> "joschu/modular_rl"
"ugo-nama-kun/gym_torcs" -> "kennethyu2017/ddpg"
"ugo-nama-kun/gym_torcs" -> "miyosuda/async_deep_reinforce"
"ugo-nama-kun/gym_torcs" -> "eleurent/rl-agents" ["e"=1]
"miyosuda/unreal" -> "miyosuda/async_deep_reinforce"
"miyosuda/unreal" -> "NVlabs/GA3C"
"miyosuda/unreal" -> "muupan/async-rl"
"miyosuda/unreal" -> "awjuliani/Meta-RL"
"miyosuda/unreal" -> "steveKapturowski/tensorflow-rl"
"miyosuda/unreal" -> "TheAbhiKumar/tensorflow-value-iteration-networks"
"miyosuda/unreal" -> "openai/universe-starter-agent"
"miyosuda/unreal" -> "dmakian/feudal_networks"
"miyosuda/unreal" -> "pathak22/noreward-rl"
"miyosuda/unreal" -> "tgangwani/GA3C-DeepNavigation"
"miyosuda/unreal" -> "IntelVCL/DirectFuturePrediction"
"miyosuda/unreal" -> "Ardavans/DSR"
"miyosuda/unreal" -> "openai/rllab"
"miyosuda/unreal" -> "coreylynch/async-rl"
"miyosuda/unreal" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"zhongwen/predictron" -> "TheAbhiKumar/tensorflow-value-iteration-networks"
"zhongwen/predictron" -> "junhyukoh/value-prediction-network"
"zhongwen/predictron" -> "oxwhirl/treeqn" ["e"=1]
"zhongwen/predictron" -> "avivt/VIN"
"cuhkrlcourse/RLexample" -> "zhoubolei/introRL"
"cuhkrlcourse/RLexample" -> "cuhkrlcourse/DeepRL-Tutorials"
"cuhkrlcourse/RLexample" -> "cuhkrlcourse/ierg6130-assignment"
"cuhkrlcourse/RLexample" -> "sfujim/TD3"
"cuhkrlcourse/RLexample" -> "datawhalechina/leedeeprl-notes"
"cuhkrlcourse/RLexample" -> "NeuronDance/DeepRL"
"cuhkrlcourse/RLexample" -> "datawhalechina/easy-rl"
"cuhkrlcourse/RLexample" -> "ZhiqingXiao/rl-book"
"cuhkrlcourse/RLexample" -> "zhangchuheng123/Reinforcement-Implementation"
"cuhkrlcourse/RLexample" -> "wangshusen/DRL"
"cuhkrlcourse/RLexample" -> "wwxFromTju/awesome-reinforcement-learning-zh"
"cuhkrlcourse/RLexample" -> "thu-ml/tianshou"
"cuhkrlcourse/RLexample" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"cuhkrlcourse/RLexample" -> "qqiang00/Reinforce"
"cuhkrlcourse/RLexample" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"datawhalechina/leedeeprl-notes" -> "cuhkrlcourse/RLexample"
"datawhalechina/leedeeprl-notes" -> "zhoubolei/introRL"
"datawhalechina/leedeeprl-notes" -> "NeuronDance/DeepRL"
"datawhalechina/leedeeprl-notes" -> "wwxFromTju/awesome-reinforcement-learning-zh"
"datawhalechina/leedeeprl-notes" -> "ZhiqingXiao/rl-book"
"datawhalechina/leedeeprl-notes" -> "datawhalechina/competition-baseline" ["e"=1]
"datawhalechina/leedeeprl-notes" -> "wangshusen/DRL"
"datawhalechina/leedeeprl-notes" -> "PaddlePaddle/PARL"
"datawhalechina/leedeeprl-notes" -> "Yonv1943/ElegantRL"
"datawhalechina/leedeeprl-notes" -> "thu-ml/tianshou"
"datawhalechina/leedeeprl-notes" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"datawhalechina/leedeeprl-notes" -> "datawhalechina/leeml-notes" ["e"=1]
"datawhalechina/leedeeprl-notes" -> "qqiang00/Reinforce"
"datawhalechina/leedeeprl-notes" -> "Sakura-gh/ML-notes" ["e"=1]
"datawhalechina/leedeeprl-notes" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"yenchenlin1994/DeepLearningFlappyBird" -> "asrivat1/DeepLearningVideoGames"
"yenchenlin1994/DeepLearningFlappyBird" -> "nivwusquorum/tensorflow-deepq"
"yenchenlin1994/DeepLearningFlappyBird" -> "kuz/DeepMind-Atari-Deep-Q-Learner"
"yenchenlin1994/DeepLearningFlappyBird" -> "spragunr/deep_q_rl"
"yenchenlin1994/DeepLearningFlappyBird" -> "Rochester-NRT/AlphaGo" ["e"=1]
"yenchenlin1994/DeepLearningFlappyBird" -> "pavelgonchar/colornet" ["e"=1]
"yenchenlin1994/DeepLearningFlappyBird" -> "coreylynch/async-rl"
"yenchenlin1994/DeepLearningFlappyBird" -> "awentzonline/image-analogies" ["e"=1]
"yenchenlin1994/DeepLearningFlappyBird" -> "torch/tutorials" ["e"=1]
"yenchenlin1994/DeepLearningFlappyBird" -> "tensorflow/skflow" ["e"=1]
"yenchenlin1994/DeepLearningFlappyBird" -> "alexjc/neural-doodle" ["e"=1]
"yenchenlin1994/DeepLearningFlappyBird" -> "rllab/rllab"
"yenchenlin1994/DeepLearningFlappyBird" -> "yenchenlin1994/awesome-watchos"
"yenchenlin1994/DeepLearningFlappyBird" -> "zer0n/deepframeworks" ["e"=1]
"yenchenlin1994/DeepLearningFlappyBird" -> "soumith/convnet-benchmarks" ["e"=1]
"cbfinn/gps" -> "rllab/rllab"
"cbfinn/gps" -> "joschu/modular_rl"
"cbfinn/gps" -> "justinjfu/inverse_rl"
"cbfinn/gps" -> "nrontsis/PILCO"
"cbfinn/gps" -> "WilsonWangTHU/mbbl"
"cbfinn/gps" -> "kchua/handful-of-trials"
"cbfinn/gps" -> "carpedm20/NAF-tensorflow"
"cbfinn/gps" -> "erlerobot/gym-gazebo"
"cbfinn/gps" -> "shaneshixiang/rllabplusplus"
"cbfinn/gps" -> "tianheyu927/mil"
"cbfinn/gps" -> "cbfinn/maml_rl" ["e"=1]
"cbfinn/gps" -> "andyzeng/visual-pushing-grasping" ["e"=1]
"cbfinn/gps" -> "avivt/VIN"
"cbfinn/gps" -> "araffin/robotics-rl-srl"
"cbfinn/gps" -> "rail-berkeley/softlearning"
"kentsommer/pytorch-value-iteration-networks" -> "avivt/VIN"
"kentsommer/pytorch-value-iteration-networks" -> "zuoxingdong/VIN_PyTorch_Visdom"
"kentsommer/pytorch-value-iteration-networks" -> "TheAbhiKumar/tensorflow-value-iteration-networks"
"kentsommer/pytorch-value-iteration-networks" -> "junhyukoh/value-prediction-network"
"kentsommer/pytorch-value-iteration-networks" -> "zuoxingdong/VIN_TensorFlow"
"tychovdo/PacmanDQN" -> "mrkulk/deepQN_tensorflow"
"nuno-faria/tetris-ai" -> "uvipen/Tetris-deep-Q-learning-pytorch"
"nuno-faria/tetris-ai" -> "michiel-cox/Tetris-DQN"
"stepjam/RLBench" -> "stepjam/PyRep"
"stepjam/RLBench" -> "rlworkgroup/metaworld"
"stepjam/RLBench" -> "clvrai/furniture"
"stepjam/RLBench" -> "google-research/ravens"
"stepjam/RLBench" -> "ARISE-Initiative/robosuite"
"stepjam/RLBench" -> "andyzeng/visual-pushing-grasping" ["e"=1]
"stepjam/RLBench" -> "stepjam/ARM"
"stepjam/RLBench" -> "StanfordVL/robosuite"
"stepjam/RLBench" -> "ARISE-Initiative/robomimic"
"stepjam/RLBench" -> "benelot/pybullet-gym"
"stepjam/RLBench" -> "vitchyr/multiworld"
"stepjam/RLBench" -> "clvrai/awesome-rl-envs"
"stepjam/RLBench" -> "NVIDIA-Omniverse/IsaacGymEnvs"
"stepjam/RLBench" -> "peract/peract"
"stepjam/RLBench" -> "cliport/cliport"
"facebookresearch/drqv2" -> "denisyarats/drq"
"facebookresearch/drqv2" -> "denisyarats/dmc2gym"
"facebookresearch/drqv2" -> "facebookresearch/deep_bisim4control"
"facebookresearch/drqv2" -> "rll-research/url_benchmark"
"facebookresearch/drqv2" -> "MishaLaskin/curl"
"facebookresearch/drqv2" -> "MishaLaskin/rad"
"facebookresearch/drqv2" -> "denisyarats/pytorch_sac_ae"
"facebookresearch/drqv2" -> "nicklashansen/dmcontrol-generalization-benchmark"
"facebookresearch/drqv2" -> "ikostrikov/jaxrl"
"facebookresearch/drqv2" -> "conglu1997/v-d4rl"
"facebookresearch/drqv2" -> "RajGhugare19/dreamerv2"
"facebookresearch/drqv2" -> "jannerm/trajectory-transformer"
"facebookresearch/drqv2" -> "openai/phasic-policy-gradient"
"facebookresearch/drqv2" -> "rail-berkeley/d4rl_evaluations"
"facebookresearch/drqv2" -> "nicklashansen/tdmpc"
"marload/DeepRL-TensorFlow2" -> "anita-hu/TF2-RL"
"marload/DeepRL-TensorFlow2" -> "keiohta/tf2rl"
"marload/DeepRL-TensorFlow2" -> "pythonlessons/Reinforcement_Learning"
"marload/DeepRL-TensorFlow2" -> "PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook"
"marload/DeepRL-TensorFlow2" -> "StepNeverStop/RLs"
"marload/DeepRL-TensorFlow2" -> "RITCHIEHuang/DeepRL_Algorithms"
"marload/DeepRL-TensorFlow2" -> "quantumiracle/Popular-RL-Algorithms"
"marload/DeepRL-TensorFlow2" -> "Kaixhin/imitation-learning"
"marload/DeepRL-TensorFlow2" -> "dongminlee94/deep_rl"
"marload/DeepRL-TensorFlow2" -> "iffiX/machin"
"marload/DeepRL-TensorFlow2" -> "Rafael1s/Deep-Reinforcement-Learning-Algorithms"
"marload/DeepRL-TensorFlow2" -> "tensorforce/tensorforce"
"marload/DeepRL-TensorFlow2" -> "abhisheksuran/Reinforcement_Learning"
"marload/DeepRL-TensorFlow2" -> "Huixxi/TensorFlow2.0-for-Deep-Reinforcement-Learning"
"marload/DeepRL-TensorFlow2" -> "inoryy/tensorflow2-deep-reinforcement-learning"
"Kchu/DeepRL_PyTorch" -> "ku2482/fqf-iqn-qrdqn.pytorch"
"Kchu/DeepRL_PyTorch" -> "sungyubkim/Deep_RL_with_pytorch"
"Kchu/DeepRL_PyTorch" -> "Kchu/LifelongRL"
"Kchu/DeepRL_PyTorch" -> "BY571/FQF-and-Extensions"
"philtabor/Actor-Critic-Methods-Paper-To-Code" -> "philtabor/Deep-Q-Learning-Paper-To-Code"
"philtabor/Actor-Critic-Methods-Paper-To-Code" -> "philtabor/Reinforcement-Learning-In-Motion"
"rosewang2008/gym-cooking" -> "Stanford-ILIAD/PantheonRL"
"rosewang2008/gym-cooking" -> "xavierpuigf/watch_and_help" ["e"=1]
"ChangyWen/wolpertinger_ddpg" -> "jimkon/Deep-Reinforcement-Learning-in-Large-Discrete-Action-Spaces"
"ChangyWen/wolpertinger_ddpg" -> "nikhil3456/Deep-Reinforcement-Learning-in-Large-Discrete-Action-Spaces"
"ChangyWen/wolpertinger_ddpg" -> "chenhaokun/TPGR" ["e"=1]
"ZhiqingXiao/rl-book" -> "ZhiqingXiao/pytorch-book"
"ZhiqingXiao/rl-book" -> "NeuronDance/DeepRL"
"ZhiqingXiao/rl-book" -> "cuhkrlcourse/RLexample"
"ZhiqingXiao/rl-book" -> "StepNeverStop/RLs"
"ZhiqingXiao/rl-book" -> "zhoubolei/introRL"
"ZhiqingXiao/rl-book" -> "wwxFromTju/awesome-reinforcement-learning-zh"
"ZhiqingXiao/rl-book" -> "qqiang00/Reinforce"
"ZhiqingXiao/rl-book" -> "Yonv1943/ElegantRL"
"ZhiqingXiao/rl-book" -> "wangshusen/DRL"
"ZhiqingXiao/rl-book" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"ZhiqingXiao/rl-book" -> "datawhalechina/leedeeprl-notes"
"ZhiqingXiao/rl-book" -> "sfujim/TD3"
"ZhiqingXiao/rl-book" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"ZhiqingXiao/rl-book" -> "keiohta/tf2rl"
"ZhiqingXiao/rl-book" -> "kaixindelele/DRLib"
"jimkon/Deep-Reinforcement-Learning-in-Large-Discrete-Action-Spaces" -> "ChangyWen/wolpertinger_ddpg"
"jimkon/Deep-Reinforcement-Learning-in-Large-Discrete-Action-Spaces" -> "nikhil3456/Deep-Reinforcement-Learning-in-Large-Discrete-Action-Spaces"
"jimkon/Deep-Reinforcement-Learning-in-Large-Discrete-Action-Spaces" -> "stevenpjg/ddpg-aigym"
"jimkon/Deep-Reinforcement-Learning-in-Large-Discrete-Action-Spaces" -> "xinshi-chen/GenerativeAdversarialUserModel" ["e"=1]
"nikhil3456/Deep-Reinforcement-Learning-in-Large-Discrete-Action-Spaces" -> "ChangyWen/wolpertinger_ddpg"
"ARISE-Initiative/robosuite" -> "stepjam/RLBench"
"ARISE-Initiative/robosuite" -> "google-research/ravens"
"ARISE-Initiative/robosuite" -> "rlworkgroup/metaworld"
"ARISE-Initiative/robosuite" -> "deepmind/mujoco_menagerie"
"ARISE-Initiative/robosuite" -> "ARISE-Initiative/robomimic"
"ARISE-Initiative/robosuite" -> "clvrai/furniture"
"ARISE-Initiative/robosuite" -> "NVIDIA-Omniverse/IsaacGymEnvs"
"ARISE-Initiative/robosuite" -> "PaulDanielML/MuJoCo_RL_UR5"
"ARISE-Initiative/robosuite" -> "StanfordVL/iGibson" ["e"=1]
"ARISE-Initiative/robosuite" -> "wangcongrobot/awesome-isaac-gym"
"ARISE-Initiative/robosuite" -> "deepmind/dm_control"
"ARISE-Initiative/robosuite" -> "clvrai/awesome-rl-envs"
"ARISE-Initiative/robosuite" -> "PKU-MARL/DexterousHands"
"ARISE-Initiative/robosuite" -> "rail-berkeley/d4rl"
"ARISE-Initiative/robosuite" -> "haosulab/ManiSkill2"
"BazkieBumpercar/GameplayFootball" -> "BazkieBumpercar/Blunted2"
"BazkieBumpercar/GameplayFootball" -> "vi3itor/GameplayFootball"
"HumanCompatibleAI/adversarial-policies" -> "openai/train-procgen"
"HumanCompatibleAI/adversarial-policies" -> "chenhongge/StateAdvDRL"
"HumanCompatibleAI/adversarial-policies" -> "huanzhang12/ATLA_robust_RL"
"HumanCompatibleAI/adversarial-policies" -> "ermongroup/MA-AIRL"
"deepmind/lab2d" -> "deepmind/meltingpot"
"deepmind/lab2d" -> "alex-petrenko/sample-factory"
"deepmind/lab2d" -> "kandouss/marlgrid"
"deepmind/lab2d" -> "eugenevinitsky/sequential_social_dilemma_games"
"deepmind/lab2d" -> "deepmind/pycolab"
"deepmind/lab2d" -> "PettingZoo-Team/PettingZoo" ["e"=1]
"deepmind/lab2d" -> "rosewang2008/gym-cooking"
"deepmind/lab2d" -> "deepmind/dm_env" ["e"=1]
"deepmind/lab2d" -> "danijar/crafter"
"deepmind/lab2d" -> "koulanurag/ma-gym"
"fabiopardo/tonic" -> "WilsonWangTHU/mbbl"
"fabiopardo/tonic" -> "rail-berkeley/d4rl"
"fabiopardo/tonic" -> "facebookresearch/mbrl-lib"
"fabiopardo/tonic" -> "JannerM/mbpo"
"fabiopardo/tonic" -> "PettingZoo-Team/PettingZoo" ["e"=1]
"fabiopardo/tonic" -> "deepmind/bsuite"
"fabiopardo/tonic" -> "openai/safety-gym"
"fabiopardo/tonic" -> "astooke/rlpyt"
"fabiopardo/tonic" -> "stepjam/RLBench"
"fabiopardo/tonic" -> "sail-sg/envpool"
"fabiopardo/tonic" -> "araffin/srl-zoo"
"openai/mujoco-worldgen" -> "openai/multi-agent-emergence-environments"
"openai/mujoco-worldgen" -> "openai/mujoco-py"
"openai/mujoco-worldgen" -> "openai/train-procgen"
"openai/mujoco-worldgen" -> "koulanurag/ma-gym"
"openai/mujoco-worldgen" -> "clvrai/furniture"
"openai/mujoco-worldgen" -> "mingfeisun/DeepMimic_mujoco" ["e"=1]
"openai/mujoco-worldgen" -> "openai/robogym"
"openai/mujoco-worldgen" -> "deepmind/lab2d"
"openai/safety-gym" -> "openai/safety-starter-agents"
"openai/safety-gym" -> "chauncygu/Safe-Reinforcement-Learning-Baselines"
"openai/safety-gym" -> "jachiam/cpo"
"openai/safety-gym" -> "utiasDSL/safe-control-gym"
"openai/safety-gym" -> "deepmind/ai-safety-gridworlds"
"openai/safety-gym" -> "SvenGronauer/Bullet-Safety-Gym"
"openai/safety-gym" -> "befelix/safe_learning"
"openai/safety-gym" -> "rail-berkeley/d4rl"
"openai/safety-gym" -> "WilsonWangTHU/mbbl"
"openai/safety-gym" -> "PKU-MARL/Safe-Policy-Optimization"
"openai/safety-gym" -> "openai/procgen"
"openai/safety-gym" -> "rlworkgroup/metaworld"
"openai/safety-gym" -> "sfujim/TD3_BC"
"openai/safety-gym" -> "maximecb/gym-minigrid"
"openai/safety-gym" -> "facebookresearch/mbrl-lib"
"pfnet/pfrl" -> "astooke/rlpyt"
"pfnet/pfrl" -> "rail-berkeley/d4rl"
"pfnet/pfrl" -> "chainer/chainerrl"
"pfnet/pfrl" -> "vwxyzjn/cleanrl"
"pfnet/pfrl" -> "sail-sg/envpool"
"pfnet/pfrl" -> "takuseno/d3rlpy"
"pfnet/pfrl" -> "deepmind/acme"
"pfnet/pfrl" -> "facebookresearch/mbrl-lib"
"pfnet/pfrl" -> "cpnota/autonomous-learning-library"
"pfnet/pfrl" -> "facebookresearch/torchbeast"
"pfnet/pfrl" -> "maximecb/gym-minigrid"
"pfnet/pfrl" -> "google-research/batch_rl"
"pfnet/pfrl" -> "PettingZoo-Team/PettingZoo" ["e"=1]
"pfnet/pfrl" -> "hanjuku-kaso/awesome-offline-rl"
"pfnet/pfrl" -> "danijar/dreamerv2"
"rail-berkeley/d4rl" -> "rail-berkeley/d4rl_evaluations"
"rail-berkeley/d4rl" -> "takuseno/d3rlpy"
"rail-berkeley/d4rl" -> "aviralkumar2907/CQL"
"rail-berkeley/d4rl" -> "sfujim/BCQ"
"rail-berkeley/d4rl" -> "hanjuku-kaso/awesome-offline-rl"
"rail-berkeley/d4rl" -> "sfujim/TD3_BC"
"rail-berkeley/d4rl" -> "google-research/batch_rl"
"rail-berkeley/d4rl" -> "rlworkgroup/metaworld"
"rail-berkeley/d4rl" -> "kzl/decision-transformer"
"rail-berkeley/d4rl" -> "MishaLaskin/curl"
"rail-berkeley/d4rl" -> "rail-berkeley/softlearning"
"rail-berkeley/d4rl" -> "vitchyr/rlkit"
"rail-berkeley/d4rl" -> "JannerM/mbpo"
"rail-berkeley/d4rl" -> "rll-research/url_benchmark"
"rail-berkeley/d4rl" -> "clvrai/awesome-rl-envs"
"Kaixhin/PlaNet" -> "juliusfrost/dreamer-pytorch"
"Kaixhin/PlaNet" -> "yusukeurakami/dreamer-pytorch"
"Kaixhin/PlaNet" -> "google-research/planet"
"Kaixhin/PlaNet" -> "ctallec/world-models"
"Kaixhin/PlaNet" -> "danijar/dreamer"
"Kaixhin/PlaNet" -> "denisyarats/dmc2gym"
"Kaixhin/PlaNet" -> "alexlee-gk/slac"
"Kaixhin/PlaNet" -> "jsikyoon/dreamer-torch"
"Kaixhin/PlaNet" -> "kchua/handful-of-trials"
"Kaixhin/PlaNet" -> "vitchyr/multiworld"
"Kaixhin/PlaNet" -> "google-research/dreamer"
"Kaixhin/PlaNet" -> "ramanans1/plan2explore"
"Kaixhin/PlaNet" -> "quanvuong/handful-of-trials-pytorch"
"Kaixhin/PlaNet" -> "MishaLaskin/curl"
"Kaixhin/PlaNet" -> "JannerM/mbpo"
"google-research/planet" -> "Kaixhin/PlaNet"
"google-research/planet" -> "danijar/dreamer"
"google-research/planet" -> "hardmaru/WorldModelsExperiments"
"google-research/planet" -> "google-research/dreamer"
"google-research/planet" -> "rail-berkeley/softlearning"
"google-research/planet" -> "vitchyr/rlkit"
"google-research/planet" -> "WilsonWangTHU/mbbl"
"google-research/planet" -> "kchua/handful-of-trials"
"google-research/planet" -> "astooke/rlpyt"
"google-research/planet" -> "danijar/dreamerv2"
"google-research/planet" -> "deepmind/dm_control"
"google-research/planet" -> "JannerM/mbpo"
"google-research/planet" -> "deepmind/bsuite"
"google-research/planet" -> "ctallec/world-models"
"google-research/planet" -> "openai/random-network-distillation"
"haarnoja/sac" -> "rail-berkeley/softlearning"
"haarnoja/sac" -> "vitchyr/rlkit"
"haarnoja/sac" -> "sfujim/TD3"
"haarnoja/sac" -> "pranz24/pytorch-soft-actor-critic"
"haarnoja/sac" -> "haarnoja/softqlearning"
"haarnoja/sac" -> "rll/rllab"
"haarnoja/sac" -> "openai/random-network-distillation"
"haarnoja/sac" -> "denisyarats/pytorch_sac"
"haarnoja/sac" -> "joschu/modular_rl"
"haarnoja/sac" -> "katerakelly/oyster"
"haarnoja/sac" -> "cbfinn/maml_rl" ["e"=1]
"haarnoja/sac" -> "openai/multiagent-particle-envs"
"haarnoja/sac" -> "openai/maddpg"
"haarnoja/sac" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"haarnoja/sac" -> "kchua/handful-of-trials"
"polixir/NeoRL" -> "polixir/OfflineRL"
"nicklashansen/policy-adaptation-during-deployment" -> "nicklashansen/dmcontrol-generalization-benchmark"
"m5823779/MotionPlannerUsingDDPG" -> "m5823779/DDPG"
"m5823779/MotionPlannerUsingDDPG" -> "m5823779/PoseEstimation"
"hanruihua/rl_rvo_nav" -> "hanruihua/intelligent-robot-simulator"
"cyanrain7/TRPO-in-MARL" -> "schroederdewitt/multiagent_mujoco"
"cyanrain7/TRPO-in-MARL" -> "uoe-agents/epymarl"
"cyanrain7/TRPO-in-MARL" -> "chauncygu/Multi-Agent-Constrained-Policy-Optimisation"
"cyanrain7/TRPO-in-MARL" -> "oxwhirl/facmac"
"cyanrain7/TRPO-in-MARL" -> "PKU-MARL/Multi-Agent-Transformer"
"cyanrain7/TRPO-in-MARL" -> "marlbenchmark/off-policy"
"cyanrain7/TRPO-in-MARL" -> "marlbenchmark/on-policy"
"cyanrain7/TRPO-in-MARL" -> "oxwhirl/smacv2"
"marlbenchmark/on-policy" -> "marlbenchmark/off-policy"
"marlbenchmark/on-policy" -> "tinyzqh/light_mappo"
"marlbenchmark/on-policy" -> "oxwhirl/pymarl"
"marlbenchmark/on-policy" -> "uoe-agents/epymarl"
"marlbenchmark/on-policy" -> "starry-sky6688/StarCraft"
"marlbenchmark/on-policy" -> "oxwhirl/smac"
"marlbenchmark/on-policy" -> "openai/multiagent-particle-envs"
"marlbenchmark/on-policy" -> "hijkzzz/pymarl2"
"marlbenchmark/on-policy" -> "Replicable-MARL/MARLlib"
"marlbenchmark/on-policy" -> "schroederdewitt/multiagent_mujoco"
"marlbenchmark/on-policy" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"marlbenchmark/on-policy" -> "sjtu-marl/malib"
"marlbenchmark/on-policy" -> "shariqiqbal2810/maddpg-pytorch"
"marlbenchmark/on-policy" -> "openai/maddpg"
"marlbenchmark/on-policy" -> "cyanrain7/TRPO-in-MARL"
"schroederdewitt/multiagent_mujoco" -> "cyanrain7/TRPO-in-MARL"
"schroederdewitt/multiagent_mujoco" -> "marlbenchmark/on-policy"
"schroederdewitt/multiagent_mujoco" -> "uoe-agents/epymarl"
"schroederdewitt/multiagent_mujoco" -> "PKU-MARL/DexterousHands"
"schroederdewitt/multiagent_mujoco" -> "TonghanWang/ROMA"
"schroederdewitt/multiagent_mujoco" -> "marlbenchmark/off-policy"
"schroederdewitt/multiagent_mujoco" -> "starry-sky6688/StarCraft"
"schroederdewitt/multiagent_mujoco" -> "semitable/lb-foraging"
"schroederdewitt/multiagent_mujoco" -> "wendelinboehmer/dcg"
"schroederdewitt/multiagent_mujoco" -> "koulanurag/ma-gym"
"schroederdewitt/multiagent_mujoco" -> "PKU-MARL/Multi-Agent-Transformer"
"schroederdewitt/multiagent_mujoco" -> "shariqiqbal2810/MAAC"
"schroederdewitt/multiagent_mujoco" -> "PKU-AI-Edge/DGN"
"schroederdewitt/multiagent_mujoco" -> "TonghanWang/RODE"
"karpathy/recurrentjs" -> "karpathy/reinforcejs"
"karpathy/recurrentjs" -> "karpathy/svmjs" ["e"=1]
"karpathy/recurrentjs" -> "karpathy/tsnejs" ["e"=1]
"karpathy/recurrentjs" -> "karpathy/convnetjs" ["e"=1]
"karpathy/recurrentjs" -> "wojciechz/learning_to_execute" ["e"=1]
"karpathy/recurrentjs" -> "facebook/eyescream" ["e"=1]
"karpathy/recurrentjs" -> "kaishengtai/torch-ntm" ["e"=1]
"karpathy/recurrentjs" -> "karpathy/neuraltalk" ["e"=1]
"karpathy/recurrentjs" -> "karpathy/forestjs" ["e"=1]
"karpathy/recurrentjs" -> "jcjohnson/cnn-vis" ["e"=1]
"karpathy/recurrentjs" -> "cazala/synaptic" ["e"=1]
"karpathy/recurrentjs" -> "waylonflinn/weblas" ["e"=1]
"karpathy/recurrentjs" -> "facebook/fbcunn" ["e"=1]
"karpathy/recurrentjs" -> "Element-Research/rnn" ["e"=1]
"karpathy/recurrentjs" -> "nicholas-leonard/dp" ["e"=1]
"songrotek/DDPG" -> "stevenpjg/ddpg-aigym"
"songrotek/DDPG" -> "rmst/ddpg"
"songrotek/DDPG" -> "MOCR/DDPG"
"songrotek/DDPG" -> "shaneshixiang/rllabplusplus"
"stepjam/PyRep" -> "stepjam/RLBench"
"stepjam/PyRep" -> "robotlearn/pyrobolearn"
"stepjam/PyRep" -> "andyzeng/visual-pushing-grasping" ["e"=1]
"stepjam/PyRep" -> "dougsm/ggcnn" ["e"=1]
"stepjam/PyRep" -> "araffin/robotics-rl-srl"
"stepjam/PyRep" -> "tianheyu927/mil"
"stepjam/PyRep" -> "dougsm/mvp_grasp" ["e"=1]
"stepjam/PyRep" -> "rlworkgroup/metaworld"
"stepjam/PyRep" -> "GeorgeDu/vision-based-robotic-grasping" ["e"=1]
"stepjam/PyRep" -> "stepjam/ARM"
"stepjam/PyRep" -> "google-research/ravens"
"stepjam/PyRep" -> "erlerobot/gym-gazebo"
"stepjam/PyRep" -> "StanfordVL/robosuite"
"stepjam/PyRep" -> "clvrai/furniture"
"stepjam/PyRep" -> "robotology-playground/pybullet-robot-envs"
"gxnk/reinforcement-learning-code" -> "zhuliquan/reinforcement_learning_basic_book"
"gxnk/reinforcement-learning-code" -> "qqiang00/reinforce"
"gxnk/reinforcement-learning-code" -> "borninfreedom/DeepLearning"
"gxnk/reinforcement-learning-code" -> "mlii/mfrl"
"gxnk/reinforcement-learning-code" -> "eyounx/VirtualTaobao" ["e"=1]
"gxnk/reinforcement-learning-code" -> "StepNeverStop/RLs"
"gxnk/reinforcement-learning-code" -> "openai/maddpg"
"gxnk/reinforcement-learning-code" -> "GAOYANGAU/DRLPytorch"
"gxnk/reinforcement-learning-code" -> "cuhkrlcourse/RLexample"
"ikostrikov/jaxrl" -> "ikostrikov/implicit_q_learning"
"ikostrikov/jaxrl" -> "RobertTLange/gymnax"
"ikostrikov/jaxrl" -> "araffin/sbx"
"ikostrikov/jaxrl" -> "facebookresearch/drqv2"
"ikostrikov/jaxrl" -> "twni2016/pomdp-baselines"
"ikostrikov/jaxrl" -> "sfujim/TD3_BC"
"ikostrikov/jaxrl" -> "denisyarats/dmc2gym"
"ikostrikov/jaxrl" -> "ikostrikov/rlpd"
"ikostrikov/jaxrl" -> "ikostrikov/walk_in_the_park"
"ikostrikov/jaxrl" -> "deepmind/rlax" ["e"=1]
"ikostrikov/jaxrl" -> "google-research/rliable"
"ikostrikov/jaxrl" -> "rail-berkeley/d4rl"
"ikostrikov/jaxrl" -> "coax-dev/coax"
"ikostrikov/jaxrl" -> "young-geng/CQL"
"ikostrikov/jaxrl" -> "sail-sg/envpool"
"rail-berkeley/softlearning" -> "haarnoja/sac"
"rail-berkeley/softlearning" -> "vitchyr/rlkit"
"rail-berkeley/softlearning" -> "haarnoja/softqlearning"
"rail-berkeley/softlearning" -> "pranz24/pytorch-soft-actor-critic"
"rail-berkeley/softlearning" -> "sfujim/TD3"
"rail-berkeley/softlearning" -> "rlworkgroup/garage"
"rail-berkeley/softlearning" -> "rail-berkeley/d4rl"
"rail-berkeley/softlearning" -> "katerakelly/oyster"
"rail-berkeley/softlearning" -> "rll/rllab"
"rail-berkeley/softlearning" -> "rail-berkeley/rlkit"
"rail-berkeley/softlearning" -> "google-research/planet"
"rail-berkeley/softlearning" -> "astooke/rlpyt"
"rail-berkeley/softlearning" -> "rlworkgroup/metaworld"
"rail-berkeley/softlearning" -> "hill-a/stable-baselines"
"rail-berkeley/softlearning" -> "denisyarats/pytorch_sac"
"ZhiqingXiao/pytorch-book" -> "ZhiqingXiao/rl-book"
"Kojoley/atari-py" -> "openai/atari-py"
"takuseno/ppo" -> "uidilr/ppo_tf"
"takuseno/ppo" -> "shareeff/PPO"
"takuseno/ppo" -> "imai-laboratory/rlsaber"
"williamFalcon/DeepRLHacks" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"williamFalcon/DeepRLHacks" -> "joschu/modular_rl"
"williamFalcon/DeepRLHacks" -> "pathak22/noreward-rl"
"williamFalcon/DeepRLHacks" -> "reinforceio/tensorforce"
"williamFalcon/DeepRLHacks" -> "NervanaSystems/coach"
"williamFalcon/DeepRLHacks" -> "junhyukoh/deep-reinforcement-learning-papers"
"williamFalcon/DeepRLHacks" -> "rll/rllab"
"williamFalcon/DeepRLHacks" -> "berkeleydeeprlcourse/homework"
"williamFalcon/DeepRLHacks" -> "astooke/rlpyt"
"williamFalcon/DeepRLHacks" -> "facebookresearch/ELF"
"williamFalcon/DeepRLHacks" -> "openai/rllab"
"williamFalcon/DeepRLHacks" -> "miyosuda/async_deep_reinforce"
"williamFalcon/DeepRLHacks" -> "vitchyr/rlkit"
"williamFalcon/DeepRLHacks" -> "carpedm20/deep-rl-tensorflow"
"williamFalcon/DeepRLHacks" -> "tensorflow/agents"
"qqiang00/Reinforce" -> "jidiai/ai_lib"
"qqiang00/Reinforce" -> "ZhiqingXiao/rl-book"
"qqiang00/Reinforce" -> "cuhkrlcourse/RLexample"
"qqiang00/Reinforce" -> "YJLAugus/Reinforcement-Learning-Notes" ["e"=1]
"qqiang00/Reinforce" -> "louisnino/RLcode"
"qqiang00/Reinforce" -> "zhangchuheng123/Reinforcement-Implementation"
"qqiang00/Reinforce" -> "shariqiqbal2810/maddpg-pytorch"
"qqiang00/Reinforce" -> "starry-sky6688/StarCraft"
"qqiang00/Reinforce" -> "NeuronDance/DeepRL"
"qqiang00/Reinforce" -> "sjtu-marl/malib"
"qqiang00/Reinforce" -> "wangshusen/DRL"
"qqiang00/Reinforce" -> "datawhalechina/easy-rl"
"qqiang00/Reinforce" -> "boyu-ai/Hands-on-RL"
"qqiang00/Reinforce" -> "datawhalechina/leedeeprl-notes"
"CoderWangcai/DRL_Path_Planning" -> "naderAsadi/Optimal-Path-Planning-Deep-Reinforcement-Learning"
"CoderWangcai/DRL_Path_Planning" -> "m5823779/MotionPlannerUsingDDPG"
"CoderWangcai/DRL_Path_Planning" -> "mit-acl/cadrl_ros"
"CoderWangcai/DRL_Path_Planning" -> "yxBeginner/RL-and-Robot"
"CoderWangcai/DRL_Path_Planning" -> "sichkar-valentyn/Reinforcement_Learning_in_Python"
"TimeBreaker/Multi-Agent-Reinforcement-Learning-papers" -> "TimeBreaker/MARL-resources-collection"
"TimeBreaker/Multi-Agent-Reinforcement-Learning-papers" -> "TimeBreaker/MARL-papers-with-code"
"TonghanWang/ROMA" -> "TonghanWang/RODE"
"TonghanWang/ROMA" -> "oxwhirl/smacv2"
"TonghanWang/ROMA" -> "TonghanWang/DOP"
"TonghanWang/ROMA" -> "facebookresearch/CollaQ"
"TonghanWang/ROMA" -> "oxwhirl/wqmix"
"TonghanWang/ROMA" -> "wendelinboehmer/dcg"
"TonghanWang/ROMA" -> "wjh720/QPLEX"
"TonghanWang/ROMA" -> "TonghanWang/NDQ"
"TonghanWang/ROMA" -> "schroederdewitt/multiagent_mujoco"
"TonghanWang/ROMA" -> "Sonkyunghwan/QTRAN"
"TonghanWang/ROMA" -> "starry-sky6688/StarCraft"
"hijkzzz/pymarl2" -> "WorldDbs/specs-actors" ["e"=1]
"hijkzzz/pymarl2" -> "uoe-agents/epymarl"
"hijkzzz/pymarl2" -> "yl-yue/yue-library" ["e"=1]
"hijkzzz/pymarl2" -> "starry-sky6688/StarCraft"
"hijkzzz/pymarl2" -> "PercyJon/PercyJon.github.io" ["e"=1]
"hijkzzz/pymarl2" -> "marlbenchmark/on-policy"
"hijkzzz/pymarl2" -> "oxwhirl/pymarl"
"hijkzzz/pymarl2" -> "v2ray-links/v2ray-free" ["e"=1]
"hijkzzz/pymarl2" -> "springmonster/RestfulTool-Retrofit" ["e"=1]
"hijkzzz/pymarl2" -> "HeisenbergEmpire/studynote" ["e"=1]
"hijkzzz/pymarl2" -> "fanyuan/MyMp3Convert" ["e"=1]
"hijkzzz/pymarl2" -> "Zaxblog/MinerProxy" ["e"=1]
"hijkzzz/pymarl2" -> "TanaStudy/Java-Study" ["e"=1]
"hijkzzz/pymarl2" -> "oxwhirl/smac"
"hijkzzz/pymarl2" -> "gatewayorg/blue" ["e"=1]
"wjh720/QPLEX" -> "TonghanWang/RODE"
"wjh720/QPLEX" -> "oxwhirl/wqmix"
"pathak22/noreward-rl" -> "openai/large-scale-curiosity"
"pathak22/noreward-rl" -> "openai/random-network-distillation"
"pathak22/noreward-rl" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"pathak22/noreward-rl" -> "ikostrikov/pytorch-a3c"
"pathak22/noreward-rl" -> "openai/universe-starter-agent"
"pathak22/noreward-rl" -> "miyosuda/unreal"
"pathak22/noreward-rl" -> "williamFalcon/DeepRLHacks"
"pathak22/noreward-rl" -> "deepmind/scalable_agent"
"pathak22/noreward-rl" -> "miyosuda/async_deep_reinforce"
"pathak22/noreward-rl" -> "rll/rllab"
"pathak22/noreward-rl" -> "joschu/modular_rl"
"pathak22/noreward-rl" -> "NVlabs/GA3C"
"pathak22/noreward-rl" -> "rail-berkeley/softlearning"
"pathak22/noreward-rl" -> "Kaixhin/Rainbow"
"pathak22/noreward-rl" -> "vitchyr/rlkit"
"borninfreedom/kuka-reach-drl" -> "mahyaret/kuka_rl"
"borninfreedom/kuka-reach-drl" -> "BarisYazici/deep-rl-grasping"
"borninfreedom/kuka-reach-drl" -> "PiggyCh/RL_arm_under_sparse_reward"
"RunxinXu/Make-Information-Extraction-Great-Again" -> "PKUnlp-icler/SCL-RAI"
"dbsxdbsx/rl-intro-book-chinese" -> "rl-cn/rl-cn"
"zhuliquan/reinforcement_learning_basic_book" -> "gxnk/reinforcement-learning-code"
"zhuliquan/reinforcement_learning_basic_book" -> "applenob/rl_learn"
"zhuliquan/reinforcement_learning_basic_book" -> "apachecn/stanford-cs234-notes-zh"
"dxyang/DQN_pytorch" -> "transedward/pytorch-dqn"
"dxyang/DQN_pytorch" -> "cyoon1729/deep-Q-networks"
"dxyang/DQN_pytorch" -> "TianhongDai/reinforcement-learning-algorithms"
"dxyang/DQN_pytorch" -> "gouxiangchen/dueling-DQN-pytorch"
"dxyang/DQN_pytorch" -> "higgsfield/RL-Adventure"
"dxyang/DQN_pytorch" -> "ghliu/pytorch-ddpg"
"dxyang/DQN_pytorch" -> "Kchu/DeepRL_PyTorch"
"dxyang/DQN_pytorch" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"dxyang/DQN_pytorch" -> "starry-sky6688/MADDPG"
"dxyang/DQN_pytorch" -> "openai/phasic-policy-gradient"
"dxyang/DQN_pytorch" -> "Kaixhin/Rainbow"
"dxyang/DQN_pytorch" -> "pranz24/pytorch-soft-actor-critic"
"DLR-RM/rl-baselines3-zoo" -> "DLR-RM/stable-baselines3"
"DLR-RM/rl-baselines3-zoo" -> "araffin/rl-baselines-zoo"
"DLR-RM/rl-baselines3-zoo" -> "Stable-Baselines-Team/stable-baselines3-contrib"
"DLR-RM/rl-baselines3-zoo" -> "vwxyzjn/cleanrl"
"DLR-RM/rl-baselines3-zoo" -> "araffin/rl-tutorial-jnrr19"
"DLR-RM/rl-baselines3-zoo" -> "Farama-Foundation/PettingZoo"
"DLR-RM/rl-baselines3-zoo" -> "HumanCompatibleAI/imitation"
"DLR-RM/rl-baselines3-zoo" -> "rail-berkeley/rlkit"
"DLR-RM/rl-baselines3-zoo" -> "facebookresearch/mbrl-lib"
"DLR-RM/rl-baselines3-zoo" -> "hill-a/stable-baselines"
"DLR-RM/rl-baselines3-zoo" -> "deepmind/dm_control"
"DLR-RM/rl-baselines3-zoo" -> "takuseno/d3rlpy"
"DLR-RM/rl-baselines3-zoo" -> "qgallouedec/panda-gym"
"DLR-RM/rl-baselines3-zoo" -> "google/brax"
"DLR-RM/rl-baselines3-zoo" -> "Farama-Foundation/Gymnasium"
"WilsonWangTHU/mbbl" -> "JannerM/mbpo"
"WilsonWangTHU/mbbl" -> "kchua/handful-of-trials"
"WilsonWangTHU/mbbl" -> "WilsonWangTHU/POPLIN"
"WilsonWangTHU/mbbl" -> "facebookresearch/mbrl-lib"
"WilsonWangTHU/mbbl" -> "thanard/me-trpo"
"WilsonWangTHU/mbbl" -> "nrontsis/PILCO"
"WilsonWangTHU/mbbl" -> "nagaban2/nn_dynamics"
"WilsonWangTHU/mbbl" -> "quanvuong/handful-of-trials-pytorch"
"WilsonWangTHU/mbbl" -> "aravindr93/mjrl"
"WilsonWangTHU/mbbl" -> "mcgillmrl/prob_mbrl"
"WilsonWangTHU/mbbl" -> "fabiopardo/tonic"
"WilsonWangTHU/mbbl" -> "rail-berkeley/d4rl"
"WilsonWangTHU/mbbl" -> "danijar/dreamer"
"WilsonWangTHU/mbbl" -> "Xingyu-Lin/mbpo_pytorch"
"WilsonWangTHU/mbbl" -> "google-research/planet"
"araffin/rl-baselines-zoo" -> "hill-a/stable-baselines"
"araffin/rl-baselines-zoo" -> "DLR-RM/rl-baselines3-zoo"
"araffin/rl-baselines-zoo" -> "araffin/rl-tutorial-jnrr19"
"araffin/rl-baselines-zoo" -> "maximecb/gym-minigrid"
"araffin/rl-baselines-zoo" -> "vitchyr/rlkit"
"araffin/rl-baselines-zoo" -> "rlworkgroup/garage"
"araffin/rl-baselines-zoo" -> "astooke/rlpyt"
"araffin/rl-baselines-zoo" -> "benelot/pybullet-gym"
"araffin/rl-baselines-zoo" -> "rail-berkeley/softlearning"
"araffin/rl-baselines-zoo" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"araffin/rl-baselines-zoo" -> "sfujim/TD3"
"araffin/rl-baselines-zoo" -> "Stable-Baselines-Team/stable-baselines"
"araffin/rl-baselines-zoo" -> "araffin/robotics-rl-srl"
"araffin/rl-baselines-zoo" -> "DLR-RM/stable-baselines3"
"araffin/rl-baselines-zoo" -> "rll/rllab"
"araffin/rl-tutorial-jnrr19" -> "Stable-Baselines-Team/rl-colab-notebooks"
"araffin/rl-tutorial-jnrr19" -> "DLR-RM/rl-baselines3-zoo"
"araffin/rl-tutorial-jnrr19" -> "araffin/rl-baselines-zoo"
"araffin/rl-tutorial-jnrr19" -> "Stable-Baselines-Team/stable-baselines3-contrib"
"araffin/rl-tutorial-jnrr19" -> "HumanCompatibleAI/imitation"
"araffin/rl-tutorial-jnrr19" -> "hill-a/stable-baselines"
"araffin/rl-tutorial-jnrr19" -> "DLR-RM/stable-baselines3"
"araffin/rl-tutorial-jnrr19" -> "rlworkgroup/garage"
"araffin/rl-tutorial-jnrr19" -> "vwxyzjn/cleanrl"
"araffin/rl-tutorial-jnrr19" -> "dongminlee94/deep_rl"
"araffin/rl-tutorial-jnrr19" -> "qgallouedec/panda-gym"
"araffin/rl-tutorial-jnrr19" -> "takuseno/d3rlpy"
"araffin/rl-tutorial-jnrr19" -> "google-research/batch_rl"
"araffin/rl-tutorial-jnrr19" -> "Denys88/rl_games"
"araffin/rl-tutorial-jnrr19" -> "openai/safety-gym"
"aravindr93/mjrl" -> "aravindr93/hand_dapg"
"aravindr93/mjrl" -> "vikashplus/mj_envs"
"aravindr93/mjrl" -> "tianheyu927/mopo"
"aravindr93/mjrl" -> "JannerM/mbpo"
"aravindr93/mjrl" -> "aravindr93/trajopt"
"aravindr93/mjrl" -> "WilsonWangTHU/mbbl"
"aravindr93/mjrl" -> "kchua/handful-of-trials"
"aravindr93/mjrl" -> "rail-berkeley/d4rl"
"aravindr93/mjrl" -> "quanvuong/handful-of-trials-pytorch"
"aravindr93/mjrl" -> "WilsonWangTHU/POPLIN"
"aravindr93/mjrl" -> "facebookresearch/slbo"
"aravindr93/mjrl" -> "Xingyu-Lin/mbpo_pytorch"
"aravindr93/mjrl" -> "aviralkumar2907/CQL"
"aravindr93/mjrl" -> "SwapnilPande/MOReL"
"aravindr93/mjrl" -> "mcgillmrl/prob_mbrl"
"denisyarats/pytorch_sac" -> "denisyarats/pytorch_sac_ae"
"denisyarats/pytorch_sac" -> "MishaLaskin/rad"
"denisyarats/pytorch_sac" -> "denisyarats/drq"
"denisyarats/pytorch_sac" -> "pranz24/pytorch-soft-actor-critic"
"denisyarats/pytorch_sac" -> "danijar/dreamer"
"denisyarats/pytorch_sac" -> "ku2482/sac-discrete.pytorch"
"denisyarats/pytorch_sac" -> "MishaLaskin/curl"
"denisyarats/pytorch_sac" -> "denisyarats/dmc2gym"
"denisyarats/pytorch_sac" -> "aviralkumar2907/CQL"
"denisyarats/pytorch_sac" -> "rail-berkeley/softlearning"
"denisyarats/pytorch_sac" -> "facebookresearch/drqv2"
"denisyarats/pytorch_sac" -> "nicklashansen/dmcontrol-generalization-benchmark"
"denisyarats/pytorch_sac" -> "rail-berkeley/rlkit"
"denisyarats/pytorch_sac" -> "haarnoja/sac"
"denisyarats/pytorch_sac" -> "sfujim/BCQ"
"facebookresearch/torchbeast" -> "deepmind/scalable_agent"
"facebookresearch/torchbeast" -> "google-research/seed_rl"
"facebookresearch/torchbeast" -> "astooke/rlpyt"
"facebookresearch/torchbeast" -> "maximecb/gym-minigrid"
"facebookresearch/torchbeast" -> "alex-petrenko/sample-factory"
"facebookresearch/torchbeast" -> "rll-research/url_benchmark"
"facebookresearch/torchbeast" -> "facebookresearch/nle"
"facebookresearch/torchbeast" -> "google-research/rliable"
"facebookresearch/torchbeast" -> "google-research/batch_rl"
"facebookresearch/torchbeast" -> "deepmind/reverb"
"facebookresearch/torchbeast" -> "facebookresearch/drqv2"
"facebookresearch/torchbeast" -> "sail-sg/envpool"
"facebookresearch/torchbeast" -> "rlworkgroup/metaworld"
"facebookresearch/torchbeast" -> "MishaLaskin/curl"
"facebookresearch/torchbeast" -> "deepmind/bsuite"
"hanjuku-kaso/awesome-offline-rl" -> "takuseno/d3rlpy"
"hanjuku-kaso/awesome-offline-rl" -> "rail-berkeley/d4rl"
"hanjuku-kaso/awesome-offline-rl" -> "tinkoff-ai/CORL"
"hanjuku-kaso/awesome-offline-rl" -> "aviralkumar2907/CQL"
"hanjuku-kaso/awesome-offline-rl" -> "sfujim/BCQ"
"hanjuku-kaso/awesome-offline-rl" -> "sfujim/TD3_BC"
"hanjuku-kaso/awesome-offline-rl" -> "google-research/batch_rl"
"hanjuku-kaso/awesome-offline-rl" -> "polixir/OfflineRL"
"hanjuku-kaso/awesome-offline-rl" -> "st-tech/zr-obp" ["e"=1]
"hanjuku-kaso/awesome-offline-rl" -> "tianheyu927/mopo"
"hanjuku-kaso/awesome-offline-rl" -> "clvrai/awesome-rl-envs"
"hanjuku-kaso/awesome-offline-rl" -> "apexrl/Batch-Offline--RL-Paper-Lists"
"hanjuku-kaso/awesome-offline-rl" -> "rlworkgroup/metaworld"
"hanjuku-kaso/awesome-offline-rl" -> "ikostrikov/implicit_q_learning"
"hanjuku-kaso/awesome-offline-rl" -> "kzl/decision-transformer"
"ikostrikov/pytorch-a3c" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"ikostrikov/pytorch-a3c" -> "MorvanZhou/pytorch-A3C"
"ikostrikov/pytorch-a3c" -> "dgriff777/rl_a3c_pytorch"
"ikostrikov/pytorch-a3c" -> "jingweiz/pytorch-rl"
"ikostrikov/pytorch-a3c" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"ikostrikov/pytorch-a3c" -> "NVlabs/GA3C"
"ikostrikov/pytorch-a3c" -> "Kaixhin/Rainbow"
"ikostrikov/pytorch-a3c" -> "pathak22/noreward-rl"
"ikostrikov/pytorch-a3c" -> "ikostrikov/pytorch-trpo"
"ikostrikov/pytorch-a3c" -> "ShangtongZhang/DeepRL"
"ikostrikov/pytorch-a3c" -> "ghliu/pytorch-ddpg"
"ikostrikov/pytorch-a3c" -> "openai/universe-starter-agent"
"ikostrikov/pytorch-a3c" -> "miyosuda/unreal"
"ikostrikov/pytorch-a3c" -> "miyosuda/async_deep_reinforce"
"ikostrikov/pytorch-a3c" -> "vitchyr/rlkit"
"ikostrikov/pytorch-trpo" -> "mjacar/pytorch-trpo"
"ikostrikov/pytorch-trpo" -> "joschu/modular_rl"
"ikostrikov/pytorch-trpo" -> "Khrylx/PyTorch-RL"
"ikostrikov/pytorch-trpo" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"ikostrikov/pytorch-trpo" -> "ikostrikov/pytorch-a3c"
"ikostrikov/pytorch-trpo" -> "pranz24/pytorch-soft-actor-critic"
"ikostrikov/pytorch-trpo" -> "wojzaremba/trpo"
"ikostrikov/pytorch-trpo" -> "TianhongDai/reinforcement-learning-algorithms"
"ikostrikov/pytorch-trpo" -> "ghliu/pytorch-ddpg"
"ikostrikov/pytorch-trpo" -> "shaneshixiang/rllabplusplus"
"ikostrikov/pytorch-trpo" -> "zuoxingdong/VIN_PyTorch_Visdom"
"ikostrikov/pytorch-trpo" -> "pat-coady/trpo"
"ikostrikov/pytorch-trpo" -> "jachiam/cpo"
"ikostrikov/pytorch-trpo" -> "tristandeleu/pytorch-maml-rl" ["e"=1]
"ikostrikov/pytorch-trpo" -> "Kaixhin/ACER"
"sfujim/TD3_BC" -> "ikostrikov/implicit_q_learning"
"sfujim/TD3_BC" -> "aviralkumar2907/CQL"
"sfujim/TD3_BC" -> "young-geng/CQL"
"sfujim/TD3_BC" -> "watchernyu/REDQ"
"sfujim/TD3_BC" -> "rail-berkeley/d4rl"
"sfujim/TD3_BC" -> "sfujim/BCQ"
"sfujim/TD3_BC" -> "tianheyu927/mopo"
"sfujim/TD3_BC" -> "snu-mllab/EDAC"
"sfujim/TD3_BC" -> "hanjuku-kaso/awesome-offline-rl"
"sfujim/TD3_BC" -> "aviralkumar2907/BEAR"
"tinkoff-ai/CORL" -> "hanjuku-kaso/awesome-offline-rl"
"tinkoff-ai/CORL" -> "vwxyzjn/cleanrl"
"tinkoff-ai/CORL" -> "takuseno/d3rlpy"
"tinkoff-ai/CORL" -> "Farama-Foundation/D4RL"
"tinkoff-ai/CORL" -> "sail-sg/envpool"
"tinkoff-ai/CORL" -> "ikostrikov/implicit_q_learning"
"tinkoff-ai/CORL" -> "araffin/sbx"
"tinkoff-ai/CORL" -> "google-research/rliable"
"tinkoff-ai/CORL" -> "facebookresearch/mbrl-lib"
"tinkoff-ai/CORL" -> "RobertTLange/gymnax"
"tinkoff-ai/CORL" -> "rail-berkeley/d4rl"
"tinkoff-ai/CORL" -> "openrlbenchmark/openrlbenchmark"
"tinkoff-ai/CORL" -> "aviralkumar2907/CQL"
"tinkoff-ai/CORL" -> "ikostrikov/jaxrl"
"tinkoff-ai/CORL" -> "sfujim/TD3_BC"
"MishaLaskin/curl" -> "MishaLaskin/rad"
"MishaLaskin/curl" -> "denisyarats/drq"
"MishaLaskin/curl" -> "aravindsrinivas/curl_rainbow"
"MishaLaskin/curl" -> "denisyarats/pytorch_sac_ae"
"MishaLaskin/curl" -> "facebookresearch/drqv2"
"MishaLaskin/curl" -> "denisyarats/dmc2gym"
"MishaLaskin/curl" -> "mila-iqia/atari-representation-learning"
"MishaLaskin/curl" -> "rlworkgroup/metaworld"
"MishaLaskin/curl" -> "danijar/dreamer"
"MishaLaskin/curl" -> "rail-berkeley/d4rl"
"MishaLaskin/curl" -> "denisyarats/pytorch_sac"
"MishaLaskin/curl" -> "alexlee-gk/slac"
"MishaLaskin/curl" -> "facebookresearch/deep_bisim4control"
"MishaLaskin/curl" -> "sfujim/BCQ"
"MishaLaskin/curl" -> "nicklashansen/dmcontrol-generalization-benchmark"
"sichkar-valentyn/Reinforcement_Learning_in_Python" -> "CoderWangcai/DRL_Path_Planning"
"sichkar-valentyn/Reinforcement_Learning_in_Python" -> "juanblak/Deep-Reinforcement-Learning"
"sichkar-valentyn/Reinforcement_Learning_in_Python" -> "mehdimo/path_planning_GAN"
"sichkar-valentyn/Reinforcement_Learning_in_Python" -> "balcilar/Multi-Robot-Path-Planning-on-Graphs"
"sichkar-valentyn/Reinforcement_Learning_in_Python" -> "Strange-AI/frenet_path_planning" ["e"=1]
"sichkar-valentyn/Reinforcement_Learning_in_Python" -> "atb033/multi_agent_path_planning" ["e"=1]
"sichkar-valentyn/Reinforcement_Learning_in_Python" -> "naderAsadi/Optimal-Path-Planning-Deep-Reinforcement-Learning"
"sichkar-valentyn/Reinforcement_Learning_in_Python" -> "kavehkamali/RobotPath"
"sichkar-valentyn/Reinforcement_Learning_in_Python" -> "a7b23/Autonomous-MazePathFinder-using-DQN"
"sichkar-valentyn/Reinforcement_Learning_in_Python" -> "lok-i/DRLPathPlanner"
"marcino239/pilco" -> "zuoxingdong/DeepPILCO"
"harvitronix/reinforcement-learning-car" -> "kanakkabara/Autonomous-Drifting"
"harvitronix/reinforcement-learning-car" -> "harvitronix/rl-rc-car"
"harvitronix/reinforcement-learning-car" -> "MLJejuCamp2017/DRL_based_SelfDrivingCarControl" ["e"=1]
"harvitronix/reinforcement-learning-car" -> "asrivat1/DeepLearningVideoGames"
"harvitronix/reinforcement-learning-car" -> "miyosuda/async_deep_reinforce"
"harvitronix/reinforcement-learning-car" -> "jangirrishabh/toyCarIRL"
"harvitronix/reinforcement-learning-car" -> "ugo-nama-kun/gym_torcs"
"harvitronix/reinforcement-learning-car" -> "thibo73800/metacar" ["e"=1]
"harvitronix/reinforcement-learning-car" -> "yanpanlau/DDPG-Keras-Torcs"
"harvitronix/reinforcement-learning-car" -> "kaihuchen/DRL-AutonomousVehicles"
"harvitronix/reinforcement-learning-car" -> "muupan/deep-reinforcement-learning-papers"
"harvitronix/reinforcement-learning-car" -> "spragunr/deep_q_rl"
"harvitronix/reinforcement-learning-car" -> "araffin/robotics-rl-srl"
"harvitronix/reinforcement-learning-car" -> "araffin/learning-to-drive-in-5-minutes" ["e"=1]
"harvitronix/reinforcement-learning-car" -> "stanfordnmbl/osim-rl"
"microsoft/maro" -> "microsoft/FOST"
"microsoft/maro" -> "hubbs5/or-gym" ["e"=1]
"microsoft/maro" -> "instadeepai/Mava"
"microsoft/maro" -> "marlbenchmark/on-policy"
"microsoft/maro" -> "ds4dm/ecole" ["e"=1]
"microsoft/maro" -> "oxwhirl/pymarl"
"microsoft/maro" -> "PettingZoo-Team/PettingZoo" ["e"=1]
"microsoft/maro" -> "rail-berkeley/d4rl"
"microsoft/maro" -> "huawei-noah/xingtian"
"microsoft/maro" -> "oxwhirl/smac"
"microsoft/maro" -> "HumanCompatibleAI/overcooked_ai"
"microsoft/maro" -> "salesforce/warp-drive"
"microsoft/maro" -> "starry-sky6688/StarCraft"
"microsoft/maro" -> "sjtu-marl/malib"
"microsoft/maro" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"mlii/mfrl" -> "shariqiqbal2810/MAAC"
"mlii/mfrl" -> "PKU-AI-Edge/DGN"
"mlii/mfrl" -> "geek-ai/MAgent"
"mlii/mfrl" -> "xuehy/pytorch-maddpg"
"mlii/mfrl" -> "sisl/MADRL"
"mlii/mfrl" -> "minqi/learning-to-communicate-pytorch"
"mlii/mfrl" -> "oxwhirl/pymarl"
"mlii/mfrl" -> "oxwhirl/smac"
"mlii/mfrl" -> "starry-sky6688/StarCraft"
"mlii/mfrl" -> "openai/multiagent-particle-envs"
"mlii/mfrl" -> "sjtu-marl/malib"
"mlii/mfrl" -> "shariqiqbal2810/maddpg-pytorch"
"mlii/mfrl" -> "openai/maddpg"
"mlii/mfrl" -> "PettingZoo-Team/MAgent"
"mlii/mfrl" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients" -> "DKuan/MADDPG_torch"
"philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients" -> "starry-sky6688/MADDPG"
"philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients" -> "marlbenchmark/off-policy"
"philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients" -> "Lizhi-sjtu/MARL-code-pytorch"
"philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients" -> "shariqiqbal2810/maddpg-pytorch"
"philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients" -> "xuehy/pytorch-maddpg"
"philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients" -> "tinyzqh/light_mappo"
"sjtu-marl/malib" -> "marlbenchmark/on-policy"
"sjtu-marl/malib" -> "Replicable-MARL/MARLlib"
"sjtu-marl/malib" -> "uoe-agents/epymarl"
"sjtu-marl/malib" -> "instadeepai/Mava"
"sjtu-marl/malib" -> "starry-sky6688/StarCraft"
"sjtu-marl/malib" -> "oxwhirl/pymarl"
"sjtu-marl/malib" -> "sail-sg/envpool"
"sjtu-marl/malib" -> "rlchina/RLCN"
"sjtu-marl/malib" -> "mlii/mfrl"
"sjtu-marl/malib" -> "PettingZoo-Team/PettingZoo" ["e"=1]
"sjtu-marl/malib" -> "oxwhirl/smac"
"sjtu-marl/malib" -> "marlbenchmark/off-policy"
"sjtu-marl/malib" -> "shariqiqbal2810/MAAC"
"sjtu-marl/malib" -> "jidiai/ai_lib"
"sjtu-marl/malib" -> "PKU-MARL/Multi-Agent-Transformer"
"floodsung/DDPG" -> "stevenpjg/ddpg-aigym"
"floodsung/DDPG" -> "yanpanlau/DDPG-Keras-Torcs"
"floodsung/DDPG" -> "ghliu/pytorch-ddpg"
"floodsung/DDPG" -> "kennethyu2017/ddpg"
"floodsung/DDPG" -> "ikostrikov/pytorch-ddpg-naf"
"floodsung/DDPG" -> "pemami4911/deep-rl"
"floodsung/DDPG" -> "rll/rllab"
"floodsung/DDPG" -> "m5823779/DDPG"
"floodsung/DDPG" -> "liampetti/DDPG"
"floodsung/DDPG" -> "MOCR/DDPG"
"floodsung/DDPG" -> "jimkon/Deep-Reinforcement-Learning-in-Large-Discrete-Action-Spaces"
"floodsung/DDPG" -> "anita-hu/TF2-RL"
"floodsung/DDPG" -> "sfujim/TD3"
"starry-sky6688/MADDPG" -> "shariqiqbal2810/maddpg-pytorch"
"starry-sky6688/MADDPG" -> "DKuan/MADDPG_torch"
"starry-sky6688/MADDPG" -> "Lizhi-sjtu/MARL-code-pytorch"
"starry-sky6688/MADDPG" -> "xuehy/pytorch-maddpg"
"starry-sky6688/MADDPG" -> "philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients"
"starry-sky6688/MADDPG" -> "tinyzqh/light_mappo"
"starry-sky6688/MADDPG" -> "marlbenchmark/on-policy"
"starry-sky6688/MADDPG" -> "starry-sky6688/MARL-Algorithms"
"starry-sky6688/MADDPG" -> "shariqiqbal2810/MAAC"
"starry-sky6688/MADDPG" -> "starry-sky6688/StarCraft"
"starry-sky6688/MADDPG" -> "sisl/MADRL"
"starry-sky6688/MADDPG" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"starry-sky6688/MADDPG" -> "openai/maddpg"
"starry-sky6688/MADDPG" -> "openai/multiagent-particle-envs"
"starry-sky6688/MADDPG" -> "cts198859/deeprl_network"
"xuehy/pytorch-maddpg" -> "shariqiqbal2810/maddpg-pytorch"
"xuehy/pytorch-maddpg" -> "sisl/MADRL"
"xuehy/pytorch-maddpg" -> "openai/maddpg"
"xuehy/pytorch-maddpg" -> "openai/multiagent-particle-envs"
"xuehy/pytorch-maddpg" -> "starry-sky6688/MADDPG"
"xuehy/pytorch-maddpg" -> "shariqiqbal2810/MAAC"
"xuehy/pytorch-maddpg" -> "minqi/learning-to-communicate-pytorch"
"xuehy/pytorch-maddpg" -> "DKuan/MADDPG_torch"
"xuehy/pytorch-maddpg" -> "starry-sky6688/StarCraft"
"xuehy/pytorch-maddpg" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"xuehy/pytorch-maddpg" -> "marlbenchmark/on-policy"
"xuehy/pytorch-maddpg" -> "mlii/mfrl"
"xuehy/pytorch-maddpg" -> "oxwhirl/pymarl"
"xuehy/pytorch-maddpg" -> "oxwhirl/smac"
"xuehy/pytorch-maddpg" -> "ChenglongChen/pytorch-madrl"
"opherlieber/rltime" -> "facebookresearch/rela"
"marooncn/learning_note" -> "marooncn/navbot"
"sumitsk/marl_transfer" -> "PKU-AI-Edge/DGN"
"mpSchrader/gym-sokoban" -> "kenjyoung/MinAtar"
"mpSchrader/gym-sokoban" -> "maximecb/gym-minigrid"
"mpSchrader/gym-sokoban" -> "vitchyr/multiworld"
"mpSchrader/gym-sokoban" -> "deepmind/pycolab"
"mpSchrader/gym-sokoban" -> "mila-iqia/atari-representation-learning"
"mpSchrader/gym-sokoban" -> "openai/procgen"
"mpSchrader/gym-sokoban" -> "Bam4d/Griddly"
"mpSchrader/gym-sokoban" -> "amidos2006/gym-pcgrl"
"mpSchrader/gym-sokoban" -> "rll-research/url_benchmark"
"ROBOTIS-GIT/turtlebot3_machine_learning" -> "dranaju/project"
"ROBOTIS-GIT/turtlebot3_machine_learning" -> "Crawford-fang/turtlebot3_DQN"
"jr-robotics/robo-gym" -> "qgallouedec/panda-gym"
"jr-robotics/robo-gym" -> "robotology/gym-ignition"
"jr-robotics/robo-gym" -> "jr-robotics/robo-gym-robot-servers"
"jr-robotics/robo-gym" -> "AndrejOrsula/drl_grasping"
"jr-robotics/robo-gym" -> "araffin/robotics-rl-srl"
"jr-robotics/robo-gym" -> "AcutronicRobotics/gym-gazebo2"
"jr-robotics/robo-gym" -> "stepjam/RLBench"
"jr-robotics/robo-gym" -> "PaulDanielML/MuJoCo_RL_UR5"
"jr-robotics/robo-gym" -> "stepjam/PyRep"
"jr-robotics/robo-gym" -> "robotlearn/pyrobolearn"
"jr-robotics/robo-gym" -> "vita-epfl/CrowdNav"
"ku2482/slac.pytorch" -> "alexlee-gk/slac"
"facebookresearch/digit-design" -> "facebookresearch/digit-interface"
"facebookresearch/digit-design" -> "facebookresearch/tacto"
"facebookresearch/digit-design" -> "mcubelab/gelslim"
"StarInitial/xpcheck" -> "HaveIBeenPwned/PwnedPasswordsCloudflareWorker"
"StarInitial/xpcheck" -> "deepmind/android_env"
"StarInitial/xpcheck" -> "vxunderground/WinAPI-Tricks" ["e"=1]
"StarInitial/xpcheck" -> "itorr/homo" ["e"=1]
"shariqiqbal2810/maddpg-pytorch" -> "xuehy/pytorch-maddpg"
"shariqiqbal2810/maddpg-pytorch" -> "starry-sky6688/MADDPG"
"shariqiqbal2810/maddpg-pytorch" -> "shariqiqbal2810/MAAC"
"shariqiqbal2810/maddpg-pytorch" -> "openai/maddpg"
"shariqiqbal2810/maddpg-pytorch" -> "DKuan/MADDPG_torch"
"shariqiqbal2810/maddpg-pytorch" -> "starry-sky6688/StarCraft"
"shariqiqbal2810/maddpg-pytorch" -> "marlbenchmark/on-policy"
"shariqiqbal2810/maddpg-pytorch" -> "openai/multiagent-particle-envs"
"shariqiqbal2810/maddpg-pytorch" -> "marlbenchmark/off-policy"
"shariqiqbal2810/maddpg-pytorch" -> "oxwhirl/pymarl"
"shariqiqbal2810/maddpg-pytorch" -> "sisl/MADRL"
"shariqiqbal2810/maddpg-pytorch" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"shariqiqbal2810/maddpg-pytorch" -> "koulanurag/ma-gym"
"shariqiqbal2810/maddpg-pytorch" -> "oxwhirl/smac"
"shariqiqbal2810/maddpg-pytorch" -> "uoe-agents/epymarl"
"li-haoran/DRL-FlappyBird" -> "zhaw/neural_style"
"andrew-j-levy/Hierarchical-Actor-Critc-HAC-" -> "nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch"
"andrew-j-levy/Hierarchical-Actor-Critc-HAC-" -> "AboudyKreidieh/h-baselines"
"andrew-j-levy/Hierarchical-Actor-Critc-HAC-" -> "yadrimz/option-critic"
"andrew-j-levy/Hierarchical-Actor-Critc-HAC-" -> "hoangminhle/hierarchical_IL_RL"
"andrew-j-levy/Hierarchical-Actor-Critc-HAC-" -> "openai/mlsh"
"andrew-j-levy/Hierarchical-Actor-Critc-HAC-" -> "YuhangSong/DEHRL"
"andrew-j-levy/Hierarchical-Actor-Critc-HAC-" -> "watakandai/hiro_pytorch"
"andrew-j-levy/Hierarchical-Actor-Critc-HAC-" -> "jcwleo/curiosity-driven-exploration-pytorch"
"danijar/dreamerv2" -> "danijar/dreamer"
"danijar/dreamerv2" -> "danijar/dreamerv3"
"danijar/dreamerv2" -> "RajGhugare19/dreamerv2"
"danijar/dreamerv2" -> "google-research/dreamer"
"danijar/dreamerv2" -> "jurgisp/pydreamer"
"danijar/dreamerv2" -> "facebookresearch/mbrl-lib"
"danijar/dreamerv2" -> "YeWR/EfficientZero"
"danijar/dreamerv2" -> "juliusfrost/dreamer-pytorch"
"danijar/dreamerv2" -> "danijar/crafter"
"danijar/dreamerv2" -> "facebookresearch/drqv2"
"danijar/dreamerv2" -> "eloialonso/iris"
"danijar/dreamerv2" -> "rail-berkeley/d4rl"
"danijar/dreamerv2" -> "rlworkgroup/metaworld"
"danijar/dreamerv2" -> "MishaLaskin/curl"
"danijar/dreamerv2" -> "google-research/planet"
"nivwusquorum/tensorflow-deepq" -> "coreylynch/async-rl"
"nivwusquorum/tensorflow-deepq" -> "spragunr/deep_q_rl"
"nivwusquorum/tensorflow-deepq" -> "rllab/rllab"
"nivwusquorum/tensorflow-deepq" -> "asrivat1/DeepLearningVideoGames"
"nivwusquorum/tensorflow-deepq" -> "tambetm/simple_dqn"
"nivwusquorum/tensorflow-deepq" -> "devsisters/DQN-tensorflow"
"nivwusquorum/tensorflow-deepq" -> "carpedm20/deep-rl-tensorflow"
"nivwusquorum/tensorflow-deepq" -> "miyosuda/async_deep_reinforce"
"nivwusquorum/tensorflow-deepq" -> "muupan/async-rl"
"nivwusquorum/tensorflow-deepq" -> "google/prettytensor" ["e"=1]
"nivwusquorum/tensorflow-deepq" -> "kuz/DeepMind-Atari-Deep-Q-Learner"
"nivwusquorum/tensorflow-deepq" -> "matthiasplappert/keras-rl"
"nivwusquorum/tensorflow-deepq" -> "muupan/deep-reinforcement-learning-papers"
"nivwusquorum/tensorflow-deepq" -> "joschu/modular_rl"
"nivwusquorum/tensorflow-deepq" -> "carpedm20/NTM-tensorflow" ["e"=1]
"openai/vime" -> "openai/imitation"
"openai/vime" -> "openai/iaf" ["e"=1]
"openai/vime" -> "joschu/modular_rl"
"openai/vime" -> "rllab/rllab"
"openai/vime" -> "openai/robosumo"
"openai/vime" -> "openai/EPG"
"openai/vime" -> "haarnoja/softqlearning"
"openai/vime" -> "Ardavans/DSR"
"openai/vime" -> "openai/rllab"
"openai/vime" -> "TheAbhiKumar/tensorflow-value-iteration-networks"
"openai/vime" -> "shaneshixiang/rllabplusplus"
"openai/vime" -> "openai/InfoGAN" ["e"=1]
"openai/vime" -> "steveKapturowski/tensorflow-rl"
"openai/vime" -> "avivt/VIN"
"openai/vime" -> "muupan/async-rl"
"rllab/rllab" -> "muupan/async-rl"
"rllab/rllab" -> "coreylynch/async-rl"
"rllab/rllab" -> "nivwusquorum/tensorflow-deepq"
"rllab/rllab" -> "joschu/modular_rl"
"rllab/rllab" -> "cbfinn/gps"
"rllab/rllab" -> "osh/kerlym"
"rllab/rllab" -> "openai/vime"
"rllab/rllab" -> "Marqt/ViZDoom"
"rllab/rllab" -> "muupan/deep-reinforcement-learning-papers"
"rllab/rllab" -> "spragunr/deep_q_rl"
"rllab/rllab" -> "twitter/torch-twrl" ["e"=1]
"rllab/rllab" -> "joschu/cgt" ["e"=1]
"rllab/rllab" -> "songrotek/DDPG"
"rllab/rllab" -> "miyosuda/async_deep_reinforce"
"rllab/rllab" -> "Kaixhin/rlenvs" ["e"=1]
"stevenpjg/ddpg-aigym" -> "songrotek/DDPG"
"stevenpjg/ddpg-aigym" -> "rmst/ddpg"
"stevenpjg/ddpg-aigym" -> "jimkon/Deep-Reinforcement-Learning-in-Large-Discrete-Action-Spaces"
"stevenpjg/ddpg-aigym" -> "floodsung/DDPG"
"stevenpjg/ddpg-aigym" -> "pemami4911/deep-rl"
"stevenpjg/ddpg-aigym" -> "yanpanlau/DDPG-Keras-Torcs"
"stevenpjg/ddpg-aigym" -> "liampetti/DDPG"
"stevenpjg/ddpg-aigym" -> "stevenpjg/QlearningRobocodeNN"
"stevenpjg/ddpg-aigym" -> "pat-coady/trpo"
"stevenpjg/ddpg-aigym" -> "joschu/modular_rl"
"stevenpjg/ddpg-aigym" -> "carpedm20/NAF-tensorflow"
"stevenpjg/ddpg-aigym" -> "spiglerg/DQN_DDQN_Dueling_and_DDPG_Tensorflow"
"tambetm/simple_dqn" -> "devsisters/DQN-tensorflow"
"tambetm/simple_dqn" -> "spragunr/deep_q_rl"
"tambetm/simple_dqn" -> "nivwusquorum/tensorflow-deepq"
"tambetm/simple_dqn" -> "carpedm20/deep-rl-tensorflow"
"tambetm/simple_dqn" -> "coreylynch/async-rl"
"tambetm/simple_dqn" -> "muupan/deep-reinforcement-learning-papers"
"tambetm/simple_dqn" -> "kuz/DeepMind-Atari-Deep-Q-Learner"
"tambetm/simple_dqn" -> "asrivat1/DeepLearningVideoGames"
"tambetm/simple_dqn" -> "miyosuda/async_deep_reinforce"
"tambetm/simple_dqn" -> "rllab/rllab"
"tambetm/simple_dqn" -> "mgbellemare/Arcade-Learning-Environment"
"tambetm/simple_dqn" -> "muupan/async-rl"
"tambetm/simple_dqn" -> "openai/universe-starter-agent"
"tambetm/simple_dqn" -> "matthiasplappert/keras-rl"
"tambetm/simple_dqn" -> "Kaixhin/Atari"
"Breakend/gym-extensions" -> "shaneshixiang/rllabplusplus"
"Breakend/gym-extensions" -> "Breakend/DeepReinforcementLearningThatMatters"
"GT-RIPL/Awesome-LLM-Robotics" -> "peract/peract"
"GT-RIPL/Awesome-LLM-Robotics" -> "vimalabs/VIMA"
"GT-RIPL/Awesome-LLM-Robotics" -> "facebookresearch/r3m"
"GT-RIPL/Awesome-LLM-Robotics" -> "google-research/robotics_transformer"
"GT-RIPL/Awesome-LLM-Robotics" -> "google-research/ravens"
"GT-RIPL/Awesome-LLM-Robotics" -> "cliport/cliport"
"GT-RIPL/Awesome-LLM-Robotics" -> "zubair-irshad/Awesome-Implicit-NeRF-Robotics" ["e"=1]
"GT-RIPL/Awesome-LLM-Robotics" -> "haosulab/ManiSkill2"
"GT-RIPL/Awesome-LLM-Robotics" -> "NVIDIA-Omniverse/Orbit"
"GT-RIPL/Awesome-LLM-Robotics" -> "NVIDIA-Omniverse/IsaacGymEnvs"
"GT-RIPL/Awesome-LLM-Robotics" -> "stepjam/RLBench"
"GT-RIPL/Awesome-LLM-Robotics" -> "wangcongrobot/awesome-isaac-gym"
"GT-RIPL/Awesome-LLM-Robotics" -> "google-research/robopianist"
"GT-RIPL/Awesome-LLM-Robotics" -> "columbia-ai-robotics/diffusion_policy"
"GT-RIPL/Awesome-LLM-Robotics" -> "vimalabs/VIMABench"
"erlerobot/gym-gazebo" -> "AcutronicRobotics/gym-gazebo2"
"erlerobot/gym-gazebo" -> "vmayoral/basic_reinforcement_learning"
"erlerobot/gym-gazebo" -> "cbfinn/gps"
"erlerobot/gym-gazebo" -> "stepjam/PyRep"
"erlerobot/gym-gazebo" -> "dusty-nv/jetson-reinforcement" ["e"=1]
"erlerobot/gym-gazebo" -> "openai/roboschool"
"erlerobot/gym-gazebo" -> "araffin/robotics-rl-srl"
"erlerobot/gym-gazebo" -> "vita-epfl/CrowdNav"
"erlerobot/gym-gazebo" -> "rll/rllab"
"erlerobot/gym-gazebo" -> "rail-berkeley/softlearning"
"erlerobot/gym-gazebo" -> "stanfordnmbl/osim-rl"
"erlerobot/gym-gazebo" -> "mit-acl/cadrl_ros"
"erlerobot/gym-gazebo" -> "robotlearn/pyrobolearn"
"erlerobot/gym-gazebo" -> "jr-robotics/robo-gym"
"erlerobot/gym-gazebo" -> "andyzeng/visual-pushing-grasping" ["e"=1]
"mila-iqia/babyai" -> "lcswillems/rl-starter-files"
"mila-iqia/babyai" -> "mila-iqia/atari-representation-learning"
"mila-iqia/babyai" -> "maximecb/gym-minigrid"
"mila-iqia/babyai" -> "rll-research/url_benchmark"
"mila-iqia/babyai" -> "huangwl18/language-planner" ["e"=1]
"mila-iqia/babyai" -> "askforalfred/alfred" ["e"=1]
"mila-iqia/babyai" -> "google-research/clevr_robot_env" ["e"=1]
"mila-iqia/babyai" -> "Farama-Foundation/Minigrid"
"mila-iqia/babyai" -> "openai/train-procgen"
"mila-iqia/babyai" -> "microsoft/TextWorld" ["e"=1]
"mila-iqia/babyai" -> "tambetm/gym-minecraft"
"mila-iqia/babyai" -> "tkipf/c-swm" ["e"=1]
"mila-iqia/babyai" -> "jacobandreas/psketch" ["e"=1]
"mila-iqia/babyai" -> "deepmind/spriteworld"
"mit-acl/cadrl_ros" -> "mit-acl/gym-collision-avoidance"
"mit-acl/cadrl_ros" -> "mit-acl/rl_collision_avoidance"
"mit-acl/cadrl_ros" -> "vita-epfl/CrowdNav"
"mit-acl/cadrl_ros" -> "srl-freiburg/pedsim_ros"
"mit-acl/cadrl_ros" -> "Acmece/rl-collision-avoidance"
"mit-acl/cadrl_ros" -> "LeeKeyu/sarl_star"
"mit-acl/cadrl_ros" -> "RGring/drl_local_planner_ros_stable_baselines"
"mit-acl/cadrl_ros" -> "ethz-asl/navrep"
"mit-acl/cadrl_ros" -> "ChanganVR/RelationalGraphLearning"
"mit-acl/cadrl_ros" -> "Shuijing725/CrowdNav_DSRNN"
"mit-acl/cadrl_ros" -> "ignc-research/arena-rosnav"
"mit-acl/cadrl_ros" -> "RoblabWh/RobLearn"
"mit-acl/cadrl_ros" -> "CoderWangcai/DRL_Path_Planning"
"mit-acl/cadrl_ros" -> "NithishkumarS/DWA-RL"
"mit-acl/cadrl_ros" -> "sybrenstuvel/Python-RVO2"
"zhangchuheng123/Reinforcement-Implementation" -> "Yonv1943/ElegantRL"
"zhangchuheng123/Reinforcement-Implementation" -> "StepNeverStop/RLs"
"zhangchuheng123/Reinforcement-Implementation" -> "kaixindelele/DRLib"
"zhangchuheng123/Reinforcement-Implementation" -> "starry-sky6688/StarCraft"
"zhangchuheng123/Reinforcement-Implementation" -> "NeuronDance/DeepRL"
"zhangchuheng123/Reinforcement-Implementation" -> "sfujim/TD3"
"zhangchuheng123/Reinforcement-Implementation" -> "louisnino/RLcode"
"zhangchuheng123/Reinforcement-Implementation" -> "AndyYue1893/Deep-reinforcement-learning-with-pytorch"
"zhangchuheng123/Reinforcement-Implementation" -> "dongminlee94/deep_rl"
"zhangchuheng123/Reinforcement-Implementation" -> "tigerneil/awesome-deep-rl"
"zhangchuheng123/Reinforcement-Implementation" -> "marlbenchmark/on-policy"
"zhangchuheng123/Reinforcement-Implementation" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"zhangchuheng123/Reinforcement-Implementation" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"zhangchuheng123/Reinforcement-Implementation" -> "sjtu-marl/malib"
"zhangchuheng123/Reinforcement-Implementation" -> "oxwhirl/pymarl"
"apachecn/stanford-cs234-notes-zh" -> "apachecn/ucb-cs294-112-notes-zh"
"apachecn/stanford-cs234-notes-zh" -> "applenob/rl_learn"
"aravindr93/hand_dapg" -> "aravindr93/mjrl"
"aravindr93/hand_dapg" -> "vikashplus/mj_envs"
"aravindr93/hand_dapg" -> "jangirrishabh/Overcoming-exploration-from-demos"
"kevinzakka/dexterity" -> "yzqin/dexmv-sim"
"Healthcare-Robotics/assistive-gym" -> "caelan/pybullet-planning"
"Healthcare-Robotics/assistive-gym" -> "StanfordVL/iGibson" ["e"=1]
"Healthcare-Robotics/assistive-gym" -> "google-research/ravens"
"Healthcare-Robotics/assistive-gym" -> "google-research/relay-policy-learning"
"Healthcare-Robotics/assistive-gym" -> "Xingyu-Lin/softgym"
"starry-sky6688/StarCraft" -> "oxwhirl/pymarl"
"starry-sky6688/StarCraft" -> "oxwhirl/smac"
"starry-sky6688/StarCraft" -> "marlbenchmark/on-policy"
"starry-sky6688/StarCraft" -> "hijkzzz/pymarl2"
"starry-sky6688/StarCraft" -> "shariqiqbal2810/MAAC"
"starry-sky6688/StarCraft" -> "openai/multiagent-particle-envs"
"starry-sky6688/StarCraft" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"starry-sky6688/StarCraft" -> "shariqiqbal2810/maddpg-pytorch"
"starry-sky6688/StarCraft" -> "PKU-AI-Edge/DGN"
"starry-sky6688/StarCraft" -> "openai/maddpg"
"starry-sky6688/StarCraft" -> "oxwhirl/wqmix"
"starry-sky6688/StarCraft" -> "uoe-agents/epymarl"
"starry-sky6688/StarCraft" -> "TonghanWang/ROMA"
"starry-sky6688/StarCraft" -> "xuehy/pytorch-maddpg"
"starry-sky6688/StarCraft" -> "sisl/MADRL"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "brynhayder/reinforcement_learning_an_introduction"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "iamhectorotero/rlai-exercises"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "vojtamolda/reinforcement-learning-an-introduction"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "NeuronDance/DeepRL"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "thu-ml/tianshou"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "vwxyzjn/cleanrl"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "DLR-RM/stable-baselines3"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "oxwhirl/pymarl"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "starry-sky6688/StarCraft"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "dennybritz/reinforcement-learning"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "openai/spinningup"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "sfujim/TD3"
"Acmece/rl-collision-avoidance" -> "mit-acl/gym-collision-avoidance"
"Acmece/rl-collision-avoidance" -> "vita-epfl/CrowdNav"
"Acmece/rl-collision-avoidance" -> "mit-acl/cadrl_ros"
"Acmece/rl-collision-avoidance" -> "ChanganVR/CADRL"
"Acmece/rl-collision-avoidance" -> "mit-acl/rl_collision_avoidance"
"Acmece/rl-collision-avoidance" -> "ChanganVR/RelationalGraphLearning"
"Acmece/rl-collision-avoidance" -> "mfe7/cadrl_ros"
"Acmece/rl-collision-avoidance" -> "ethz-asl/navrep"
"Acmece/rl-collision-avoidance" -> "sybrenstuvel/Python-RVO2"
"Acmece/rl-collision-avoidance" -> "daenny/collvoid"
"Acmece/rl-collision-avoidance" -> "ignc-research/arena-rosnav"
"Acmece/rl-collision-avoidance" -> "RGring/drl_local_planner_ros_stable_baselines"
"Acmece/rl-collision-avoidance" -> "marooncn/navbot"
"Acmece/rl-collision-avoidance" -> "gsartoretti/distributedRL_MAPF" ["e"=1]
"Acmece/rl-collision-avoidance" -> "MengGuo/RVO_Py_MAS"
"Huixxi/CS234-Reinforcement-Learning-Winter-2019" -> "Zhenye-Na/reinforcement-learning-stanford"
"Huixxi/CS234-Reinforcement-Learning-Winter-2019" -> "zlpure/CS234"
"Huixxi/CS234-Reinforcement-Learning-Winter-2019" -> "arowdy98/Stanford-CS234"
"Huixxi/CS234-Reinforcement-Learning-Winter-2019" -> "changebo/CS234-2020"
"Huixxi/CS234-Reinforcement-Learning-Winter-2019" -> "ksang/cs234-assignments"
"Huixxi/CS234-Reinforcement-Learning-Winter-2019" -> "tallamjr/stanford-cs234"
"Zhenye-Na/reinforcement-learning-stanford" -> "Huixxi/CS234-Reinforcement-Learning-Winter-2019"
"Zhenye-Na/reinforcement-learning-stanford" -> "zlpure/CS234"
"Zhenye-Na/reinforcement-learning-stanford" -> "arowdy98/Stanford-CS234"
"Zhenye-Na/reinforcement-learning-stanford" -> "changebo/CS234-2020"
"Zhenye-Na/reinforcement-learning-stanford" -> "apachecn/stanford-cs234-notes-zh"
"jannerm/trajectory-transformer" -> "kzl/decision-transformer"
"jannerm/trajectory-transformer" -> "nikhilbarhate99/min-decision-transformer"
"jannerm/trajectory-transformer" -> "jannerm/diffuser"
"jannerm/trajectory-transformer" -> "Howuhh/faster-trajectory-transformer"
"jannerm/trajectory-transformer" -> "facebookresearch/drqv2"
"jannerm/trajectory-transformer" -> "ikostrikov/implicit_q_learning"
"jannerm/trajectory-transformer" -> "frt03/generalized_dt"
"jannerm/trajectory-transformer" -> "Farama-Foundation/D4RL"
"jannerm/trajectory-transformer" -> "rail-berkeley/d4rl"
"jannerm/trajectory-transformer" -> "dhruvramani/Transformers-RL"
"jannerm/trajectory-transformer" -> "facebookresearch/deep_bisim4control"
"jannerm/trajectory-transformer" -> "google-research/batch_rl"
"jannerm/trajectory-transformer" -> "eloialonso/iris"
"jannerm/trajectory-transformer" -> "opendilab/awesome-decision-transformer" ["e"=1]
"MorvanZhou/train-robot-arm-from-scratch" -> "zenetio/DeepRL-Robotic"
"MorvanZhou/train-robot-arm-from-scratch" -> "araffin/robotics-rl-srl"
"MorvanZhou/train-robot-arm-from-scratch" -> "kindredresearch/SenseAct"
"MorvanZhou/train-robot-arm-from-scratch" -> "abr/abr_control"
"MorvanZhou/train-robot-arm-from-scratch" -> "yao62995/AS_6Dof_Arm"
"MorvanZhou/train-robot-arm-from-scratch" -> "mahyaret/kuka_rl"
"MorvanZhou/train-robot-arm-from-scratch" -> "MorvanZhou/train-classifier-from-scratch" ["e"=1]
"MorvanZhou/train-robot-arm-from-scratch" -> "Shawntl/Kuka_Robotics_Arms"
"MorvanZhou/train-robot-arm-from-scratch" -> "PiggyCh/RL_arm_under_sparse_reward"
"rtv/Stage" -> "playerproject/player"
"rtv/Stage" -> "jennyhasahat/Player-Stage-Manual"
"rtv/Stage" -> "Acmece/rl-collision-avoidance"
"rtv/Stage" -> "vita-epfl/CrowdNav"
"sisl/MADRL" -> "xuehy/pytorch-maddpg"
"sisl/MADRL" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"sisl/MADRL" -> "openai/multiagent-particle-envs"
"sisl/MADRL" -> "openai/maddpg"
"sisl/MADRL" -> "shariqiqbal2810/MAAC"
"sisl/MADRL" -> "marlbenchmark/on-policy"
"sisl/MADRL" -> "shariqiqbal2810/maddpg-pytorch"
"sisl/MADRL" -> "geek-ai/MAgent"
"sisl/MADRL" -> "starry-sky6688/StarCraft"
"sisl/MADRL" -> "oxwhirl/pymarl"
"sisl/MADRL" -> "starry-sky6688/MADDPG"
"sisl/MADRL" -> "mlii/mfrl"
"sisl/MADRL" -> "minqi/learning-to-communicate-pytorch"
"sisl/MADRL" -> "koulanurag/ma-gym"
"sisl/MADRL" -> "iassael/learning-to-communicate"
"MorvanZhou/pytorch-A3C" -> "ikostrikov/pytorch-a3c"
"MorvanZhou/pytorch-A3C" -> "dgriff777/a3c_continuous"
"MorvanZhou/pytorch-A3C" -> "vy007vikas/PyTorch-ActorCriticRL"
"MorvanZhou/pytorch-A3C" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"MorvanZhou/pytorch-A3C" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"MorvanZhou/pytorch-A3C" -> "jingweiz/pytorch-rl"
"MorvanZhou/pytorch-A3C" -> "ghliu/pytorch-ddpg"
"MorvanZhou/pytorch-A3C" -> "dgriff777/rl_a3c_pytorch"
"MorvanZhou/pytorch-A3C" -> "sfujim/TD3"
"MorvanZhou/pytorch-A3C" -> "nikhilbarhate99/PPO-PyTorch"
"MorvanZhou/pytorch-A3C" -> "uvipen/Super-mario-bros-A3C-pytorch"
"MorvanZhou/pytorch-A3C" -> "Khrylx/PyTorch-RL"
"MorvanZhou/pytorch-A3C" -> "ShangtongZhang/DeepRL"
"MorvanZhou/pytorch-A3C" -> "TianhongDai/reinforcement-learning-algorithms"
"MorvanZhou/pytorch-A3C" -> "astooke/rlpyt"
"openai/universe-starter-agent" -> "miyosuda/async_deep_reinforce"
"openai/universe-starter-agent" -> "openai/rllab"
"openai/universe-starter-agent" -> "openai/universe"
"openai/universe-starter-agent" -> "coreylynch/async-rl"
"openai/universe-starter-agent" -> "muupan/async-rl"
"openai/universe-starter-agent" -> "awjuliani/DeepRL-Agents"
"openai/universe-starter-agent" -> "NVlabs/GA3C"
"openai/universe-starter-agent" -> "pathak22/noreward-rl"
"openai/universe-starter-agent" -> "miyosuda/unreal"
"openai/universe-starter-agent" -> "joschu/modular_rl"
"openai/universe-starter-agent" -> "reinforceio/tensorforce"
"openai/universe-starter-agent" -> "openai/evolution-strategies-starter" ["e"=1]
"openai/universe-starter-agent" -> "carpedm20/deep-rl-tensorflow"
"openai/universe-starter-agent" -> "ikostrikov/pytorch-a3c"
"openai/universe-starter-agent" -> "matthiasplappert/keras-rl"
"uvipen/Super-mario-bros-A3C-pytorch" -> "uvipen/Super-mario-bros-PPO-pytorch"
"uvipen/Super-mario-bros-A3C-pytorch" -> "Kautenja/gym-super-mario-bros"
"uvipen/Super-mario-bros-A3C-pytorch" -> "ikostrikov/pytorch-a3c"
"uvipen/Super-mario-bros-A3C-pytorch" -> "uvipen/Street-fighter-A3C-ICM-pytorch"
"uvipen/Super-mario-bros-A3C-pytorch" -> "MorvanZhou/pytorch-A3C"
"uvipen/Super-mario-bros-A3C-pytorch" -> "Kautenja/nes-py"
"uvipen/Super-mario-bros-A3C-pytorch" -> "uvipen/Flappy-bird-deep-Q-learning-pytorch"
"reinforcement-learning-kr/pg_travel" -> "reinforcement-learning-kr/lets-do-irl"
"reinforcement-learning-kr/pg_travel" -> "ikostrikov/pytorch-trpo"
"reinforcement-learning-kr/pg_travel" -> "Khrylx/PyTorch-RL"
"reinforcement-learning-kr/pg_travel" -> "chagmgang/distributed_reinforcement_learning"
"reinforcement-learning-kr/pg_travel" -> "medipixel/rl_algorithms"
"reinforcement-learning-kr/pg_travel" -> "dongminlee94/deep_rl"
"reinforcement-learning-kr/pg_travel" -> "chagmgang/pytorch_ppo_rl"
"reinforcement-learning-kr/pg_travel" -> "rlcode/reinforcement-learning-kr" ["e"=1]
"reinforcement-learning-kr/pg_travel" -> "TianhongDai/reinforcement-learning-algorithms"
"reinforcement-learning-kr/pg_travel" -> "takuseno/ppo"
"Shmuma/ptan" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition"
"Shmuma/ptan" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On"
"Shmuma/ptan" -> "yandexdataschool/AgentNet"
"Shmuma/ptan" -> "Kaixhin/Rainbow"
"Shmuma/ptan" -> "jingweiz/pytorch-rl"
"Shmuma/ptan" -> "kengz/SLM-Lab"
"Shmuma/ptan" -> "MillionIntegrals/vel"
"Shmuma/ptan" -> "Khrylx/PyTorch-RL"
"Shmuma/ptan" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"Shmuma/ptan" -> "dgriff777/rl_a3c_pytorch"
"Shmuma/ptan" -> "higgsfield/RL-Adventure"
"Shmuma/ptan" -> "ShangtongZhang/DeepRL"
"Shmuma/ptan" -> "navneet-nmk/pytorch-rl"
"Shmuma/ptan" -> "transedward/pytorch-dqn"
"Shmuma/ptan" -> "rll/rllab"
"iclavera/learning_to_adapt" -> "jonasrothfuss/ProMP"
"iclavera/learning_to_adapt" -> "younggyoseo/CaDM"
"iclavera/learning_to_adapt" -> "katerakelly/oyster"
"iclavera/learning_to_adapt" -> "Knoxantropicen/model-based-meta-rl"
"iclavera/learning_to_adapt" -> "JannerM/mbpo"
"iclavera/learning_to_adapt" -> "tristandeleu/pytorch-maml-rl" ["e"=1]
"iclavera/learning_to_adapt" -> "WilsonWangTHU/POPLIN"
"iclavera/learning_to_adapt" -> "quanvuong/handful-of-trials-pytorch"
"iclavera/learning_to_adapt" -> "lmzintgraf/varibad"
"iclavera/learning_to_adapt" -> "rlworkgroup/metaworld"
"sungyubkim/Deep_RL_with_pytorch" -> "Kchu/DeepRL_PyTorch"
"sungyubkim/Deep_RL_with_pytorch" -> "ku2482/fqf-iqn-qrdqn.pytorch"
"sungyubkim/Deep_RL_with_pytorch" -> "dannysdeng/dqn-pytorch"
"AboudyKreidieh/h-baselines" -> "nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch"
"AboudyKreidieh/h-baselines" -> "andrew-j-levy/Hierarchical-Actor-Critc-HAC-"
"AboudyKreidieh/h-baselines" -> "JannerM/mbpo"
"AboudyKreidieh/h-baselines" -> "vitchyr/multiworld"
"AboudyKreidieh/h-baselines" -> "schroederdewitt/multiagent_mujoco"
"AboudyKreidieh/h-baselines" -> "stepjam/RLBench"
"AboudyKreidieh/h-baselines" -> "clvrai/awesome-rl-envs"
"AboudyKreidieh/h-baselines" -> "HorizonRobotics/alf"
"AboudyKreidieh/h-baselines" -> "uoe-agents/epymarl"
"TimeBreaker/MARL-resources-collection" -> "TimeBreaker/Multi-Agent-Reinforcement-Learning-papers"
"TimeBreaker/MARL-resources-collection" -> "TimeBreaker/MARL-papers-with-code"
"StanfordVL/robosuite" -> "SurrealAI/surreal"
"StanfordVL/robosuite" -> "SudeepDasari/visual_foresight"
"StanfordVL/robosuite" -> "stepjam/RLBench"
"StanfordVL/robosuite" -> "vitchyr/multiworld"
"StanfordVL/robosuite" -> "PSVL/DoorGym"
"StanfordVL/robosuite" -> "araffin/robotics-rl-srl"
"StanfordVL/robosuite" -> "montrealrobotics/active-domainrand"
"StanfordVL/robosuite" -> "kindredresearch/SenseAct"
"StanfordVL/robosuite" -> "robotlearn/pyrobolearn"
"StanfordVL/robosuite" -> "clvrai/furniture"
"StanfordVL/robosuite" -> "WilsonWangTHU/mbbl"
"StanfordVL/robosuite" -> "jangirrishabh/Overcoming-exploration-from-demos"
"StanfordVL/robosuite" -> "rail-berkeley/softlearning"
"qiwihui/reinforcement-learning-an-introduction-chinese" -> "dbsxdbsx/rl-intro-book-chinese"
"qiwihui/reinforcement-learning-an-introduction-chinese" -> "rl-cn/rl-cn"
"qiwihui/reinforcement-learning-an-introduction-chinese" -> "wwxFromTju/awesome-reinforcement-learning-zh"
"qiwihui/reinforcement-learning-an-introduction-chinese" -> "NeuronDance/DeepRL"
"qiwihui/reinforcement-learning-an-introduction-chinese" -> "GAOYANGAU/DRLPytorch"
"qiwihui/reinforcement-learning-an-introduction-chinese" -> "qqiang00/ReinforcemengLearningPractice"
"qiwihui/reinforcement-learning-an-introduction-chinese" -> "apachecn/stanford-cs234-notes-zh"
"rl-cn/rl-cn" -> "dbsxdbsx/rl-intro-book-chinese"
"jmichaux/dqn-pytorch" -> "KaleabTessera/DQN-Atari"
"Kaixhin/Atari" -> "Kaixhin/rlenvs" ["e"=1]
"Kaixhin/Atari" -> "twitter/torch-twrl" ["e"=1]
"Kaixhin/Atari" -> "muupan/async-rl"
"Kaixhin/Atari" -> "ludc/rltorch" ["e"=1]
"Kaixhin/Atari" -> "iassael/torch-bootstrapped-dqn"
"Kaixhin/Atari" -> "miyosuda/async_deep_reinforce"
"Kaixhin/Atari" -> "deepmind/alewrap"
"Kaixhin/Atari" -> "yobibyte/atarigrandchallenge"
"Kaixhin/Atari" -> "vivanov879/draw" ["e"=1]
"Kaixhin/Atari" -> "fmassa/optimize-net" ["e"=1]
"justinjfu/inverse_rl" -> "ahq1993/inverse_rl"
"justinjfu/inverse_rl" -> "yrlu/irl-imitation"
"justinjfu/inverse_rl" -> "MatthewJA/Inverse-Reinforcement-Learning"
"justinjfu/inverse_rl" -> "andrewliao11/gail-tf"
"justinjfu/inverse_rl" -> "ermongroup/MA-AIRL"
"justinjfu/inverse_rl" -> "stormmax/irl-imitation"
"justinjfu/inverse_rl" -> "vvanirudh/IRL-Toolkit"
"justinjfu/inverse_rl" -> "ermongroup/MetaIRL"
"justinjfu/inverse_rl" -> "reinforcement-learning-kr/lets-do-irl"
"justinjfu/inverse_rl" -> "cbfinn/gps"
"justinjfu/inverse_rl" -> "haarnoja/softqlearning"
"justinjfu/inverse_rl" -> "YunzhuLi/InfoGAIL"
"justinjfu/inverse_rl" -> "Div99/IQ-Learn"
"justinjfu/inverse_rl" -> "openai/imitation"
"justinjfu/inverse_rl" -> "sisl/gail-driver"
"luchris429/purejaxrl" -> "RobertTLange/gymnax"
"luchris429/purejaxrl" -> "instadeepai/jumanji"
"luchris429/purejaxrl" -> "araffin/sbx"
"luchris429/purejaxrl" -> "sotetsuk/pgx" ["e"=1]
"luchris429/purejaxrl" -> "ikostrikov/rlpd"
"luchris429/purejaxrl" -> "facebookresearch/online-dt"
"luchris429/purejaxrl" -> "danijar/dreamerv3"
"luchris429/purejaxrl" -> "twni2016/pomdp-baselines"
"luchris429/purejaxrl" -> "tinkoff-ai/CORL"
"luchris429/purejaxrl" -> "openrlbenchmark/openrlbenchmark"
"luchris429/purejaxrl" -> "RobertTLange/evosax"
"luchris429/purejaxrl" -> "sail-sg/envpool"
"luchris429/purejaxrl" -> "ikostrikov/jaxrl"
"facebookresearch/craftassist" -> "microsoft/malmo"
"facebookresearch/craftassist" -> "facebookresearch/droidlet" ["e"=1]
"facebookresearch/craftassist" -> "minerllabs/minerl"
"facebookresearch/craftassist" -> "Tigermouthbear/Theia" ["e"=1]
"facebookresearch/craftassist" -> "microsoft/TextWorld" ["e"=1]
"facebookresearch/craftassist" -> "crowdAI/marLo"
"zlpure/CS234" -> "SoulLights/SLPlayer" ["e"=1]
"Derekabc/MARL_CAVs" -> "jlubars/RL-MPC-LaneMerging"
"araffin/robotics-rl-srl" -> "araffin/srl-zoo"
"araffin/robotics-rl-srl" -> "andyzeng/visual-pushing-grasping" ["e"=1]
"araffin/robotics-rl-srl" -> "robotlearn/pyrobolearn"
"araffin/robotics-rl-srl" -> "avisingh599/reward-learning-rl"
"araffin/robotics-rl-srl" -> "kindredresearch/SenseAct"
"araffin/robotics-rl-srl" -> "StanfordVL/robosuite"
"araffin/robotics-rl-srl" -> "rail-berkeley/softlearning"
"araffin/robotics-rl-srl" -> "stepjam/PyRep"
"araffin/robotics-rl-srl" -> "stepjam/RLBench"
"araffin/robotics-rl-srl" -> "araffin/learning-to-drive-in-5-minutes" ["e"=1]
"araffin/robotics-rl-srl" -> "benelot/pybullet-gym"
"araffin/robotics-rl-srl" -> "erwincoumans/pybullet_robots"
"araffin/robotics-rl-srl" -> "SurrealAI/surreal"
"araffin/robotics-rl-srl" -> "jr-robotics/robo-gym"
"araffin/robotics-rl-srl" -> "BarisYazici/deep-rl-grasping"
"Farama-Foundation/PettingZoo" -> "oxwhirl/pymarl"
"Farama-Foundation/PettingZoo" -> "openai/multiagent-particle-envs"
"Farama-Foundation/PettingZoo" -> "marlbenchmark/on-policy"
"Farama-Foundation/PettingZoo" -> "Farama-Foundation/SuperSuit"
"Farama-Foundation/PettingZoo" -> "oxwhirl/smac"
"Farama-Foundation/PettingZoo" -> "uoe-agents/epymarl"
"Farama-Foundation/PettingZoo" -> "vwxyzjn/cleanrl"
"Farama-Foundation/PettingZoo" -> "Replicable-MARL/MARLlib"
"Farama-Foundation/PettingZoo" -> "DLR-RM/rl-baselines3-zoo"
"Farama-Foundation/PettingZoo" -> "Farama-Foundation/Gymnasium"
"Farama-Foundation/PettingZoo" -> "instadeepai/Mava"
"Farama-Foundation/PettingZoo" -> "openai/maddpg"
"Farama-Foundation/PettingZoo" -> "marlbenchmark/off-policy"
"Farama-Foundation/PettingZoo" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"Farama-Foundation/PettingZoo" -> "hijkzzz/pymarl2"
"koulanurag/ma-gym" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"koulanurag/ma-gym" -> "PettingZoo-Team/PettingZoo" ["e"=1]
"koulanurag/ma-gym" -> "starry-sky6688/StarCraft"
"koulanurag/ma-gym" -> "shariqiqbal2810/maddpg-pytorch"
"koulanurag/ma-gym" -> "oxwhirl/pymarl"
"koulanurag/ma-gym" -> "PKU-AI-Edge/DGN"
"koulanurag/ma-gym" -> "sisl/MADRL"
"koulanurag/ma-gym" -> "openai/multiagent-particle-envs"
"koulanurag/ma-gym" -> "schroederdewitt/multiagent_mujoco"
"koulanurag/ma-gym" -> "eugenevinitsky/sequential_social_dilemma_games"
"koulanurag/ma-gym" -> "oxwhirl/smac"
"koulanurag/ma-gym" -> "xuehy/pytorch-maddpg"
"koulanurag/ma-gym" -> "deepmind/meltingpot"
"koulanurag/ma-gym" -> "shariqiqbal2810/MAAC"
"koulanurag/ma-gym" -> "koulanurag/minimal-marl"
"AI4Finance-Foundation/ElegantRL" -> "thu-ml/tianshou"
"AI4Finance-Foundation/ElegantRL" -> "AI4Finance-Foundation/FinRL" ["e"=1]
"AI4Finance-Foundation/ElegantRL" -> "DLR-RM/stable-baselines3"
"AI4Finance-Foundation/ElegantRL" -> "marlbenchmark/on-policy"
"AI4Finance-Foundation/ElegantRL" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"AI4Finance-Foundation/ElegantRL" -> "vwxyzjn/cleanrl"
"AI4Finance-Foundation/ElegantRL" -> "datawhalechina/easy-rl"
"AI4Finance-Foundation/ElegantRL" -> "NeuronDance/DeepRL"
"AI4Finance-Foundation/ElegantRL" -> "wangshusen/DRL"
"AI4Finance-Foundation/ElegantRL" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"AI4Finance-Foundation/ElegantRL" -> "Lizhi-sjtu/DRL-code-pytorch"
"AI4Finance-Foundation/ElegantRL" -> "DLR-RM/rl-baselines3-zoo"
"AI4Finance-Foundation/ElegantRL" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"AI4Finance-Foundation/ElegantRL" -> "AI4Finance-Foundation/FinRL-Meta" ["e"=1]
"AI4Finance-Foundation/ElegantRL" -> "oxwhirl/pymarl"
"berkeleydeeprlcourse/homework_fall2021" -> "vamsianumula/cs285-deeprl-ucberkeley-2021"
"ikostrikov/implicit_q_learning" -> "sfujim/TD3_BC"
"ikostrikov/implicit_q_learning" -> "young-geng/CQL"
"ikostrikov/implicit_q_learning" -> "snu-mllab/EDAC"
"ikostrikov/implicit_q_learning" -> "gwthomas/IQL-PyTorch"
"ikostrikov/implicit_q_learning" -> "ikostrikov/rlpd"
"ikostrikov/implicit_q_learning" -> "young-geng/JaxCQL"
"uvipen/Super-mario-bros-PPO-pytorch" -> "uvipen/Super-mario-bros-A3C-pytorch"
"uvipen/Super-mario-bros-PPO-pytorch" -> "nikhilbarhate99/PPO-PyTorch"
"uvipen/Super-mario-bros-PPO-pytorch" -> "Kautenja/gym-super-mario-bros"
"uvipen/Super-mario-bros-PPO-pytorch" -> "starry-sky6688/StarCraft"
"uvipen/Super-mario-bros-PPO-pytorch" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"uvipen/Super-mario-bros-PPO-pytorch" -> "ericyangyu/PPO-for-Beginners"
"uvipen/Super-mario-bros-PPO-pytorch" -> "uvipen/Tetris-deep-Q-learning-pytorch"
"uvipen/Super-mario-bros-PPO-pytorch" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"uvipen/Super-mario-bros-PPO-pytorch" -> "StepNeverStop/RLs"
"uvipen/Super-mario-bros-PPO-pytorch" -> "uvipen/Flappy-bird-deep-Q-learning-pytorch"
"uvipen/Super-mario-bros-PPO-pytorch" -> "sfujim/TD3_BC"
"uvipen/Super-mario-bros-PPO-pytorch" -> "google-research/realworldrl_suite"
"uvipen/Super-mario-bros-PPO-pytorch" -> "openai/multiagent-particle-envs"
"uvipen/Super-mario-bros-PPO-pytorch" -> "openai/phasic-policy-gradient"
"uvipen/Super-mario-bros-PPO-pytorch" -> "marlbenchmark/on-policy"
"ChanganVR/RelationalGraphLearning" -> "vita-epfl/CrowdNav"
"ChanganVR/RelationalGraphLearning" -> "Shuijing725/CrowdNav_DSRNN"
"ChanganVR/RelationalGraphLearning" -> "ChanganVR/CADRL"
"ChanganVR/RelationalGraphLearning" -> "LeeKeyu/sarl_star"
"ChanganVR/RelationalGraphLearning" -> "sybrenstuvel/Python-RVO2"
"ChanganVR/RelationalGraphLearning" -> "vita-epfl/social-nce"
"ChanganVR/RelationalGraphLearning" -> "ml-lab-cuny/menge_ros"
"HumanCompatibleAI/imitation" -> "Kaixhin/imitation-learning"
"HumanCompatibleAI/imitation" -> "reinforcement-learning-kr/lets-do-irl"
"HumanCompatibleAI/imitation" -> "kristery/Awesome-Imitation-Learning"
"HumanCompatibleAI/imitation" -> "facebookresearch/mbrl-lib"
"HumanCompatibleAI/imitation" -> "DLR-RM/rl-baselines3-zoo"
"HumanCompatibleAI/imitation" -> "MatthewJA/Inverse-Reinforcement-Learning"
"HumanCompatibleAI/imitation" -> "rlworkgroup/metaworld"
"HumanCompatibleAI/imitation" -> "stepjam/RLBench"
"HumanCompatibleAI/imitation" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"HumanCompatibleAI/imitation" -> "Khrylx/PyTorch-RL"
"HumanCompatibleAI/imitation" -> "rlworkgroup/garage"
"HumanCompatibleAI/imitation" -> "yrlu/irl-imitation"
"HumanCompatibleAI/imitation" -> "HumanCompatibleAI/overcooked_ai"
"HumanCompatibleAI/imitation" -> "takuseno/d3rlpy"
"HumanCompatibleAI/imitation" -> "openai/imitation"
"vita-epfl/CrowdNav" -> "ChanganVR/RelationalGraphLearning"
"vita-epfl/CrowdNav" -> "sybrenstuvel/Python-RVO2"
"vita-epfl/CrowdNav" -> "mit-acl/gym-collision-avoidance"
"vita-epfl/CrowdNav" -> "mit-acl/cadrl_ros"
"vita-epfl/CrowdNav" -> "mfe7/cadrl_ros"
"vita-epfl/CrowdNav" -> "srl-freiburg/pedsim_ros"
"vita-epfl/CrowdNav" -> "ChanganVR/CADRL"
"vita-epfl/CrowdNav" -> "Acmece/rl-collision-avoidance"
"vita-epfl/CrowdNav" -> "Shuijing725/CrowdNav_DSRNN"
"vita-epfl/CrowdNav" -> "mit-acl/rl_collision_avoidance"
"vita-epfl/CrowdNav" -> "onlytailei/gym_ped_sim"
"vita-epfl/CrowdNav" -> "RoblabWh/RobLearn"
"vita-epfl/CrowdNav" -> "RGring/drl_local_planner_ros_stable_baselines"
"vita-epfl/CrowdNav" -> "ethz-asl/navrep"
"vita-epfl/CrowdNav" -> "LeeKeyu/sarl_star"
"openai/atari-py" -> "bbitmaster/ale_python_interface"
"openai/atari-py" -> "mgbellemare/Arcade-Learning-Environment"
"openai/atari-py" -> "openai/doom-py"
"openai/atari-py" -> "Kojoley/atari-py"
"xiaochus/Deep-Reinforcement-Learning-Practice" -> "tdmdal/rl-hedge-2019"
"SarvagyaVaish/FlappyBirdRL" -> "enhuiz/flappybird-ql"
"SarvagyaVaish/FlappyBirdRL" -> "chncyhn/flappybird-qlearning-bot"
"SarvagyaVaish/FlappyBirdRL" -> "songrotek/DRL-FlappyBird"
"SarvagyaVaish/FlappyBirdRL" -> "yenchenlin/DeepLearningFlappyBird"
"SarvagyaVaish/FlappyBirdRL" -> "mrspeaker/Omega500"
"SarvagyaVaish/FlappyBirdRL" -> "asrivat1/DeepLearningVideoGames"
"SarvagyaVaish/FlappyBirdRL" -> "sourabhv/FlapPyBird"
"SarvagyaVaish/FlappyBirdRL" -> "Enhuiz/flappybird-ql"
"SarvagyaVaish/FlappyBirdRL" -> "kuz/DeepMind-Atari-Deep-Q-Learner"
"SarvagyaVaish/FlappyBirdRL" -> "pascanur/GroundHog" ["e"=1]
"SarvagyaVaish/FlappyBirdRL" -> "muupan/deep-reinforcement-learning-papers"
"SarvagyaVaish/FlappyBirdRL" -> "yenchenlin1994/DeepLearningFlappyBird"
"SarvagyaVaish/FlappyBirdRL" -> "jdeng/rbm-mnist" ["e"=1]
"SarvagyaVaish/FlappyBirdRL" -> "nivwusquorum/tensorflow-deepq"
"SarvagyaVaish/FlappyBirdRL" -> "kristjankorjus/Replicating-DeepMind"
"CETC-TFAI/MaCA" -> "starry-sky6688/StarCraft"
"CETC-TFAI/MaCA" -> "sjtu-marl/malib"
"CETC-TFAI/MaCA" -> "sujiongming/starcraftAI"
"CETC-TFAI/MaCA" -> "oxwhirl/smac"
"Khrylx/PyTorch-RL" -> "ikostrikov/pytorch-trpo"
"Khrylx/PyTorch-RL" -> "reinforcement-learning-kr/lets-do-irl"
"Khrylx/PyTorch-RL" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"Khrylx/PyTorch-RL" -> "TianhongDai/reinforcement-learning-algorithms"
"Khrylx/PyTorch-RL" -> "pranz24/pytorch-soft-actor-critic"
"Khrylx/PyTorch-RL" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"Khrylx/PyTorch-RL" -> "dongminlee94/deep_rl"
"Khrylx/PyTorch-RL" -> "ShangtongZhang/DeepRL"
"Khrylx/PyTorch-RL" -> "jingweiz/pytorch-rl"
"Khrylx/PyTorch-RL" -> "sfujim/TD3"
"Khrylx/PyTorch-RL" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"Khrylx/PyTorch-RL" -> "astooke/rlpyt"
"Khrylx/PyTorch-RL" -> "MatthewJA/Inverse-Reinforcement-Learning"
"Khrylx/PyTorch-RL" -> "HumanCompatibleAI/imitation"
"Khrylx/PyTorch-RL" -> "andrewliao11/gail-tf"
"PaulDanielML/MuJoCo_RL_UR5" -> "qgallouedec/panda-gym"
"PaulDanielML/MuJoCo_RL_UR5" -> "wangcongrobot/dual_ur5_husky_mujoco"
"PaulDanielML/MuJoCo_RL_UR5" -> "ARISE-Initiative/robosuite"
"PaulDanielML/MuJoCo_RL_UR5" -> "AndrejOrsula/drl_grasping"
"PaulDanielML/MuJoCo_RL_UR5" -> "andyzeng/visual-pushing-grasping" ["e"=1]
"PaulDanielML/MuJoCo_RL_UR5" -> "abr/abr_control"
"PaulDanielML/MuJoCo_RL_UR5" -> "ElectronicElephant/pybullet_ur5_robotiq" ["e"=1]
"PaulDanielML/MuJoCo_RL_UR5" -> "stepjam/RLBench"
"PaulDanielML/MuJoCo_RL_UR5" -> "deepmind/mujoco_menagerie"
"PaulDanielML/MuJoCo_RL_UR5" -> "google-research/ravens"
"XinJingHao/RL-Algorithms-by-Pytorch" -> "XinJingHao/PPO-Continuous-Pytorch"
"XinJingHao/RL-Algorithms-by-Pytorch" -> "XinJingHao/PPO-Discrete-Pytorch"
"ac-93/tactile_gym" -> "facebookresearch/tacto"
"ac-93/tactile_gym" -> "CMURoboTouch/Taxim"
"ac-93/tactile_gym" -> "danfergo/gelsight_simulation"
"ac-93/tactile_gym" -> "zixichen007115/Tacchi"
"ac-93/tactile_gym" -> "yikaiw/EIP"
"chauncygu/Safe-Reinforcement-Learning-Baselines" -> "openai/safety-starter-agents"
"chauncygu/Safe-Reinforcement-Learning-Baselines" -> "PKU-MARL/Safe-Policy-Optimization"
"chauncygu/Safe-Reinforcement-Learning-Baselines" -> "utiasDSL/safe-control-gym"
"chauncygu/Safe-Reinforcement-Learning-Baselines" -> "openai/safety-gym"
"chauncygu/Safe-Reinforcement-Learning-Baselines" -> "rcheng805/RL-CBF"
"chauncygu/Safe-Reinforcement-Learning-Baselines" -> "chauncygu/Multi-Agent-Constrained-Policy-Optimisation"
"chauncygu/Safe-Reinforcement-Learning-Baselines" -> "jachiam/cpo"
"chauncygu/Safe-Reinforcement-Learning-Baselines" -> "zlr20/saferl_kit" ["e"=1]
"chauncygu/Safe-Reinforcement-Learning-Baselines" -> "gwthomas/Safe-MBPO"
"chauncygu/Safe-Reinforcement-Learning-Baselines" -> "AgrawalAmey/safe-explorer"
"chauncygu/Safe-Reinforcement-Learning-Baselines" -> "SvenGronauer/Bullet-Safety-Gym"
"facebookresearch/tacto" -> "facebookresearch/PyTouch"
"facebookresearch/tacto" -> "ac-93/tactile_gym"
"facebookresearch/tacto" -> "CMURoboTouch/Taxim"
"facebookresearch/tacto" -> "facebookresearch/digit-design"
"facebookresearch/tacto" -> "facebookresearch/pybulletX"
"facebookresearch/tacto" -> "facebookresearch/digit-interface"
"facebookresearch/tacto" -> "google-research/ravens"
"facebookresearch/tacto" -> "eleramp/pybullet-object-models"
"facebookresearch/tacto" -> "yikaiw/EIP"
"facebookresearch/tacto" -> "caelan/pybullet-planning"
"facebookresearch/tacto" -> "sea-bass/ycb-tools"
"facebookresearch/tacto" -> "stanford-iprl-lab/multimodal_representation"
"facebookresearch/tacto" -> "PKU-MARL/DexterousHands"
"facebookresearch/tacto" -> "iamlab-cmu/isaacgym-utils"
"openai/robogym" -> "qgallouedec/panda-gym"
"openai/robogym" -> "Xingyu-Lin/softgym"
"openai/robogym" -> "stepjam/RLBench"
"openai/robogym" -> "denisyarats/dmc2gym"
"openai/robogym" -> "google-research/relay-policy-learning"
"openai/robogym" -> "NVIDIA-Omniverse/IsaacGymEnvs"
"openai/robogym" -> "PKU-MARL/DexterousHands"
"openai/robogym" -> "PaulDanielML/MuJoCo_RL_UR5"
"openai/robogym" -> "rll-research/url_benchmark"
"openai/robogym" -> "Farama-Foundation/Gym-Robotics"
"openai/robogym" -> "google-research/ravens"
"openai/robogym" -> "MishaLaskin/rad"
"openai/robogym" -> "rlworkgroup/metaworld"
"openai/robogym" -> "ramanans1/plan2explore"
"robotlearn/pyrobolearn" -> "stepjam/PyRep"
"robotlearn/pyrobolearn" -> "araffin/robotics-rl-srl"
"robotlearn/pyrobolearn" -> "caelan/pybullet-planning"
"robotlearn/pyrobolearn" -> "clvrai/furniture"
"robotlearn/pyrobolearn" -> "facebookresearch/differentiable-robot-model" ["e"=1]
"robotlearn/pyrobolearn" -> "robotology-playground/pybullet-robot-envs"
"robotlearn/pyrobolearn" -> "stepjam/RLBench"
"robotlearn/pyrobolearn" -> "StanfordVL/robosuite"
"robotlearn/pyrobolearn" -> "facebookresearch/tacto"
"robotlearn/pyrobolearn" -> "mahyaret/gym-panda"
"robotlearn/pyrobolearn" -> "studywolf/pydmps" ["e"=1]
"robotlearn/pyrobolearn" -> "wangcongrobot/awesome-isaac-gym"
"robotlearn/pyrobolearn" -> "google-research/ravens"
"robotlearn/pyrobolearn" -> "qgallouedec/panda-gym"
"robotlearn/pyrobolearn" -> "erwincoumans/pybullet_robots"
"robotology-playground/pybullet-robot-envs" -> "mahyaret/gym-panda"
"vy007vikas/PyTorch-ActorCriticRL" -> "MorvanZhou/pytorch-A3C"
"vy007vikas/PyTorch-ActorCriticRL" -> "ghliu/pytorch-ddpg"
"vy007vikas/PyTorch-ActorCriticRL" -> "ikostrikov/pytorch-ddpg-naf"
"vy007vikas/PyTorch-ActorCriticRL" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"vy007vikas/PyTorch-ActorCriticRL" -> "dgriff777/a3c_continuous"
"vy007vikas/PyTorch-ActorCriticRL" -> "ikostrikov/pytorch-a3c"
"vy007vikas/PyTorch-ActorCriticRL" -> "ChenglongChen/pytorch-madrl"
"vy007vikas/PyTorch-ActorCriticRL" -> "Khrylx/PyTorch-RL"
"vy007vikas/PyTorch-ActorCriticRL" -> "sfujim/TD3"
"vy007vikas/PyTorch-ActorCriticRL" -> "nikhilbarhate99/Actor-Critic-PyTorch"
"vy007vikas/PyTorch-ActorCriticRL" -> "paarthneekhara/Weather-From-Map" ["e"=1]
"vy007vikas/PyTorch-ActorCriticRL" -> "alexis-jacq/Pytorch-DPPO"
"vy007vikas/PyTorch-ActorCriticRL" -> "TianhongDai/reinforcement-learning-algorithms"
"openai/large-scale-curiosity" -> "openai/random-network-distillation"
"openai/large-scale-curiosity" -> "pathak22/noreward-rl"
"openai/large-scale-curiosity" -> "uber-research/go-explore"
"openai/large-scale-curiosity" -> "pathak22/exploration-by-disagreement"
"openai/large-scale-curiosity" -> "deepmind/scalable_agent"
"openai/large-scale-curiosity" -> "openai/vime"
"openai/large-scale-curiosity" -> "google-research/planet"
"openai/large-scale-curiosity" -> "ml-jku/baselines-rudder"
"openai/large-scale-curiosity" -> "openai/coinrun"
"openai/large-scale-curiosity" -> "rail-berkeley/softlearning"
"openai/large-scale-curiosity" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"openai/large-scale-curiosity" -> "google-research/episodic-curiosity"
"openai/large-scale-curiosity" -> "openai/multiagent-competition"
"openai/large-scale-curiosity" -> "openai/mlsh"
"openai/large-scale-curiosity" -> "haarnoja/sac"
"ignitionrobotics/ign-gazebo" -> "ignitionrobotics/ign-sensors"
"ignitionrobotics/ign-gazebo" -> "robotology/gym-ignition"
"ignitionrobotics/ign-gazebo" -> "osrf/sdformat" ["e"=1]
"ignitionrobotics/ign-gazebo" -> "ignitionrobotics/ign-rendering"
"ignitionrobotics/ign-gazebo" -> "ignitionrobotics/ign-physics"
"ignitionrobotics/ign-gazebo" -> "ignitionrobotics/ign-math"
"ignitionrobotics/ign-gazebo" -> "ignitionrobotics/ros_ign"
"ignitionrobotics/ign-gazebo" -> "ros/sdformat_urdf"
"ignitionrobotics/ign-gazebo" -> "ignitionrobotics/ign-gui"
"onlytailei/gym_ped_sim" -> "srl-freiburg/pedsim_ros"
"onlytailei/gym_ped_sim" -> "ml-lab-cuny/menge_ros"
"onlytailei/gym_ped_sim" -> "marinaKollmitz/human_aware_navigation" ["e"=1]
"onlytailei/gym_ped_sim" -> "onlytailei/gym_style_gazebo"
"AcutronicRobotics/gym-gazebo2" -> "AcutronicRobotics/ros2learn"
"AcutronicRobotics/gym-gazebo2" -> "erlerobot/gym-gazebo"
"AcutronicRobotics/gym-gazebo2" -> "AcutronicRobotics/moveit2"
"AcutronicRobotics/gym-gazebo2" -> "jr-robotics/robo-gym"
"AcutronicRobotics/gym-gazebo2" -> "andyzeng/visual-pushing-grasping" ["e"=1]
"AcutronicRobotics/gym-gazebo2" -> "robotology/gym-ignition"
"AcutronicRobotics/gym-gazebo2" -> "AcutronicRobotics/MARA_threat_model"
"AcutronicRobotics/gym-gazebo2" -> "araffin/robotics-rl-srl"
"AcutronicRobotics/gym-gazebo2" -> "stepjam/RLBench"
"AcutronicRobotics/gym-gazebo2" -> "jangirrishabh/Overcoming-exploration-from-demos"
"AcutronicRobotics/gym-gazebo2" -> "Acmece/rl-collision-avoidance"
"AcutronicRobotics/gym-gazebo2" -> "AcutronicRobotics/MARA"
"AcutronicRobotics/gym-gazebo2" -> "avisingh599/reward-learning-rl"
"AcutronicRobotics/gym-gazebo2" -> "robotlearn/pyrobolearn"
"kootenpv/neural_complete" -> "jostmey/rwa" ["e"=1]
"kootenpv/neural_complete" -> "deepmind/dnc"
"kootenpv/neural_complete" -> "uclmr/pycodesuggest"
"kootenpv/neural_complete" -> "bioinf-jku/SNNs" ["e"=1]
"kootenpv/neural_complete" -> "silicon-valley-data-science/RNN-Tutorial" ["e"=1]
"kootenpv/neural_complete" -> "deepmind/learning-to-learn"
"kootenpv/neural_complete" -> "deepmind/sonnet"
"kootenpv/neural_complete" -> "joeddav/devol" ["e"=1]
"kootenpv/neural_complete" -> "chiphuyen/tf-stanford-tutorials" ["e"=1]
"kootenpv/neural_complete" -> "JulianGaal/python-cheat-sheet" ["e"=1]
"kootenpv/neural_complete" -> "rlcode/reinforcement-learning"
"kootenpv/neural_complete" -> "facebookresearch/CommAI-env" ["e"=1]
"kootenpv/neural_complete" -> "openai/generating-reviews-discovering-sentiment" ["e"=1]
"kootenpv/neural_complete" -> "Babylonpartners/fastText_multilingual" ["e"=1]
"kootenpv/neural_complete" -> "thomasj02/DeepLearningProjectWorkflow" ["e"=1]
"yanpanlau/DDPG-Keras-Torcs" -> "ugo-nama-kun/gym_torcs"
"yanpanlau/DDPG-Keras-Torcs" -> "miyosuda/async_deep_reinforce"
"yanpanlau/DDPG-Keras-Torcs" -> "stevenpjg/ddpg-aigym"
"yanpanlau/DDPG-Keras-Torcs" -> "songrotek/DDPG"
"yanpanlau/DDPG-Keras-Torcs" -> "floodsung/DDPG"
"yanpanlau/DDPG-Keras-Torcs" -> "pemami4911/deep-rl"
"yanpanlau/DDPG-Keras-Torcs" -> "germain-hug/Deep-RL-Keras"
"yanpanlau/DDPG-Keras-Torcs" -> "rmst/ddpg"
"yanpanlau/DDPG-Keras-Torcs" -> "openai/imitation"
"yanpanlau/DDPG-Keras-Torcs" -> "YurongYou/rlTORCS"
"yanpanlau/DDPG-Keras-Torcs" -> "openai/rllab"
"yanpanlau/DDPG-Keras-Torcs" -> "joschu/modular_rl"
"yanpanlau/DDPG-Keras-Torcs" -> "openai/universe-starter-agent"
"yanpanlau/DDPG-Keras-Torcs" -> "coreylynch/async-rl"
"yanpanlau/DDPG-Keras-Torcs" -> "muupan/async-rl"
"microsoft/PromptCraft-Robotics" -> "google-research/robotics_transformer"
"microsoft/PromptCraft-Robotics" -> "GT-RIPL/Awesome-LLM-Robotics"
"microsoft/PromptCraft-Robotics" -> "vimalabs/VIMA"
"microsoft/PromptCraft-Robotics" -> "haosulab/ManiSkill2"
"microsoft/PromptCraft-Robotics" -> "facebookresearch/eai-vc"
"microsoft/PromptCraft-Robotics" -> "NVIDIA-Omniverse/Orbit"
"microsoft/PromptCraft-Robotics" -> "stepjam/RLBench"
"microsoft/PromptCraft-Robotics" -> "NVIDIA-Omniverse/IsaacGymEnvs"
"microsoft/PromptCraft-Robotics" -> "cliport/cliport"
"microsoft/PromptCraft-Robotics" -> "microsoft/ChatGPT-Robot-Manipulation-Prompts"
"microsoft/PromptCraft-Robotics" -> "microsoft/MM-REACT" ["e"=1]
"microsoft/PromptCraft-Robotics" -> "lucidrains/robotic-transformer-pytorch"
"microsoft/PromptCraft-Robotics" -> "wangcongrobot/awesome-isaac-gym"
"microsoft/PromptCraft-Robotics" -> "vlmaps/vlmaps"
"microsoft/PromptCraft-Robotics" -> "peract/peract"
"DeepRLChinese/DeepRL-Chinese" -> "wangshusen/DRL"
"mit-acl/gym-collision-avoidance" -> "mit-acl/rl_collision_avoidance"
"mit-acl/gym-collision-avoidance" -> "mit-acl/cadrl_ros"
"mit-acl/gym-collision-avoidance" -> "vita-epfl/CrowdNav"
"mit-acl/gym-collision-avoidance" -> "Acmece/rl-collision-avoidance"
"mit-acl/gym-collision-avoidance" -> "ethz-asl/navrep"
"mit-acl/gym-collision-avoidance" -> "sybrenstuvel/Python-RVO2"
"mit-acl/gym-collision-avoidance" -> "ChanganVR/CADRL"
"mit-acl/gym-collision-avoidance" -> "ChanganVR/RelationalGraphLearning"
"mit-acl/gym-collision-avoidance" -> "Shuijing725/CrowdNav_DSRNN"
"mit-acl/gym-collision-avoidance" -> "hanruihua/rl_rvo_nav"
"mit-acl/gym-collision-avoidance" -> "RGring/drl_local_planner_ros_stable_baselines"
"mit-acl/gym-collision-avoidance" -> "yuxiang-gao/PySocialForce"
"sjchoi86/irl_rocks" -> "stormmax/irl-imitation"
"marlbenchmark/off-policy" -> "marlbenchmark/on-policy"
"marlbenchmark/off-policy" -> "uoe-agents/epymarl"
"marlbenchmark/off-policy" -> "starry-sky6688/MARL-Algorithms"
"marlbenchmark/off-policy" -> "Lizhi-sjtu/MARL-code-pytorch"
"marlbenchmark/off-policy" -> "Replicable-MARL/MARLlib"
"marlbenchmark/off-policy" -> "tinyzqh/light_mappo"
"marlbenchmark/off-policy" -> "cyanrain7/TRPO-in-MARL"
"marlbenchmark/off-policy" -> "hijkzzz/pymarl2"
"marlbenchmark/off-policy" -> "schroederdewitt/multiagent_mujoco"
"marlbenchmark/off-policy" -> "PKU-MARL/Multi-Agent-Transformer"
"marlbenchmark/off-policy" -> "shariqiqbal2810/maddpg-pytorch"
"marlbenchmark/off-policy" -> "chauncygu/Multi-Agent-Constrained-Policy-Optimisation"
"marlbenchmark/off-policy" -> "starry-sky6688/StarCraft"
"marlbenchmark/off-policy" -> "oxwhirl/pymarl"
"marlbenchmark/off-policy" -> "lich14/CDS"
"google/brax" -> "NVIDIA-Omniverse/IsaacGymEnvs"
"google/brax" -> "sail-sg/envpool"
"google/brax" -> "google-research/tiny-differentiable-simulator" ["e"=1]
"google/brax" -> "deepmind/rlax" ["e"=1]
"google/brax" -> "deepmind/mujoco_menagerie"
"google/brax" -> "RobertTLange/gymnax"
"google/brax" -> "facebookresearch/mbrl-lib"
"google/brax" -> "deepmind/dm_control"
"google/brax" -> "Denys88/rl_games"
"google/brax" -> "vwxyzjn/cleanrl"
"google/brax" -> "rlworkgroup/metaworld"
"google/brax" -> "stepjam/RLBench"
"google/brax" -> "deepmind/mujoco"
"google/brax" -> "rail-berkeley/d4rl"
"google/brax" -> "ikostrikov/jaxrl"
"weightagnostic/weightagnostic.github.io" -> "google/brain-tokyo-workshop"
"weightagnostic/weightagnostic.github.io" -> "henry8527/HCOT" ["e"=1]
"weightagnostic/weightagnostic.github.io" -> "IpsumDominum/super-brain-weight-agnostic-neural-networks"
"SurrealAI/surreal" -> "StanfordVL/robosuite"
"SurrealAI/surreal" -> "stepjam/RLBench"
"SurrealAI/surreal" -> "araffin/robotics-rl-srl"
"SurrealAI/surreal" -> "vitchyr/multiworld"
"SurrealAI/surreal" -> "rail-berkeley/softlearning"
"SurrealAI/surreal" -> "robotlearn/pyrobolearn"
"SurrealAI/surreal" -> "rlworkgroup/metaworld"
"SurrealAI/surreal" -> "kindredresearch/SenseAct"
"SurrealAI/surreal" -> "vitchyr/rlkit"
"SurrealAI/surreal" -> "SudeepDasari/RoboNet"
"SurrealAI/surreal" -> "stepjam/PyRep"
"SurrealAI/surreal" -> "rlworkgroup/garage"
"SurrealAI/surreal" -> "iclavera/learning_to_adapt"
"SurrealAI/surreal" -> "rlgraph/rlgraph"
"SurrealAI/surreal" -> "tianheyu927/mil"
"openai/coinrun" -> "openai/train-procgen"
"openai/coinrun" -> "pokaxpoka/netrand"
"openai/coinrun" -> "openai/random-network-distillation"
"openai/coinrun" -> "deepmind/scalable_agent"
"openai/coinrun" -> "iclavera/learning_to_adapt"
"openai/coinrun" -> "openai/large-scale-curiosity"
"openai/coinrun" -> "vitchyr/multiworld"
"RoblabWh/RobLearn" -> "ethz-asl/navrep"
"RoblabWh/RobLearn" -> "marooncn/navbot"
"RoblabWh/RobLearn" -> "vita-epfl/CrowdNav"
"RoblabWh/RobLearn" -> "m5823779/DDPG"
"RoblabWh/RobLearn" -> "LeeKeyu/sarl_star"
"RoblabWh/RobLearn" -> "RGring/drl_local_planner_ros_stable_baselines"
"RoblabWh/RobLearn" -> "m5823779/motion-planner-reinforcement-learning"
"RoblabWh/RobLearn" -> "mit-acl/cadrl_ros"
"RoblabWh/RobLearn" -> "reiniscimurs/DRL-robot-navigation"
"google-research/robopianist" -> "haosulab/ManiSkill2"
"google-research/robopianist" -> "deepmind/mujoco_menagerie"
"google-research/robopianist" -> "columbia-ai-robotics/diffusion_policy"
"google-research/robopianist" -> "kevinzakka/dexterity"
"google-research/robopianist" -> "deepmind/mujoco_mpc"
"google-research/robopianist" -> "zalo/mujoco_wasm"
"google-research/robopianist" -> "PKU-MARL/DexterousHands"
"google-research/robopianist" -> "NVIDIA-Omniverse/Orbit"
"google-research/robopianist" -> "peract/peract"
"google-research/robopianist" -> "GT-RIPL/Awesome-LLM-Robotics"
"google-research/robopianist" -> "wwxFromTju/awesome-reinforcement-learning-lib"
"google-research/robopianist" -> "ToruOwO/minimal-stable-PPO"
"google-research/robopianist" -> "facebookresearch/r3m"
"google-research/robopianist" -> "araffin/sbx"
"google-research/robopianist" -> "sfujim/TD3_BC"
"GAOYANGAU/DRLPytorch" -> "catziyan/DRLPytorch-"
"GAOYANGAU/DRLPytorch" -> "Teacher-Guo/RL_code"
"IC3Net/IC3Net" -> "rhoowd/sched_net"
"IC3Net/IC3Net" -> "PKU-AI-Edge/I2C"
"IC3Net/IC3Net" -> "minqi/learning-to-communicate-pytorch"
"IC3Net/IC3Net" -> "facebookarchive/CommNet"
"IC3Net/IC3Net" -> "PKU-AI-Edge/DGN"
"IC3Net/IC3Net" -> "saizhang0218/VBC"
"IC3Net/IC3Net" -> "sumitsk/marl_transfer"
"IC3Net/IC3Net" -> "wwxFromTju/deepmind_MAS_enviroment"
"IC3Net/IC3Net" -> "shariqiqbal2810/MAAC"
"IC3Net/IC3Net" -> "Coac/CommNet-BiCnet"
"IC3Net/IC3Net" -> "apsdehal/ic3net-envs"
"mrkulk/deepQN_tensorflow" -> "gliese581gg/DQN_tensorflow"
"PKU-AI-Edge/DGN" -> "sumitsk/marl_transfer"
"PKU-AI-Edge/DGN" -> "jiechuanjiang/pytorch_DGN"
"PKU-AI-Edge/DGN" -> "starry-sky6688/StarCraft"
"PKU-AI-Edge/DGN" -> "cts198859/deeprl_network"
"PKU-AI-Edge/DGN" -> "tegg89/magnet"
"PKU-AI-Edge/DGN" -> "shariqiqbal2810/MAAC"
"PKU-AI-Edge/DGN" -> "IC3Net/IC3Net"
"PKU-AI-Edge/DGN" -> "mlii/mfrl"
"PKU-AI-Edge/DGN" -> "koulanurag/ma-gym"
"PKU-AI-Edge/DGN" -> "minqi/learning-to-communicate-pytorch"
"PKU-AI-Edge/DGN" -> "oxwhirl/pymarl"
"PKU-AI-Edge/DGN" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"PKU-AI-Edge/DGN" -> "schroederdewitt/multiagent_mujoco"
"PKU-AI-Edge/DGN" -> "TonghanWang/ROMA"
"PKU-AI-Edge/DGN" -> "shariqiqbal2810/maddpg-pytorch"
"PKU-MARL/Multi-Agent-Transformer" -> "cyanrain7/TRPO-in-MARL"
"PKU-MARL/Multi-Agent-Transformer" -> "marlbenchmark/off-policy"
"PKU-MARL/Multi-Agent-Transformer" -> "marlbenchmark/on-policy"
"YuhangSong/Arena-Baselines" -> "YuhangSong/Arena-BuildingToolkit"
"jiechuanjiang/pytorch_DGN" -> "PKU-RL/DGN"
"jiechuanjiang/pytorch_DGN" -> "PKU-AI-Edge/DGN"
"minqi/learning-to-communicate-pytorch" -> "iassael/learning-to-communicate"
"minqi/learning-to-communicate-pytorch" -> "IC3Net/IC3Net"
"minqi/learning-to-communicate-pytorch" -> "xuehy/pytorch-maddpg"
"minqi/learning-to-communicate-pytorch" -> "shariqiqbal2810/MAAC"
"minqi/learning-to-communicate-pytorch" -> "sisl/MADRL"
"minqi/learning-to-communicate-pytorch" -> "eugenevinitsky/sequential_social_dilemma_games"
"minqi/learning-to-communicate-pytorch" -> "cts198859/deeprl_network"
"minqi/learning-to-communicate-pytorch" -> "mlii/mfrl"
"minqi/learning-to-communicate-pytorch" -> "PKU-AI-Edge/DGN"
"minqi/learning-to-communicate-pytorch" -> "starry-sky6688/StarCraft"
"minqi/learning-to-communicate-pytorch" -> "wwxFromTju/deepmind_MAS_enviroment"
"minqi/learning-to-communicate-pytorch" -> "rhoowd/sched_net"
"minqi/learning-to-communicate-pytorch" -> "facebookarchive/CommNet"
"minqi/learning-to-communicate-pytorch" -> "openai/multiagent-particle-envs"
"minqi/learning-to-communicate-pytorch" -> "oxwhirl/pymarl"
"rhoowd/sched_net" -> "saizhang0218/VBC"
"rhoowd/sched_net" -> "facebookarchive/CommNet"
"rhoowd/sched_net" -> "tuladhay/ATOC_COMA_PyTorch"
"inoryy/tensorflow2-deep-reinforcement-learning" -> "Huixxi/TensorFlow2.0-for-Deep-Reinforcement-Learning"
"chncyhn/flappybird-qlearning-bot" -> "mihaibivol/Q-learning-tic-tac-toe"
"chncyhn/flappybird-qlearning-bot" -> "sourabhv/FlapPyBird"
"chncyhn/flappybird-qlearning-bot" -> "SarvagyaVaish/FlappyBirdRL"
"chncyhn/flappybird-qlearning-bot" -> "kyokin78/rl-flappybird"
"YangRui2015/Sparse-Reward-Algorithms" -> "YangRui2015/Modular_HER"
"miyosuda/async_deep_reinforce" -> "muupan/async-rl"
"miyosuda/async_deep_reinforce" -> "NVlabs/GA3C"
"miyosuda/async_deep_reinforce" -> "coreylynch/async-rl"
"miyosuda/async_deep_reinforce" -> "miyosuda/unreal"
"miyosuda/async_deep_reinforce" -> "openai/universe-starter-agent"
"miyosuda/async_deep_reinforce" -> "joschu/modular_rl"
"miyosuda/async_deep_reinforce" -> "yao62995/A3C"
"miyosuda/async_deep_reinforce" -> "TheAbhiKumar/tensorflow-value-iteration-networks"
"miyosuda/async_deep_reinforce" -> "openai/rllab"
"miyosuda/async_deep_reinforce" -> "carpedm20/deep-rl-tensorflow"
"miyosuda/async_deep_reinforce" -> "Kaixhin/Atari"
"miyosuda/async_deep_reinforce" -> "steveKapturowski/tensorflow-rl"
"miyosuda/async_deep_reinforce" -> "devsisters/DQN-tensorflow"
"miyosuda/async_deep_reinforce" -> "yanpanlau/DDPG-Keras-Torcs"
"miyosuda/async_deep_reinforce" -> "Ardavans/DSR"
"deepmind/spriteworld" -> "deepmind/bsuite"
"deepmind/spriteworld" -> "deepmind/multi_object_datasets" ["e"=1]
"deepmind/spriteworld" -> "jcoreyes/OP3" ["e"=1]
"deepmind/spriteworld" -> "deepmind/open_spiel"
"deepmind/spriteworld" -> "tkipf/c-swm" ["e"=1]
"deepmind/spriteworld" -> "deepmind/pycolab"
"deepmind/spriteworld" -> "deepmind/hanabi-learning-environment"
"deepmind/spriteworld" -> "deepmind/rlax" ["e"=1]
"nav74neet/gail_gym" -> "uidilr/gail_ppo_tf"
"pat-coady/trpo" -> "joschu/modular_rl"
"pat-coady/trpo" -> "wojzaremba/trpo"
"pat-coady/trpo" -> "shaneshixiang/rllabplusplus"
"pat-coady/trpo" -> "openai/vime"
"pat-coady/trpo" -> "ikostrikov/pytorch-trpo"
"pat-coady/trpo" -> "pemami4911/deep-rl"
"pat-coady/trpo" -> "uidilr/gail_ppo_tf"
"pat-coady/trpo" -> "stevenpjg/ddpg-aigym"
"pat-coady/trpo" -> "NVlabs/GA3C"
"pat-coady/trpo" -> "openai/imitation"
"pat-coady/trpo" -> "dgriff777/a3c_continuous"
"pat-coady/trpo" -> "Kaixhin/NoisyNet-A3C"
"pat-coady/trpo" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"pat-coady/trpo" -> "miyosuda/async_deep_reinforce"
"pat-coady/trpo" -> "nagaban2/nn_dynamics"
"rmst/ddpg" -> "songrotek/DDPG"
"rmst/ddpg" -> "stevenpjg/ddpg-aigym"
"rmst/ddpg" -> "MOCR/DDPG"
"uidilr/gail_ppo_tf" -> "andrewliao11/gail-tf"
"uidilr/gail_ppo_tf" -> "nav74neet/gail_gym"
"uidilr/gail_ppo_tf" -> "ahq1993/inverse_rl"
"uidilr/gail_ppo_tf" -> "YunzhuLi/InfoGAIL"
"vitchyr/multiworld" -> "vitchyr/rlkit"
"vitchyr/multiworld" -> "rlworkgroup/metaworld"
"vitchyr/multiworld" -> "TianhongDai/hindsight-experience-replay"
"vitchyr/multiworld" -> "stepjam/RLBench"
"vitchyr/multiworld" -> "StanfordVL/robosuite"
"vitchyr/multiworld" -> "vitchyr/viskit"
"vitchyr/multiworld" -> "JannerM/mbpo"
"vitchyr/multiworld" -> "Kaixhin/PlaNet"
"vitchyr/multiworld" -> "haarnoja/softqlearning"
"vitchyr/multiworld" -> "AboudyKreidieh/h-baselines"
"vitchyr/multiworld" -> "mengf1/DHER"
"vitchyr/multiworld" -> "rail-berkeley/softlearning"
"vitchyr/multiworld" -> "vikashplus/mj_envs"
"pytorch/rl" -> "facebookresearch/mbrl-lib"
"pytorch/rl" -> "Farama-Foundation/Gymnasium"
"pytorch/rl" -> "danijar/dreamerv3"
"pytorch/rl" -> "pytorch-labs/tensordict"
"pytorch/rl" -> "alex-petrenko/sample-factory"
"pytorch/rl" -> "google-research/rliable"
"pytorch/rl" -> "Farama-Foundation/D4RL"
"pytorch/rl" -> "vwxyzjn/cleanrl"
"pytorch/rl" -> "sail-sg/envpool"
"pytorch/rl" -> "luchris429/purejaxrl"
"pytorch/rl" -> "deepmind/rlax" ["e"=1]
"pytorch/rl" -> "araffin/sbx"
"pytorch/rl" -> "opendilab/awesome-decision-transformer" ["e"=1]
"pytorch/rl" -> "tinkoff-ai/CORL"
"pytorch/rl" -> "deepmind/mujoco_mpc"
"ChenglongChen/pytorch-madrl" -> "xuehy/pytorch-maddpg"
"ChenglongChen/pytorch-madrl" -> "Khrylx/PyTorch-RL"
"ChenglongChen/pytorch-madrl" -> "alexis-jacq/Pytorch-DPPO"
"ChenglongChen/pytorch-madrl" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"ChenglongChen/pytorch-madrl" -> "rohan-sawhney/multi-agent-rl"
"Kaixhin/NoisyNet-A3C" -> "andrewliao11/NoisyNet-DQN"
"MultiAgentLearning/playground" -> "openai/multiagent-particle-envs"
"MultiAgentLearning/playground" -> "geek-ai/MAgent"
"MultiAgentLearning/playground" -> "oxwhirl/smac"
"MultiAgentLearning/playground" -> "oxwhirl/pymarl"
"MultiAgentLearning/playground" -> "sisl/MADRL"
"MultiAgentLearning/playground" -> "deepmind/hanabi-learning-environment"
"MultiAgentLearning/playground" -> "maximecb/gym-minigrid"
"MultiAgentLearning/playground" -> "mlii/mfrl"
"MultiAgentLearning/playground" -> "openai/multiagent-competition"
"MultiAgentLearning/playground" -> "openai/maddpg"
"MultiAgentLearning/playground" -> "tambetm/pommerman-baselines"
"MultiAgentLearning/playground" -> "LantaoYu/MARL-Papers"
"MultiAgentLearning/playground" -> "crowdAI/marLo"
"MultiAgentLearning/playground" -> "openai/multi-agent-emergence-environments"
"MultiAgentLearning/playground" -> "starry-sky6688/StarCraft"
"VinF/deer" -> "rlpy/rlpy" ["e"=1]
"VinF/deer" -> "ADGEfficiency/energy-py" ["e"=1]
"VinF/deer" -> "rllab/rllab"
"VinF/deer" -> "Ardavans/DSR"
"VinF/deer" -> "muupan/async-rl"
"VinF/deer" -> "Matrixeigs/energy_management_system" ["e"=1]
"dgriff777/rl_a3c_pytorch" -> "dgriff777/a3c_continuous"
"dgriff777/rl_a3c_pytorch" -> "ikostrikov/pytorch-a3c"
"dgriff777/rl_a3c_pytorch" -> "NVlabs/GA3C"
"dgriff777/rl_a3c_pytorch" -> "jingweiz/pytorch-rl"
"dgriff777/rl_a3c_pytorch" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"dgriff777/rl_a3c_pytorch" -> "ikostrikov/pytorch-ddpg-naf"
"dgriff777/rl_a3c_pytorch" -> "miyosuda/async_deep_reinforce"
"dgriff777/rl_a3c_pytorch" -> "Kaixhin/ACER"
"dgriff777/rl_a3c_pytorch" -> "greydanus/baby-a3c"
"dgriff777/rl_a3c_pytorch" -> "Kaixhin/Rainbow"
"dgriff777/rl_a3c_pytorch" -> "jingweiz/pytorch-dnc" ["e"=1]
"dgriff777/rl_a3c_pytorch" -> "pathak22/noreward-rl"
"dgriff777/rl_a3c_pytorch" -> "miyosuda/unreal"
"dgriff777/rl_a3c_pytorch" -> "awjuliani/Meta-RL"
"dgriff777/rl_a3c_pytorch" -> "jaara/AI-blog"
"erwincoumans/pybullet_robots" -> "robotology-playground/pybullet-robot-envs"
"erwincoumans/pybullet_robots" -> "bulletphysics/pybullet_robots"
"erwincoumans/pybullet_robots" -> "benelot/pybullet-gym"
"erwincoumans/pybullet_robots" -> "google-research/motion_imitation" ["e"=1]
"erwincoumans/pybullet_robots" -> "nicrusso7/rex-gym" ["e"=1]
"erwincoumans/pybullet_robots" -> "araffin/robotics-rl-srl"
"erwincoumans/pybullet_robots" -> "robotlearn/pyrobolearn"
"erwincoumans/pybullet_robots" -> "oscar-lima/pybullet_ros"
"erwincoumans/pybullet_robots" -> "caelan/pybullet-planning"
"erwincoumans/pybullet_robots" -> "Derek-TH-Wang/quadruped_ctrl" ["e"=1]
"erwincoumans/pybullet_robots" -> "abr/abr_control"
"erwincoumans/pybullet_robots" -> "stepjam/PyRep"
"erwincoumans/pybullet_robots" -> "OpenQuadruped/spot_mini_mini" ["e"=1]
"erwincoumans/pybullet_robots" -> "google-research/ravens"
"erwincoumans/pybullet_robots" -> "jr-robotics/robo-gym"
"go2sea/C51DQN" -> "Kiwoo/distributional_perspective_on_RL"
"jaara/AI-blog" -> "Damcy/prioritized-experience-replay"
"jaara/AI-blog" -> "miyosuda/async_deep_reinforce"
"jaara/AI-blog" -> "pemami4911/deep-rl"
"jaara/AI-blog" -> "muupan/async-rl"
"jaara/AI-blog" -> "alexis-jacq/Pytorch-DPPO"
"jaara/AI-blog" -> "miyosuda/unreal"
"jangirrishabh/toyCarIRL" -> "yrlu/irl-imitation"
"jangirrishabh/toyCarIRL" -> "ahq1993/inverse_rl"
"jangirrishabh/toyCarIRL" -> "reinforcement-learning-kr/lets-do-irl"
"jangirrishabh/toyCarIRL" -> "MatthewJA/Inverse-Reinforcement-Learning"
"jangirrishabh/toyCarIRL" -> "gemst1/IRL"
"jangirrishabh/toyCarIRL" -> "yfzhang/vehicle-motion-forecasting"
"jangirrishabh/toyCarIRL" -> "stormmax/irl-imitation"
"jangirrishabh/toyCarIRL" -> "andrewliao11/gail-tf"
"jangirrishabh/toyCarIRL" -> "sjchoi86/irl_rocks"
"mpatacchiola/dissecting-reinforcement-learning" -> "vmayoral/basic_reinforcement_learning"
"mpatacchiola/dissecting-reinforcement-learning" -> "rlcode/reinforcement-learning"
"mpatacchiola/dissecting-reinforcement-learning" -> "carpedm20/deep-rl-tensorflow"
"mpatacchiola/dissecting-reinforcement-learning" -> "NervanaSystems/coach"
"mpatacchiola/dissecting-reinforcement-learning" -> "sawcordwell/pymdptoolbox" ["e"=1]
"mpatacchiola/dissecting-reinforcement-learning" -> "mimoralea/applied-reinforcement-learning"
"mpatacchiola/dissecting-reinforcement-learning" -> "awjuliani/oreilly-rl-tutorial" ["e"=1]
"mpatacchiola/dissecting-reinforcement-learning" -> "maximecb/gym-minigrid"
"mpatacchiola/dissecting-reinforcement-learning" -> "awjuliani/DeepRL-Agents"
"mpatacchiola/dissecting-reinforcement-learning" -> "muupan/deep-reinforcement-learning-papers"
"mpatacchiola/dissecting-reinforcement-learning" -> "clvrai/awesome-rl-envs"
"mpatacchiola/dissecting-reinforcement-learning" -> "junhyukoh/deep-reinforcement-learning-papers"
"mpatacchiola/dissecting-reinforcement-learning" -> "tychovdo/PacmanDQN"
"mpatacchiola/dissecting-reinforcement-learning" -> "AboudyKreidieh/h-baselines"
"mpatacchiola/dissecting-reinforcement-learning" -> "MrSyee/pg-is-all-you-need"
"ntasfi/PyGame-Learning-Environment" -> "lusob/gym-ple"
"ntasfi/PyGame-Learning-Environment" -> "mgbellemare/Arcade-Learning-Environment"
"ntasfi/PyGame-Learning-Environment" -> "coreylynch/async-rl"
"ntasfi/PyGame-Learning-Environment" -> "muupan/async-rl"
"ntasfi/PyGame-Learning-Environment" -> "NVlabs/GA3C"
"ntasfi/PyGame-Learning-Environment" -> "carpedm20/deep-rl-tensorflow"
"ntasfi/PyGame-Learning-Environment" -> "miyosuda/unreal"
"ntasfi/PyGame-Learning-Environment" -> "Mekire/pygame-samples" ["e"=1]
"ntasfi/PyGame-Learning-Environment" -> "kidscancode/pygame_tutorials" ["e"=1]
"ntasfi/PyGame-Learning-Environment" -> "miyosuda/async_deep_reinforce"
"ntasfi/PyGame-Learning-Environment" -> "spragunr/deep_q_rl"
"ntasfi/PyGame-Learning-Environment" -> "devsisters/DQN-tensorflow"
"ntasfi/PyGame-Learning-Environment" -> "asrivat1/DeepLearningVideoGames"
"ntasfi/PyGame-Learning-Environment" -> "rllab/rllab"
"ntasfi/PyGame-Learning-Environment" -> "rlcode/reinforcement-learning"
"osh/kerlym" -> "sherjilozair/dqn"
"osh/kerlym" -> "coreylynch/async-rl"
"osh/kerlym" -> "rllab/rllab"
"osh/kerlym" -> "datalogai/recurrentshop" ["e"=1]
"stanfordnmbl/osim-rl" -> "opensim-org/opensim-core" ["e"=1]
"stanfordnmbl/osim-rl" -> "ctmakro/stanford-osrl"
"stanfordnmbl/osim-rl" -> "openai/roboschool"
"stanfordnmbl/osim-rl" -> "benelot/pybullet-gym"
"stanfordnmbl/osim-rl" -> "nnaisense/2017-learning-to-run"
"stanfordnmbl/osim-rl" -> "deepmind/dm_control"
"stanfordnmbl/osim-rl" -> "erlerobot/gym-gazebo"
"stanfordnmbl/osim-rl" -> "Scitator/Run-Skeleton-Run"
"stanfordnmbl/osim-rl" -> "pathak22/noreward-rl"
"stanfordnmbl/osim-rl" -> "joschu/modular_rl"
"stanfordnmbl/osim-rl" -> "williamFalcon/DeepRLHacks"
"stanfordnmbl/osim-rl" -> "matthiasplappert/keras-rl"
"stanfordnmbl/osim-rl" -> "rll/rllab"
"stanfordnmbl/osim-rl" -> "pat-coady/trpo"
"stanfordnmbl/osim-rl" -> "maximecb/gym-miniworld"
"YurongYou/rlTORCS" -> "bhanuvikasr/Deep-RL-TORCS"
"YurongYou/rlTORCS" -> "popovicidaniela/Master-Thesis"
"YurongYou/rlTORCS" -> "xinleipan/VirtualtoReal-RL"
"abhisheknaik96/MultiAgentTORCS" -> "madras-simulator/MADRaS"
"abhisheknaik96/MultiAgentTORCS" -> "YurongYou/rlTORCS"
"PKU-TANGENT/nlp-tutorial" -> "Zce1112zslx/ChID_baseline"
"PKU-TANGENT/nlp-tutorial" -> "dqxiu/CaliNet"
"PKU-TANGENT/nlp-tutorial" -> "RunxinXu/Make-Information-Extraction-Great-Again"
"PKU-TANGENT/nlp-tutorial" -> "dqxiu/ICL_PaperList" ["e"=1]
"PKU-TANGENT/nlp-tutorial" -> "chenllliang/MLS"
"PKU-TANGENT/nlp-tutorial" -> "OpenMindClub/awesome-scholarly-productivity" ["e"=1]
"PKU-TANGENT/nlp-tutorial" -> "PKUnlp-icler/SCL-RAI"
"PKU-TANGENT/nlp-tutorial" -> "lancopku/clip-openness"
"PKU-TANGENT/nlp-tutorial" -> "PKU-TANGENT/ConFiguRe"
"PKU-TANGENT/nlp-tutorial" -> "wzh9969/HPT" ["e"=1]
"PKU-TANGENT/nlp-tutorial" -> "reorx/cht-colors"
"PKU-TANGENT/nlp-tutorial" -> "RunxinXu/ChildTuning"
"awjuliani/DeepRL-Agents" -> "carpedm20/deep-rl-tensorflow"
"awjuliani/DeepRL-Agents" -> "devsisters/DQN-tensorflow"
"awjuliani/DeepRL-Agents" -> "matthiasplappert/keras-rl"
"awjuliani/DeepRL-Agents" -> "reinforceio/tensorforce"
"awjuliani/DeepRL-Agents" -> "rlcode/reinforcement-learning"
"awjuliani/DeepRL-Agents" -> "openai/universe-starter-agent"
"awjuliani/DeepRL-Agents" -> "miyosuda/async_deep_reinforce"
"awjuliani/DeepRL-Agents" -> "junhyukoh/deep-reinforcement-learning-papers"
"awjuliani/DeepRL-Agents" -> "openai/baselines"
"awjuliani/DeepRL-Agents" -> "awjuliani/Meta-RL"
"awjuliani/DeepRL-Agents" -> "dennybritz/reinforcement-learning"
"awjuliani/DeepRL-Agents" -> "aikorea/awesome-rl"
"awjuliani/DeepRL-Agents" -> "rll/rllab"
"awjuliani/DeepRL-Agents" -> "coreylynch/async-rl"
"awjuliani/DeepRL-Agents" -> "NVlabs/GA3C"
"kristery/Awesome-Imitation-Learning" -> "apexrl/Imitation-Learning-Paper-Lists"
"kristery/Awesome-Imitation-Learning" -> "Kaixhin/imitation-learning"
"kristery/Awesome-Imitation-Learning" -> "reinforcement-learning-kr/lets-do-irl"
"kristery/Awesome-Imitation-Learning" -> "HumanCompatibleAI/imitation"
"kristery/Awesome-Imitation-Learning" -> "Ericonaldo/ILSwiss"
"kristery/Awesome-Imitation-Learning" -> "kristery/Imitation-Learning-from-Imperfect-Demonstration"
"kristery/Awesome-Imitation-Learning" -> "carla-simulator/imitation-learning" ["e"=1]
"kristery/Awesome-Imitation-Learning" -> "openai/imitation"
"kristery/Awesome-Imitation-Learning" -> "junhyukoh/self-imitation-learning"
"kristery/Awesome-Imitation-Learning" -> "Div99/IQ-Learn"
"kristery/Awesome-Imitation-Learning" -> "stepjam/RLBench"
"kristery/Awesome-Imitation-Learning" -> "KamyarGh/rl_swiss"
"kristery/Awesome-Imitation-Learning" -> "ARISE-Initiative/robomimic"
"kristery/Awesome-Imitation-Learning" -> "Khrylx/PyTorch-RL"
"kristery/Awesome-Imitation-Learning" -> "ermongroup/multiagent-gail"
"HorizonRobotics/alf" -> "HorizonRobotics/SocialRobot"
"HorizonRobotics/alf" -> "twni2016/pomdp-baselines"
"HorizonRobotics/alf" -> "rll-research/url_benchmark"
"HorizonRobotics/alf" -> "JannerM/mbpo"
"HorizonRobotics/alf" -> "rll-research/cic"
"muupan/deep-reinforcement-learning-papers" -> "junhyukoh/deep-reinforcement-learning-papers"
"muupan/deep-reinforcement-learning-papers" -> "muupan/async-rl"
"muupan/deep-reinforcement-learning-papers" -> "spragunr/deep_q_rl"
"muupan/deep-reinforcement-learning-papers" -> "rllab/rllab"
"muupan/deep-reinforcement-learning-papers" -> "carpedm20/deep-rl-tensorflow"
"muupan/deep-reinforcement-learning-papers" -> "tambetm/simple_dqn"
"muupan/deep-reinforcement-learning-papers" -> "miyosuda/async_deep_reinforce"
"muupan/deep-reinforcement-learning-papers" -> "openai/rllab"
"muupan/deep-reinforcement-learning-papers" -> "andrewliao11/Deep-Reinforcement-Learning-Survey"
"muupan/deep-reinforcement-learning-papers" -> "coreylynch/async-rl"
"muupan/deep-reinforcement-learning-papers" -> "nivwusquorum/tensorflow-deepq"
"muupan/deep-reinforcement-learning-papers" -> "matthiasplappert/keras-rl"
"muupan/deep-reinforcement-learning-papers" -> "joschu/modular_rl"
"muupan/deep-reinforcement-learning-papers" -> "devsisters/DQN-tensorflow"
"muupan/deep-reinforcement-learning-papers" -> "williamFalcon/DeepRLHacks"
"denisyarats/drq" -> "facebookresearch/drqv2"
"denisyarats/drq" -> "MishaLaskin/curl"
"denisyarats/drq" -> "denisyarats/dmc2gym"
"denisyarats/drq" -> "MishaLaskin/rad"
"denisyarats/drq" -> "denisyarats/pytorch_sac_ae"
"denisyarats/drq" -> "nicklashansen/dmcontrol-generalization-benchmark"
"denisyarats/drq" -> "facebookresearch/deep_bisim4control"
"denisyarats/drq" -> "denisyarats/pytorch_sac"
"denisyarats/drq" -> "tianheyu927/mopo"
"denisyarats/drq" -> "mila-iqia/spr"
"denisyarats/drq" -> "ramanans1/plan2explore"
"denisyarats/drq" -> "sfujim/BCQ"
"denisyarats/drq" -> "aravindsrinivas/curl_rainbow"
"denisyarats/drq" -> "nicklashansen/policy-adaptation-during-deployment"
"denisyarats/drq" -> "aviralkumar2907/CQL"
"vietnguyen91/Super-mario-bros-A3C-pytorch" -> "vietnguyen91/Flappy-bird-deep-Q-learning-pytorch"
"vietnguyen91/Super-mario-bros-A3C-pytorch" -> "Kautenja/gym-super-mario-bros"
"BY571/Soft-Actor-Critic-and-Extensions" -> "BY571/DQN-Atari-Agents"
"BY571/Soft-Actor-Critic-and-Extensions" -> "pranz24/pytorch-soft-actor-critic"
"BY571/Soft-Actor-Critic-and-Extensions" -> "denisyarats/pytorch_sac"
"BY571/Soft-Actor-Critic-and-Extensions" -> "ku2482/sac-discrete.pytorch"
"BY571/Soft-Actor-Critic-and-Extensions" -> "dongminlee94/deep_rl"
"ctallec/world-models" -> "hardmaru/WorldModelsExperiments"
"ctallec/world-models" -> "Kaixhin/PlaNet"
"ctallec/world-models" -> "AppliedDataSciencePartners/WorldModels"
"ctallec/world-models" -> "MishaLaskin/curl"
"ctallec/world-models" -> "ramanans1/plan2explore"
"ctallec/world-models" -> "google-research/planet"
"ctallec/world-models" -> "google-research/dreamer"
"ctallec/world-models" -> "danijar/dreamer"
"ctallec/world-models" -> "AdeelMufti/WorldModels"
"ctallec/world-models" -> "dylandjian/retro-contest-sonic"
"ctallec/world-models" -> "danijar/dreamerv2"
"ctallec/world-models" -> "higgsfield/Imagination-Augmented-Agents"
"ctallec/world-models" -> "ctallec/pyvarinf" ["e"=1]
"ctallec/world-models" -> "Deepest-Project/WorldModels-A3C"
"ctallec/world-models" -> "zacwellmer/WorldModels" ["e"=1]
"hardmaru/WorldModelsExperiments" -> "ctallec/world-models"
"hardmaru/WorldModelsExperiments" -> "worldmodels/worldmodels.github.io"
"hardmaru/WorldModelsExperiments" -> "AppliedDataSciencePartners/WorldModels"
"hardmaru/WorldModelsExperiments" -> "google-research/planet"
"hardmaru/WorldModelsExperiments" -> "zacwellmer/WorldModels" ["e"=1]
"hardmaru/WorldModelsExperiments" -> "hardmaru/estool" ["e"=1]
"hardmaru/WorldModelsExperiments" -> "Kaixhin/PlaNet"
"hardmaru/WorldModelsExperiments" -> "danijar/dreamer"
"hardmaru/WorldModelsExperiments" -> "danijar/dreamerv2"
"hardmaru/WorldModelsExperiments" -> "dylandjian/retro-contest-sonic"
"hardmaru/WorldModelsExperiments" -> "openai/random-network-distillation"
"hardmaru/WorldModelsExperiments" -> "google-research/dreamer"
"hardmaru/WorldModelsExperiments" -> "AdeelMufti/WorldModels"
"hardmaru/WorldModelsExperiments" -> "hardmaru/pytorch_notebooks" ["e"=1]
"hardmaru/WorldModelsExperiments" -> "openai/large-scale-curiosity"
"openai/phasic-policy-gradient" -> "rraileanu/idaac"
"openai/phasic-policy-gradient" -> "lucidrains/phasic-policy-gradient"
"openai/phasic-policy-gradient" -> "openai/train-procgen"
"openai/phasic-policy-gradient" -> "facebookresearch/drqv2"
"LeeKeyu/sarl_star" -> "Shuijing725/CrowdNav_DSRNN"
"LeeKeyu/sarl_star" -> "ethz-asl/rl-navigation"
"LeeKeyu/sarl_star" -> "ChanganVR/RelationalGraphLearning"
"MattChanTK/gym-maze" -> "MattChanTK/ai-gym"
"MattChanTK/gym-maze" -> "zuoxingdong/mazelab"
"MattChanTK/gym-maze" -> "maximecb/gym-minigrid"
"MattChanTK/gym-maze" -> "maximecb/gym-miniworld"
"MattChanTK/gym-maze" -> "lcswillems/rl-starter-files"
"MattChanTK/gym-maze" -> "mpSchrader/gym-sokoban"
"a7b23/Autonomous-MazePathFinder-using-DQN" -> "Mehdi0xC/PathFinding-Agent-with-Deep-Reinforcement-Learning"
"wwxFromTju/awesome-reinforcement-learning-lib" -> "openrlbenchmark/openrlbenchmark"
"wwxFromTju/awesome-reinforcement-learning-lib" -> "araffin/sbx"
"openai/Video-Pre-Training" -> "minerllabs/minerl"
"openai/Video-Pre-Training" -> "MineDojo/MineDojo"
"openai/Video-Pre-Training" -> "MineDojo/MineCLIP"
"openai/Video-Pre-Training" -> "eloialonso/iris"
"openai/Video-Pre-Training" -> "danijar/dreamerv3"
"openai/Video-Pre-Training" -> "YeWR/EfficientZero"
"openai/Video-Pre-Training" -> "minerllabs/basalt-2022-behavioural-cloning-baseline"
"openai/Video-Pre-Training" -> "danijar/crafter"
"openai/Video-Pre-Training" -> "danijar/dreamerv2"
"openai/Video-Pre-Training" -> "google-research/rliable"
"openai/Video-Pre-Training" -> "kzl/decision-transformer"
"openai/Video-Pre-Training" -> "rll-research/url_benchmark"
"openai/Video-Pre-Training" -> "TeaPearce/Counter-Strike_Behavioural_Cloning"
"openai/Video-Pre-Training" -> "microsoft/malmo"
"openai/Video-Pre-Training" -> "rlworkgroup/metaworld"
"AndrejOrsula/drl_grasping" -> "BarisYazici/deep-rl-grasping"
"AndrejOrsula/drl_grasping" -> "robotology/gym-ignition"
"AndrejOrsula/drl_grasping" -> "andyzeng/visual-pushing-grasping" ["e"=1]
"AndrejOrsula/drl_grasping" -> "PaulDanielML/MuJoCo_RL_UR5"
"AndrejOrsula/drl_grasping" -> "NVlabs/contact_graspnet" ["e"=1]
"AndrejOrsula/drl_grasping" -> "liruiw/GA-DDPG" ["e"=1]
"AndrejOrsula/drl_grasping" -> "jr-robotics/robo-gym"
"AndrejOrsula/drl_grasping" -> "jhu-lcsr/good_robot"
"AndrejOrsula/drl_grasping" -> "skumra/robotic-grasping" ["e"=1]
"eleramp/pybullet-object-models" -> "harvard-microrobotics/object2urdf"
"hhexiy/opponent" -> "alshedivat/lola"
"hhexiy/opponent" -> "alexis-jacq/LOLA_DiCE"
"ehrenbrav/DeepQNetwork" -> "ehrenbrav/FCEUX_Learning_Environment"
"ehrenbrav/DeepQNetwork" -> "ppaquette/gym-super-mario"
"ehrenbrav/DeepQNetwork" -> "xushsh163/A3CSuperMario_Windows"
"ppaquette/gym-super-mario" -> "Kautenja/gym-super-mario-bros"
"ppaquette/gym-super-mario" -> "llSourcell/deep_q_learning"
"ppaquette/gym-super-mario" -> "ehrenbrav/DeepQNetwork"
"ppaquette/gym-super-mario" -> "ppaquette/gym-doom"
"ppaquette/gym-super-mario" -> "zuoxingdong/gym-maze"
"ppaquette/gym-super-mario" -> "robmsylvester/Super-Mario-Bros-DQN"
"ppaquette/gym-super-mario" -> "ppaquette/gym-pull"
"ppaquette/gym-super-mario" -> "openai/large-scale-curiosity"
"mahyaret/kuka_rl" -> "mahyaret/gym-panda"
"mahyaret/kuka_rl" -> "borninfreedom/kuka-reach-drl"
"qgallouedec/panda-gym" -> "jr-robotics/robo-gym"
"qgallouedec/panda-gym" -> "PaulDanielML/MuJoCo_RL_UR5"
"qgallouedec/panda-gym" -> "mahyaret/gym-panda"
"qgallouedec/panda-gym" -> "google-research/ravens"
"qgallouedec/panda-gym" -> "BarisYazici/deep-rl-grasping"
"qgallouedec/panda-gym" -> "openai/robogym"
"qgallouedec/panda-gym" -> "wangcongrobot/awesome-isaac-gym"
"qgallouedec/panda-gym" -> "ir413/mvp"
"qgallouedec/panda-gym" -> "caelan/pybullet-planning"
"qgallouedec/panda-gym" -> "borninfreedom/kuka-reach-drl"
"qgallouedec/panda-gym" -> "TianhongDai/hindsight-experience-replay"
"qgallouedec/panda-gym" -> "IanYangChina/pybullet_multigoal_gym"
"qgallouedec/panda-gym" -> "stepjam/RLBench"
"qgallouedec/panda-gym" -> "NVIDIA-Omniverse/IsaacGymEnvs"
"qgallouedec/panda-gym" -> "Stable-Baselines-Team/stable-baselines3-contrib"
"caelan/pddlstream" -> "caelan/pybullet-planning"
"caelan/pddlstream" -> "caelan/SS-Replan"
"caelan/pddlstream" -> "caelan/motion-planners"
"caelan/pddlstream" -> "caelan/LTAMP"
"caelan/pddlstream" -> "tomsilver/pddlgym" ["e"=1]
"caelan/pddlstream" -> "MarcToussaint/18-RSS-PhysicalManipulation"
"caelan/pddlstream" -> "yijiangh/pybullet_planning"
"caelan/pddlstream" -> "zi-w/Kitchen2D"
"caelan/pddlstream" -> "aibasel/pyperplan" ["e"=1]
"cliport/cliport" -> "google-research/ravens"
"cliport/cliport" -> "mees/calvin"
"cliport/cliport" -> "peract/peract"
"cliport/cliport" -> "vimalabs/VIMA"
"cliport/cliport" -> "ir413/mvp"
"cliport/cliport" -> "facebookresearch/r3m"
"cliport/cliport" -> "haosulab/ManiSkill2"
"cliport/cliport" -> "google-research/language-table"
"cliport/cliport" -> "columbia-ai-robotics/diffusion_policy"
"cliport/cliport" -> "ARISE-Initiative/robomimic"
"cliport/cliport" -> "allenai/manipulathor" ["e"=1]
"cliport/cliport" -> "vimalabs/VIMABench"
"cliport/cliport" -> "stepjam/ARM"
"cliport/cliport" -> "GT-RIPL/Awesome-LLM-Robotics"
"cliport/cliport" -> "PKU-MARL/DexterousHands"
"google-research/ravens" -> "DanielTakeshi/deformable-ravens"
"google-research/ravens" -> "cliport/cliport"
"google-research/ravens" -> "stepjam/RLBench"
"google-research/ravens" -> "haosulab/ManiSkill2"
"google-research/ravens" -> "peract/peract"
"google-research/ravens" -> "google-research/ibc"
"google-research/ravens" -> "ARISE-Initiative/robosuite"
"google-research/ravens" -> "qgallouedec/panda-gym"
"google-research/ravens" -> "facebookresearch/tacto"
"google-research/ravens" -> "BarisYazici/deep-rl-grasping"
"google-research/ravens" -> "stepjam/ARM"
"google-research/ravens" -> "NVIDIA-Omniverse/OmniIsaacGymEnvs"
"google-research/ravens" -> "columbia-ai-robotics/diffusion_policy"
"google-research/ravens" -> "vimalabs/VIMA"
"google-research/ravens" -> "mees/calvin"
"mfe7/cadrl_ros" -> "vita-epfl/CrowdNav"
"mfe7/cadrl_ros" -> "sybrenstuvel/Python-RVO2"
"mfe7/cadrl_ros" -> "gkahn13/gcg"
"mfe7/cadrl_ros" -> "ChanganVR/CADRL"
"mfe7/cadrl_ros" -> "ChanganVR/RelationalGraphLearning"
"mfe7/cadrl_ros" -> "srl-freiburg/pedsim_ros"
"mfe7/cadrl_ros" -> "MengGuo/RVO_Py_MAS"
"mfe7/cadrl_ros" -> "RoblabWh/RobLearn"
"mfe7/cadrl_ros" -> "Acmece/rl-collision-avoidance"
"mfe7/cadrl_ros" -> "marooncn/navbot"
"mfe7/cadrl_ros" -> "RGring/drl_local_planner_ros_stable_baselines"
"ugurkanates/awesome-real-world-rl" -> "seungjaeryanlee/awesome-rl-competitions"
"ugurkanates/awesome-real-world-rl" -> "araffin/srl-zoo"
"ugurkanates/awesome-real-world-rl" -> "araffin/robotics-rl-srl"
"ugurkanates/awesome-real-world-rl" -> "jr-robotics/robo-gym"
"ugurkanates/awesome-real-world-rl" -> "bennylp/RL-Taxonomy"
"ugurkanates/awesome-real-world-rl" -> "opherlieber/rltime"
"ugurkanates/awesome-real-world-rl" -> "rail-berkeley/d4rl"
"ugurkanates/awesome-real-world-rl" -> "clvrai/awesome-rl-envs"
"Observerspy/CS294" -> "Observerspy/CS234"
"Observerspy/CS294" -> "xuwd11/cs294-112_hws"
"aviralkumar2907/CQL" -> "sfujim/BCQ"
"aviralkumar2907/CQL" -> "aviralkumar2907/BEAR"
"aviralkumar2907/CQL" -> "rail-berkeley/d4rl_evaluations"
"aviralkumar2907/CQL" -> "young-geng/CQL"
"aviralkumar2907/CQL" -> "sfujim/TD3_BC"
"aviralkumar2907/CQL" -> "rail-berkeley/d4rl"
"aviralkumar2907/CQL" -> "google-research/batch_rl"
"aviralkumar2907/CQL" -> "takuseno/d3rlpy"
"aviralkumar2907/CQL" -> "hanjuku-kaso/awesome-offline-rl"
"aviralkumar2907/CQL" -> "snu-mllab/EDAC"
"aviralkumar2907/CQL" -> "ikostrikov/implicit_q_learning"
"aviralkumar2907/CQL" -> "tianheyu927/mopo"
"aviralkumar2907/CQL" -> "BY571/CQL"
"aviralkumar2907/CQL" -> "polixir/OfflineRL"
"aviralkumar2907/CQL" -> "rail-berkeley/rlkit"
"rail-berkeley/d4rl_evaluations" -> "rail-berkeley/d4rl"
"rail-berkeley/d4rl_evaluations" -> "aviralkumar2907/BEAR"
"rail-berkeley/d4rl_evaluations" -> "aviralkumar2907/CQL"
"rail-berkeley/d4rl_evaluations" -> "takuseno/d4rl-pybullet"
"sfujim/BCQ" -> "aviralkumar2907/CQL"
"sfujim/BCQ" -> "aviralkumar2907/BEAR"
"sfujim/BCQ" -> "rail-berkeley/d4rl"
"sfujim/BCQ" -> "google-research/batch_rl"
"sfujim/BCQ" -> "sfujim/TD3_BC"
"sfujim/BCQ" -> "hanjuku-kaso/awesome-offline-rl"
"sfujim/BCQ" -> "sfujim/TD3"
"sfujim/BCQ" -> "takuseno/d3rlpy"
"sfujim/BCQ" -> "rail-berkeley/d4rl_evaluations"
"sfujim/BCQ" -> "JannerM/mbpo"
"sfujim/BCQ" -> "tianheyu927/mopo"
"sfujim/BCQ" -> "denisyarats/drq"
"sfujim/BCQ" -> "vitchyr/rlkit"
"sfujim/BCQ" -> "katerakelly/oyster"
"sfujim/BCQ" -> "rlworkgroup/metaworld"
"MishaLaskin/rad" -> "MishaLaskin/curl"
"MishaLaskin/rad" -> "denisyarats/drq"
"MishaLaskin/rad" -> "denisyarats/dmc2gym"
"MishaLaskin/rad" -> "facebookresearch/drqv2"
"MishaLaskin/rad" -> "denisyarats/pytorch_sac_ae"
"MishaLaskin/rad" -> "denisyarats/pytorch_sac"
"MishaLaskin/rad" -> "nicklashansen/dmcontrol-generalization-benchmark"
"MishaLaskin/rad" -> "ramanans1/plan2explore"
"MishaLaskin/rad" -> "danijar/dreamer"
"MishaLaskin/rad" -> "nicklashansen/policy-adaptation-during-deployment"
"MishaLaskin/rad" -> "rraileanu/auto-drac"
"MishaLaskin/rad" -> "aravindsrinivas/curl_rainbow"
"MishaLaskin/rad" -> "facebookresearch/deep_bisim4control"
"MishaLaskin/rad" -> "google-research/dreamer"
"MishaLaskin/rad" -> "openai/phasic-policy-gradient"
"Stable-Baselines-Team/stable-baselines3-contrib" -> "DLR-RM/rl-baselines3-zoo"
"Stable-Baselines-Team/stable-baselines3-contrib" -> "Stable-Baselines-Team/rl-colab-notebooks"
"Stable-Baselines-Team/stable-baselines3-contrib" -> "qgallouedec/panda-gym"
"Stable-Baselines-Team/stable-baselines3-contrib" -> "takuseno/d4rl-pybullet"
"Stable-Baselines-Team/stable-baselines3-contrib" -> "Stable-Baselines-Team/stable-baselines"
"Stable-Baselines-Team/stable-baselines3-contrib" -> "openrlbenchmark/openrlbenchmark"
"Stable-Baselines-Team/stable-baselines3-contrib" -> "araffin/rl-tutorial-jnrr19"
"Stable-Baselines-Team/stable-baselines3-contrib" -> "HumanCompatibleAI/imitation"
"Stable-Baselines-Team/stable-baselines3-contrib" -> "Farama-Foundation/SuperSuit"
"Zeta36/muzero" -> "johan-gras/MuZero"
"danijar/dreamer" -> "google-research/dreamer"
"danijar/dreamer" -> "danijar/dreamerv2"
"danijar/dreamer" -> "juliusfrost/dreamer-pytorch"
"danijar/dreamer" -> "yusukeurakami/dreamer-pytorch"
"danijar/dreamer" -> "google-research/planet"
"danijar/dreamer" -> "denisyarats/pytorch_sac_ae"
"danijar/dreamer" -> "ramanans1/plan2explore"
"danijar/dreamer" -> "JannerM/mbpo"
"danijar/dreamer" -> "MishaLaskin/curl"
"danijar/dreamer" -> "MishaLaskin/rad"
"danijar/dreamer" -> "denisyarats/pytorch_sac"
"danijar/dreamer" -> "Kaixhin/PlaNet"
"danijar/dreamer" -> "kchua/handful-of-trials"
"danijar/dreamer" -> "rail-berkeley/d4rl"
"danijar/dreamer" -> "rlworkgroup/metaworld"
"instadeepai/Mava" -> "instadeepai/jumanji"
"instadeepai/Mava" -> "instadeepai/catx"
"instadeepai/Mava" -> "adaptive-intelligent-robotics/QDax"
"instadeepai/Mava" -> "uoe-agents/epymarl"
"instadeepai/Mava" -> "sjtu-marl/malib"
"instadeepai/Mava" -> "marlbenchmark/on-policy"
"instadeepai/Mava" -> "oxwhirl/pymarl"
"instadeepai/Mava" -> "PettingZoo-Team/PettingZoo" ["e"=1]
"instadeepai/Mava" -> "RobertTLange/gymnax"
"instadeepai/Mava" -> "deepmind/meltingpot"
"instadeepai/Mava" -> "Farama-Foundation/PettingZoo"
"instadeepai/Mava" -> "oxwhirl/smac"
"instadeepai/Mava" -> "semitable/robotic-warehouse"
"instadeepai/Mava" -> "salesforce/warp-drive"
"instadeepai/Mava" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"jannerm/mbpo" -> "Xingyu-Lin/mbpo_pytorch"
"gkahn13/gcg" -> "mfe7/cadrl_ros"
"Skylark0924/Rofunc" -> "Skylark0924/Reinforcement-Learning-in-Robotics"
"ethz-asl/navrep" -> "mit-acl/rl_collision_avoidance"
"ethz-asl/navrep" -> "ignc-research/arena-rosnav"
"Marqt/ViZDoom" -> "rllab/rllab"
"Marqt/ViZDoom" -> "Ardavans/DSR"
"Marqt/ViZDoom" -> "coreylynch/async-rl"
"Marqt/ViZDoom" -> "miyosuda/async_deep_reinforce"
"Marqt/ViZDoom" -> "muupan/async-rl"
"Marqt/ViZDoom" -> "twitter/torch-autograd" ["e"=1]
"Marqt/ViZDoom" -> "osh/kerlym"
"Marqt/ViZDoom" -> "yueatsprograms/Stochastic_Depth" ["e"=1]
"Denys88/rl_games" -> "NVIDIA-Omniverse/IsaacGymEnvs"
"Denys88/rl_games" -> "NVIDIA-Omniverse/OmniIsaacGymEnvs"
"Denys88/rl_games" -> "wangcongrobot/awesome-isaac-gym"
"Denys88/rl_games" -> "Toni-SM/skrl"
"Denys88/rl_games" -> "PKU-MARL/DexterousHands"
"Denys88/rl_games" -> "leggedrobotics/legged_gym" ["e"=1]
"Denys88/rl_games" -> "nv-tlabs/ASE" ["e"=1]
"Denys88/rl_games" -> "sail-sg/envpool"
"Denys88/rl_games" -> "iamlab-cmu/isaacgym-utils"
"Denys88/rl_games" -> "inspirai/TimeChamber"
"Denys88/rl_games" -> "google/brax"
"Denys88/rl_games" -> "leggedrobotics/rsl_rl" ["e"=1]
"Denys88/rl_games" -> "alex-petrenko/sample-factory"
"Denys88/rl_games" -> "NVIDIA-Omniverse/Orbit"
"Denys88/rl_games" -> "ikostrikov/jaxrl"
"facebookresearch/moolib" -> "facebookresearch/rlmeta"
"facebookresearch/moolib" -> "facebookresearch/minihack"
"facebookresearch/moolib" -> "google-research/rliable"
"facebookresearch/moolib" -> "facebookresearch/torchbeast"
"facebookresearch/moolib" -> "RobertTLange/gymnax"
"facebookresearch/moolib" -> "ucl-dark/paired"
"facebookresearch/moolib" -> "facebookresearch/rl"
"facebookresearch/rlmeta" -> "facebookresearch/moolib"
"facebookresearch/rlmeta" -> "facebookresearch/mbrl-lib"
"facebookresearch/rlmeta" -> "facebookresearch/rl"
"facebookresearch/rlmeta" -> "facebookresearch/minihack"
"clvrai/furniture" -> "stepjam/RLBench"
"clvrai/furniture" -> "ARISE-Initiative/robomimic"
"clvrai/furniture" -> "ARISE-Initiative/robosuite"
"clvrai/furniture" -> "PSVL/DoorGym"
"clvrai/furniture" -> "robotlearn/pyrobolearn"
"clvrai/furniture" -> "mees/calvin"
"clvrai/furniture" -> "google-research/ravens"
"clvrai/furniture" -> "StanfordVL/robosuite"
"clvrai/furniture" -> "rlworkgroup/metaworld"
"clvrai/furniture" -> "clvrai/awesome-rl-envs"
"clvrai/furniture" -> "qgallouedec/panda-gym"
"clvrai/furniture" -> "google-research/relay-policy-learning"
"clvrai/furniture" -> "caelan/pybullet-planning"
"clvrai/furniture" -> "deepmind/mujoco_menagerie"
"clvrai/furniture" -> "columbia-ai-robotics/diffusion_policy"
"duckietown/gym-duckietown" -> "maximecb/gym-miniworld"
"duckietown/gym-duckietown" -> "maximecb/gym-minigrid"
"duckietown/gym-duckietown" -> "eleurent/highway-env" ["e"=1]
"duckietown/gym-duckietown" -> "eleurent/rl-agents" ["e"=1]
"duckietown/gym-duckietown" -> "araffin/learning-to-drive-in-5-minutes" ["e"=1]
"duckietown/gym-duckietown" -> "araffin/robotics-rl-srl"
"duckietown/gym-duckietown" -> "duckietown/Software"
"duckietown/gym-duckietown" -> "cjy1992/gym-carla" ["e"=1]
"duckietown/gym-duckietown" -> "google-research/batch_rl"
"duckietown/gym-duckietown" -> "eleurent/phd-bibliography" ["e"=1]
"duckietown/gym-duckietown" -> "nplan/gym-line-follower"
"duckietown/gym-duckietown" -> "stanfordnmbl/osim-rl"
"duckietown/gym-duckietown" -> "praveen-palanisamy/macad-gym" ["e"=1]
"duckietown/gym-duckietown" -> "benelot/pybullet-gym"
"duckietown/gym-duckietown" -> "thibo73800/metacar" ["e"=1]
"ignc-research/arena-fsm-ego-planner" -> "ignc-research/arena-tools"
"ignc-research/arena-fsm-ego-planner" -> "ignc-research/navsafe-arena"
"ignc-research/arena-fsm-ego-planner" -> "ignc-research/all-in-one-DRL-planner"
"ignc-research/arena-fsm-ego-planner" -> "ignc-research/arena-rosnav-3D"
"adaptive-intelligent-robotics/QDax" -> "instadeepai/catx"
"adaptive-intelligent-robotics/QDax" -> "instadeepai/jumanji"
"adaptive-intelligent-robotics/QDax" -> "icaros-usc/pyribs"
"adaptive-intelligent-robotics/QDax" -> "RobertTLange/evosax"
"adaptive-intelligent-robotics/QDax" -> "instadeepai/Mava"
"adaptive-intelligent-robotics/QDax" -> "instadeepai/fastpbrl"
"adaptive-intelligent-robotics/QDax" -> "icaros-usc/dqd"
"adaptive-intelligent-robotics/QDax" -> "ollenilsson19/QDgym"
"instadeepai/jumanji" -> "instadeepai/catx"
"instadeepai/jumanji" -> "adaptive-intelligent-robotics/QDax"
"instadeepai/jumanji" -> "RobertTLange/gymnax"
"instadeepai/jumanji" -> "instadeepai/Mava"
"instadeepai/jumanji" -> "araffin/sbx"
"instadeepai/jumanji" -> "instadeepai/poppy"
"instadeepai/jumanji" -> "instadeepai/manyfold"
"instadeepai/jumanji" -> "instadeepai/nucleotide-transformer"
"instadeepai/jumanji" -> "luchris429/purejaxrl"
"princeton-nlp/CoFiPruning" -> "RunxinXu/ContrastivePruning"
"princeton-nlp/CoFiPruning" -> "princeton-nlp/TRIME" ["e"=1]
"princeton-nlp/CoFiPruning" -> "sai-prasanna/bert-experiments"
"princeton-nlp/CoFiPruning" -> "WoosukKwon/retraining-free-pruning" ["e"=1]
"facebookresearch/CommNet" -> "iassael/learning-to-communicate"
"openai/requests-for-research" -> "dennybritz/deeplearning-papernotes" ["e"=1]
"openai/requests-for-research" -> "openai/universe"
"openai/requests-for-research" -> "openai/roboschool"
"openai/requests-for-research" -> "openai/improved-gan" ["e"=1]
"openai/requests-for-research" -> "openai/cleverhans" ["e"=1]
"openai/requests-for-research" -> "chiphuyen/tf-stanford-tutorials" ["e"=1]
"openai/requests-for-research" -> "openai/InfoGAN" ["e"=1]
"openai/requests-for-research" -> "deepmind/learning-to-learn"
"openai/requests-for-research" -> "openai/universe-starter-agent"
"openai/requests-for-research" -> "openai/evolution-strategies-starter" ["e"=1]
"openai/requests-for-research" -> "facebookresearch/CommAI-env" ["e"=1]
"openai/requests-for-research" -> "tensorflow/fold"
"openai/requests-for-research" -> "harvardnlp/seq2seq-attn" ["e"=1]
"openai/requests-for-research" -> "openai/rllab"
"openai/requests-for-research" -> "carpedm20/deep-rl-tensorflow"
"openai/rllab" -> "joschu/modular_rl"
"openai/rllab" -> "openai/universe-starter-agent"
"openai/rllab" -> "miyosuda/async_deep_reinforce"
"openai/rllab" -> "shaneshixiang/rllabplusplus"
"openai/rllab" -> "matthiasplappert/keras-rl"
"openai/rllab" -> "coreylynch/async-rl"
"openai/rllab" -> "muupan/async-rl"
"openai/rllab" -> "openai/vime"
"openai/rllab" -> "rmst/ddpg"
"openai/rllab" -> "steveKapturowski/tensorflow-rl"
"openai/rllab" -> "carpedm20/deep-rl-tensorflow"
"openai/rllab" -> "miyosuda/unreal"
"openai/rllab" -> "TheAbhiKumar/tensorflow-value-iteration-networks"
"openai/rllab" -> "openai/imitation"
"openai/rllab" -> "muupan/deep-reinforcement-learning-papers"
"tambetm/gym-minecraft" -> "crowdAI/marLo"
"tambetm/gym-minecraft" -> "microsoft/malmo"
"tambetm/gym-minecraft" -> "vincentberaud/Minecraft-Reinforcement-Learning"
"tambetm/gym-minecraft" -> "minerllabs/minerl"
"tambetm/gym-minecraft" -> "muupan/async-rl"
"tambetm/gym-minecraft" -> "zuoxingdong/gym-maze"
"tambetm/gym-minecraft" -> "pathak22/modular-assemblies"
"wojzaremba/trpo" -> "ilyasu123/trpo"
"google-research/rliable" -> "RobertTLange/gymnax"
"google-research/rliable" -> "rll-research/url_benchmark"
"google-research/rliable" -> "kenjyoung/MinAtar"
"google-research/rliable" -> "rail-berkeley/d4rl"
"google-research/rliable" -> "danijar/dreamerv3"
"google-research/rliable" -> "sail-sg/envpool"
"google-research/rliable" -> "sfujim/TD3_BC"
"google-research/rliable" -> "YeWR/EfficientZero"
"google-research/rliable" -> "takuseno/d3rlpy"
"google-research/rliable" -> "facebookresearch/mbrl-lib"
"google-research/rliable" -> "ikostrikov/jaxrl"
"google-research/rliable" -> "deepmind/mujoco_menagerie"
"google-research/rliable" -> "araffin/sbx"
"google-research/rliable" -> "tinkoff-ai/CORL"
"google-research/rliable" -> "rlworkgroup/metaworld"
"openai/train-procgen" -> "openai/procgen"
"openai/train-procgen" -> "openai/gym3"
"openai/train-procgen" -> "pokaxpoka/netrand"
"openai/train-procgen" -> "openai/phasic-policy-gradient"
"anita-hu/TF2-RL" -> "marload/DeepRL-TensorFlow2"
"anita-hu/TF2-RL" -> "keiohta/tf2rl"
"anita-hu/TF2-RL" -> "RITCHIEHuang/DeepRL_Algorithms"
"anita-hu/TF2-RL" -> "StepNeverStop/RLs"
"anita-hu/TF2-RL" -> "louisnino/RLcode"
"anita-hu/TF2-RL" -> "philtabor/Actor-Critic-Methods-Paper-To-Code"
"anita-hu/TF2-RL" -> "chagmgang/tf2.0_reinforcement_learning"
"kvfrans/parallel-trpo" -> "ilyasu123/trpo"
"mit-acl/rl_collision_avoidance" -> "mit-acl/gym-collision-avoidance"
"mit-acl/rl_collision_avoidance" -> "mit-acl/cadrl_ros"
"mit-acl/rl_collision_avoidance" -> "ethz-asl/navrep"
"berkeleydeeprlcourse/homework_fall2020" -> "xuanlinli17/CS285_Fa19_Deep_Reinforcement_Learning"
"berkeleydeeprlcourse/homework_fall2020" -> "berkeleydeeprlcourse/homework_fall2019"
"berkeleydeeprlcourse/homework_fall2020" -> "mdeib/berkeley-deep-RL-pytorch-solutions"
"berkeleydeeprlcourse/homework_fall2020" -> "erfanMhi/Deep-Reinforcement-Learning-CS285-Pytorch"
"berkeleydeeprlcourse/homework_fall2020" -> "mdeib/berkeley-deep-RL-pytorch-starter"
"berkeleydeeprlcourse/homework_fall2020" -> "berkeleydeeprlcourse/homework_fall2021"
"avisingh599/reward-learning-rl" -> "araffin/robotics-rl-srl"
"avisingh599/reward-learning-rl" -> "rail-berkeley/softlearning"
"avisingh599/reward-learning-rl" -> "SudeepDasari/visual_foresight"
"avisingh599/reward-learning-rl" -> "stepjam/RLBench"
"avisingh599/reward-learning-rl" -> "sharadmv/parasol"
"avisingh599/reward-learning-rl" -> "tianheyu927/mil"
"avisingh599/reward-learning-rl" -> "iclavera/learning_to_adapt"
"avisingh599/reward-learning-rl" -> "andyzeng/visual-pushing-grasping" ["e"=1]
"avisingh599/reward-learning-rl" -> "JanMatas/Rainbow_ddpg"
"avisingh599/reward-learning-rl" -> "ramanans1/plan2explore"
"avisingh599/reward-learning-rl" -> "StanfordVL/robosuite"
"avisingh599/reward-learning-rl" -> "jangirrishabh/Overcoming-exploration-from-demos"
"avisingh599/reward-learning-rl" -> "kindredresearch/SenseAct"
"avisingh599/reward-learning-rl" -> "vitchyr/multiworld"
"avisingh599/reward-learning-rl" -> "robotlearn/pyrobolearn"
"haosulab/ManiSkill-Learn" -> "haosulab/ManiSkill"
"haosulab/ManiSkill-Learn" -> "haosulab/ManiSkill2-Learn"
"ramanans1/plan2explore" -> "yusukeurakami/plan2explore-pytorch"
"ramanans1/plan2explore" -> "danijar/dreamer"
"ramanans1/plan2explore" -> "MishaLaskin/rad"
"ramanans1/plan2explore" -> "google-research/dreamer"
"ramanans1/plan2explore" -> "denisyarats/drq"
"ramanans1/plan2explore" -> "denisyarats/dmc2gym"
"vikashplus/mj_envs" -> "aravindr93/hand_dapg"
"vikashplus/mj_envs" -> "aravindr93/mjrl"
"watchernyu/REDQ" -> "sfujim/TD3_BC"
"amoudgl/short-jokes-dataset" -> "amoudgl/funnybot"
"amoudgl/short-jokes-dataset" -> "taivop/joke-dataset"
"amoudgl/short-jokes-dataset" -> "CrowdTruth/Short-Text-Corpus-For-Humor-Detection"
"transedward/pytorch-dqn" -> "dxyang/DQN_pytorch"
"transedward/pytorch-dqn" -> "jingweiz/pytorch-rl"
"transedward/pytorch-dqn" -> "ikostrikov/pytorch-a3c"
"transedward/pytorch-dqn" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"transedward/pytorch-dqn" -> "dgriff777/rl_a3c_pytorch"
"transedward/pytorch-dqn" -> "ikostrikov/pytorch-trpo"
"transedward/pytorch-dqn" -> "AndersonJo/dqn-pytorch"
"transedward/pytorch-dqn" -> "ghliu/pytorch-ddpg"
"transedward/pytorch-dqn" -> "berkeleydeeprlcourse/homework"
"alexis-jacq/Pytorch-DPPO" -> "TianhongDai/distributed-ppo"
"deepmind/dqn_zoo" -> "deepmind/rlax" ["e"=1]
"deepmind/dqn_zoo" -> "deepmind/reverb"
"deepmind/dqn_zoo" -> "RobertTLange/gymnax"
"deepmind/dqn_zoo" -> "sail-sg/envpool"
"deepmind/dqn_zoo" -> "google-research/seed_rl"
"deepmind/dqn_zoo" -> "danijar/dreamerv3"
"deepmind/dqn_zoo" -> "deepmind/alphastar" ["e"=1]
"deepmind/dqn_zoo" -> "google-research/rliable"
"deepmind/dqn_zoo" -> "chagmgang/distributed_reinforcement_learning"
"xuanlinli17/CS285_Fa19_Deep_Reinforcement_Learning" -> "berkeleydeeprlcourse/homework_fall2019"
"xuanlinli17/CS285_Fa19_Deep_Reinforcement_Learning" -> "mdeib/berkeley-deep-RL-pytorch-solutions"
"xuanlinli17/CS285_Fa19_Deep_Reinforcement_Learning" -> "erfanMhi/Deep-Reinforcement-Learning-CS285-Pytorch"
"xuwd11/cs294-112_hws" -> "daggertye/CS294_homework"
"MrSyee/pg-is-all-you-need" -> "Curt-Park/rainbow-is-all-you-need"
"MrSyee/pg-is-all-you-need" -> "medipixel/rl_algorithms"
"MrSyee/pg-is-all-you-need" -> "clvrai/awesome-rl-envs"
"MrSyee/pg-is-all-you-need" -> "dongminlee94/deep_rl"
"MrSyee/pg-is-all-you-need" -> "qfettes/DeepRL-Tutorials"
"MrSyee/pg-is-all-you-need" -> "seungeunrho/minimalRL"
"MrSyee/pg-is-all-you-need" -> "vwxyzjn/cleanrl"
"MrSyee/pg-is-all-you-need" -> "seungjaeryanlee/awesome-rl-competitions"
"MrSyee/pg-is-all-you-need" -> "Khrylx/PyTorch-RL"
"MrSyee/pg-is-all-you-need" -> "reinforcement-learning-kr/lets-do-irl"
"MrSyee/pg-is-all-you-need" -> "higgsfield/RL-Adventure"
"MrSyee/pg-is-all-you-need" -> "quantumiracle/Popular-RL-Algorithms"
"MrSyee/pg-is-all-you-need" -> "higgsfield/RL-Adventure-2"
"MrSyee/pg-is-all-you-need" -> "TianhongDai/reinforcement-learning-algorithms"
"MrSyee/pg-is-all-you-need" -> "StepNeverStop/RLs"
"kengz/SLM-Lab" -> "NervanaSystems/coach"
"kengz/SLM-Lab" -> "vitchyr/rlkit"
"kengz/SLM-Lab" -> "deepmind/bsuite"
"kengz/SLM-Lab" -> "rlworkgroup/garage"
"kengz/SLM-Lab" -> "maximecb/gym-minigrid"
"kengz/SLM-Lab" -> "rail-berkeley/softlearning"
"kengz/SLM-Lab" -> "ShangtongZhang/DeepRL"
"kengz/SLM-Lab" -> "kengz/awesome-deep-rl"
"kengz/SLM-Lab" -> "astooke/rlpyt"
"kengz/SLM-Lab" -> "zuoxingdong/lagom"
"kengz/SLM-Lab" -> "higgsfield/RL-Adventure-2"
"kengz/SLM-Lab" -> "Khrylx/PyTorch-RL"
"kengz/SLM-Lab" -> "rll/rllab"
"kengz/SLM-Lab" -> "hill-a/stable-baselines"
"kengz/SLM-Lab" -> "facebookresearch/Horizon"
"AnujMahajanOxf/MAVEN" -> "IouJenLiu/CMAE"
"AnujMahajanOxf/MAVEN" -> "Sonkyunghwan/QTRAN"
"flyyufelix/C51-DDQN-Keras" -> "go2sea/C51DQN"
"flyyufelix/C51-DDQN-Keras" -> "Silvicek/distributional-dqn"
"flyyufelix/C51-DDQN-Keras" -> "floringogianu/categorical-dqn"
"flyyufelix/C51-DDQN-Keras" -> "itaicaspi/keras-dqn-doom"
"jcwleo/random-network-distillation-pytorch" -> "jcwleo/curiosity-driven-exploration-pytorch"
"jcwleo/random-network-distillation-pytorch" -> "openai/random-network-distillation"
"jcwleo/random-network-distillation-pytorch" -> "wizdom13/RND-Pytorch"
"jcwleo/random-network-distillation-pytorch" -> "orrivlin/MountainCar_DQN_RND"
"JannerM/mbpo" -> "kchua/handful-of-trials"
"JannerM/mbpo" -> "WilsonWangTHU/mbbl"
"JannerM/mbpo" -> "quanvuong/handful-of-trials-pytorch"
"JannerM/mbpo" -> "Xingyu-Lin/mbpo_pytorch"
"JannerM/mbpo" -> "tianheyu927/mopo"
"JannerM/mbpo" -> "thanard/me-trpo"
"JannerM/mbpo" -> "aravindr93/mjrl"
"JannerM/mbpo" -> "mcgillmrl/prob_mbrl"
"JannerM/mbpo" -> "iclavera/learning_to_adapt"
"JannerM/mbpo" -> "danijar/dreamer"
"JannerM/mbpo" -> "google-research/pddm"
"JannerM/mbpo" -> "sfujim/BCQ"
"JannerM/mbpo" -> "WilsonWangTHU/POPLIN"
"JannerM/mbpo" -> "nrontsis/PILCO"
"JannerM/mbpo" -> "rail-berkeley/d4rl"
"Stable-Baselines-Team/stable-baselines" -> "Stable-Baselines-Team/stable-baselines3-contrib"
"Stable-Baselines-Team/stable-baselines" -> "araffin/rl-baselines-zoo"
"Stable-Baselines-Team/stable-baselines" -> "Stable-Baselines-Team/rl-colab-notebooks"
"Stable-Baselines-Team/stable-baselines" -> "YuhangSong/Arena-Baselines"
"cts198859/deeprl_network" -> "cts198859/deeprl_signal_control" ["e"=1]
"cts198859/deeprl_network" -> "PKU-AI-Edge/DGN"
"cts198859/deeprl_network" -> "shariqiqbal2810/MAAC"
"cts198859/deeprl_network" -> "minqi/learning-to-communicate-pytorch"
"cts198859/deeprl_network" -> "starry-sky6688/StarCraft"
"cts198859/deeprl_network" -> "starry-sky6688/MADDPG"
"cts198859/deeprl_network" -> "sisl/MADRL"
"cts198859/deeprl_network" -> "LucasAlegre/sumo-rl" ["e"=1]
"cts198859/deeprl_network" -> "xuehy/pytorch-maddpg"
"cts198859/deeprl_network" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"cts198859/deeprl_network" -> "mohammadasghari/dqn-multi-agent-rl"
"cts198859/deeprl_network" -> "mlii/mfrl"
"cts198859/deeprl_network" -> "docwza/sumolights" ["e"=1]
"cts198859/deeprl_network" -> "wingsweihua/colight" ["e"=1]
"cts198859/deeprl_network" -> "marlbenchmark/on-policy"
"stormmax/irl-imitation" -> "stormmax/reinforcement_learning"
"stormmax/irl-imitation" -> "andrewliao11/gail-tf"
"stormmax/irl-imitation" -> "vvanirudh/IRL-Toolkit"
"stormmax/irl-imitation" -> "sjchoi86/irl_rocks"
"Hunter-DDM/knowledge-neurons" -> "dqxiu/CaliNet"
"Hunter-DDM/knowledge-neurons" -> "EleutherAI/knowledge-neurons"
"icaros-usc/pyribs" -> "adaptive-intelligent-robotics/QDax"
"icaros-usc/pyribs" -> "resibots/pymap_elites" ["e"=1]
"icaros-usc/pyribs" -> "icaros-usc/dqd"
"juliusfrost/dreamer-pytorch" -> "yusukeurakami/dreamer-pytorch"
"juliusfrost/dreamer-pytorch" -> "danijar/dreamer"
"juliusfrost/dreamer-pytorch" -> "Kaixhin/PlaNet"
"juliusfrost/dreamer-pytorch" -> "cross32768/Dreamer_PyTorch"
"juliusfrost/dreamer-pytorch" -> "zhaoyi11/dreamer-pytorch"
"juliusfrost/dreamer-pytorch" -> "google-research/dreamer"
"juliusfrost/dreamer-pytorch" -> "jsikyoon/dreamer-torch"
"juliusfrost/dreamer-pytorch" -> "denisyarats/pytorch_sac_ae"
"juliusfrost/dreamer-pytorch" -> "quanvuong/handful-of-trials-pytorch"
"juliusfrost/dreamer-pytorch" -> "Xingyu-Lin/mbpo_pytorch"
"yusukeurakami/dreamer-pytorch" -> "juliusfrost/dreamer-pytorch"
"yusukeurakami/dreamer-pytorch" -> "cross32768/Dreamer_PyTorch"
"yusukeurakami/dreamer-pytorch" -> "jsikyoon/dreamer-torch"
"yusukeurakami/dreamer-pytorch" -> "Kaixhin/PlaNet"
"yusukeurakami/dreamer-pytorch" -> "danijar/dreamer"
"yusukeurakami/dreamer-pytorch" -> "ku2482/slac.pytorch"
"yusukeurakami/dreamer-pytorch" -> "jurgisp/pydreamer"
"yusukeurakami/dreamer-pytorch" -> "zhaoyi11/dreamer-pytorch"
"deepmind/mujoco_mpc" -> "deepmind/mujoco_menagerie"
"deepmind/mujoco_mpc" -> "dojo-sim/Dojo.jl" ["e"=1]
"deepmind/mujoco_mpc" -> "Simple-Robotics/proxsuite" ["e"=1]
"deepmind/mujoco_mpc" -> "tasts-robots/vulp"
"deepmind/mujoco_mpc" -> "google-research/robopianist"
"deepmind/mujoco_mpc" -> "loco-3d/crocoddyl" ["e"=1]
"deepmind/mujoco_mpc" -> "qiayuanliao/legged_control" ["e"=1]
"deepmind/mujoco_mpc" -> "stephane-caron/robot_descriptions.py" ["e"=1]
"deepmind/mujoco_mpc" -> "haosulab/ManiSkill2"
"deepmind/mujoco_mpc" -> "mayataka/robotoc" ["e"=1]
"deepmind/mujoco_mpc" -> "google-research/ibc"
"Observerspy/CS234" -> "Observerspy/CS294"
"Observerspy/CS234" -> "zlpure/CS234"
"typoverflow/UtilsRL" -> "typoverflow/OfflineRL-Lib"
"algorithmdog/Reinforcement_Learning_Blog" -> "songrotek/DDPG"
"algorithmdog/Reinforcement_Learning_Blog" -> "andrewliao11/Deep-Reinforcement-Learning-Survey"
"algorithmdog/Reinforcement_Learning_Blog" -> "songrotek/DRL-FlappyBird"
"mohammadasghari/dqn-multi-agent-rl" -> "ChenglongChen/pytorch-DRL"
"mohammadasghari/dqn-multi-agent-rl" -> "sisl/MADRL"
"mohammadasghari/dqn-multi-agent-rl" -> "cts198859/deeprl_network"
"mohammadasghari/dqn-multi-agent-rl" -> "shkrwnd/Deep-Reinforcement-Learning-for-Dynamic-Spectrum-Access" ["e"=1]
"mohammadasghari/dqn-multi-agent-rl" -> "thesouther/MARL"
"mohammadasghari/dqn-multi-agent-rl" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"mohammadasghari/dqn-multi-agent-rl" -> "minqi/learning-to-communicate-pytorch"
"mohammadasghari/dqn-multi-agent-rl" -> "cyoon1729/Multi-agent-reinforcement-learning"
"mohammadasghari/dqn-multi-agent-rl" -> "xuehy/pytorch-maddpg"
"vietnguyen91/Flappy-bird-deep-Q-learning-pytorch" -> "vietnguyen91/Super-mario-bros-A3C-pytorch"
"dranaju/project" -> "Crawford-fang/turtlebot3_DQN"
"dranaju/project" -> "Crawford-fang/ROS_pytorch_RL"
"ChanganVR/CADRL" -> "vita-epfl/CrowdNav"
"ChanganVR/CADRL" -> "ChanganVR/RelationalGraphLearning"
"RGring/drl_local_planner_ros_stable_baselines" -> "ignc-research/arena-rosnav"
"RGring/drl_local_planner_ros_stable_baselines" -> "ignc-research/arena-rosnav-3D"
"RGring/drl_local_planner_ros_stable_baselines" -> "ethz-asl/navrep"
"RGring/drl_local_planner_ros_stable_baselines" -> "ignc-research/navsafe-arena"
"RITCHIEHuang/DeepRL_Algorithms" -> "anita-hu/TF2-RL"
"RITCHIEHuang/DeepRL_Algorithms" -> "StepNeverStop/RLs"
"RITCHIEHuang/DeepRL_Algorithms" -> "dongminlee94/deep_rl"
"RITCHIEHuang/DeepRL_Algorithms" -> "starry-sky6688/MADDPG"
"RITCHIEHuang/DeepRL_Algorithms" -> "quantumiracle/Popular-RL-Algorithms"
"RITCHIEHuang/DeepRL_Algorithms" -> "iffiX/machin"
"RITCHIEHuang/DeepRL_Algorithms" -> "BY571/DQN-Atari-Agents"
"RITCHIEHuang/DeepRL_Algorithms" -> "TianhongDai/reinforcement-learning-algorithms"
"RITCHIEHuang/DeepRL_Algorithms" -> "BY571/Soft-Actor-Critic-and-Extensions"
"RITCHIEHuang/DeepRL_Algorithms" -> "mlii/mfrl"
"Shuijing725/CrowdNav_DSRNN" -> "Shuijing725/CrowdNav_Prediction_AttnGraph"
"Shuijing725/CrowdNav_DSRNN" -> "ChanganVR/RelationalGraphLearning"
"dhruvramani/Transformers-RL" -> "jerrodparker20/adaptive-transformers-in-rl"
"dhruvramani/Transformers-RL" -> "alantess/gtrxl-torch"
"dhruvramani/Transformers-RL" -> "kevslinger/DTQN"
"dhruvramani/Transformers-RL" -> "yashbonde/Transformer-RL"
"ignc-research/arena-rosnav" -> "ignc-research/arena-rosnav-3D"
"ignc-research/arena-rosnav" -> "ignc-research/arena-tools"
"ignc-research/arena-rosnav" -> "ignc-research/navsafe-arena"
"ignc-research/arena-rosnav" -> "ignc-research/arena-fsm-ego-planner"
"ignc-research/arena-rosnav" -> "ignc-research/all-in-one-DRL-planner"
"ignc-research/arena-rosnav" -> "ignc-research/arena-bench"
"ignc-research/arena-rosnav" -> "RGring/drl_local_planner_ros_stable_baselines"
"ignc-research/arena-rosnav" -> "ethz-asl/navrep"
"ignc-research/navsafe-arena" -> "ignc-research/arena-tools"
"jachiam/cpo" -> "openai/safety-starter-agents"
"jachiam/cpo" -> "openai/safety-gym"
"jachiam/cpo" -> "chauncygu/Safe-Reinforcement-Learning-Baselines"
"jachiam/cpo" -> "ajlangley/cpo-pytorch"
"jachiam/cpo" -> "zbzhu99/Constrained-Decision-Making-Paper-List"
"jachiam/cpo" -> "rcheng805/RL-CBF"
"jachiam/cpo" -> "SapanaChaudhary/PyTorch-CPO"
"jachiam/cpo" -> "befelix/safe_learning"
"jachiam/cpo" -> "AlgTUDelft/WCSAC"
"jachiam/cpo" -> "SvenGronauer/Bullet-Safety-Gym"
"jachiam/cpo" -> "chauncygu/Multi-Agent-Constrained-Policy-Optimisation"
"jachiam/cpo" -> "ikostrikov/pytorch-trpo"
"jachiam/cpo" -> "PKU-MARL/Safe-Policy-Optimization"
"jachiam/cpo" -> "hari-sikchi/safeRL"
"openai/safety-starter-agents" -> "openai/safety-gym"
"openai/safety-starter-agents" -> "chauncygu/Safe-Reinforcement-Learning-Baselines"
"openai/safety-starter-agents" -> "jachiam/cpo"
"openai/safety-starter-agents" -> "utiasDSL/safe-control-gym"
"openai/safety-starter-agents" -> "liuzuxin/safe-mbrl"
"openai/safety-starter-agents" -> "SapanaChaudhary/PyTorch-CPO"
"openai/safety-starter-agents" -> "AgrawalAmey/safe-explorer"
"openai/safety-starter-agents" -> "PKU-MARL/Safe-Policy-Optimization"
"openai/safety-starter-agents" -> "SvenGronauer/Bullet-Safety-Gym"
"openai/safety-starter-agents" -> "AlgTUDelft/WCSAC"
"openai/safety-starter-agents" -> "zbzhu99/Constrained-Decision-Making-Paper-List"
"openai/safety-starter-agents" -> "schroederdewitt/multiagent_mujoco"
"openai/safety-starter-agents" -> "befelix/safe_learning"
"llSourcell/deep_q_learning" -> "ppaquette/gym-super-mario"
"NVlabs/cule" -> "facebookresearch/rela"
"ARISE-Initiative/robomimic" -> "peract/peract"
"ARISE-Initiative/robomimic" -> "stepjam/RLBench"
"ARISE-Initiative/robomimic" -> "ARISE-Initiative/robosuite"
"ARISE-Initiative/robomimic" -> "facebookresearch/r3m"
"ARISE-Initiative/robomimic" -> "clvrai/furniture"
"ARISE-Initiative/robomimic" -> "cliport/cliport"
"ARISE-Initiative/robomimic" -> "ir413/mvp"
"ARISE-Initiative/robomimic" -> "mees/calvin"
"ARISE-Initiative/robomimic" -> "google-research/ravens"
"ARISE-Initiative/robomimic" -> "UT-Austin-RPL/deoxys_control"
"ARISE-Initiative/robomimic" -> "haosulab/ManiSkill2"
"clvrai/spirl" -> "google-research/relay-policy-learning"
"jaybutera/tetrisRL" -> "lusob/gym-tetris"
"mees/calvin" -> "lukashermann/hulc"
"mees/calvin" -> "facebookresearch/r3m"
"mees/calvin" -> "cliport/cliport"
"mees/calvin" -> "stepjam/ARM"
"mees/calvin" -> "peract/peract"
"mees/calvin" -> "google-research/relay-policy-learning"
"mees/calvin" -> "ir413/mvp"
"TianhongDai/hindsight-experience-replay" -> "vitchyr/multiworld"
"TianhongDai/hindsight-experience-replay" -> "mengf1/DHER"
"TianhongDai/hindsight-experience-replay" -> "florensacc/rllab-curriculum"
"TianhongDai/hindsight-experience-replay" -> "qgallouedec/panda-gym"
"TianhongDai/hindsight-experience-replay" -> "ruizhaogit/EnergyBasedPrioritization"
"TianhongDai/hindsight-experience-replay" -> "mengf1/CHER"
"TianhongDai/hindsight-experience-replay" -> "quanvuong/handful-of-trials-pytorch"
"TianhongDai/hindsight-experience-replay" -> "google-research/dads"
"TianhongDai/hindsight-experience-replay" -> "rll-research/url_benchmark"
"TianhongDai/hindsight-experience-replay" -> "TianhongDai/reinforcement-learning-algorithms"
"TianhongDai/hindsight-experience-replay" -> "paulorauber/hpg"
"TianhongDai/hindsight-experience-replay" -> "sfujim/BCQ"
"aviralkumar2907/BEAR" -> "aviralkumar2907/CQL"
"aviralkumar2907/BEAR" -> "sfujim/BCQ"
"aviralkumar2907/BEAR" -> "rail-berkeley/d4rl_evaluations"
"google-research/dads" -> "ben-eysenbach/sac"
"google-research/dads" -> "rll-research/url_benchmark"
"google-research/dads" -> "alirezakazemipour/DIAYN-PyTorch"
"snu-mllab/EDAC" -> "Baichenjia/PBRL"
"DKuan/MADDPG_torch" -> "starry-sky6688/MADDPG"
"DKuan/MADDPG_torch" -> "shariqiqbal2810/maddpg-pytorch"
"DKuan/MADDPG_torch" -> "philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients"
"DKuan/MADDPG_torch" -> "xuehy/pytorch-maddpg"
"DKuan/MADDPG_torch" -> "marlbenchmark/off-policy"
"DKuan/MADDPG_torch" -> "sisl/MADRL"
"DKuan/MADDPG_torch" -> "shariqiqbal2810/MAAC"
"TimeBreaker/MARL-papers-with-code" -> "TimeBreaker/MARL-resources-collection"
"TimeBreaker/MARL-papers-with-code" -> "TimeBreaker/Multi-Agent-Reinforcement-Learning-papers"
"qqiang00/reinforce" -> "gxnk/reinforcement-learning-code"
"qqiang00/reinforce" -> "borgwang/reinforce_py"
"qqiang00/reinforce" -> "wangshusen/deep-rl"
"berkeleydeeprlcourse/homework_fall2019" -> "xuanlinli17/CS285_Fa19_Deep_Reinforcement_Learning"
"berkeleydeeprlcourse/homework_fall2019" -> "berkeleydeeprlcourse/homework_fall2020"
"berkeleydeeprlcourse/homework_fall2019" -> "silencial/DeepRL"
"berkeleydeeprlcourse/homework_fall2019" -> "berkeleydeeprlcourse/homework"
"berkeleydeeprlcourse/homework_fall2019" -> "xuwd11/cs294-112_hws"
"deepmind/mujoco_menagerie" -> "deepmind/mujoco_mpc"
"deepmind/mujoco_menagerie" -> "NVIDIA-Omniverse/IsaacGymEnvs"
"deepmind/mujoco_menagerie" -> "google-research/robopianist"
"deepmind/mujoco_menagerie" -> "ikostrikov/walk_in_the_park"
"deepmind/mujoco_menagerie" -> "PKU-MARL/DexterousHands"
"deepmind/mujoco_menagerie" -> "kevinzakka/dexterity"
"deepmind/mujoco_menagerie" -> "leggedrobotics/legged_gym" ["e"=1]
"deepmind/mujoco_menagerie" -> "rohanpsingh/mujoco-python-viewer"
"deepmind/mujoco_menagerie" -> "NVIDIA-Omniverse/OmniIsaacGymEnvs"
"deepmind/mujoco_menagerie" -> "ARISE-Initiative/robosuite"
"deepmind/mujoco_menagerie" -> "google/brax"
"deepmind/mujoco_menagerie" -> "NVIDIA-Omniverse/Orbit"
"deepmind/mujoco_menagerie" -> "wangcongrobot/awesome-isaac-gym"
"deepmind/mujoco_menagerie" -> "danijar/dreamerv3"
"deepmind/mujoco_menagerie" -> "nicklashansen/tdmpc"
"kristjankorjus/Replicating-DeepMind" -> "spragunr/deep_q_rl"
"kristjankorjus/Replicating-DeepMind" -> "muupan/dqn-in-the-caffe"
"kristjankorjus/Replicating-DeepMind" -> "kuz/DeepMind-Atari-Deep-Q-Learner"
"kristjankorjus/Replicating-DeepMind" -> "brian473/neural_rl"
"kristjankorjus/Replicating-DeepMind" -> "shawntan/neural-turing-machines" ["e"=1]
"kristjankorjus/Replicating-DeepMind" -> "TorontoDeepLearning/convnet" ["e"=1]
"kristjankorjus/Replicating-DeepMind" -> "nivwusquorum/tensorflow-deepq"
"kristjankorjus/Replicating-DeepMind" -> "tambetm/simple_dqn"
"kristjankorjus/Replicating-DeepMind" -> "facebook/fbcunn" ["e"=1]
"kristjankorjus/Replicating-DeepMind" -> "jbornschein/draw" ["e"=1]
"kristjankorjus/Replicating-DeepMind" -> "facebook/iTorch" ["e"=1]
"kristjankorjus/Replicating-DeepMind" -> "muupan/deep-reinforcement-learning-papers"
"kristjankorjus/Replicating-DeepMind" -> "benanne/kaggle-ndsb" ["e"=1]
"kristjankorjus/Replicating-DeepMind" -> "wojciechz/learning_to_execute" ["e"=1]
"kristjankorjus/Replicating-DeepMind" -> "benanne/kaggle-galaxies" ["e"=1]
"MushroomRL/mushroom-rl" -> "cpnota/autonomous-learning-library"
"MushroomRL/mushroom-rl" -> "IntelLabs/coach"
"MushroomRL/mushroom-rl" -> "facebookresearch/mbrl-lib"
"MushroomRL/mushroom-rl" -> "clvrai/awesome-rl-envs"
"MushroomRL/mushroom-rl" -> "maximecb/gym-minigrid"
"MushroomRL/mushroom-rl" -> "utiasDSL/safe-control-gym"
"MushroomRL/mushroom-rl" -> "deepmind/bsuite"
"MushroomRL/mushroom-rl" -> "takuseno/d3rlpy"
"MushroomRL/mushroom-rl" -> "learnables/cherry"
"MushroomRL/mushroom-rl" -> "rlworkgroup/garage"
"MushroomRL/mushroom-rl" -> "google-research/realworldrl_suite"
"MushroomRL/mushroom-rl" -> "kengz/awesome-deep-rl"
"MushroomRL/mushroom-rl" -> "fabiopardo/tonic"
"MushroomRL/mushroom-rl" -> "JannerM/mbpo"
"MushroomRL/mushroom-rl" -> "aravindr93/mjrl"
"ikostrikov/pytorch-a2c-ppo-acktr" -> "ikostrikov/pytorch-a3c"
"ikostrikov/pytorch-a2c-ppo-acktr" -> "jingweiz/pytorch-rl"
"ikostrikov/pytorch-a2c-ppo-acktr" -> "williamFalcon/DeepRLHacks"
"ikostrikov/pytorch-a2c-ppo-acktr" -> "Kaixhin/Rainbow"
"ikostrikov/pytorch-a2c-ppo-acktr" -> "vitchyr/rlkit"
"ikostrikov/pytorch-a2c-ppo-acktr" -> "ikostrikov/pytorch-trpo"
"ikostrikov/pytorch-a2c-ppo-acktr" -> "ShangtongZhang/DeepRL"
"ikostrikov/pytorch-a2c-ppo-acktr" -> "dgriff777/rl_a3c_pytorch"
"ikostrikov/pytorch-a2c-ppo-acktr" -> "rll/rllab"
"ikostrikov/pytorch-a2c-ppo-acktr" -> "pathak22/noreward-rl"
"ikostrikov/pytorch-a2c-ppo-acktr" -> "joschu/modular_rl"
"ikostrikov/pytorch-a2c-ppo-acktr" -> "facebookresearch/ELF"
"ikostrikov/pytorch-a2c-ppo-acktr" -> "lanpa/tensorboard-pytorch" ["e"=1]
"ikostrikov/pytorch-a2c-ppo-acktr" -> "higgsfield/RL-Adventure-2"
"ikostrikov/pytorch-a2c-ppo-acktr" -> "reinforceio/tensorforce"
"facebookresearch/salina" -> "google-research/rliable"
"facebookresearch/salina" -> "rll-research/url_benchmark"
"facebookresearch/salina" -> "rll-research/cic"
"facebookresearch/salina" -> "mle-infrastructure/mle-hyperopt"
"mlfoundations/task_vectors" -> "mlfoundations/patching"
"mlfoundations/task_vectors" -> "naver-ai/coco-annotation-tool" ["e"=1]
"yihaosun1124/OfflineRL-Kit" -> "typoverflow/OfflineRL-Lib"
"yihaosun1124/OfflineRL-Kit" -> "yihaosun1124/pytorch-mopo"
"sisl/gail-driver" -> "sisl/ngsim_env" ["e"=1]
"PSVL/DoorGym" -> "MiguelARD/DoorDetect-Dataset"
"SudeepDasari/RoboNet" -> "SudeepDasari/visual_foresight"
"SudeepDasari/RoboNet" -> "PSVL/DoorGym"
"nicklashansen/tdmpc" -> "facebookresearch/modem"
"nicklashansen/tdmpc" -> "nicklashansen/dmcontrol-generalization-benchmark"
"PKU-MARL/DexterousHands" -> "NVIDIA-Omniverse/IsaacGymEnvs"
"PKU-MARL/DexterousHands" -> "Denys88/rl_games"
"PKU-MARL/DexterousHands" -> "schroederdewitt/multiagent_mujoco"
"PKU-MARL/DexterousHands" -> "wangcongrobot/awesome-isaac-gym"
"PKU-MARL/DexterousHands" -> "NVIDIA-Omniverse/OmniIsaacGymEnvs"
"PKU-MARL/DexterousHands" -> "NVIDIA-Omniverse/Orbit"
"PKU-MARL/DexterousHands" -> "iamlab-cmu/isaacgym-utils"
"PKU-MARL/DexterousHands" -> "yzqin/dexmv-sim"
"PKU-MARL/DexterousHands" -> "deepmind/mujoco_menagerie"
"PKU-MARL/DexterousHands" -> "Toni-SM/skrl"
"PKU-MARL/DexterousHands" -> "haosulab/ManiSkill2"
"PKU-MARL/DexterousHands" -> "vimalabs/VIMA"
"PKU-MARL/DexterousHands" -> "caelan/pybullet-planning"
"PKU-MARL/DexterousHands" -> "sail-sg/envpool"
"PKU-MARL/DexterousHands" -> "kevinzakka/dexterity"
"young-geng/CQL" -> "young-geng/JaxCQL"
"young-geng/CQL" -> "snu-mllab/EDAC"
"young-geng/CQL" -> "ikostrikov/implicit_q_learning"
"young-geng/CQL" -> "aviralkumar2907/CQL"
"Bam4d/Griddly" -> "facebookresearch/dcd"
"Bam4d/Griddly" -> "ArnaudFickinger/gym-multigrid"
"Bam4d/Griddly" -> "facebookresearch/minihack"
"Bam4d/Griddly" -> "vwxyzjn/gym-microrts" ["e"=1]
"schaul/py-vgdl" -> "rubenvereecken/py-vgdl"
"schaul/py-vgdl" -> "GAIGResearch/GVGAI"
"schaul/py-vgdl" -> "EssexUniversityMCTS/gvgai"
"Farama-Foundation/SuperSuit" -> "Farama-Foundation/PettingZoo"
"lucidrains/dreamerv3-pytorch" -> "danijar/dreamerv3"
"lucidrains/dreamerv3-pytorch" -> "NM512/dreamerv3-torch"
"lucidrains/dreamerv3-pytorch" -> "jurgisp/pydreamer"
"dalmia/David-Silver-Reinforcement-learning" -> "enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning"
"dalmia/David-Silver-Reinforcement-learning" -> "sudharsan13296/Hands-On-Reinforcement-Learning-With-Python"
"dalmia/David-Silver-Reinforcement-learning" -> "qfettes/DeepRL-Tutorials"
"dalmia/David-Silver-Reinforcement-learning" -> "omerbsezer/Reinforcement_learning_tutorial_with_demo"
"dalmia/David-Silver-Reinforcement-learning" -> "udacity/deep-reinforcement-learning"
"dalmia/David-Silver-Reinforcement-learning" -> "rlworkgroup/garage"
"dalmia/David-Silver-Reinforcement-learning" -> "simoninithomas/Deep_reinforcement_learning_Course"
"dalmia/David-Silver-Reinforcement-learning" -> "astooke/rlpyt"
"dalmia/David-Silver-Reinforcement-learning" -> "LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions"
"dalmia/David-Silver-Reinforcement-learning" -> "dennybritz/reinforcement-learning"
"dalmia/David-Silver-Reinforcement-learning" -> "andri27-ts/Reinforcement-Learning"
"dalmia/David-Silver-Reinforcement-learning" -> "higgsfield/RL-Adventure"
"dalmia/David-Silver-Reinforcement-learning" -> "rlcode/reinforcement-learning"
"dalmia/David-Silver-Reinforcement-learning" -> "keras-rl/keras-rl"
"dalmia/David-Silver-Reinforcement-learning" -> "MrSyee/pg-is-all-you-need"
"chenhongge/StateAdvDRL" -> "huanzhang12/ATLA_robust_RL"
"chenhongge/StateAdvDRL" -> "chenhongge/SA_DQN"
"chenhongge/StateAdvDRL" -> "tuomaso/radial_rl"
"chenhongge/StateAdvDRL" -> "lan-lc/adversarial_example_of_Go"
"chenhongge/StateAdvDRL" -> "tuomaso/radial_rl_v2"
"Kautenja/nes-py" -> "Kautenja/gym-super-mario-bros"
"bulletphysics/pybullet_robots" -> "erwincoumans/pybullet_robots"
"bulletphysics/pybullet_robots" -> "robotology-playground/pybullet-robot-envs"
"pemami4911/deep-rl" -> "liampetti/DDPG"
"pemami4911/deep-rl" -> "stevenpjg/ddpg-aigym"
"pemami4911/deep-rl" -> "songrotek/DDPG"
"pemami4911/deep-rl" -> "yanpanlau/DDPG-Keras-Torcs"
"pemami4911/deep-rl" -> "pat-coady/trpo"
"pemami4911/deep-rl" -> "MG2033/A2C"
"pemami4911/deep-rl" -> "yukezhu/tensorflow-reinforce"
"pemami4911/deep-rl" -> "jaara/AI-blog"
"pemami4911/deep-rl" -> "rmst/ddpg"
"pemami4911/deep-rl" -> "miyosuda/async_deep_reinforce"
"pemami4911/deep-rl" -> "Damcy/prioritized-experience-replay"
"pemami4911/deep-rl" -> "MOCR/DDPG"
"WuTheFWasThat/hanabi.rs" -> "Quuxplusone/Hanabi"
"coreylynch/async-rl" -> "miyosuda/async_deep_reinforce"
"coreylynch/async-rl" -> "muupan/async-rl"
"coreylynch/async-rl" -> "matthiasplappert/keras-rl"
"coreylynch/async-rl" -> "nivwusquorum/tensorflow-deepq"
"coreylynch/async-rl" -> "carpedm20/deep-rl-tensorflow"
"coreylynch/async-rl" -> "rllab/rllab"
"coreylynch/async-rl" -> "openai/universe-starter-agent"
"coreylynch/async-rl" -> "osh/kerlym"
"coreylynch/async-rl" -> "joschu/modular_rl"
"coreylynch/async-rl" -> "NVlabs/GA3C"
"coreylynch/async-rl" -> "tambetm/simple_dqn"
"coreylynch/async-rl" -> "spragunr/deep_q_rl"
"coreylynch/async-rl" -> "openai/rllab"
"coreylynch/async-rl" -> "miyosuda/unreal"
"coreylynch/async-rl" -> "devsisters/DQN-tensorflow"
"rlcode/per" -> "alirezamika/evostra" ["e"=1]
"rlcode/per" -> "Damcy/prioritized-experience-replay"
"rlcode/per" -> "pranz24/pytorch-soft-actor-critic"
"rlcode/per" -> "cardwing/Codes-for-RL-PER"
"rlcode/per" -> "qfettes/DeepRL-Tutorials"
"rlcode/per" -> "ku2482/soft-actor-critic.pytorch"
"rlcode/per" -> "ghliu/pytorch-ddpg"
"rlcode/per" -> "BY571/Soft-Actor-Critic-and-Extensions"
"google-research/ibc" -> "kevinzakka/ibc"
"google-research/ibc" -> "google-research/ravens"
"google-research/ibc" -> "deepmind/mujoco_mpc"
"google-research/ibc" -> "mees/calvin"
"google-research/ibc" -> "kevinzakka/dexterity"
"ikostrikov/pytorch-ddpg-naf" -> "ghliu/pytorch-ddpg"
"ikostrikov/pytorch-ddpg-naf" -> "dgriff777/rl_a3c_pytorch"
"ikostrikov/pytorch-ddpg-naf" -> "ikostrikov/pytorch-trpo"
"ikostrikov/pytorch-ddpg-naf" -> "jcwleo/curiosity-driven-exploration-pytorch"
"ikostrikov/pytorch-ddpg-naf" -> "vy007vikas/PyTorch-ActorCriticRL"
"Farama-Foundation/D4RL" -> "Farama-Foundation/D4RL-Evaluations"
"Farama-Foundation/D4RL" -> "tinkoff-ai/CORL"
"Farama-Foundation/D4RL" -> "takuseno/d3rlpy"
"Farama-Foundation/D4RL" -> "aviralkumar2907/CQL"
"Farama-Foundation/D4RL" -> "jannerm/trajectory-transformer"
"Farama-Foundation/D4RL" -> "Farama-Foundation/Gymnasium-Robotics"
"Farama-Foundation/D4RL" -> "rll-research/url_benchmark"
"Farama-Foundation/D4RL" -> "dmksjfl/MCQ"
"Farama-Foundation/D4RL" -> "Farama-Foundation/Metaworld"
"Farama-Foundation/D4RL" -> "anuragajay/decision-diffuser"
"Farama-Foundation/D4RL" -> "hanjuku-kaso/awesome-offline-rl"
"Farama-Foundation/D4RL" -> "jannerm/diffuser"
"Farama-Foundation/D4RL" -> "Farama-Foundation/Minari"
"Farama-Foundation/D4RL" -> "denisyarats/exorl"
"Farama-Foundation/D4RL" -> "columbia-ai-robotics/diffusion_policy"
"inspirai/TimeChamber" -> "heyuanYao-pku/Control-VAE" ["e"=1]
"openai/robosumo" -> "openai/EPG"
"openai/robosumo" -> "openai/multiagent-competition"
"openai/robosumo" -> "cbfinn/maml_rl" ["e"=1]
"openai/robosumo" -> "junhyukoh/value-prediction-network"
"openai/robosumo" -> "openai/mlsh"
"openai/robosumo" -> "junhyukoh/self-imitation-learning"
"openai/robosumo" -> "openai/vime"
"openai/robosumo" -> "iclavera/learning_to_adapt"
"openai/robosumo" -> "openai/doom-py"
"openai/robosumo" -> "jonasrothfuss/ProMP"
"openai/robosumo" -> "pathak22/modular-assemblies"
"openai/robosumo" -> "alshedivat/lola"
"NVIDIA-Omniverse/OmniIsaacGymEnvs" -> "NVIDIA-Omniverse/IsaacGymEnvs"
"NVIDIA-Omniverse/OmniIsaacGymEnvs" -> "Denys88/rl_games"
"NVIDIA-Omniverse/OmniIsaacGymEnvs" -> "wangcongrobot/awesome-isaac-gym"
"NVIDIA-Omniverse/OmniIsaacGymEnvs" -> "NVIDIA-Omniverse/Orbit"
"NVIDIA-Omniverse/OmniIsaacGymEnvs" -> "Toni-SM/skrl"
"NVIDIA-Omniverse/OmniIsaacGymEnvs" -> "leggedrobotics/legged_gym" ["e"=1]
"NVIDIA-Omniverse/OmniIsaacGymEnvs" -> "PKU-MARL/DexterousHands"
"NVIDIA-Omniverse/OmniIsaacGymEnvs" -> "iamlab-cmu/isaacgym-utils"
"NVIDIA-Omniverse/OmniIsaacGymEnvs" -> "nv-tlabs/ASE" ["e"=1]
"NVIDIA-Omniverse/OmniIsaacGymEnvs" -> "deepmind/mujoco_menagerie"
"NVIDIA-Omniverse/OmniIsaacGymEnvs" -> "google-research/ravens"
"NVIDIA-Omniverse/OmniIsaacGymEnvs" -> "erwincoumans/motion_imitation" ["e"=1]
"NVIDIA-Omniverse/OmniIsaacGymEnvs" -> "Improbable-AI/walk-these-ways" ["e"=1]
"NVIDIA-Omniverse/OmniIsaacGymEnvs" -> "pairlab/leibnizgym"
"NVIDIA-Omniverse/OmniIsaacGymEnvs" -> "cliport/cliport"
"iamlab-cmu/isaacgym-utils" -> "pairlab/leibnizgym"
"yijiangh/pybullet_planning" -> "caelan/pybullet-planning"
"worldmodels/worldmodels.github.io" -> "hardmaru/WorldModelsExperiments"
"worldmodels/worldmodels.github.io" -> "AdeelMufti/WorldModels"
"worldmodels/worldmodels.github.io" -> "google-research/planet"
"worldmodels/worldmodels.github.io" -> "AppliedDataSciencePartners/WorldModels"
"worldmodels/worldmodels.github.io" -> "ctallec/world-models"
"worldmodels/worldmodels.github.io" -> "pathak22/noreward-rl"
"facebookresearch/Horizon" -> "deepmind/trfl"
"facebookresearch/Horizon" -> "NervanaSystems/coach"
"facebookresearch/Horizon" -> "google/dopamine"
"facebookresearch/Horizon" -> "vitchyr/rlkit"
"facebookresearch/Horizon" -> "astooke/rlpyt"
"facebookresearch/Horizon" -> "tensorflow/adanet" ["e"=1]
"facebookresearch/Horizon" -> "reinforceio/tensorforce"
"facebookresearch/Horizon" -> "rll/rllab"
"facebookresearch/Horizon" -> "rlworkgroup/garage"
"facebookresearch/Horizon" -> "facebookresearch/pytext" ["e"=1]
"facebookresearch/Horizon" -> "facebookresearch/nevergrad" ["e"=1]
"facebookresearch/Horizon" -> "kengz/SLM-Lab"
"facebookresearch/Horizon" -> "facebookresearch/pythia" ["e"=1]
"facebookresearch/Horizon" -> "ShangtongZhang/DeepRL"
"facebookresearch/Horizon" -> "deepmind/scalable_agent"
"rlgraph/rlgraph" -> "ray-project/rl-experiments"
"rlgraph/rlgraph" -> "PKU-AI-Edge/DGN"
"rlgraph/rlgraph" -> "tegg89/magnet"
"xbpeng/DeepTerrainRL" -> "xbpeng/DeepLoco"
"xbpeng/DeepTerrainRL" -> "xbpeng/DeepMimic" ["e"=1]
"xbpeng/DeepTerrainRL" -> "songrotek/DDPG"
"xbpeng/DeepTerrainRL" -> "rllab/rllab"
"xbpeng/DeepTerrainRL" -> "miyosuda/async_deep_reinforce"
"xbpeng/DeepTerrainRL" -> "Kaixhin/Atari"
"xbpeng/DeepTerrainRL" -> "akanazawa/motion_reconstruction" ["e"=1]
"xbpeng/DeepTerrainRL" -> "UBCMOCCA/TerrainRLSim"
"xbpeng/DeepTerrainRL" -> "openai/rllab"
"xbpeng/DeepTerrainRL" -> "joschu/modular_rl"
"xbpeng/DeepTerrainRL" -> "xbpeng/awr"
"xbpeng/DeepTerrainRL" -> "miyosuda/unreal"
"wangshusen/AdvancedAlgorithms" -> "wangshusen/RecommenderSystem" ["e"=1]
"wangshusen/AdvancedAlgorithms" -> "wangshusen/DeepLearning"
"wangshusen/AdvancedAlgorithms" -> "stevens-cs546-cs554/CS-554"
"rwightman/pytorch-pommerman-rl" -> "BorealisAI/pommerman-baseline"
"rwightman/pytorch-pommerman-rl" -> "tambetm/pommerman-baselines"
"rwightman/pytorch-pommerman-rl" -> "eugene/pommerman"
"mjacar/pytorch-trpo" -> "ikostrikov/pytorch-trpo"
"facebookresearch/CollaQ" -> "TonghanWang/ROMA"
"facebookresearch/CollaQ" -> "oxwhirl/wqmix"
"facebookresearch/CollaQ" -> "isp1tze/MAProj"
"facebookresearch/CollaQ" -> "TonghanWang/DOP"
"maximecb/gym-miniworld" -> "maximecb/gym-minigrid"
"maximecb/gym-miniworld" -> "kenjyoung/MinAtar"
"maximecb/gym-miniworld" -> "lcswillems/rl-starter-files"
"maximecb/gym-miniworld" -> "openai/procgen"
"maximecb/gym-miniworld" -> "rlworkgroup/metaworld"
"maximecb/gym-miniworld" -> "vitchyr/multiworld"
"maximecb/gym-miniworld" -> "JannerM/mbpo"
"maximecb/gym-miniworld" -> "MattChanTK/gym-maze"
"maximecb/gym-miniworld" -> "rll-research/url_benchmark"
"maximecb/gym-miniworld" -> "rail-berkeley/d4rl"
"maximecb/gym-miniworld" -> "deepmind/bsuite"
"maximecb/gym-miniworld" -> "google-research/realworldrl_suite"
"maximecb/gym-miniworld" -> "duckietown/gym-duckietown"
"maximecb/gym-miniworld" -> "araffin/robotics-rl-srl"
"maximecb/gym-miniworld" -> "clvrai/awesome-rl-envs"
"alex-petrenko/megaverse" -> "shacklettbp/bps-nav"
"5vision/deep-reinforcement-learning-networks" -> "5vision/DARQN"
"5vision/deep-reinforcement-learning-networks" -> "miyosuda/async_deep_reinforce"
"5vision/deep-reinforcement-learning-networks" -> "IntelVCL/DirectFuturePrediction"
"AIRLab-POLIMI/mushroom" -> "MushroomRL/mushroom-rl-benchmark"
"AIRLab-POLIMI/mushroom" -> "zuoxingdong/lagom"
"abr/abr_control" -> "abr/abr_jaco2"
"abr/abr_control" -> "PaulDanielML/MuJoCo_RL_UR5"
"abr/abr_control" -> "studywolf/pydmps" ["e"=1]
"abr/abr_control" -> "stepjam/PyRep"
"abr/abr_control" -> "erwincoumans/pybullet_robots"
"abr/abr_control" -> "caelan/pybullet-planning"
"abr/abr_control" -> "araffin/robotics-rl-srl"
"abr/abr_control" -> "robotlearn/pyrobolearn"
"abr/abr_control" -> "jr-robotics/robo-gym"
"YuMan-Tam/deep-hedging" -> "jsg71/Deep-Hedging"
"YuMan-Tam/deep-hedging" -> "pfnet-research/pfhedge"
"YuMan-Tam/deep-hedging" -> "pfnet-research/NoTransactionBandNetwork"
"YuMan-Tam/deep-hedging" -> "hansbuehler/deephedging"
"YuMan-Tam/deep-hedging" -> "tdmdal/rl-hedge-2019"
"robotology/gym-ignition" -> "ignitionrobotics/ign-gazebo"
"robotology/gym-ignition" -> "AndrejOrsula/drl_grasping"
"robotology/gym-ignition" -> "ignitionrobotics/ign-physics"
"robotology/gym-ignition" -> "jr-robotics/robo-gym"
"facebookresearch/eai-vc" -> "haosulab/ManiSkill2"
"facebookresearch/eai-vc" -> "NVIDIA-Omniverse/Orbit"
"facebookresearch/eai-vc" -> "StanfordVL/OmniGibson"
"facebookresearch/eai-vc" -> "ir413/mvp"
"facebookresearch/eai-vc" -> "peract/peract"
"facebookresearch/eai-vc" -> "facebookresearch/r3m"
"facebookresearch/eai-vc" -> "google-research/robotics_transformer"
"facebookresearch/eai-vc" -> "vimalabs/VIMA"
"facebookresearch/eai-vc" -> "tonyzhaozh/act"
"facebookresearch/eai-vc" -> "columbia-ai-robotics/diffusion_policy"
"facebookresearch/eai-vc" -> "devendrachaplot/Object-Goal-Navigation" ["e"=1]
"facebookresearch/eai-vc" -> "lucidrains/robotic-transformer-pytorch"
"facebookresearch/eai-vc" -> "showlab/EgoVLP" ["e"=1]
"facebookresearch/eai-vc" -> "facebookresearch/habitat-challenge" ["e"=1]
"microsoft/ChatGPT-Robot-Manipulation-Prompts" -> "microsoft/cohesion-based-robot-teaching-interface"
"lcswillems/rl-starter-files" -> "lcswillems/torch-ac"
"lcswillems/rl-starter-files" -> "maximecb/gym-minigrid"
"lcswillems/rl-starter-files" -> "maximecb/gym-miniworld"
"lcswillems/rl-starter-files" -> "Farama-Foundation/Minigrid"
"lcswillems/rl-starter-files" -> "mila-iqia/babyai"
"lcswillems/rl-starter-files" -> "quanvuong/handful-of-trials-pytorch"
"lcswillems/rl-starter-files" -> "denisyarats/pytorch_sac"
"lcswillems/rl-starter-files" -> "Kaixhin/PlaNet"
"lcswillems/rl-starter-files" -> "MattChanTK/gym-maze"
"lcswillems/rl-starter-files" -> "facebookresearch/impact-driven-exploration"
"lcswillems/rl-starter-files" -> "vitchyr/multiworld"
"lcswillems/rl-starter-files" -> "facebookresearch/torchbeast"
"dgriff777/a3c_continuous" -> "dgriff777/rl_a3c_pytorch"
"dgriff777/a3c_continuous" -> "greydanus/baby-a3c"
"dgriff777/a3c_continuous" -> "MorvanZhou/pytorch-A3C"
"dgriff777/a3c_continuous" -> "shaneshixiang/rllabplusplus"
"cxy1997/Robotiq-UR5" -> "j96w/MuJoCo_Unity_UR5"
"cxy1997/Robotiq-UR5" -> "roboticsleeds/mujoco_ur5_model"
"openai/gym3" -> "openai/train-procgen"
"xbpeng/DeepLoco" -> "xbpeng/DeepTerrainRL"
"xbpeng/DeepLoco" -> "UBCMOCCA/TerrainRLSim"
"xbpeng/DeepLoco" -> "VincentYu68/SymmetryCurriculumLocomotion"
"M-J-Murray/MAMEToolkit" -> "uvipen/Street-fighter-A3C-ICM-pytorch"
"M-J-Murray/MAMEToolkit" -> "alito/mamele"
"M-J-Murray/MAMEToolkit" -> "M-J-Murray/SFAgents"
"M-J-Murray/MAMEToolkit" -> "deepmind/trfl"
"M-J-Murray/MAMEToolkit" -> "TorchCraft/TorchCraftAI" ["e"=1]
"M-J-Murray/MAMEToolkit" -> "vitchyr/rlkit"
"M-J-Murray/MAMEToolkit" -> "openai/retro"
"M-J-Murray/MAMEToolkit" -> "ctallec/world-models"
"M-J-Murray/MAMEToolkit" -> "facebookresearch/Horizon"
"M-J-Murray/MAMEToolkit" -> "openai/large-scale-curiosity"
"M-J-Murray/MAMEToolkit" -> "MultiAgentLearning/playground"
"M-J-Murray/MAMEToolkit" -> "rail-berkeley/softlearning"
"M-J-Murray/MAMEToolkit" -> "openai/random-network-distillation"
"M-J-Murray/MAMEToolkit" -> "ppaquette/gym-super-mario"
"m5823779/DDPG" -> "m5823779/MotionPlannerUsingDDPG"
"m5823779/DDPG" -> "m5823779/PoseEstimation"
"m5823779/PoseEstimation" -> "m5823779/stereo_image_generator_from_single_image"
"Stanford-ILIAD/PantheonRL" -> "HumanCompatibleAI/human_aware_rl"
"Kaixhin/imitation-learning" -> "kristery/Awesome-Imitation-Learning"
"Kaixhin/imitation-learning" -> "HumanCompatibleAI/imitation"
"Kaixhin/imitation-learning" -> "reinforcement-learning-kr/lets-do-irl"
"Kaixhin/imitation-learning" -> "CherryPieSexy/imitation_learning"
"Kaixhin/imitation-learning" -> "yrlu/irl-imitation"
"Kaixhin/imitation-learning" -> "Ericonaldo/ILSwiss"
"Kaixhin/imitation-learning" -> "seolhokim/InverseRL-Pytorch"
"Kaixhin/imitation-learning" -> "ku2482/gail-airl-ppo.pytorch"
"Kaixhin/imitation-learning" -> "sfujim/TD3_BC"
"Kaixhin/imitation-learning" -> "rlworkgroup/metaworld"
"Kaixhin/imitation-learning" -> "juliusfrost/dreamer-pytorch"
"Kaixhin/imitation-learning" -> "clvrai/spirl"
"JanMatas/Rainbow_ddpg" -> "wilson1yan/rlpyt"
"NVlabs/DefGraspSim" -> "NVlabs/biotac_sim"
"TheAbhiKumar/tensorflow-value-iteration-networks" -> "avivt/VIN"
"TheAbhiKumar/tensorflow-value-iteration-networks" -> "kentsommer/pytorch-value-iteration-networks"
"TheAbhiKumar/tensorflow-value-iteration-networks" -> "zhongwen/predictron"
"TheAbhiKumar/tensorflow-value-iteration-networks" -> "joschu/modular_rl"
"TheAbhiKumar/tensorflow-value-iteration-networks" -> "zuoxingdong/VIN_PyTorch_Visdom"
"TheAbhiKumar/tensorflow-value-iteration-networks" -> "miyosuda/async_deep_reinforce"
"TheAbhiKumar/tensorflow-value-iteration-networks" -> "miyosuda/unreal"
"TheAbhiKumar/tensorflow-value-iteration-networks" -> "openai/vime"
"TheAbhiKumar/tensorflow-value-iteration-networks" -> "openai/imitation"
"TheAbhiKumar/tensorflow-value-iteration-networks" -> "openai/rllab"
"TheAbhiKumar/tensorflow-value-iteration-networks" -> "carpedm20/deep-rl-tensorflow"
"TheAbhiKumar/tensorflow-value-iteration-networks" -> "muupan/async-rl"
"TheAbhiKumar/tensorflow-value-iteration-networks" -> "Mostafa-Samir/DNC-tensorflow" ["e"=1]
"TheAbhiKumar/tensorflow-value-iteration-networks" -> "NVlabs/GA3C"
"TheAbhiKumar/tensorflow-value-iteration-networks" -> "openai/universe-starter-agent"
"denisyarats/pytorch_sac_ae" -> "denisyarats/dmc2gym"
"denisyarats/pytorch_sac_ae" -> "denisyarats/drq"
"denisyarats/pytorch_sac_ae" -> "denisyarats/pytorch_sac"
"denisyarats/pytorch_sac_ae" -> "MishaLaskin/curl"
"denisyarats/pytorch_sac_ae" -> "aravindsrinivas/curl_rainbow"
"denisyarats/pytorch_sac_ae" -> "juliusfrost/dreamer-pytorch"
"denisyarats/pytorch_sac_ae" -> "MishaLaskin/rad"
"denisyarats/pytorch_sac_ae" -> "danijar/dreamer"
"denisyarats/pytorch_sac_ae" -> "facebookresearch/drqv2"
"denisyarats/pytorch_sac_ae" -> "facebookresearch/deep_bisim4control"
"denisyarats/pytorch_sac_ae" -> "ku2482/sac-discrete.pytorch"
"denisyarats/pytorch_sac_ae" -> "martius-lab/SMORL"
"mcgillmrl/prob_mbrl" -> "mcgillmrl/kusanagi"
"montrealrobotics/active-domainrand" -> "montrealrobotics/domain-randomizer"
"nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch" -> "andrew-j-levy/Hierarchical-Actor-Critc-HAC-"
"nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch" -> "AboudyKreidieh/h-baselines"
"nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch" -> "lweitkamp/option-critic-pytorch"
"nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch" -> "skumar9876/Hierarchical-DQN"
"quantumiracle/SOTA-RL-Algorithms" -> "dongminlee94/deep_rl"
"quantumiracle/SOTA-RL-Algorithms" -> "pranz24/pytorch-soft-actor-critic"
"quanvuong/handful-of-trials-pytorch" -> "kchua/handful-of-trials"
"quanvuong/handful-of-trials-pytorch" -> "JannerM/mbpo"
"quanvuong/handful-of-trials-pytorch" -> "Xingyu-Lin/mbpo_pytorch"
"quanvuong/handful-of-trials-pytorch" -> "tianheyu927/mopo"
"quanvuong/handful-of-trials-pytorch" -> "WilsonWangTHU/mbbl"
"quanvuong/handful-of-trials-pytorch" -> "iclavera/learning_to_adapt"
"quanvuong/handful-of-trials-pytorch" -> "juliusfrost/dreamer-pytorch"
"yukezhu/tensorflow-reinforce" -> "joschu/modular_rl"
"yukezhu/tensorflow-reinforce" -> "miyosuda/async_deep_reinforce"
"yukezhu/tensorflow-reinforce" -> "pemami4911/deep-rl"
"yukezhu/tensorflow-reinforce" -> "steveKapturowski/tensorflow-rl"
"yukezhu/tensorflow-reinforce" -> "openai/rllab"
"yukezhu/tensorflow-reinforce" -> "carpedm20/deep-rl-tensorflow"
"yukezhu/tensorflow-reinforce" -> "stormmax/reinforcement_learning"
"yukezhu/tensorflow-reinforce" -> "coreylynch/async-rl"
"yukezhu/tensorflow-reinforce" -> "awjuliani/DeepRL-Agents"
"yukezhu/tensorflow-reinforce" -> "muupan/async-rl"
"yukezhu/tensorflow-reinforce" -> "ikostrikov/pytorch-a2c-ppo-acktr"
"yukezhu/tensorflow-reinforce" -> "facebookresearch/MIXER" ["e"=1]
"yukezhu/tensorflow-reinforce" -> "pat-coady/trpo"
"yukezhu/tensorflow-reinforce" -> "reinforceio/tensorforce"
"yukezhu/tensorflow-reinforce" -> "williamFalcon/DeepRLHacks"
"kandouss/marlgrid" -> "ArnaudFickinger/gym-multigrid"
"sven1977/rllib_tutorials" -> "DerwenAI/rllib_tutorials"
"XinJingHao/PPO-Continuous-Pytorch" -> "XinJingHao/RL-Algorithms-by-Pytorch"
"oxwhirl/wqmix" -> "wjh720/QPLEX"
"oxwhirl/wqmix" -> "Sonkyunghwan/QTRAN"
"oxwhirl/wqmix" -> "TonghanWang/DOP"
"oxwhirl/wqmix" -> "wendelinboehmer/dcg"
"oxwhirl/wqmix" -> "TonghanWang/ROMA"
"oxwhirl/wqmix" -> "jingranburangyongzhongwen/torchMARL"
"oxwhirl/wqmix" -> "facebookresearch/CollaQ"
"cuhkrlcourse/ierg6130-assignment" -> "cuhkrlcourse/DeepRL-Tutorials"
"cuhkrlcourse/ierg6130-assignment" -> "dayekuaipao/ierg6130-assignment"
"cuhkrlcourse/ierg6130-assignment" -> "cuhkrlcourse/RLexample"
"cuhkrlcourse/ierg6130-assignment" -> "Aguin/cuhkrlcourse-ierg6130"
"shaneshixiang/rllabplusplus" -> "joschu/modular_rl"
"shaneshixiang/rllabplusplus" -> "Breakend/DeepReinforcementLearningThatMatters"
"shaneshixiang/rllabplusplus" -> "junhyukoh/self-imitation-learning"
"shaneshixiang/rllabplusplus" -> "wojzaremba/trpo"
"shaneshixiang/rllabplusplus" -> "haarnoja/softqlearning"
"shaneshixiang/rllabplusplus" -> "junhyukoh/value-prediction-network"
"NeuralMMO/environment" -> "NeuralMMO/baselines"
"NeuralMMO/environment" -> "PufferAI/PufferLib"
"NeuralMMO/environment" -> "Netease-Games-AI-Lab-Guangzhou/realikun"
"lich14/CDS" -> "mansicer/MAIC"
"semitable/lb-foraging" -> "uoe-agents/epymarl"
"uoe-agents/epymarl" -> "marlbenchmark/on-policy"
"uoe-agents/epymarl" -> "hijkzzz/pymarl2"
"uoe-agents/epymarl" -> "cyanrain7/TRPO-in-MARL"
"uoe-agents/epymarl" -> "marlbenchmark/off-policy"
"uoe-agents/epymarl" -> "Replicable-MARL/MARLlib"
"uoe-agents/epymarl" -> "semitable/lb-foraging"
"uoe-agents/epymarl" -> "oxwhirl/pymarl"
"uoe-agents/epymarl" -> "oxwhirl/smacv2"
"uoe-agents/epymarl" -> "oxwhirl/smac"
"uoe-agents/epymarl" -> "lich14/CDS"
"uoe-agents/epymarl" -> "schroederdewitt/multiagent_mujoco"
"uoe-agents/epymarl" -> "oxwhirl/wqmix"
"uoe-agents/epymarl" -> "semitable/robotic-warehouse"
"uoe-agents/epymarl" -> "starry-sky6688/MARL-Algorithms"
"uoe-agents/epymarl" -> "starry-sky6688/StarCraft"
"samuela/git-re-basin" -> "themrzmaster/git-re-basin-pytorch"
"samuela/git-re-basin" -> "stanislavfort/dissect-git-re-basin"
"samuela/git-re-basin" -> "ogkalu2/Merge-Stable-Diffusion-models-without-distortion" ["e"=1]
"samuela/git-re-basin" -> "mlfoundations/task_vectors"
"samuela/git-re-basin" -> "ethancaballero/broken_neural_scaling_laws"
"samuela/git-re-basin" -> "kmeng01/rome"
"samuela/git-re-basin" -> "nnaisense/evotorch"
"samuela/git-re-basin" -> "sidak/otfusion"
"synpon/prog_nn" -> "seann999/progressive_a3c"
"jeanharb/option_critic" -> "lweitkamp/option-critic-pytorch"
"jeanharb/option_critic" -> "yadrimz/option-critic"
"jeanharb/option_critic" -> "jeanharb/a2oc_delib"
"jeanharb/option_critic" -> "alversafa/option-critic-arch"
"jeanharb/option_critic" -> "junhyukoh/value-prediction-network"
"jeanharb/option_critic" -> "dmakian/feudal_networks"
"jeanharb/option_critic" -> "kkhetarpal/ioc"
"jeanharb/option_critic" -> "mklissa/PPOC"
"jeanharb/option_critic" -> "florensacc/snn4hrl"
"jeanharb/option_critic" -> "EthanMacdonald/h-DQN"
"NVIDIA-Omniverse/Orbit" -> "NVIDIA-Omniverse/OmniIsaacGymEnvs"
"NVIDIA-Omniverse/Orbit" -> "facebookresearch/r3m"
"NVIDIA-Omniverse/Orbit" -> "peract/peract"
"NVIDIA-Omniverse/Orbit" -> "wangcongrobot/awesome-isaac-gym"
"NVIDIA-Omniverse/Orbit" -> "PKU-MARL/DexterousHands"
"NVIDIA-Omniverse/Orbit" -> "haosulab/ManiSkill2"
"NVIDIA-Omniverse/Orbit" -> "ir413/mvp"
"NVIDIA-Omniverse/Orbit" -> "facebookresearch/eai-vc"
"NVIDIA-Omniverse/Orbit" -> "NVIDIA-Omniverse/IsaacGymEnvs"
"NVIDIA-Omniverse/Orbit" -> "google-research/robotics_transformer"
"NVIDIA-Omniverse/Orbit" -> "deepmind/mujoco_menagerie"
"NVIDIA-Omniverse/Orbit" -> "columbia-ai-robotics/diffusion_policy"
"NVIDIA-Omniverse/Orbit" -> "google-research/ravens"
"NVIDIA-Omniverse/Orbit" -> "GT-RIPL/Awesome-LLM-Robotics"
"NVIDIA-Omniverse/Orbit" -> "leggedrobotics/legged_gym" ["e"=1]
"shivaverma/OpenAIGym" -> "shivaverma/Orbit"
"pythonlessons/Reinforcement_Learning" -> "marload/DeepRL-TensorFlow2"
"pythonlessons/Reinforcement_Learning" -> "abhisheksuran/Reinforcement_Learning"
"pythonlessons/Reinforcement_Learning" -> "shivaverma/OpenAIGym"
"pythonlessons/Reinforcement_Learning" -> "philtabor/Youtube-Code-Repository"
"pythonlessons/Reinforcement_Learning" -> "germain-hug/Deep-RL-Keras"
"pythonlessons/Reinforcement_Learning" -> "Rafael1s/Deep-Reinforcement-Learning-Algorithms"
"pythonlessons/Reinforcement_Learning" -> "pythonlessons/RL-Bitcoin-trading-bot" ["e"=1]
"AcutronicRobotics/MARA" -> "AcutronicRobotics/ros2learn"
"AcutronicRobotics/MARA" -> "AcutronicRobotics/moveit2"
"pfnet-research/pfhedge" -> "pfnet-research/NoTransactionBandNetwork"
"pfnet-research/pfhedge" -> "YuMan-Tam/deep-hedging"
"pfnet-research/pfhedge" -> "hansbuehler/deephedging"
"stevens-cs546-cs554/CS-546" -> "stevens-cs546-cs554/CS-554"
"facebookresearch/Hanabi_SPARTA" -> "facebookresearch/hanabi_SAD"
"facebookresearch/Hanabi_SPARTA" -> "Quuxplusone/Hanabi"
"facebookresearch/hanabi_SAD" -> "facebookresearch/Hanabi_SPARTA"
"facebookresearch/hanabi_SAD" -> "aronsar/hoad"
"facebookresearch/hanabi_SAD" -> "facebookresearch/jps"
"facebookresearch/hanabi_SAD" -> "facebookresearch/off-belief-learning"
"facebookresearch/hanabi_SAD" -> "Quuxplusone/Hanabi"
"vojtamolda/reinforcement-learning-an-introduction" -> "brynhayder/reinforcement_learning_an_introduction"
"vojtamolda/reinforcement-learning-an-introduction" -> "LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions"
"utiasDSL/safe-control-gym" -> "utiasDSL/gym-pybullet-drones" ["e"=1]
"utiasDSL/safe-control-gym" -> "chauncygu/Safe-Reinforcement-Learning-Baselines"
"utiasDSL/safe-control-gym" -> "openai/safety-starter-agents"
"utiasDSL/safe-control-gym" -> "openai/safety-gym"
"utiasDSL/safe-control-gym" -> "befelix/safe_learning"
"utiasDSL/safe-control-gym" -> "SvenGronauer/Bullet-Safety-Gym"
"utiasDSL/safe-control-gym" -> "rcheng805/RL-CBF"
"utiasDSL/safe-control-gym" -> "uzh-rpg/high_mpc" ["e"=1]
"utiasDSL/safe-control-gym" -> "TUM-AAS/ml-casadi" ["e"=1]
"utiasDSL/safe-control-gym" -> "UM-ARM-Lab/pytorch_mppi" ["e"=1]
"utiasDSL/safe-control-gym" -> "Shunichi09/PythonLinearNonlinearControl" ["e"=1]
"utiasDSL/safe-control-gym" -> "locuslab/mpc.pytorch" ["e"=1]
"utiasDSL/safe-control-gym" -> "HybridRobotics/cbf" ["e"=1]
"utiasDSL/safe-control-gym" -> "facebookresearch/mbrl-lib"
"farizrahman4u/qlearning4k" -> "osh/kerlym"
"farizrahman4u/qlearning4k" -> "matthiasplappert/keras-rl"
"farizrahman4u/qlearning4k" -> "nivwusquorum/tensorflow-deepq"
"farizrahman4u/qlearning4k" -> "spragunr/deep_q_rl"
"farizrahman4u/qlearning4k" -> "datalogai/recurrentshop" ["e"=1]
"farizrahman4u/qlearning4k" -> "asrivat1/DeepLearningVideoGames"
"farizrahman4u/qlearning4k" -> "EderSantana/seya" ["e"=1]
"farizrahman4u/qlearning4k" -> "coreylynch/async-rl"
"mikezhang95/ML_Assignment" -> "joyiswu/UCL-Deep-learning-ans-Reinforcement-learning"
"mikezhang95/ML_Assignment" -> "RylanSchaeffer/ucl-adv-dl-rl"
"befelix/SafeOpt" -> "befelix/safe_learning"
"befelix/SafeOpt" -> "befelix/SafeMDP"
"befelix/SafeOpt" -> "befelix/lyapunov-learning"
"befelix/SafeOpt" -> "befelix/safe-exploration"
"tianheyu927/mopo" -> "SwapnilPande/MOReL"
"tianheyu927/mopo" -> "google-research/dice_rl"
"tianheyu927/mopo" -> "yihaosun1124/pytorch-mopo"
"tianheyu927/mopo" -> "JannerM/mbpo"
"tianheyu927/mopo" -> "Xingyu-Lin/mbpo_pytorch"
"tianheyu927/mopo" -> "sfujim/TD3_BC"
"tianheyu927/mopo" -> "sparkmxy/my-offlinerl"
"tianheyu927/mopo" -> "aviralkumar2907/CQL"
"TonghanWang/RODE" -> "wjh720/QPLEX"
"TonghanWang/RODE" -> "TonghanWang/ROMA"
"ben-eysenbach/sac" -> "victorcampos7/edl"
"ben-eysenbach/sac" -> "google-research/dads"
"ben-eysenbach/sac" -> "alirezakazemipour/DIAYN-PyTorch"
"chauncygu/Multi-Agent-Constrained-Policy-Optimisation" -> "chauncygu/Safe-Multi-Agent-Mujoco"
"chauncygu/Multi-Agent-Constrained-Policy-Optimisation" -> "chauncygu/Safe-Multi-Agent-Isaac-Gym"
"chauncygu/Multi-Agent-Constrained-Policy-Optimisation" -> "cyanrain7/TRPO-in-MARL"
"chauncygu/Safe-Multi-Agent-Mujoco" -> "chauncygu/Safe-Multi-Agent-Robosuite"
"junhyukoh/value-prediction-network" -> "jeanharb/option_critic"
"junhyukoh/value-prediction-network" -> "oxwhirl/treeqn" ["e"=1]
"junhyukoh/value-prediction-network" -> "dmakian/feudal_networks"
"junhyukoh/value-prediction-network" -> "zhongwen/predictron"
"junhyukoh/value-prediction-network" -> "Breakend/DeepReinforcementLearningThatMatters"
"mansicer/MAIC" -> "DrZero0/MACC"
"uber-research/ape-x" -> "belepi93/Ape-X"
"uber-research/ape-x" -> "deepmind/scalable_agent"
"uber-research/ape-x" -> "jingweiz/pytorch-distributed"
"wendelinboehmer/dcg" -> "sisl/DICG"
"mega002/lm-debugger" -> "mega002/ff-layers"
"mega002/lm-debugger" -> "aviclu/ffn-values"
"Unity-Technologies/obstacle-tower-source" -> "unixpickle/obs-tower2"
"EvolutionGym/evogym-design-tool" -> "EvolutionGym/evolutiongym.github.io"
"EvolutionGym/evogym-design-tool" -> "EvolutionGym/evogym"
"gliese581gg/DQN_tensorflow" -> "songrotek/DQN-Atari-Tensorflow"
"gliese581gg/DQN_tensorflow" -> "mrkulk/deepQN_tensorflow"
"gliese581gg/DQN_tensorflow" -> "floodsung/DQN-Atari-Tensorflow"
"gliese581gg/DQN_tensorflow" -> "gtoubassi/dqn-atari"
"haosulab/ManiSkill" -> "haosulab/ManiSkill-Learn"
"haosulab/ManiSkill" -> "haosulab/ManiSkill2"
"haosulab/ManiSkill" -> "haosulab/ManiSkill2-Learn"
"haosulab/ManiSkill" -> "haosulab/SAPIEN-Release"
"arex18/rocket-lander" -> "zuoxingdong/gym-maze"
"YunzhuLi/InfoGAIL" -> "andrewliao11/gail-tf"
"YunzhuLi/InfoGAIL" -> "ermongroup/InfoGAIL"
"YunzhuLi/InfoGAIL" -> "sisl/gail-driver"
"YunzhuLi/InfoGAIL" -> "openai/imitation"
"YunzhuLi/InfoGAIL" -> "uidilr/gail_ppo_tf"
"YunzhuLi/InfoGAIL" -> "sisl/hgail"
"YunzhuLi/InfoGAIL" -> "justinjfu/inverse_rl"
"sherjilozair/dqn" -> "osh/kerlym"
"sherjilozair/dqn" -> "tatsuyaokubo/dqn"
"sherjilozair/dqn" -> "EthanMacdonald/h-DQN"
"sherjilozair/dqn" -> "nivwusquorum/tf-adversarial" ["e"=1]
"EssexUniversityMCTS/gvgai" -> "GAIGResearch/GVGAI"
"EssexUniversityMCTS/gvgai" -> "rubenrtorrado/GVGAI_GYM"
"EssexUniversityMCTS/gvgai" -> "schaul/py-vgdl"
"Unity-Technologies/obstacle-tower-challenge" -> "Unity-Technologies/obstacle-tower-env"
"Unity-Technologies/obstacle-tower-challenge" -> "unixpickle/obs-tower2"
"adik993/ppo-pytorch" -> "chagmgang/pytorch_ppo_rl"
"jangirrishabh/Overcoming-exploration-from-demos" -> "aravindr93/hand_dapg"
"katerakelly/oyster" -> "jonasrothfuss/ProMP"
"katerakelly/oyster" -> "lmzintgraf/varibad"
"katerakelly/oyster" -> "rlworkgroup/metaworld"
"katerakelly/oyster" -> "tristandeleu/pytorch-maml-rl" ["e"=1]
"katerakelly/oyster" -> "iclavera/learning_to_adapt"
"katerakelly/oyster" -> "cbfinn/maml_rl" ["e"=1]
"katerakelly/oyster" -> "rail-berkeley/softlearning"
"katerakelly/oyster" -> "sfujim/BCQ"
"katerakelly/oyster" -> "vitchyr/rlkit"
"katerakelly/oyster" -> "awjuliani/Meta-RL"
"katerakelly/oyster" -> "amazon-research/meta-q-learning"
"katerakelly/oyster" -> "kchua/handful-of-trials"
"katerakelly/oyster" -> "haarnoja/sac"
"katerakelly/oyster" -> "MishaLaskin/curl"
"katerakelly/oyster" -> "quanvuong/handful-of-trials-pytorch"
"openrlbenchmark/openrlbenchmark" -> "Farama-Foundation/Shimmy"
"x35f/unstable_baselines" -> "typoverflow/UtilsRL"
"x35f/unstable_baselines" -> "jiangsy/mbpo_pytorch"
"danfeiX/model-based-papers" -> "ferreirafabio/mppi_pendulum"
"MiguelARD/DoorDetect-Dataset" -> "PSVL/DoorGym"
"MiguelARD/DoorDetect-Dataset" -> "gasparramoa/DeepDoors2"
"sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python" -> "PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python"
"sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python" -> "sudharsan13296/Hands-On-Reinforcement-Learning-With-Python"
"sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python" -> "sudharsan13296/Hands-On-Meta-Learning-With-Python" ["e"=1]
"sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python" -> "sudharsan13296/Awesome-Meta-Learning" ["e"=1]
"sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python" -> "sudharsan13296/Word2vec-from-scratch"
"stepjam/ARM" -> "peract/peract"
"stepjam/ARM" -> "stepjam/YARR"
"stepjam/ARM" -> "mees/calvin"
"stepjam/ARM" -> "ir413/mvp"
"stepjam/ARM" -> "facebookresearch/r3m"
"google-research/language-table" -> "vimalabs/VIMA"
"google-research/language-table" -> "facebookresearch/r3m"
"stanford-iprl-lab/multimodal_representation" -> "Henry1iu/ierg5350_rl_course_project"
"cuhkrlcourse/DeepRL-Tutorials" -> "cuhkrlcourse/RLexample"
"cuhkrlcourse/DeepRL-Tutorials" -> "cuhkrlcourse/ierg6130-assignment"
"cuhkrlcourse/DeepRL-Tutorials" -> "xmfbit/DQN-FlappyBird"
"cuhkrlcourse/DeepRL-Tutorials" -> "sfujim/TD3"
"cuhkrlcourse/DeepRL-Tutorials" -> "zhoubolei/introRL"
"danijar/crafter" -> "kenjyoung/MinAtar"
"danijar/crafter" -> "danijar/dreamerv3"
"danijar/crafter" -> "danijar/director"
"danijar/crafter" -> "facebookresearch/minihack"
"danijar/crafter" -> "danijar/crafter-baselines"
"danijar/crafter" -> "denisyarats/dmc2gym"
"danijar/crafter" -> "RajGhugare19/dreamerv2"
"danijar/crafter" -> "orybkin/lexa"
"danijar/crafter" -> "danijar/dreamerv2"
"danijar/crafter" -> "jurgisp/memory-maze"
"danijar/crafter" -> "rll-research/url_benchmark"
"danijar/crafter" -> "jurgisp/pydreamer"
"danijar/crafter" -> "eloialonso/iris"
"denisyarats/dmc2gym" -> "facebookresearch/drqv2"
"denisyarats/dmc2gym" -> "denisyarats/drq"
"denisyarats/dmc2gym" -> "denisyarats/pytorch_sac_ae"
"denisyarats/dmc2gym" -> "facebookresearch/deep_bisim4control"
"denisyarats/dmc2gym" -> "nicklashansen/dmcontrol-generalization-benchmark"
"denisyarats/dmc2gym" -> "MishaLaskin/rad"
"denisyarats/dmc2gym" -> "MishaLaskin/curl"
"rll-research/cic" -> "artberryx/CSD-public"
"rll-research/url_benchmark" -> "denisyarats/exorl"
"rll-research/url_benchmark" -> "facebookresearch/drqv2"
"rll-research/url_benchmark" -> "google-research/dads"
"rll-research/url_benchmark" -> "rll-research/cic"
"rll-research/url_benchmark" -> "facebookresearch/denoised_mdp"
"rll-research/url_benchmark" -> "google-research/rliable"
"rll-research/url_benchmark" -> "denisyarats/proto"
"rll-research/url_benchmark" -> "danijar/dreamerv3"
"rll-research/url_benchmark" -> "rail-berkeley/d4rl"
"rll-research/url_benchmark" -> "YeWR/EfficientZero"
"rll-research/url_benchmark" -> "facebookresearch/mbrl-lib"
"ucl-dark/paired" -> "facebookresearch/dcd"
"kengz/awesome-deep-rl" -> "clvrai/awesome-rl-envs"
"kengz/awesome-deep-rl" -> "kengz/SLM-Lab"
"kengz/awesome-deep-rl" -> "cpnota/autonomous-learning-library"
"kengz/awesome-deep-rl" -> "AboudyKreidieh/h-baselines"
"kengz/awesome-deep-rl" -> "quantumiracle/Popular-RL-Algorithms"
"kengz/awesome-deep-rl" -> "tshrjn/env-zoo"
"kengz/awesome-deep-rl" -> "andyljones/reinforcement-learning-discord-wiki"
"kengz/awesome-deep-rl" -> "MrSyee/pg-is-all-you-need"
"kengz/awesome-deep-rl" -> "tinkoff-ai/CORL"
"kengz/awesome-deep-rl" -> "rlworkgroup/metaworld"
"kengz/awesome-deep-rl" -> "hanjuku-kaso/awesome-offline-rl"
"kengz/awesome-deep-rl" -> "rail-berkeley/d4rl"
"kengz/awesome-deep-rl" -> "PettingZoo-Team/PettingZoo" ["e"=1]
"kengz/awesome-deep-rl" -> "MushroomRL/mushroom-rl"
"kengz/awesome-deep-rl" -> "denisyarats/dmc2gym"
"haosulab/ManiSkill2" -> "haosulab/ManiSkill2-Learn"
"haosulab/ManiSkill2" -> "haosulab/ManiSkill"
"haosulab/ManiSkill2" -> "columbia-ai-robotics/diffusion_policy"
"haosulab/ManiSkill2" -> "haosulab/ManiSkill-Learn"
"haosulab/ManiSkill2" -> "peract/peract"
"haosulab/ManiSkill2" -> "haosulab/SAPIEN"
"haosulab/ManiSkill2" -> "facebookresearch/r3m"
"haosulab/ManiSkill2" -> "ir413/mvp"
"haosulab/ManiSkill2" -> "yzqin/isaacgym-stubs"
"haosulab/ManiSkill2" -> "facebookresearch/eai-vc"
"haosulab/ManiSkill2" -> "google-research/robopianist"
"haosulab/ManiSkill2" -> "google-research/ravens"
"apexrl/Imitation-Learning-Paper-Lists" -> "Ericonaldo/ILSwiss"
"apexrl/Imitation-Learning-Paper-Lists" -> "kristery/Awesome-Imitation-Learning"
"zuoxingdong/mazelab" -> "MattChanTK/gym-maze"
"taivop/joke-dataset" -> "amoudgl/short-jokes-dataset"
"taivop/joke-dataset" -> "CrowdTruth/Short-Text-Corpus-For-Humor-Detection"
"taivop/joke-dataset" -> "openai/requests-for-research"
"taivop/joke-dataset" -> "amoudgl/funnybot"
"uvipen/Tetris-deep-Q-learning-pytorch" -> "nuno-faria/tetris-ai"
"uvipen/Tetris-deep-Q-learning-pytorch" -> "uvipen/Super-mario-bros-PPO-pytorch"
"uvipen/Tetris-deep-Q-learning-pytorch" -> "lusob/gym-tetris"
"uvipen/Tetris-deep-Q-learning-pytorch" -> "uvipen/Flappy-bird-deep-Q-learning-pytorch"
"junhyukoh/self-imitation-learning" -> "TianhongDai/self-imitation-learning-pytorch"
"junhyukoh/self-imitation-learning" -> "pathak22/zeroshot-imitation"
"junhyukoh/self-imitation-learning" -> "shaneshixiang/rllabplusplus"
"junhyukoh/self-imitation-learning" -> "tianheyu927/mil"
"junhyukoh/self-imitation-learning" -> "openai/atari-reset"
"junhyukoh/self-imitation-learning" -> "openai/robosumo"
"junhyukoh/self-imitation-learning" -> "openai/EPG"
"junhyukoh/self-imitation-learning" -> "hoangminhle/hierarchical_IL_RL"
"junhyukoh/self-imitation-learning" -> "haarnoja/softqlearning"
"junhyukoh/self-imitation-learning" -> "junhyukoh/value-prediction-network"
"MengGuo/RVO_Py_MAS" -> "sybrenstuvel/Python-RVO2"
"MengGuo/RVO_Py_MAS" -> "snape/HRVO" ["e"=1]
"MengGuo/RVO_Py_MAS" -> "daenny/collvoid"
"MengGuo/RVO_Py_MAS" -> "suraj2596/RVO_rospy"
"MengGuo/RVO_Py_MAS" -> "hanruihua/rl_rvo_nav"
"daenny/collvoid" -> "ferherranz/multi_robot"
"higgsfield/Imagination-Augmented-Agents" -> "yilundu/imagination_augmented_agents"
"zuoxingdong/lagom" -> "MillionIntegrals/vel"
"zuoxingdong/lagom" -> "vitchyr/multiworld"
"zuoxingdong/lagom" -> "ml-jku/baselines-rudder"
"zuoxingdong/lagom" -> "arraiy/torchgeometry" ["e"=1]
"google-research/episodic-curiosity" -> "nsavinov/SPTM" ["e"=1]
"kchua/handful-of-trials" -> "JannerM/mbpo"
"kchua/handful-of-trials" -> "quanvuong/handful-of-trials-pytorch"
"kchua/handful-of-trials" -> "WilsonWangTHU/mbbl"
"kchua/handful-of-trials" -> "WilsonWangTHU/POPLIN"
"kchua/handful-of-trials" -> "nrontsis/PILCO"
"kchua/handful-of-trials" -> "nagaban2/nn_dynamics"
"kchua/handful-of-trials" -> "facebookresearch/mbrl-lib"
"kchua/handful-of-trials" -> "mcgillmrl/prob_mbrl"
"kchua/handful-of-trials" -> "aravindr93/mjrl"
"kchua/handful-of-trials" -> "danijar/dreamer"
"kchua/handful-of-trials" -> "thanard/me-trpo"
"kchua/handful-of-trials" -> "iclavera/learning_to_adapt"
"kchua/handful-of-trials" -> "google-research/planet"
"kchua/handful-of-trials" -> "Kaixhin/PlaNet"
"kchua/handful-of-trials" -> "katerakelly/oyster"
"nrontsis/PILCO" -> "kchua/handful-of-trials"
"nrontsis/PILCO" -> "WilsonWangTHU/mbbl"
"nrontsis/PILCO" -> "zuoxingdong/DeepPILCO"
"nrontsis/PILCO" -> "JannerM/mbpo"
"nrontsis/PILCO" -> "ICL-SML/pilco-matlab"
"nrontsis/PILCO" -> "mcgillmrl/prob_mbrl"
"nrontsis/PILCO" -> "WilsonWangTHU/POPLIN"
"nrontsis/PILCO" -> "jaztsong/PILCO-gpytorch"
"nrontsis/PILCO" -> "cbfinn/gps"
"nrontsis/PILCO" -> "mcgillmrl/kusanagi"
"nrontsis/PILCO" -> "nagaban2/nn_dynamics"
"nrontsis/PILCO" -> "danfeiX/model-based-papers"
"nrontsis/PILCO" -> "aidanscannell/pilco-tensorflow"
"nrontsis/PILCO" -> "locuslab/mpc.pytorch" ["e"=1]
"nrontsis/PILCO" -> "helgeanl/GP-MPC" ["e"=1]
"david-abel/simple_rl" -> "junhyukoh/value-prediction-network"
"amidos2006/gym-pcgrl" -> "pbontrager/GenerativePlayingNetworks"
"amidos2006/gym-pcgrl" -> "smearle/control-pcgrl"
"mila-iqia/spr" -> "facebookresearch/deep_bisim4control"
"mila-iqia/spr" -> "conglu1997/v-d4rl"
"marooncn/navbot" -> "marooncn/RL"
"marooncn/navbot" -> "ethz-asl/rl-navigation"
"marooncn/navbot" -> "RoblabWh/RobLearn"
"marooncn/navbot" -> "jkulhanek/visual-navigation-agent-pytorch" ["e"=1]
"marooncn/navbot" -> "vita-epfl/CrowdNav"
"marooncn/navbot" -> "yushu-liu/icra2017-visual-navigation" ["e"=1]
"marooncn/navbot" -> "m5823779/MotionPlannerUsingDDPG"
"nikhilbarhate99/min-decision-transformer" -> "facebookresearch/online-dt"
"nikhilbarhate99/min-decision-transformer" -> "jannerm/trajectory-transformer"
"nikhilbarhate99/min-decision-transformer" -> "daniellawson9999/online-decision-transformer"
"nikhilbarhate99/min-decision-transformer" -> "Howuhh/faster-trajectory-transformer"
"MengeCrowdSim/Menge" -> "ml-lab-cuny/menge_ros"
"mimoralea/applied-reinforcement-learning" -> "mimoralea/gdrl"
"mimoralea/applied-reinforcement-learning" -> "rldm/rldm_tutorials"
"mimoralea/applied-reinforcement-learning" -> "pushkar/ABAGAIL" ["e"=1]
"songrotek/DRL-FlappyBird" -> "songrotek/DQN-Atari-Tensorflow"
"songrotek/DRL-FlappyBird" -> "li-haoran/DRL-FlappyBird"
"songrotek/DRL-FlappyBird" -> "algorithmdog/Reinforcement_Learning_Blog"
"songrotek/DRL-FlappyBird" -> "gliese581gg/DQN_tensorflow"
"songrotek/DRL-FlappyBird" -> "asrivat1/DeepLearningVideoGames"
"songrotek/DRL-FlappyBird" -> "yenchenlin/DeepLearningFlappyBird"
"songrotek/DRL-FlappyBird" -> "songrotek/Meta-Learning-Papers" ["e"=1]
"YaChienChang/Neural-Lyapunov-Control" -> "befelix/safe_learning"
"YaChienChang/Neural-Lyapunov-Control" -> "MIT-REALM/neural_clbf"
"jonasrothfuss/ProMP" -> "katerakelly/oyster"
"jonasrothfuss/ProMP" -> "tristandeleu/pytorch-maml-rl" ["e"=1]
"jonasrothfuss/ProMP" -> "iclavera/learning_to_adapt"
"jonasrothfuss/ProMP" -> "cbfinn/maml_rl" ["e"=1]
"jonasrothfuss/ProMP" -> "lmzintgraf/varibad"
"jonasrothfuss/ProMP" -> "amazon-research/meta-q-learning"
"jonasrothfuss/ProMP" -> "russellmendonca/maesn_suite"
"jonasrothfuss/ProMP" -> "lhao499/taming-maml"
"openai/atari-reset" -> "openai/atari-demo"
"openai/atari-reset" -> "junhyukoh/self-imitation-learning"
"jurgisp/pydreamer" -> "jsikyoon/dreamer-torch"
"jurgisp/pydreamer" -> "danijar/daydreamer"
"jurgisp/pydreamer" -> "RajGhugare19/dreamerv2"
"jurgisp/pydreamer" -> "danijar/dreamerv3"
"jurgisp/pydreamer" -> "yusukeurakami/dreamer-pytorch"
"jurgisp/pydreamer" -> "jurgisp/memory-maze"
"jurgisp/pydreamer" -> "vincent-thevenin/DreamerV2-Pytorch"
"jurgisp/pydreamer" -> "danijar/dreamerv2"
"5vision/DARQN" -> "iassael/torch-bootstrapped-dqn"
"5vision/DARQN" -> "mhauskn/dqn"
"5vision/DARQN" -> "5vision/deep-reinforcement-learning-networks"
"5vision/DARQN" -> "Kaixhin/rlenvs" ["e"=1]
"danijar/dreamerv3" -> "lucidrains/dreamerv3-pytorch"
"danijar/dreamerv3" -> "danijar/dreamerv2"
"danijar/dreamerv3" -> "jurgisp/pydreamer"
"danijar/dreamerv3" -> "danijar/crafter"
"danijar/dreamerv3" -> "danijar/daydreamer"
"danijar/dreamerv3" -> "rll-research/url_benchmark"
"danijar/dreamerv3" -> "facebookresearch/mbrl-lib"
"danijar/dreamerv3" -> "google-research/rliable"
"danijar/dreamerv3" -> "RobertTLange/gymnax"
"danijar/dreamerv3" -> "YeWR/EfficientZero"
"danijar/dreamerv3" -> "araffin/sbx"
"danijar/dreamerv3" -> "juliusfrost/dreamer-pytorch"
"danijar/dreamerv3" -> "eloialonso/iris"
"danijar/dreamerv3" -> "RajGhugare19/dreamerv2"
"danijar/dreamerv3" -> "sail-sg/envpool"
"ignc-research/arena-rosnav-3D" -> "ignc-research/arena-tools"
"ignc-research/arena-rosnav-3D" -> "ignc-research/navsafe-arena"
"ignc-research/arena-rosnav-3D" -> "ignc-research/arena-rosnav"
"ignc-research/arena-rosnav-3D" -> "ignc-research/all-in-one-DRL-planner"
"ignc-research/arena-rosnav-3D" -> "ignc-research/arena-fsm-ego-planner"
"ignc-research/arena-rosnav-3D" -> "ignc-research/arena-bench"
"JKCooper2/rlai-exercises" -> "iamhectorotero/rlai-exercises"
"JKCooper2/rlai-exercises" -> "matteocasolari/reinforcement-learning-an-introduction-solutions"
"JKCooper2/rlai-exercises" -> "diegoalejogm/Reinforcement-Learning"
"MillionIntegrals/vel" -> "zuoxingdong/lagom"
"facebookarchive/CommNet" -> "rhoowd/sched_net"
"younggyoseo/Ape-X" -> "jingweiz/pytorch-distributed"
"SudeepDasari/visual_foresight" -> "anxie/meta_classifier"
"google-research/batch-ppo" -> "takuseno/ppo"
"google-research/batch-ppo" -> "google-research/policy-learning-landscape"
"hiwonjoon/ICML2019-TREX" -> "dsbrown1331/CoRL2019-DREX"
"kindredresearch/SenseAct" -> "araffin/robotics-rl-srl"
"kindredresearch/SenseAct" -> "StanfordVL/robosuite"
"kindredresearch/SenseAct" -> "cxy1997/Robotiq-UR5"
"AppliedDataSciencePartners/WorldModels" -> "hardmaru/WorldModelsExperiments"
"AppliedDataSciencePartners/WorldModels" -> "ctallec/world-models"
"AppliedDataSciencePartners/WorldModels" -> "hardmaru/estool" ["e"=1]
"ac-93/soft-actor-critic" -> "ku2482/sac-discrete.pytorch"
"ac-93/soft-actor-critic" -> "yining043/SAC-discrete"
"ahq1993/inverse_rl" -> "ermongroup/MetaIRL"
"ahq1993/inverse_rl" -> "jangirrishabh/toyCarIRL"
"ahq1993/inverse_rl" -> "justinjfu/inverse_rl"
"ahq1993/inverse_rl" -> "yrlu/irl-imitation"
"jcwleo/curiosity-driven-exploration-pytorch" -> "jcwleo/random-network-distillation-pytorch"
"jcwleo/curiosity-driven-exploration-pytorch" -> "chagmgang/pytorch_ppo_rl"
"ku2482/sac-discrete.pytorch" -> "ac-93/soft-actor-critic"
"ku2482/sac-discrete.pytorch" -> "denisyarats/pytorch_sac_ae"
"ku2482/sac-discrete.pytorch" -> "denisyarats/pytorch_sac"
"ku2482/sac-discrete.pytorch" -> "yining043/SAC-discrete"
"ku2482/sac-discrete.pytorch" -> "alirezakazemipour/Discrete-SAC-PyTorch"
"lmzintgraf/varibad" -> "katerakelly/oyster"
"lmzintgraf/varibad" -> "twni2016/pomdp-baselines"
"lmzintgraf/varibad" -> "jonasrothfuss/ProMP"
"lmzintgraf/varibad" -> "lmzintgraf/hyperx"
"lmzintgraf/varibad" -> "LanqingLi1993/FOCAL-ICLR"
"rubenrtorrado/GVGAI_GYM" -> "GAIGResearch/GVGAI"
"rubenrtorrado/GVGAI_GYM" -> "pbontrager/GenerativePlayingNetworks"
"chenllliang/MLS" -> "chenllliang/ATP"
"chenllliang/MLS" -> "PKUnlp-icler/SCL-RAI"
"facebookresearch/minihack" -> "facebookresearch/nle"
"facebookresearch/minihack" -> "danijar/crafter"
"facebookresearch/minihack" -> "facebookresearch/moolib"
"facebookresearch/minihack" -> "google-research/rliable"
"facebookresearch/minihack" -> "Bam4d/Griddly"
"facebookresearch/minihack" -> "danijar/dreamerv3"
"facebookresearch/minihack" -> "facebookresearch/impact-driven-exploration"
"facebookresearch/minihack" -> "facebookresearch/torchbeast"
"facebookresearch/minihack" -> "facebookresearch/dcd"
"facebookresearch/minihack" -> "ikostrikov/jaxrl"
"EvolutionGym/evogym" -> "EvolutionGym/evogym-design-tool"
"AdamStelmaszczyk/learning2run" -> "Scitator/Run-Skeleton-Run"
"Breakend/DeepReinforcementLearningThatMatters" -> "shaneshixiang/rllabplusplus"
"Breakend/DeepReinforcementLearningThatMatters" -> "junhyukoh/value-prediction-network"
"Breakend/DeepReinforcementLearningThatMatters" -> "Kaixhin/NoisyNet-A3C"
"ctmakro/stanford-osrl" -> "nnaisense/2017-learning-to-run"
"gtoubassi/dqn-atari" -> "floodsung/DQN-Atari-Tensorflow"
"nnaisense/2017-learning-to-run" -> "ctmakro/stanford-osrl"
"tambetm/pommerman-baselines" -> "rwightman/pytorch-pommerman-rl"
"tambetm/pommerman-baselines" -> "BorealisAI/pommerman-baseline"
"wwxFromTju/deepmind_MAS_enviroment" -> "IC3Net/IC3Net"
"mahyaret/gym-panda" -> "mahyaret/kuka_rl"
"akolishchak/doom-net-pytorch" -> "IntelVCL/DirectFuturePrediction"
"vitchyr/viskit" -> "justinjfu/doodad"
"facebookresearch/impact-driven-exploration" -> "tianjunz/NovelD"
"facebookresearch/impact-driven-exploration" -> "facebookresearch/adversarially-motivated-intrinsic-goals"
"facebookresearch/impact-driven-exploration" -> "yfletberliac/adversarially-guided-actor-critic"
"kenjyoung/MinAtar" -> "danijar/crafter"
"kenjyoung/MinAtar" -> "JohanSamir/revisiting_rainbow"
"kenjyoung/MinAtar" -> "google-research/rliable"
"kenjyoung/MinAtar" -> "RobertTLange/gymnax"
"kenjyoung/MinAtar" -> "RajGhugare19/dreamerv2"
"kenjyoung/MinAtar" -> "danijar/dreamerv3"
"kenjyoung/MinAtar" -> "mpSchrader/gym-sokoban"
"kenjyoung/MinAtar" -> "maximecb/gym-miniworld"
"kenjyoung/MinAtar" -> "ikostrikov/rlpd"
"AgileRL/AgileRL" -> "danijar/dreamerv3"
"AgileRL/AgileRL" -> "vwxyzjn/cleanba"
"google-research/reincarnating_rl" -> "vwxyzjn/cleanba"
"ku2482/fqf-iqn-qrdqn.pytorch" -> "microsoft/FQF"
"ku2482/fqf-iqn-qrdqn.pytorch" -> "xtma/dsac"
"ku2482/fqf-iqn-qrdqn.pytorch" -> "Kchu/DeepRL_PyTorch"
"xbpeng/awr" -> "rail-berkeley/d4rl_evaluations"
"xbpeng/awr" -> "aviralkumar2907/BEAR"
"RajGhugare19/dreamerv2" -> "IvLabs/Natural-Language-Processing" ["e"=1]
"RajGhugare19/dreamerv2" -> "IvLabs/Sahayak-v3" ["e"=1]
"RajGhugare19/dreamerv2" -> "HiPatil/Policy-based-RL" ["e"=1]
"RajGhugare19/dreamerv2" -> "IvLabs/stagewise-knowledge-distillation" ["e"=1]
"RajGhugare19/dreamerv2" -> "jurgisp/pydreamer"
"RajGhugare19/dreamerv2" -> "jsikyoon/dreamer-torch"
"RajGhugare19/dreamerv2" -> "HiPatil/Autonomous-Delivery-Robot" ["e"=1]
"RajGhugare19/dreamerv2" -> "HiPatil/Value-based-RL" ["e"=1]
"RajGhugare19/dreamerv2" -> "IvLabs/ResearchPaperNotes" ["e"=1]
"RajGhugare19/dreamerv2" -> "RajGhugare19/gym-OctaKing"
"RajGhugare19/dreamerv2" -> "HiPatil/number-plate-detection-system" ["e"=1]
"RajGhugare19/dreamerv2" -> "IvLabs/Quantum-Machine-Learning" ["e"=1]
"RajGhugare19/dreamerv2" -> "RajGhugare19/Classical-RL"
"fawwazbmn/SocialForceModel" -> "robotics-upo/lightsfm"
"google-research/realworldrl_suite" -> "JannerM/mbpo"
"google-research/realworldrl_suite" -> "rail-berkeley/d4rl_evaluations"
"google-research/realworldrl_suite" -> "rail-berkeley/d4rl"
"google-research/realworldrl_suite" -> "rlworkgroup/metaworld"
"google-research/realworldrl_suite" -> "vitchyr/multiworld"
"google-research/realworldrl_suite" -> "deepmind/dm_hard_eight"
"google-research/realworldrl_suite" -> "facebookresearch/mbrl-lib"
"google-research/realworldrl_suite" -> "utiasDSL/safe-control-gym"
"google-research/realworldrl_suite" -> "sfujim/TD3_BC"
"google-research/realworldrl_suite" -> "google-research/dice_rl"
"rcheng805/RL-CBF" -> "unstable-zeros/learning-cbfs" ["e"=1]
"rcheng805/RL-CBF" -> "yemam3/SAC-RCBF"
"rcheng805/RL-CBF" -> "befelix/safe_learning"
"yemam3/SAC-RCBF" -> "yemam3/Mod-RL-RCBF"
"iamhectorotero/rlai-exercises" -> "JKCooper2/rlai-exercises"
"iamhectorotero/rlai-exercises" -> "brynhayder/reinforcement_learning_an_introduction"
"iamhectorotero/rlai-exercises" -> "LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions"
"iamhectorotero/rlai-exercises" -> "mharbuz/rlbook-exercises"
"iamhectorotero/rlai-exercises" -> "diegoalejogm/Reinforcement-Learning"
"iamhectorotero/rlai-exercises" -> "matteocasolari/reinforcement-learning-an-introduction-solutions"
"Quuxplusone/Hanabi" -> "WuTheFWasThat/hanabi.rs"
"Quuxplusone/Hanabi" -> "SliceOfBread/Hanabi"
"EleutherAI/knowledge-neurons" -> "Hunter-DDM/knowledge-neurons"
"MarcToussaint/rai" -> "MarcToussaint/rai-python"
"nagaban2/nn_dynamics" -> "kchua/handful-of-trials"
"nagaban2/nn_dynamics" -> "WilsonWangTHU/mbbl"
"nagaban2/nn_dynamics" -> "thanard/me-trpo"
"nagaban2/nn_dynamics" -> "JannerM/mbpo"
"nagaban2/nn_dynamics" -> "nrontsis/PILCO"
"nagaban2/nn_dynamics" -> "locuslab/mpc.pytorch" ["e"=1]
"nagaban2/nn_dynamics" -> "resibots/blackdrops" ["e"=1]
"nagaban2/nn_dynamics" -> "higgsfield/Imagination-Augmented-Agents"
"nagaban2/nn_dynamics" -> "junhyukoh/value-prediction-network"
"DerwenAI/rllib_tutorials" -> "sven1977/rllib_tutorials"
"shacklettbp/bps-nav" -> "shacklettbp/bps3D"
"tianheyu927/mil" -> "pathak22/zeroshot-imitation"
"tianheyu927/mil" -> "hoangminhle/hierarchical_IL_RL"
"tianheyu927/mil" -> "wyndwarrior/imitation_from_observation"
"tianheyu927/mil" -> "junhyukoh/self-imitation-learning"
"tianheyu927/mil" -> "jangirrishabh/Overcoming-exploration-from-demos"
"tianheyu927/mil" -> "stepjam/PyRep"
"tianheyu927/mil" -> "cbfinn/maml_rl" ["e"=1]
"tianheyu927/mil" -> "andrewliao11/gail-tf"
"tianheyu927/mil" -> "pathak22/hierarchical-imitation"
"tianheyu927/mil" -> "cbfinn/gps"
"tianheyu927/mil" -> "songrotek/Meta-Learning-Papers" ["e"=1]
"tianheyu927/mil" -> "avisingh599/reward-learning-rl"
"vlmaps/vlmaps" -> "YicongHong/Discrete-Continuous-VLN" ["e"=1]
"instadeepai/nucleotide-transformer" -> "instadeepai/catx"
"Ardavans/DSR" -> "EthanMacdonald/h-DQN"
"Ardavans/DSR" -> "ikostrikov/pytorch-naf" ["e"=1]
"muupan/dqn-in-the-caffe" -> "mhauskn/dqn"
"muupan/dqn-in-the-caffe" -> "spragunr/deep_q_rl"
"muupan/dqn-in-the-caffe" -> "kristjankorjus/Replicating-DeepMind"
"muupan/dqn-in-the-caffe" -> "brian473/neural_rl"
"muupan/dqn-in-the-caffe" -> "aiworld/dqn"
"muupan/dqn-in-the-caffe" -> "ugo-nama-kun/DQN-chainer" ["e"=1]
"muupan/dqn-in-the-caffe" -> "muupan/async-rl"
"muupan/dqn-in-the-caffe" -> "alrojo/lasagne_residual_network" ["e"=1]
"yandexdataschool/MLatImperial2017" -> "yandexdataschool/MLatGradDays"
"marooncn/RL" -> "marooncn/navbot"
"marooncn/RL" -> "akashdeepjassal/VREP-RL-bot"
"Zamiell/hanabi-live" -> "Zamiell/hanabi-conventions"
"Zamiell/hanabi-live" -> "WuTheFWasThat/hanabi.rs"
"thanard/me-trpo" -> "WilsonWangTHU/mbbl-metrpo"
"sybrenstuvel/Python-RVO2" -> "vita-epfl/CrowdNav"
"sybrenstuvel/Python-RVO2" -> "MengGuo/RVO_Py_MAS"
"sybrenstuvel/Python-RVO2" -> "mfe7/cadrl_ros"
"sybrenstuvel/Python-RVO2" -> "mit-acl/gym-collision-avoidance"
"sybrenstuvel/Python-RVO2" -> "srl-freiburg/pedsim_ros"
"sybrenstuvel/Python-RVO2" -> "mit-acl/rl_collision_avoidance"
"sybrenstuvel/Python-RVO2" -> "ChanganVR/RelationalGraphLearning"
"sybrenstuvel/Python-RVO2" -> "ethz-asl/navrep"
"sybrenstuvel/Python-RVO2" -> "vita-epfl/social-nce"
"sybrenstuvel/Python-RVO2" -> "yuxiang-gao/PySocialForce"
"sybrenstuvel/Python-RVO2" -> "svenkreiss/socialforce"
"sybrenstuvel/Python-RVO2" -> "ChanganVR/CADRL"
"sybrenstuvel/Python-RVO2" -> "ml-lab-cuny/menge_ros"
"sybrenstuvel/Python-RVO2" -> "onlytailei/gym_ped_sim"
"florensacc/rllab-curriculum" -> "dingyiming0427/goalgail"
"florensacc/rllab-curriculum" -> "mengf1/CHER"
"florensacc/rllab-curriculum" -> "snu-mllab/EMI"
"florensacc/rllab-curriculum" -> "florensacc/snn4hrl"
"steveKapturowski/tensorflow-rl" -> "yao62995/A3C"
"steveKapturowski/tensorflow-rl" -> "miyosuda/unreal"
"steveKapturowski/tensorflow-rl" -> "miyosuda/async_deep_reinforce"
"steveKapturowski/tensorflow-rl" -> "NVlabs/GA3C"
"steveKapturowski/tensorflow-rl" -> "dmakian/feudal_networks"
"steveKapturowski/tensorflow-rl" -> "openai/rllab"
"steveKapturowski/tensorflow-rl" -> "openai/vime"
"ignitionrobotics/ign-gui" -> "ignitionrobotics/ign-rendering"
"RylanSchaeffer/ucl-adv-dl-rl" -> "joyiswu/UCL-Deep-learning-ans-Reinforcement-learning"
"RylanSchaeffer/ucl-adv-dl-rl" -> "mikezhang95/ML_Assignment"
"google-research/rlds" -> "deepmind/envlogger"
"Xingyu-Lin/softgym" -> "Xingyu-Lin/softagent"
"Xingyu-Lin/softgym" -> "YunzhuLi/PyFleX"
"Xingyu-Lin/softgym" -> "DanielTakeshi/deformable-ravens"
"Xingyu-Lin/softgym" -> "Xingyu-Lin/VCD"
"Xingyu-Lin/softgym" -> "JanMatas/Rainbow_ddpg"
"Xingyu-Lin/softgym" -> "columbia-ai-robotics/flingbot"
"Xingyu-Lin/softgym" -> "NVlabs/DiSECt"
"jidiai/ai_lib" -> "rlchina/RLCN"
"jidiai/ai_lib" -> "jidiai/Competition_3v3snakes"
"jidiai/ai_lib" -> "CarlossShi/Competition_3v3snakes"
"jidiai/ai_lib" -> "jidiai/SummerCourse2021"
"ignc-research/arena-bench" -> "ignc-research/arena-tools"
"ignc-research/arena-bench" -> "ignc-research/navsafe-arena"
"ignc-research/arena-bench" -> "ignc-research/arena-rosnav-3D"
"ignc-research/arena-bench" -> "ignc-research/arena-evaluation"
"vita-epfl/social-nce" -> "vita-epfl/causalmotion"
"cyoon1729/Reinforcement-learning" -> "cyoon1729/Policy-Gradient-Methods"
"vimalabs/VIMA" -> "vimalabs/VIMABench"
"vimalabs/VIMA" -> "peract/peract"
"vimalabs/VIMA" -> "facebookresearch/vip"
"vimalabs/VIMA" -> "facebookresearch/r3m"
"vimalabs/VIMA" -> "cliport/cliport"
"vimalabs/VIMA" -> "google-research/language-table"
"vimalabs/VIMA" -> "ir413/mvp"
"vimalabs/VIMA" -> "mees/calvin"
"vimalabs/VIMA" -> "siddk/voltron-robotics"
"vimalabs/VIMA" -> "lukashermann/hulc"
"vimalabs/VIMA" -> "google-research/robotics_transformer"
"vimalabs/VIMA" -> "GT-RIPL/Awesome-LLM-Robotics"
"vimalabs/VIMA" -> "PKU-MARL/DexterousHands"
"flyyufelix/VizDoom-Keras-RL" -> "itaicaspi/keras-dqn-doom"
"instadeepai/catx" -> "instadeepai/jumanji"
"instadeepai/catx" -> "adaptive-intelligent-robotics/QDax"
"columbia-ai-robotics/diffusion_policy" -> "peract/peract"
"columbia-ai-robotics/diffusion_policy" -> "haosulab/ManiSkill2"
"columbia-ai-robotics/diffusion_policy" -> "jannerm/diffuser"
"columbia-ai-robotics/diffusion_policy" -> "siddk/voltron-robotics"
"columbia-ai-robotics/diffusion_policy" -> "ir413/mvp"
"columbia-ai-robotics/diffusion_policy" -> "vimalabs/VIMABench"
"YuhangSong/Arena-BuildingToolkit" -> "YuhangSong/Arena-Baselines"
"floodsung/DQN-Atari-Tensorflow" -> "gtoubassi/dqn-atari"
"floodsung/DQN-Atari-Tensorflow" -> "gliese581gg/DQN_tensorflow"
"oxwhirl/smacv2" -> "TonghanWang/ROMA"
"crowdAI/marLo" -> "tambetm/gym-minecraft"
"crowdAI/marLo" -> "microsoft/malmo"
"crowdAI/marLo" -> "minerllabs/minerl"
"crowdAI/marLo" -> "MultiAgentLearning/playground"
"deepmind/dm_memorytasks" -> "jurgisp/memory-maze"
"Xingyu-Lin/mbpo_pytorch" -> "jannerm/mbpo"
"Xingyu-Lin/mbpo_pytorch" -> "jiangsy/mbpo_pytorch"
"Xingyu-Lin/mbpo_pytorch" -> "JannerM/mbpo"
"jiangsy/mbpo_pytorch" -> "jiangsy/slbo_pytorch"
"ku2482/gail-airl-ppo.pytorch" -> "dsbrown1331/CoRL2019-DREX"
"ku2482/gail-airl-ppo.pytorch" -> "kristery/Imitation-Learning-from-Imperfect-Demonstration"
"xionghuichen/RLAssistant" -> "xionghuichen/MAPLE"
"xionghuichen/RLAssistant" -> "x35f/unstable_baselines"
"johan-gras/MuZero" -> "Zeta36/muzero"
"johan-gras/MuZero" -> "koulanurag/muzero-pytorch"
"johan-gras/MuZero" -> "kaesve/muzero"
"johan-gras/MuZero" -> "YuriCat/MuZeroJupyterExample"
"johan-gras/MuZero" -> "wulfebw/muzero"
"johan-gras/MuZero" -> "werner-duvaud/muzero-general"
"johan-gras/MuZero" -> "fidel-schaposnik/muzero"
"florensacc/snn4hrl" -> "yadrimz/option-critic"
"minerllabs/baselines" -> "minerllabs/competition_submission_template"
"minerllabs/baselines" -> "minerllabs/minerl"
"minerllabs/baselines" -> "minerllabs/competition_submission_starter_template"
"minerllabs/baselines" -> "amiranas/minerl_imitation_learning"
"GAIGResearch/GVGAI" -> "rubenrtorrado/GVGAI_GYM"
"takuseno/d4rl-atari" -> "takuseno/d4rl-pybullet"
"zuoxingdong/VIN_PyTorch_Visdom" -> "kentsommer/pytorch-value-iteration-networks"
"zuoxingdong/VIN_PyTorch_Visdom" -> "avivt/VIN"
"zuoxingdong/VIN_PyTorch_Visdom" -> "zuoxingdong/VIN_TensorFlow"
"zuoxingdong/VIN_PyTorch_Visdom" -> "junhyukoh/value-prediction-network"
"zuoxingdong/VIN_PyTorch_Visdom" -> "TheAbhiKumar/tensorflow-value-iteration-networks"
"zuoxingdong/VIN_PyTorch_Visdom" -> "jingweiz/pytorch-dnc" ["e"=1]
"Alfredvc/paac" -> "NVlabs/GA3C"
"Alfredvc/paac" -> "qbx2/PAAC.pytorch"
"Alfredvc/paac" -> "Kaixhin/NoisyNet-A3C"
"alexlee-gk/slac" -> "ku2482/slac.pytorch"
"facebookresearch/r3m" -> "peract/peract"
"facebookresearch/r3m" -> "ir413/mvp"
"facebookresearch/r3m" -> "siddk/voltron-robotics"
"facebookresearch/r3m" -> "mees/calvin"
"facebookresearch/r3m" -> "guhur/hiveformer"
"facebookresearch/r3m" -> "stepjam/ARM"
"facebookresearch/r3m" -> "vimalabs/VIMA"
"facebookresearch/r3m" -> "lukashermann/hulc"
"facebookresearch/r3m" -> "google-research/language-table"
"haosulab/ManiSkill2-Learn" -> "haosulab/ManiSkill2"
"ir413/mvp" -> "facebookresearch/r3m"
"ir413/mvp" -> "peract/peract"
"ir413/mvp" -> "stepjam/ARM"
"ir413/mvp" -> "guhur/hiveformer"
"ir413/mvp" -> "vimalabs/VIMA"
"ir413/mvp" -> "mees/calvin"
"ir413/mvp" -> "vimalabs/VIMABench"
"peract/peract" -> "stepjam/ARM"
"peract/peract" -> "facebookresearch/r3m"
"peract/peract" -> "vimalabs/VIMA"
"peract/peract" -> "ir413/mvp"
"peract/peract" -> "columbia-ai-robotics/diffusion_policy"
"peract/peract" -> "mees/calvin"
"peract/peract" -> "cliport/cliport"
"peract/peract" -> "haosulab/ManiSkill2"
"peract/peract" -> "lukashermann/hulc"
"peract/peract" -> "siddk/voltron-robotics"
"peract/peract" -> "facebookresearch/vip"
"peract/peract" -> "vimalabs/VIMABench"
"siddk/voltron-robotics" -> "siddk/voltron-evaluation"
"siddk/voltron-robotics" -> "facebookresearch/r3m"
"vimalabs/VIMABench" -> "vimalabs/VIMA"
"polixir/OfflineRL" -> "polixir/NeoRL"
"polixir/OfflineRL" -> "young-geng/CQL"
"Zamiell/hanabi-conventions" -> "Zamiell/hanabi-live"
"ArnaudFickinger/gym-multigrid" -> "kandouss/marlgrid"
"ArnaudFickinger/gym-multigrid" -> "semitable/lb-foraging"
"ArnaudFickinger/gym-multigrid" -> "koulanurag/ma-gym"
"ReinholdM/Offline-Pre-trained-Multi-Agent-Decision-Transformer" -> "YiqinYang/ICQ"
"Zhenye-Na/advanced-deep-learning-and-reinforcement-learning-deepmind" -> "joyiswu/UCL-Deep-learning-ans-Reinforcement-learning"
"Zhenye-Na/advanced-deep-learning-and-reinforcement-learning-deepmind" -> "YidingYu/UCL-DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning"
"HumanCompatibleAI/human_aware_rl" -> "HumanCompatibleAI/overcooked-demo"
"HumanCompatibleAI/human_aware_rl" -> "Stanford-ILIAD/PantheonRL"
"HumanCompatibleAI/human_aware_rl" -> "HumanCompatibleAI/overcooked_ai"
"HumanCompatibleAI/human_aware_rl" -> "HumanCompatibleAI/overcooked-hAI-exp"
"HumanCompatibleAI/human_aware_rl" -> "ruizhaogit/maximum_entropy_population_based_training"
"carpedm20/NAF-tensorflow" -> "Ardavans/DSR"
"carpedm20/NAF-tensorflow" -> "joschu/modular_rl"
"carpedm20/NAF-tensorflow" -> "songrotek/DDPG"
"facebookresearch/deep_bisim4control" -> "mila-iqia/spr"
"facebookresearch/deep_bisim4control" -> "nicklashansen/dmcontrol-generalization-benchmark"
"facebookresearch/deep_bisim4control" -> "denisyarats/dmc2gym"
"facebookresearch/deep_bisim4control" -> "facebookresearch/drqv2"
"facebookresearch/deep_bisim4control" -> "denisyarats/drq"
"anuragajay/decision-diffuser" -> "jannerm/diffuser"
"greydanus/baby-a3c" -> "greydanus/visualize_atari"
"DanielTakeshi/deformable-ravens" -> "google-research/ravens"
"DanielTakeshi/deformable-ravens" -> "JanMatas/Rainbow_ddpg"
"DanielTakeshi/deformable-ravens" -> "Xingyu-Lin/softgym"
"ikostrikov/walk_in_the_park" -> "lauramsmith/fine-tuning-locomotion" ["e"=1]
"ikostrikov/walk_in_the_park" -> "araffin/sbx"
"ikostrikov/walk_in_the_park" -> "TakuyaHiraoka/Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning"
"ikostrikov/walk_in_the_park" -> "ikostrikov/rlpd"
"ikostrikov/walk_in_the_park" -> "deepmind/mujoco_menagerie"
"ikostrikov/walk_in_the_park" -> "ethanluoyc/magi"
"ikostrikov/walk_in_the_park" -> "Improbable-AI/rapid-locomotion-rl" ["e"=1]
"ethz-asl/rl-navigation" -> "onlytailei/gym_style_gazebo"
"higgsfield/interaction_network_pytorch" -> "jsikyoon/Interaction-networks_tensorflow"
"higgsfield/interaction_network_pytorch" -> "jaesik817/Interaction-networks_tensorflow"
"higgsfield/interaction_network_pytorch" -> "ToruOwO/InteractionNetwork-pytorch"
"higgsfield/interaction_network_pytorch" -> "YunzhuLi/PropNet"
"higgsfield/interaction_network_pytorch" -> "YunzhuLi/DPI-Net"
"higgsfield/interaction_network_pytorch" -> "MrGemy95/visual-interaction-networks-pytorch"
"KaleabTessera/DQN-Atari" -> "jmichaux/dqn-pytorch"
"JBLanier/pipeline-psro" -> "indylab/nxdo"
"JBLanier/pipeline-psro" -> "diversepsro/diverse_psro"
"kmeng01/rome" -> "kmeng01/memit"
"kmeng01/rome" -> "aviclu/ffn-values"
"kmeng01/rome" -> "mega002/lm-debugger"
"kmeng01/rome" -> "eric-mitchell/mend"
"kmeng01/rome" -> "Hunter-DDM/knowledge-neurons"
"qzed/irl-maxent" -> "yrlu/irl-imitation"
"qzed/irl-maxent" -> "krasheninnikov/max-causal-ent-irl"
"qzed/irl-maxent" -> "ermongroup/MA-AIRL"
"zuoxingdong/DeepPILCO" -> "marcino239/pilco"
"ignc-research/all-in-one-DRL-planner" -> "ignc-research/arena-tools"
"ignc-research/all-in-one-DRL-planner" -> "ignc-research/navsafe-arena"
"ignc-research/all-in-one-DRL-planner" -> "ignc-research/arena-fsm-ego-planner"
"ignc-research/arena-tools" -> "ignc-research/navsafe-arena"
"YunzhuLi/PyFleX" -> "YunzhuLi/VGPL"
"YunzhuLi/PyFleX" -> "Xingyu-Lin/softgym"
"YunzhuLi/PyFleX" -> "YunzhuLi/DPI-Net"
"PettingZoo-Team/AutoROM" -> "PettingZoo-Team/Multi-Agent-ALE"
"PettingZoo-Team/AutoROM" -> "PettingZoo-Team/hanabi-learning-environment"
"PettingZoo-Team/MAgent" -> "PettingZoo-Team/Multi-Agent-ALE"
"PettingZoo-Team/MAgent" -> "PettingZoo-Team/hanabi-learning-environment"
"PettingZoo-Team/MAgent" -> "PettingZoo-Team/AutoROM"
"PettingZoo-Team/MAgent" -> "PettingZoo-Team/PettingZoo" ["e"=1]
"PettingZoo-Team/Multi-Agent-ALE" -> "PettingZoo-Team/hanabi-learning-environment"
"PettingZoo-Team/Multi-Agent-ALE" -> "PettingZoo-Team/AutoROM"
"jingweiz/pytorch-distributed" -> "younggyoseo/Ape-X"
"eric-mitchell/mend" -> "Hunter-DDM/knowledge-neurons"
"eric-mitchell/mend" -> "zjunlp/Kformer"
"kaesve/muzero" -> "johan-gras/MuZero"
"kaesve/muzero" -> "koulanurag/muzero-pytorch"
"ollenilsson19/QDgym" -> "ollenilsson19/PGA-MAP-Elites"
"yao62995/A3C" -> "synpon/prog_nn"
"yao62995/A3C" -> "seann999/progressive_a3c"
"yao62995/A3C" -> "miyosuda/async_deep_reinforce"
"yao62995/A3C" -> "steveKapturowski/tensorflow-rl"
"Silvicek/distributional-dqn" -> "flyyufelix/C51-DDQN-Keras"
"Silvicek/distributional-dqn" -> "go2sea/C51DQN"
"Silvicek/distributional-dqn" -> "floringogianu/categorical-dqn"
"Silvicek/distributional-dqn" -> "senya-ashukha/quantile-regression-dqn-pytorch"
"ikostrikov/rlpd" -> "ikostrikov/dmcgym"
"ikostrikov/rlpd" -> "conglu1997/v-d4rl"
"jsikyoon/dreamer-torch" -> "jurgisp/pydreamer"
"jsikyoon/dreamer-torch" -> "RajGhugare19/dreamerv2"
"jsikyoon/dreamer-torch" -> "yusukeurakami/dreamer-pytorch"
"Stable-Baselines-Team/rl-colab-notebooks" -> "araffin/rl-tutorial-jnrr19"
"typoverflow/OfflineRL-Lib" -> "typoverflow/UtilsRL"
"facebookresearch/PyTouch" -> "facebookresearch/tacto"
"facebookresearch/PyTouch" -> "CMURoboTouch/Taxim"
"facebookresearch/PyTouch" -> "NVlabs/biotac_sim"
"facebookresearch/PyTouch" -> "yikaiw/EIP"
"facebookresearch/PyTouch" -> "facebookresearch/pybulletX"
"facebookresearch/PyTouch" -> "facebookresearch/3D-Vision-and-Touch"
"facebookresearch/PyTouch" -> "facebookresearch/digit-interface"
"mrkulk/hierarchical-deep-RL" -> "EthanMacdonald/h-DQN"
"mrkulk/hierarchical-deep-RL" -> "skumar9876/Hierarchical-DQN"
"ollenilsson19/PGA-MAP-Elites" -> "ollenilsson19/QDgym"
"openai/EPG" -> "openai/robosumo"
"openai/EPG" -> "junhyukoh/self-imitation-learning"
"openai/EPG" -> "openai/vime"
"arowdy98/Stanford-CS234" -> "changebo/CS234-2020"
"columbia-ai-robotics/flingbot" -> "pantor/speedfolding"
"cyoon1729/RLcycle" -> "medipixel/rl_algorithms"
"cyoon1729/RLcycle" -> "cyoon1729/distributedRL"
"jerrodparker20/adaptive-transformers-in-rl" -> "dhruvramani/Transformers-RL"
"jerrodparker20/adaptive-transformers-in-rl" -> "yashbonde/Transformer-RL"
"ToruOwO/minimal-stable-PPO" -> "yzqin/dexpoint_sim"
"orybkin/lexa" -> "orybkin/lexa-benchmark"
"TobiasLee/Awesome-Efficient-PLM" -> "RunxinXu/Make-Information-Extraction-Great-Again"
"TobiasLee/Awesome-Efficient-PLM" -> "RunxinXu/ContrastivePruning"
"TobiasLee/Awesome-Efficient-PLM" -> "lancopku/MUKI"
"TobiasLee/Awesome-Efficient-PLM" -> "lancopku/well-classified-examples-are-underestimated"
"cyoon1729/Policy-Gradient-Methods" -> "cyoon1729/Reinforcement-learning"
"caelan/pybullet-planning" -> "yijiangh/pybullet_planning"
"caelan/pybullet-planning" -> "caelan/pddlstream"
"caelan/pybullet-planning" -> "lyf44/pybullet_ompl"
"caelan/pybullet-planning" -> "caelan/LTAMP"
"caelan/pybullet-planning" -> "eleramp/pybullet-object-models"
"caelan/pybullet-planning" -> "PKU-MARL/DexterousHands"
"caelan/pybullet-planning" -> "google-research/ravens"
"caelan/pybullet-planning" -> "robotlearn/pyrobolearn"
"caelan/pybullet-planning" -> "facebookresearch/tacto"
"caelan/pybullet-planning" -> "Healthcare-Robotics/assistive-gym"
"caelan/pybullet-planning" -> "facebookresearch/differentiable-robot-model" ["e"=1]
"ermongroup/MA-AIRL" -> "ermongroup/multiagent-gail"
"ermongroup/MA-AIRL" -> "ermongroup/MetaIRL"
"ermongroup/MA-AIRL" -> "yrlu/irl-imitation"
"ermongroup/MA-AIRL" -> "justinjfu/inverse_rl"
"ermongroup/MA-AIRL" -> "ahq1993/inverse_rl"
"ermongroup/MA-AIRL" -> "qzed/irl-maxent"
"ermongroup/MA-AIRL" -> "twni2016/f-IRL"
"ermongroup/multiagent-gail" -> "RITCHIEHuang/MAGAIL"
"ermongroup/multiagent-gail" -> "ermongroup/MA-AIRL"
"mdeib/berkeley-deep-RL-pytorch-solutions" -> "mdeib/berkeley-deep-RL-pytorch-starter"
"rraileanu/auto-drac" -> "rraileanu/idaac"
"unixpickle/obs-tower2" -> "Unity-Technologies/obstacle-tower-source"
"unixpickle/obs-tower2" -> "Unity-Technologies/obstacle-tower-challenge"
"unixpickle/obs-tower2" -> "Sohojoe/ppo-dash"
"songrotek/DQN-Atari-Tensorflow" -> "gliese581gg/DQN_tensorflow"
"deepmind/alewrap" -> "deepmind/torch-totem"
"mtrazzi/two-step-task" -> "mtrazzi/harlow"
"Coac/CommNet-BiCnet" -> "KornbergFresnel/CommNet"
"pathak22/zeroshot-imitation" -> "tianheyu927/mil"
"pathak22/zeroshot-imitation" -> "junhyukoh/self-imitation-learning"
"yadrimz/option-critic" -> "jeanharb/a2oc_delib"
"yadrimz/option-critic" -> "mklissa/PPOC"
"dsbrown1331/CoRL2019-DREX" -> "CORE-Robotics-Lab/SSRR"
"jaesik817/Interaction-networks_tensorflow" -> "jaesik817/visual-interaction-networks_tensorflow"
"jaesik817/Interaction-networks_tensorflow" -> "higgsfield/interaction_network_pytorch"
"ethanluoyc/magi" -> "henry-prior/jax-rl"
"MarcToussaint/18-RSS-PhysicalManipulation" -> "MarcToussaint/rai-python"
"MarcToussaint/rai-python" -> "MarcToussaint/rai"
"onlytailei/A3C-PyTorch" -> "onlytailei/pytorch-rl"
"onlytailei/A3C-PyTorch" -> "onlytailei/Value-Iteration-Networks-PyTorch"
"CORE-Robotics-Lab/SSRR" -> "dsbrown1331/CoRL2019-DREX"
"CORE-Robotics-Lab/SSRR" -> "syzhang092218-source/Confidence-Aware-Imitation-Learning"
"conglu1997/v-d4rl" -> "ikostrikov/dmcgym"
"haosulab/SAPIEN-Release" -> "haosulab/ManiSkill"
"haosulab/SAPIEN-Release" -> "jetd1/kuafu"
"haosulab/SAPIEN-Release" -> "dragonlong/articulated-pose" ["e"=1]
"yzqin/isaacgym-stubs" -> "yzqin/dexpoint_sim"
"deepmind/envlogger" -> "google-research/rlds"
"araffin/srl-zoo" -> "araffin/robotics-rl-srl"
"montrealrobotics/domain-randomizer" -> "montrealrobotics/active-domainrand"
"jaesik817/visual-interaction-networks_tensorflow" -> "jaesik817/Interaction-networks_tensorflow"
"jaesik817/visual-interaction-networks_tensorflow" -> "MrGemy95/visual-interaction-networks-pytorch"
"changebo/CS234-2020" -> "ksang/cs234-assignments"
"RunxinXu/ChildTuning" -> "RunxinXu/ContrastivePruning"
"RunxinXu/ChildTuning" -> "PKUnlp-icler/SCL-RAI"
"chauncygu/Safe-Reinforcement-Learning-Baseline" -> "chauncygu/Safe-Multi-Agent-Robosuite"
"YunzhuLi/DPI-Net" -> "YunzhuLi/VGPL-Dynamics-Prior"
"YunzhuLi/DPI-Net" -> "YunzhuLi/PropNet"
"YunzhuLi/DPI-Net" -> "YunzhuLi/PyFleX"
"YunzhuLi/DPI-Net" -> "cschenck/SmoothParticleNets"
"facebookresearch/online-dt" -> "daniellawson9999/online-decision-transformer"
"facebookresearch/online-dt" -> "nikhilbarhate99/min-decision-transformer"
"facebookresearch/online-dt" -> "ReinholdM/Offline-Pre-trained-Multi-Agent-Decision-Transformer"
"younggyoseo/CaDM" -> "younggyoseo/trajectory_mcl"
"younggyoseo/trajectory_mcl" -> "younggyoseo/CaDM"
"huanzhang12/ATLA_robust_RL" -> "tuomaso/radial_rl_v2"
"huanzhang12/ATLA_robust_RL" -> "chenhongge/StateAdvDRL"
"kmeng01/memit" -> "kmeng01/rome"
"NithishkumarS/DWA-RL" -> "zhlstone/DWA-RL"
"RITCHIEHuang/MAGAIL" -> "wsjeon/multiagent-gail"
"WilsonWangTHU/POPLIN" -> "kchua/handful-of-trials"
"WilsonWangTHU/POPLIN" -> "facebookresearch/slbo"
"WilsonWangTHU/POPLIN" -> "WilsonWangTHU/mbbl"
"CMURoboTouch/Taxim" -> "psodhi/tactile-in-hand"
"CMURoboTouch/Taxim" -> "danfergo/gelsight_simulation"
"CMURoboTouch/Taxim" -> "RVSATHU/GelSight-Sim2Real"
"yikaiw/EIP" -> "zixichen007115/Tacchi"
"facebookresearch/dcd" -> "ucl-dark/paired"
"paulorauber/hpg" -> "jimimvp/torch_rl"
"carolinahiguera/NCF" -> "psodhi/tactile-in-hand"
"Zce1112zslx/ChID_baseline" -> "Zce1112zslx/AGED"
"Zce1112zslx/ChID_baseline" -> "Zce1112zslx/KID"
"Zce1112zslx/ChID_baseline" -> "dqxiu/CaliNet"
"mengf1/CHER" -> "ruizhaogit/EnergyBasedPrioritization"
"MineDojo/MineCLIP" -> "MineDojo/TaskCreationUI"
"MineDojo/MineCLIP" -> "MineDojo/MineDojo"
"takuseno/d4rl-pybullet" -> "takuseno/d4rl-atari"
"takuseno/d4rl-pybullet" -> "rail-berkeley/d4rl_evaluations"
"takuseno/d4rl-pybullet" -> "takuseno/minerva"
"Kiwoo/distributional_perspective_on_RL" -> "go2sea/C51DQN"
"saizhang0218/VBC" -> "saizhang0218/TMC"
"danijar/daydreamer" -> "jurgisp/pydreamer"
"indylab/nxdo" -> "JBLanier/pipeline-psro"
"indylab/nxdo" -> "indylab/tabular_xdo"
"indylab/nxdo" -> "diversepsro/diverse_psro"
"AcutronicRobotics/moveit2" -> "AcutronicRobotics/MARA_threat_model"
"AcutronicRobotics/moveit2" -> "AcutronicRobotics/ros2learn"
"YunzhuLi/VGPL" -> "ToruOwO/VGPL-Visual-Prior"
"YunzhuLi/VGPL" -> "comphyreasoning/compositional_physics_learner"
"dqxiu/CaliNet" -> "PKUnlp-icler/SCL-RAI"
"dqxiu/CaliNet" -> "lancopku/clip-openness"
"AcutronicRobotics/MARA_threat_model" -> "AcutronicRobotics/moveit2"
"icaros-usc/dqd" -> "ollenilsson19/QDgym"
"HorizonRobotics/SocialRobot" -> "HorizonRobotics/alf"
"belepi93/Ape-X" -> "uber-research/ape-x"
"hansbuehler/deephedging" -> "pfnet-research/NoTransactionBandNetwork"
"BorealisAI/pommerman-baseline" -> "rwightman/pytorch-pommerman-rl"
"BorealisAI/pommerman-baseline" -> "eugene/pommerman"
"BorealisAI/pommerman-baseline" -> "tambetm/pommerman-baselines"
"lancopku/clip-openness" -> "PKUnlp-icler/SCL-RAI"
"chauncygu/Safe-Multi-Agent-Isaac-Gym" -> "chauncygu/Safe-Multi-Agent-Robosuite"
"jkflying/literumble" -> "darkcanuck/rumbleserver"
"deepmind/learning-to-learn" ["l"="24.828,35.787"]
"deepmind/lab" ["l"="24.969,35.86"]
"deepmind/sonnet" ["l"="24.865,35.774"]
"deepmind/dnc" ["l"="24.891,35.795"]
"openai/universe" ["l"="24.874,35.838"]
"carpedm20/deep-rl-tensorflow" ["l"="24.905,35.887"]
"openai/pixel-cnn" ["l"="33.772,32.54"]
"tensorflow/fold" ["l"="24.795,35.73"]
"blei-lab/edward" ["l"="25.451,33.61"]
"dennybritz/deeplearning-papernotes" ["l"="23.38,31.006"]
"devsisters/DQN-tensorflow" ["l"="24.875,35.888"]
"openai/universe-starter-agent" ["l"="24.919,35.858"]
"songrotek/Deep-Learning-Papers-Reading-Roadmap" ["l"="23.49,31.046"]
"BinRoot/TensorFlow-Book" ["l"="23.462,31.069"]
"tflearn/tflearn" ["l"="23.478,31.121"]
"google/seq2seq" ["l"="30.114,32.543"]
"ChenglongChen/pytorch-DRL" ["l"="25.353,36.044"]
"mohammadasghari/dqn-multi-agent-rl" ["l"="25.376,36.051"]
"Derekabc/MARL_CAVs" ["l"="25.463,36.115"]
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" ["l"="25.317,35.992"]
"marlbenchmark/on-policy" ["l"="25.334,35.988"]
"cts198859/deeprl_network" ["l"="25.341,36.028"]
"sisl/MADRL" ["l"="25.309,36.011"]
"xuehy/pytorch-maddpg" ["l"="25.293,36.021"]
"tinyzqh/light_mappo" ["l"="25.37,36.028"]
"starry-sky6688/MADDPG" ["l"="25.31,36.022"]
"Lizhi-sjtu/MARL-code-pytorch" ["l"="25.364,36.038"]
"marlbenchmark/off-policy" ["l"="25.374,36.006"]
"starry-sky6688/MARL-Algorithms" ["l"="25.385,36.026"]
"sanmuyang/multi-agent-PPO-on-SMAC" ["l"="25.421,36.079"]
"Lizhi-sjtu/DRL-code-pytorch" ["l"="25.315,36.047"]
"TianhongDai/distributed-ppo" ["l"="24.913,36.008"]
"alexis-jacq/Pytorch-DPPO" ["l"="24.947,35.991"]
"ericyangyu/PPO-for-Beginners" ["l"="25.278,35.961"]
"nikhilbarhate99/PPO-PyTorch" ["l"="25.232,35.983"]
"mila-iqia/spr" ["l"="25.318,35.759"]
"quantumiracle/Popular-RL-Algorithms" ["l"="25.239,35.959"]
"lmzintgraf/varibad" ["l"="25.214,35.778"]
"MrSyee/pg-is-all-you-need" ["l"="25.196,35.949"]
"RITCHIEHuang/DeepRL_Algorithms" ["l"="25.244,36.009"]
"AntoineTheb/RNN-RL" ["l"="25.369,36.063"]
"dongminlee94/deep_rl" ["l"="25.216,35.988"]
"vwxyzjn/ppo-implementation-details" ["l"="25.331,35.966"]
"ikostrikov/pytorch-a2c-ppo-acktr-gail" ["l"="25.174,35.953"]
"clvrai/awesome-rl-envs" ["l"="25.307,35.844"]
"XinJingHao/PPO-Continuous-Pytorch" ["l"="25.343,36.072"]
"sweetice/Deep-reinforcement-learning-with-pytorch" ["l"="25.195,36.022"]
"pranz24/pytorch-soft-actor-critic" ["l"="25.192,35.918"]
"sfujim/TD3" ["l"="25.174,35.966"]
"seungeunrho/minimalRL" ["l"="25.191,35.977"]
"higgsfield/RL-Adventure-2" ["l"="25.129,35.974"]
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" ["l"="25.143,36.03"]
"starry-sky6688/StarCraft" ["l"="25.323,36.003"]
"Khrylx/PyTorch-RL" ["l"="25.128,35.948"]
"zhangchuheng123/Reinforcement-Implementation" ["l"="25.27,36.044"]
"oxwhirl/pymarl" ["l"="25.295,35.991"]
"benelot/pybullet-gym" ["l"="25.243,35.867"]
"philtabor/Youtube-Code-Repository" ["l"="25.246,36.035"]
"philtabor/Deep-Q-Learning-Paper-To-Code" ["l"="25.295,36.112"]
"philtabor/Actor-Critic-Methods-Paper-To-Code" ["l"="25.275,36.114"]
"pythonlessons/Reinforcement_Learning" ["l"="25.156,36.076"]
"philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients" ["l"="25.335,36.043"]
"rail-berkeley/softlearning" ["l"="25.19,35.863"]
"marload/DeepRL-TensorFlow2" ["l"="25.202,36.044"]
"philtabor/Reinforcement-Learning-In-Motion" ["l"="25.289,36.129"]
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" ["l"="25.19,35.995"]
"oxwhirl/smac" ["l"="25.319,35.975"]
"cyanrain7/TRPO-in-MARL" ["l"="25.409,35.991"]
"chauncygu/Multi-Agent-Constrained-Policy-Optimisation" ["l"="25.438,35.962"]
"uoe-agents/epymarl" ["l"="25.376,35.979"]
"Replicable-MARL/MARLlib" ["l"="25.386,35.993"]
"TimeBreaker/MARL-papers-with-code" ["l"="25.467,36.072"]
"shariqiqbal2810/maddpg-pytorch" ["l"="25.341,36.005"]
"vwxyzjn/cleanrl" ["l"="25.271,35.899"]
"vwxyzjn/invalid-action-masking" ["l"="25.433,35.991"]
"wangcongrobot/awesome-isaac-gym" ["l"="25.477,35.84"]
"Stable-Baselines-Team/stable-baselines3-contrib" ["l"="25.351,35.89"]
"distillpub/template" ["l"="25.174,35.692"]
"deepmind/bsuite" ["l"="25.188,35.838"]
"distillpub/post--example" ["l"="25.148,35.607"]
"deepmind/dm-haiku" ["l"="23.513,33.768"]
"deepmind/jraph" ["l"="23.507,33.787"]
"arxiv-vanity/arxiv-vanity" ["l"="20.669,-34.383"]
"tensorflow/lucid" ["l"="34.11,35.539"]
"maximecb/gym-minigrid" ["l"="25.206,35.84"]
"idyll-lang/idyll" ["l"="16.723,-31.911"]
"ramanans1/plan2explore" ["l"="25.277,35.718"]
"maximecb/gym-miniworld" ["l"="25.23,35.783"]
"google/jaxopt" ["l"="23.475,33.765"]
"ermongroup/cs228-notes" ["l"="25.499,33.524"]
"worldmodels/worldmodels.github.io" ["l"="25.135,35.709"]
"dennybritz/reinforcement-learning" ["l"="24.974,36.051"]
"ShangtongZhang/reinforcement-learning-an-introduction" ["l"="25.026,36.063"]
"aikorea/awesome-rl" ["l"="24.961,36.021"]
"openai/baselines" ["l"="25.044,35.992"]
"openai/gym" ["l"="24.91,36.054"]
"MorvanZhou/Reinforcement-learning-with-tensorflow" ["l"="25.049,36.093"]
"openai/spinningup" ["l"="25.097,36.01"]
"google/dopamine" ["l"="25.013,35.966"]
"terryum/awesome-deep-learning-papers" ["l"="23.54,30.955"]
"udacity/deep-reinforcement-learning" ["l"="25.053,36.044"]
"aymericdamien/TensorFlow-Examples" ["l"="23.614,31.017"]
"yandexdataschool/Practical_RL" ["l"="25.014,36.035"]
"yunjey/pytorch-tutorial" ["l"="23.801,30.989"]
"pytorch/examples" ["l"="34.032,35.559"]
"hill-a/stable-baselines" ["l"="25.149,35.942"]
"DLR-RM/stable-baselines3" ["l"="25.211,35.961"]
"thu-ml/tianshou" ["l"="25.179,36.036"]
"openai/mujoco-py" ["l"="25.149,35.913"]
"rlworkgroup/garage" ["l"="25.184,35.897"]
"astooke/rlpyt" ["l"="25.174,35.911"]
"rll/rllab" ["l"="25.103,35.917"]
"deepmind/trfl" ["l"="25.08,35.901"]
"ray-project/ray" ["l"="25.254,33.964"]
"Farama-Foundation/Gymnasium" ["l"="25.293,35.893"]
"Farama-Foundation/PettingZoo" ["l"="25.341,35.951"]
"DLR-RM/rl-baselines3-zoo" ["l"="25.301,35.91"]
"pytorch/rl" ["l"="25.394,35.797"]
"sail-sg/envpool" ["l"="25.356,35.827"]
"deepmind/mujoco" ["l"="25.294,35.872"]
"google/brax" ["l"="25.366,35.816"]
"tinkoff-ai/CORL" ["l"="25.364,35.799"]
"NVIDIA-Omniverse/IsaacGymEnvs" ["l"="25.446,35.838"]
"deepmind/dm_control" ["l"="25.149,35.865"]
"google-research/rliable" ["l"="25.349,35.776"]
"princewen/tensorflow_practice" ["l"="22.756,37.093"]
"rlcode/reinforcement-learning" ["l"="24.966,35.977"]
"MorvanZhou/Tensorflow-Tutorial" ["l"="23.768,31.145"]
"NeuronDance/DeepRL" ["l"="25.206,36.076"]
"datawhalechina/easy-rl" ["l"="25.207,36.122"]
"Denys88/rl_games" ["l"="25.456,35.827"]
"NVIDIA-Omniverse/OmniIsaacGymEnvs" ["l"="25.507,35.834"]
"leggedrobotics/legged_gym" ["l"="28.187,42.155"]
"nv-tlabs/ASE" ["l"="35.571,35.154"]
"PKU-MARL/DexterousHands" ["l"="25.492,35.837"]
"deepmind/mujoco_menagerie" ["l"="25.457,35.795"]
"xbpeng/DeepMimic" ["l"="35.637,35.188"]
"Toni-SM/skrl" ["l"="25.513,35.847"]
"leggedrobotics/rsl_rl" ["l"="28.215,42.134"]
"NVIDIA-Omniverse/Orbit" ["l"="25.511,35.814"]
"stepjam/RLBench" ["l"="25.39,35.841"]
"ARISE-Initiative/robosuite" ["l"="25.413,35.829"]
"google-research/football" ["l"="25.295,35.954"]
"openai/multiagent-particle-envs" ["l"="25.289,35.978"]
"deepmind/open_spiel" ["l"="25.206,35.896"]
"LantaoYu/MARL-Papers" ["l"="25.223,36.007"]
"openai/maddpg" ["l"="25.289,36.006"]
"openai/multi-agent-emergence-environments" ["l"="25.283,35.935"]
"deepmind/acme" ["l"="25.208,35.868"]
"BazkieBumpercar/GameplayFootball" ["l"="25.515,36.036"]
"keon/awesome-nlp" ["l"="23.52,31.015"]
"src-d/awesome-machine-learning-on-source-code" ["l"="23.357,31.119"]
"jbhuang0604/awesome-computer-vision" ["l"="23.458,30.988"]
"GoogleTrends/data" ["l"="23.336,31.14"]
"igrigorik/decisiontree" ["l"="23.353,31.157"]
"scikit-learn-contrib/lightning" ["l"="23.355,31.174"]
"ChristosChristofidis/awesome-deep-learning" ["l"="23.54,30.912"]
"nlintz/TensorFlow-Tutorials" ["l"="23.401,31.096"]
"jtoy/awesome-tensorflow" ["l"="23.475,31.019"]
"kjw0612/awesome-deep-vision" ["l"="23.42,31"]
"PaddlePaddle/PARL" ["l"="25.175,36.084"]
"zhoubolei/introRL" ["l"="25.19,36.103"]
"wangshusen/DRL" ["l"="25.252,36.101"]
"PaddlePaddle/PGL" ["l"="21.852,38.533"]
"AI4Finance-Foundation/ElegantRL" ["l"="25.255,36.058"]
"ShangtongZhang/DeepRL" ["l"="25.109,35.971"]
"yanpanlau/Keras-FlappyBird" ["l"="24.815,35.941"]
"yanpanlau/DDPG-Keras-Torcs" ["l"="24.91,35.94"]
"matthiasplappert/keras-rl" ["l"="24.894,35.902"]
"coreylynch/async-rl" ["l"="24.867,35.873"]
"keon/deep-q-learning" ["l"="24.886,35.963"]
"farizrahman4u/qlearning4k" ["l"="24.787,35.896"]
"pytorch/pytorch" ["l"="33.791,35.479"]
"tensorflow/models" ["l"="33.821,35.342"]
"scikit-learn/scikit-learn" ["l"="33.568,35.426"]
"fchollet/keras" ["l"="23.498,31.087"]
"tensorflow/tensorflow" ["l"="33.533,35.343"]
"Unity-Technologies/ml-agents" ["l"="-13.462,40.046"]
"YeWR/EfficientZero" ["l"="25.341,35.762"]
"werner-duvaud/muzero-general" ["l"="25.31,35.821"]
"koulanurag/muzero-pytorch" ["l"="25.373,35.738"]
"eloialonso/iris" ["l"="25.338,35.749"]
"rll-research/url_benchmark" ["l"="25.333,35.733"]
"danijar/dreamerv2" ["l"="25.29,35.749"]
"danijar/dreamerv3" ["l"="25.356,35.738"]
"kzl/decision-transformer" ["l"="25.329,35.787"]
"MishaLaskin/curl" ["l"="25.278,35.757"]
"facebookresearch/mbrl-lib" ["l"="25.31,35.797"]
"danijar/crafter" ["l"="25.333,35.718"]
"jurgisp/pydreamer" ["l"="25.335,35.692"]
"deepmind/mctx" ["l"="23.562,33.748"]
"rail-berkeley/d4rl" ["l"="25.299,35.811"]
"cpnota/autonomous-learning-library" ["l"="25.266,35.863"]
"PettingZoo-Team/PettingZoo" ["l"="25.683,36.861"]
"pfnet/pfrl" ["l"="25.247,35.84"]
"MushroomRL/mushroom-rl" ["l"="25.274,35.834"]
"kengz/awesome-deep-rl" ["l"="25.243,35.857"]
"JannerM/mbpo" ["l"="25.238,35.762"]
"learnables/cherry" ["l"="25.327,35.845"]
"quantumiracle/SOTA-RL-Algorithms" ["l"="25.248,35.95"]
"fabiopardo/tonic" ["l"="25.287,35.826"]
"iffiX/machin" ["l"="25.277,35.987"]
"deepmind/spriteworld" ["l"="25.223,35.852"]
"deepmind/rlax" ["l"="23.535,33.749"]
"vitchyr/rlkit" ["l"="25.152,35.881"]
"google-research/planet" ["l"="25.191,35.784"]
"deepmind/reverb" ["l"="25.262,35.826"]
"facebookresearch/Horizon" ["l"="25.101,35.879"]
"deepmind/graph_nets" ["l"="21.74,38.632"]
"tensorflow/agents" ["l"="25.083,35.926"]
"deepmind/scalable_agent" ["l"="25.144,35.839"]
"NervanaSystems/coach" ["l"="25.079,35.885"]
"tensorflow/adanet" ["l"="25.583,33.899"]
"reinforceio/tensorforce" ["l"="24.99,35.887"]
"germain-hug/Deep-RL-Keras" ["l"="25.069,36.03"]
"keras-rl/keras-rl" ["l"="25.021,36.01"]
"keiohta/tf2rl" ["l"="25.184,36.01"]
"xiaochus/Deep-Reinforcement-Learning-Practice" ["l"="24.902,36.178"]
"wau/keras-rl2" ["l"="24.984,36.139"]
"tensorforce/tensorforce" ["l"="25.133,35.929"]
"cyoon1729/deep-Q-networks" ["l"="25.109,36.106"]
"TianhongDai/reinforcement-learning-algorithms" ["l"="25.175,35.989"]
"anita-hu/TF2-RL" ["l"="25.215,36.057"]
"floodsung/DDPG" ["l"="25,35.991"]
"miroblog/deep_rl_trader" ["l"="21.69,32.442"]
"rlworkgroup/metaworld" ["l"="25.278,35.814"]
"araffin/rl-baselines-zoo" ["l"="25.232,35.907"]
"wwxFromTju/awesome-reinforcement-learning-zh" ["l"="25.175,36.122"]
"tigerneil/awesome-deep-rl" ["l"="25.255,36.023"]
"spragunr/deep_q_rl" ["l"="24.822,35.884"]
"kristjankorjus/Replicating-DeepMind" ["l"="24.767,35.878"]
"tambetm/simple_dqn" ["l"="24.85,35.877"]
"kuz/DeepMind-Atari-Deep-Q-Learner" ["l"="24.819,35.867"]
"nivwusquorum/tensorflow-deepq" ["l"="24.832,35.894"]
"mgbellemare/Arcade-Learning-Environment" ["l"="24.929,35.84"]
"muupan/deep-reinforcement-learning-papers" ["l"="24.873,35.902"]
"asrivat1/DeepLearningVideoGames" ["l"="24.804,35.905"]
"muupan/dqn-in-the-caffe" ["l"="24.744,35.852"]
"rllab/rllab" ["l"="24.873,35.859"]
"muupan/async-rl" ["l"="24.886,35.857"]
"shawntan/neural-turing-machines" ["l"="27.077,34.27"]
"miyosuda/async_deep_reinforce" ["l"="24.899,35.87"]
"facebookresearch/ELF" ["l"="25.017,35.849"]
"pytorch/ELF" ["l"="25.455,38.808"]
"TorchCraft/TorchCraft" ["l"="24.211,37.512"]
"ikostrikov/pytorch-a2c-ppo-acktr" ["l"="25.045,35.899"]
"williamFalcon/DeepRLHacks" ["l"="25.01,35.879"]
"facebookresearch/darkforestGo" ["l"="25.436,38.773"]
"openai/roboschool" ["l"="25.048,35.87"]
"pathak22/noreward-rl" ["l"="25.045,35.841"]
"geek-ai/MAgent" ["l"="25.263,35.971"]
"boyu-ai/Hands-on-RL" ["l"="25.309,36.083"]
"cuhkrlcourse/RLexample" ["l"="25.216,36.097"]
"mli/paper-reading" ["l"="34.376,35.844"]
"wangshusen/DeepLearning" ["l"="25.215,36.19"]
"tensorflow/tensor2tensor" ["l"="30.047,32.545"]
"tensorflow/magenta" ["l"="33.66,32.394"]
"PAIR-code/facets" ["l"="25.452,33.857"]
"uber/horovod" ["l"="34.123,35.271"]
"deepmind/pysc2" ["l"="24.256,37.518"]
"facebookresearch/fastText" ["l"="30.048,32.705"]
"uber/pyro" ["l"="25.461,33.65"]
"facebookresearch/visdom" ["l"="34.137,35.481"]
"Ericonaldo/ILSwiss" ["l"="25.248,35.975"]
"apexrl/Imitation-Learning-Paper-Lists" ["l"="25.231,35.969"]
"kristery/Awesome-Imitation-Learning" ["l"="25.191,35.937"]
"Farama-Foundation/Minigrid" ["l"="25.257,35.637"]
"Farama-Foundation/Miniworld" ["l"="25.254,35.565"]
"lcswillems/rl-starter-files" ["l"="25.232,35.712"]
"Farama-Foundation/Gymnasium-Robotics" ["l"="25.353,35.631"]
"kaixindelele/DRLib" ["l"="25.326,36.058"]
"mengwanglalala/RL-algorithms" ["l"="25.342,36.111"]
"XinJingHao/RL-Algorithms-by-Pytorch" ["l"="25.333,36.097"]
"ctgk/PRML" ["l"="23.911,31.059"]
"louisnino/RLcode" ["l"="25.286,36.087"]
"tensorlayer/TensorLayer" ["l"="25.331,36.146"]
"StepNeverStop/RLs" ["l"="25.267,36.034"]
"qqiang00/Reinforce" ["l"="25.275,36.077"]
"NovemberChopin/RL_Tutorial" ["l"="25.335,36.171"]
"luozachary/drl-rec" ["l"="24.095,36.474"]
"Yonv1943/ElegantRL" ["l"="25.287,36.061"]
"openai/random-network-distillation" ["l"="25.132,35.816"]
"jcwleo/random-network-distillation-pytorch" ["l"="25.033,35.769"]
"openai/large-scale-curiosity" ["l"="25.108,35.814"]
"haarnoja/sac" ["l"="25.171,35.878"]
"uber-research/go-explore" ["l"="25.16,35.795"]
"junhyukoh/self-imitation-learning" ["l"="25.08,35.806"]
"openai/coinrun" ["l"="25.153,35.807"]
"Kaixhin/Rainbow" ["l"="25.109,35.903"]
"uber-research/ape-x" ["l"="25.099,35.745"]
"sfujim/BCQ" ["l"="25.259,35.814"]
"rail-berkeley/rlkit" ["l"="25.289,35.852"]
"takuseno/d3rlpy" ["l"="25.327,35.814"]
"aviralkumar2907/CQL" ["l"="25.325,35.772"]
"denisyarats/pytorch_sac" ["l"="25.251,35.804"]
"hanjuku-kaso/awesome-offline-rl" ["l"="25.337,35.8"]
"ikostrikov/jaxrl" ["l"="25.38,35.766"]
"qgallouedec/panda-gym" ["l"="25.433,35.855"]
"tensorlayer/RLzoo" ["l"="33.107,37.484"]
"tensorlayer/TensorLayerX" ["l"="-25.607,19.339"]
"deep-reinforcement-learning-book/Chapter4-DQN" ["l"="25.362,36.188"]
"tensorlayer/tensorlayer-chinese" ["l"="25.388,36.226"]
"KISS1996/trexminer" ["l"="22.009,27.694"]
"salesforce/ai-economist" ["l"="25.333,35.918"]
"salesforce/warp-drive" ["l"="25.392,35.894"]
"deepmind/meltingpot" ["l"="25.384,35.922"]
"microsoft/EconML" ["l"="23.793,33.195"]
"instadeepai/Mava" ["l"="25.394,35.907"]
"eugenevinitsky/sequential_social_dilemma_games" ["l"="25.35,35.984"]
"projectmesa/mesa" ["l"="17.075,39.419"]
"antontarasenko/awesome-economics" ["l"="23.016,33.084"]
"hardmaru/slimevolleygym" ["l"="25.293,35.841"]
"KennethJudd/CompEcon2020" ["l"="23.031,33.061"]
"openai/procgen" ["l"="25.224,35.817"]
"hijkzzz/pymarl2" ["l"="25.36,35.995"]
"TimeBreaker/Multi-Agent-Reinforcement-Learning-papers" ["l"="25.485,36.08"]
"PKU-MARL/Multi-Agent-Transformer" ["l"="25.4,36"]
"oxwhirl/wqmix" ["l"="25.459,36.04"]
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" ["l"="25.124,36.06"]
"Shmuma/ptan" ["l"="25.08,35.956"]
"simoninithomas/Deep_reinforcement_learning_Course" ["l"="25.065,36.067"]
"PacktPublishing/Deep-Learning-with-Keras" ["l"="22.541,29.43"]
"higgsfield/RL-Adventure" ["l"="25.123,35.996"]
"andri27-ts/Reinforcement-Learning" ["l"="25.1,36.058"]
"deepmind/android_env" ["l"="25.361,35.724"]
"HaveIBeenPwned/PwnedPasswordsCloudflareWorker" ["l"="25.425,35.594"]
"neuralmagic/sparseml" ["l"="27.728,31.009"]
"StarInitial/xpcheck" ["l"="25.416,35.626"]
"vxunderground/WinAPI-Tricks" ["l"="-8.863,-44.154"]
"sorenisanerd/gotty" ["l"="25.847,13.995"]
"stack-of-tasks/pinocchio" ["l"="28.124,42.126"]
"bulletphysics/bullet3" ["l"="-12.655,26.094"]
"RobotLocomotion/drake" ["l"="28.123,42.104"]
"tonyzhaozh/aloha" ["l"="25.606,35.757"]
"tonyzhaozh/act" ["l"="25.632,35.759"]
"haosulab/ManiSkill2" ["l"="25.559,35.793"]
"columbia-ai-robotics/diffusion_policy" ["l"="25.512,35.781"]
"facebookresearch/eai-vc" ["l"="25.578,35.788"]
"google-research/robopianist" ["l"="25.52,35.772"]
"eanswer/TactileSimulation" ["l"="-11.439,26.815"]
"columbia-ai-robotics/irp" ["l"="25.652,35.73"]
"Microsoft/malmo" ["l"="24.926,35.802"]
"facebookresearch/CommAI-env" ["l"="27.161,34.416"]
"tambetm/gym-minecraft" ["l"="25.129,35.733"]
"Microsoft/AirSim" ["l"="29.061,42.819"]
"Microsoft/CNTK" ["l"="27.089,33.876"]
"AcutronicRobotics/ros2learn" ["l"="25.544,35.966"]
"AcutronicRobotics/gym-gazebo2" ["l"="25.46,35.929"]
"AcutronicRobotics/moveit2" ["l"="25.54,35.953"]
"AcutronicRobotics/MARA_threat_model" ["l"="25.524,35.956"]
"AcutronicRobotics/MARA" ["l"="25.56,35.959"]
"AcutronicRobotics/HRIM" ["l"="25.6,35.989"]
"stanfordnlp/treelstm" ["l"="27.078,34.366"]
"clab/dynet" ["l"="30.165,32.577"]
"lanpa/tensorboard-pytorch" ["l"="34.262,35.363"]
"LantaoYu/SeqGAN" ["l"="30.043,32.267"]
"ppwwyyxx/tensorpack" ["l"="34.217,35.125"]
"tensorflow/transform" ["l"="25.555,34.175"]
"openai/cleverhans" ["l"="27.204,34.491"]
"taolei87/sru" ["l"="30.094,32.518"]
"tensorflow/skflow" ["l"="27.157,34.187"]
"openai/multiagent-competition" ["l"="25.164,35.891"]
"openai/robosumo" ["l"="25.059,35.822"]
"openai/mlsh" ["l"="25.063,35.809"]
"openai/imitation" ["l"="25.018,35.893"]
"deepmind/pycolab" ["l"="25.166,35.834"]
"MultiAgentLearning/playground" ["l"="25.251,35.93"]
"tensorlayer/tensorlayer" ["l"="25.013,36.102"]
"tensorlayer/srgan" ["l"="35.9,32.161"]
"tensorlayer/awesome-tensorlayer" ["l"="24.961,36.161"]
"tensorpack/tensorpack" ["l"="34.319,35.309"]
"awjuliani/DeepRL-Agents" ["l"="24.945,35.912"]
"joschu/modular_rl" ["l"="24.965,35.878"]
"NVlabs/GA3C" ["l"="24.946,35.865"]
"shariqiqbal2810/MAAC" ["l"="25.334,36.014"]
"chainer/chainerrl" ["l"="25.119,35.866"]
"kengz/SLM-Lab" ["l"="25.137,35.898"]
"miyosuda/unreal" ["l"="24.947,35.848"]
"Kaixhin/Atari" ["l"="24.815,35.823"]
"Ardavans/DSR" ["l"="24.918,35.824"]
"Jabberwockyll/deep_rl_ale" ["l"="24.771,35.835"]
"Zeta36/Asynchronous-Methods-for-Deep-Reinforcement-Learning" ["l"="24.768,35.811"]
"traai/async-deep-rl" ["l"="24.783,35.823"]
"openai/rllab" ["l"="24.923,35.871"]
"DeepRLChinese/DeepRL-Chinese" ["l"="25.286,36.156"]
"MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning" ["l"="25.323,36.033"]
"wangshusen/RecommenderSystem" ["l"="22.685,37.018"]
"wmathor/nlp-tutorial" ["l"="32.214,30.182"]
"dair-ai/ml-visuals" ["l"="34.293,35.778"]
"wangshusen/AdvancedAlgorithms" ["l"="25.234,36.283"]
"xmu-xiaoma666/External-Attention-pytorch" ["l"="34.458,35.803"]
"TingsongYu/PyTorch_Tutorial" ["l"="24.231,31.037"]
"tsyw/MachineLearningNotes" ["l"="24.199,31.156"]
"DA-southampton/NLP_ability" ["l"="32.11,30.203"]
"chaoyanghe/Awesome-Federated-Learning" ["l"="21.064,27.136"]
"minqi/learning-to-communicate-pytorch" ["l"="25.354,36.031"]
"social-dilemma/multiagent" ["l"="25.424,36.016"]
"koulanurag/ma-gym" ["l"="25.361,35.969"]
"IC3Net/IC3Net" ["l"="25.396,36.061"]
"YuhangSong/Arena-Baselines" ["l"="25.463,35.991"]
"alshedivat/lola" ["l"="25.149,36.058"]
"ermongroup/MA-AIRL" ["l"="25.083,35.985"]
"PKU-AI-Edge/DGN" ["l"="25.368,36.014"]
"schroederdewitt/multiagent_mujoco" ["l"="25.396,35.97"]
"HumanCompatibleAI/human_aware_rl" ["l"="25.426,35.97"]
"TonghanWang/ROMA" ["l"="25.436,36.026"]
"deepmind/hanabi-learning-environment" ["l"="25.269,35.942"]
"facebookresearch/hanabi_SAD" ["l"="25.38,36.097"]
"facebookresearch/Hanabi_SPARTA" ["l"="25.364,36.082"]
"openai/neural-mmo" ["l"="25.266,36"]
"vitchyr/multiworld" ["l"="25.234,35.804"]
"Quuxplusone/Hanabi" ["l"="25.377,36.109"]
"Scitator/Run-Skeleton-Run" ["l"="24.87,35.929"]
"fgvbrt/nips_rl" ["l"="29.345,35.758"]
"AdamStelmaszczyk/learning2run" ["l"="24.828,35.953"]
"nnaisense/2017-learning-to-run" ["l"="24.917,35.92"]
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" ["l"="25.24,36.121"]
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" ["l"="25.097,36.08"]
"qqiang00/reinforce" ["l"="25.299,36.233"]
"PacktPublishing/Reinforcement-Learning-Algorithms-with-Python" ["l"="25.235,36.219"]
"ZhiqingXiao/rl-book" ["l"="25.235,36.085"]
"gxnk/reinforcement-learning-code" ["l"="25.278,36.176"]
"PacktPublishing/Python-Reinforcement-Learning-Projects" ["l"="25.287,36.204"]
"CoderWangcai/DRL_Path_Planning" ["l"="25.56,36.167"]
"cuhkrlcourse/DeepRL-Tutorials" ["l"="25.19,36.141"]
"datawhalechina/leedeeprl-notes" ["l"="25.227,36.109"]
"openai/retro" ["l"="25.075,35.866"]
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" ["l"="25.218,36.033"]
"sjtu-marl/malib" ["l"="25.349,35.967"]
"gxywy/rl-plotter" ["l"="25.334,36.125"]
"junhyukoh/deep-reinforcement-learning-papers" ["l"="24.937,35.936"]
"udacity/deep-learning-v2-pytorch" ["l"="23.769,30.84"]
"udacity/deep-learning" ["l"="23.629,30.933"]
"andyljones/reinforcement-learning-discord-wiki" ["l"="25.179,35.662"]
"laszukdawid/ai-traineree" ["l"="25.174,35.608"]
"RchalYang/torchrl" ["l"="25.269,36.133"]
"BY571/Soft-Actor-Critic-and-Extensions" ["l"="25.216,35.944"]
"qfettes/DeepRL-Tutorials" ["l"="25.145,35.993"]
"dongminlee94/Samsung-DRL-Code" ["l"="44.58,-14.768"]
"reinforcement-learning-kr/lets-do-irl" ["l"="25.137,35.961"]
"facebookresearch/slbo" ["l"="25.21,35.631"]
"roosephu/slbo" ["l"="25.202,35.569"]
"Curt-Park/rainbow-is-all-you-need" ["l"="25.158,35.977"]
"Kautenja/gym-super-mario-bros" ["l"="25.151,35.822"]
"Kautenja/nes-py" ["l"="25.12,35.796"]
"ppaquette/gym-super-mario" ["l"="25.004,35.721"]
"uvipen/Super-mario-bros-A3C-pytorch" ["l"="25.127,35.881"]
"uvipen/Super-mario-bros-PPO-pytorch" ["l"="25.224,35.932"]
"jcwleo/mario_rl" ["l"="25.103,35.759"]
"mpSchrader/gym-sokoban" ["l"="25.24,35.753"]
"mwydmuch/ViZDoom" ["l"="25.075,35.829"]
"minerllabs/minerl" ["l"="25.258,35.732"]
"kenjyoung/MinAtar" ["l"="25.303,35.758"]
"reiniscimurs/DRL-robot-navigation" ["l"="25.642,36.288"]
"reiniscimurs/GDAE" ["l"="25.659,36.32"]
"m5823779/motion-planner-reinforcement-learning" ["l"="25.618,36.272"]
"LeeKeyu/sarl_star" ["l"="25.618,36.228"]
"Crawford-fang/turtlebot3_DQN" ["l"="25.695,36.351"]
"applenob/rl_learn" ["l"="25.187,36.239"]
"zhuliquan/reinforcement_learning_basic_book" ["l"="25.228,36.248"]
"apachecn/stanford-cs234-notes-zh" ["l"="25.174,36.293"]
"berkeleydeeprlcourse/homework" ["l"="25.068,35.942"]
"minerllabs/competition_submission_template" ["l"="25.262,35.651"]
"minerllabs/baselines" ["l"="25.248,35.667"]
"sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python" ["l"="25.036,36.188"]
"sudharsan13296/Deep-Reinforcement-Learning-With-Python" ["l"="25.046,36.165"]
"sudharsan13296/Hands-On-Meta-Learning-With-Python" ["l"="23.762,35.445"]
"omerbsezer/Reinforcement_learning_tutorial_with_demo" ["l"="25.155,36.044"]
"vmayoral/basic_reinforcement_learning" ["l"="24.997,35.942"]
"dalmia/David-Silver-Reinforcement-learning" ["l"="25.097,36.042"]
"sudharsan13296/Awesome-Meta-Learning" ["l"="23.728,35.413"]
"glample/Arnold" ["l"="24.982,35.778"]
"akolishchak/doom-net-pytorch" ["l"="24.905,35.723"]
"ruiminshen/yolo2-pytorch" ["l"="34.505,35.199"]
"Breakend/gym-extensions" ["l"="24.934,35.768"]
"IntelVCL/DirectFuturePrediction" ["l"="24.893,35.753"]
"ikostrikov/pytorch-a3c" ["l"="25.034,35.888"]
"zuoxingdong/gym-maze" ["l"="25.005,35.683"]
"itaicaspi/keras-dqn-doom" ["l"="24.811,35.607"]
"iabem97/topanga" ["l"="-49.166,-0.62"]
"Breakend/DeepReinforcementLearningThatMatters" ["l"="24.961,35.811"]
"AppliedDataSciencePartners/WorldModels" ["l"="25.12,35.695"]
"semitable/robotic-warehouse" ["l"="25.446,35.976"]
"semitable/lb-foraging" ["l"="25.422,35.958"]
"uoe-agents/robotic-warehouse" ["l"="25.504,35.998"]
"omerbsezer/Generative_Models_Tutorial_with_Demo" ["l"="25.121,36.134"]
"DeepReinforcementLearning/DeepReinforcementLearningInAction" ["l"="25.166,36.009"]
"mimoralea/gdrl" ["l"="25.113,36.029"]
"maxpumperla/deep_learning_and_the_game_of_go" ["l"="25.451,38.786"]
"andri27-ts/1-Year-MachineLearning-Journey" ["l"="25.057,36.134"]
"oxford-cs-deepnlp-2017/lectures" ["l"="23.5,30.985"]
"medipixel/rl_algorithms" ["l"="25.172,35.931"]
"opherlieber/rltime" ["l"="25.208,35.801"]
"cyoon1729/RLcycle" ["l"="25.127,36.042"]
"kakaoenterprise/JORLDY" ["l"="44.602,-14.755"]
"unixpickle/anyrl-py" ["l"="24.434,37.532"]
"reinforcement-learning-kr/pg_travel" ["l"="25.099,35.983"]
"rlgraph/rlgraph" ["l"="25.333,35.939"]
"kairproject/kair_algorithms_draft" ["l"="25.162,35.992"]
"nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch" ["l"="25.099,35.785"]
"jingweiz/pytorch-rl" ["l"="25.03,35.924"]
"yenchenlin/DeepLearningFlappyBird" ["l"="24.828,35.996"]
"lengstrom/fast-style-transfer" ["l"="33.565,32.515"]
"Rochester-NRT/RocAlphaGo" ["l"="25.396,38.804"]
"sourabhv/FlapPyBird" ["l"="24.716,35.957"]
"carpedm20/DCGAN-tensorflow" ["l"="33.729,32.615"]
"cypypccpy/Isaac-ManipulaRL" ["l"="25.59,35.853"]
"iamlab-cmu/isaacgym-utils" ["l"="25.543,35.846"]
"pairlab/leibnizgym" ["l"="25.549,35.855"]
"google-research/dreamer" ["l"="25.24,35.737"]
"danijar/dreamer" ["l"="25.252,35.746"]
"juliusfrost/dreamer-pytorch" ["l"="25.269,35.725"]
"yusukeurakami/dreamer-pytorch" ["l"="25.273,35.693"]
"MishaLaskin/rad" ["l"="25.287,35.726"]
"Kaixhin/PlaNet" ["l"="25.235,35.725"]
"kchua/handful-of-trials" ["l"="25.203,35.751"]
"ctallec/world-models" ["l"="25.184,35.715"]
"WilsonWangTHU/mbbl" ["l"="25.227,35.773"]
"katerakelly/oyster" ["l"="25.179,35.803"]
"tristandeleu/pytorch-maml-rl" ["l"="23.773,35.463"]
"iclavera/learning_to_adapt" ["l"="25.184,35.772"]
"ir413/mvp" ["l"="25.532,35.811"]
"erwincoumans/motion_imitation" ["l"="28.19,42.143"]
"google-research/ravens" ["l"="25.499,35.823"]
"Mehooz/vision4leg" ["l"="28.219,42.155"]
"suragnair/alpha-zero-general" ["l"="25.456,38.854"]
"johan-gras/MuZero" ["l"="25.41,35.707"]
"kaesve/muzero" ["l"="25.406,35.723"]
"robo-code/robocode" ["l"="24.541,36.111"]
"robocode-dev/tank-royale" ["l"="24.517,36.138"]
"turkishviking/Python-Robocode" ["l"="24.486,36.129"]
"Voidious/Diamond" ["l"="24.509,36.12"]
"jkflying/literumble" ["l"="24.509,36.107"]
"stevenpjg/QlearningRobocodeNN" ["l"="24.704,36.047"]
"clvrai/furniture" ["l"="25.429,35.838"]
"AboudyKreidieh/h-baselines" ["l"="25.23,35.837"]
"hardmaru/estool" ["l"="24.677,38.374"]
"deepmind/dqn_zoo" ["l"="25.273,35.804"]
"hardmaru/rlzoo" ["l"="25.294,35.799"]
"openai/robogym" ["l"="25.416,35.8"]
"eleurent/highway-env" ["l"="29.856,45.164"]
"rasmusbergpalm/evostrat" ["l"="24.534,38.542"]
"seungjaeryanlee/awesome-rl-competitions" ["l"="25.263,35.878"]
"2019ChenGong/RL-Paper-notes" ["l"="25.37,35.931"]
"xionghuichen/RLAssistant" ["l"="25.481,35.917"]
"wwxFromTju/awesome-reinforcement-learning-lib" ["l"="25.468,35.811"]
"chauncygu/Safe-Reinforcement-Learning-Baselines" ["l"="25.409,35.886"]
"huawei-noah/SMARTS" ["l"="29.877,45.159"]
"elleryqueenhomels/AI_for_Atari" ["l"="21.906,27.626"]
"elleryqueenhomels/google_sketcher" ["l"="21.892,27.624"]
"TracyWang95/legal-prompts-for-gpt" ["l"="21.914,27.606"]
"weiwensangsang/golang-internal" ["l"="21.893,27.646"]
"harryzhangOG/Deep-RL-Notes" ["l"="21.926,27.585"]
"Weasley-J/mydtt-plus-spring-boot-starter" ["l"="22.029,27.679"]
"the-zion/matrix-core" ["l"="21.972,27.617"]
"tuneflow/tuneflow-py" ["l"="21.948,27.583"]
"FractonProtocol/FractonV1" ["l"="21.992,27.696"]
"HackerBar-Sec/HackerBar" ["l"="-6.367,-45.31"]
"hepingood/mpu6050" ["l"="22.023,27.677"]
"huawei-noah/xingtian" ["l"="25.37,35.948"]
"tencent-ailab/TLeague" ["l"="25.446,35.944"]
"tencent-ailab/tleague_projpage" ["l"="25.374,35.904"]
"ying-wen/malib" ["l"="25.465,35.969"]
"AI4Finance-LLC/ElegantRL" ["l"="21.633,32.527"]
"YangRui2015/Sparse-Reward-Algorithms" ["l"="25.381,36.143"]
"kaixindelele/RHER" ["l"="25.362,36.114"]
"borninfreedom/kuka-reach-drl" ["l"="25.491,35.929"]
"PacktPublishing/Deep-Reinforcement-Learning-with-Python" ["l"="25.001,36.229"]
"tencent-ailab/hok_env" ["l"="25.405,35.959"]
"liuruoze/mini-AlphaStar" ["l"="24.084,37.586"]
"lich14/CDS" ["l"="25.449,36.001"]
"jidiai/ai_lib" ["l"="25.401,36.049"]
"deepmind/alphastar" ["l"="24.062,37.607"]
"Damcy/prioritized-experience-replay" ["l"="24.889,35.987"]
"takoika/PrioritizedExperienceReplay" ["l"="24.826,36.031"]
"jaara/AI-blog" ["l"="24.897,35.931"]
"fhennecker/deepdoom" ["l"="24.79,35.583"]
"chncyhn/flappybird-qlearning-bot" ["l"="24.653,35.954"]
"TimoWilken/flappy-bird-pygame" ["l"="24.637,35.981"]
"kidscancode/pygame_tutorials" ["l"="24.218,37.029"]
"mx0c/super-mario-python" ["l"="24.234,37.021"]
"ntasfi/PyGame-Learning-Environment" ["l"="24.843,35.863"]
"techwithtim/Pygame-Tutorials" ["l"="24.305,36.96"]
"justinmeister/Mario-Level-1" ["l"="24.192,37.013"]
"SarvagyaVaish/FlappyBirdRL" ["l"="24.716,35.926"]
"Mekire/pygame-samples" ["l"="24.168,37.065"]
"justinmeister/The-Stolen-Crown-RPG" ["l"="24.175,37.04"]
"floodsung/DRL-FlappyBird" ["l"="24.608,35.994"]
"nikitasrivatsan/DeepLearningVideoGames" ["l"="24.661,35.984"]
"mlii/mfrl" ["l"="25.322,36.019"]
"DKuan/MADDPG_torch" ["l"="25.306,36.034"]
"openai/train-procgen" ["l"="25.186,35.82"]
"openai/phasic-policy-gradient" ["l"="25.224,35.797"]
"openai/gym3" ["l"="25.167,35.78"]
"openai/safety-gym" ["l"="25.347,35.839"]
"DanielTakeshi/Paper_Notes" ["l"="25.096,35.854"]
"DanielTakeshi/rl_algorithms" ["l"="25.01,35.769"]
"justinjfu/inverse_rl" ["l"="25.057,35.925"]
"shaneshixiang/rllabplusplus" ["l"="24.994,35.855"]
"cbfinn/gps" ["l"="25.093,35.839"]
"hzwer/NIPS2017-LearningToRun" ["l"="24.844,35.935"]
"ctmakro/stanford-osrl" ["l"="24.925,35.913"]
"andrewliao11/Deep-Reinforcement-Learning-Survey" ["l"="24.929,35.888"]
"Mostafa-Samir/DNC-tensorflow" ["l"="27.204,34.352"]
"carpedm20/NTM-tensorflow" ["l"="27.188,34.333"]
"facebook/MemNN" ["l"="27.113,34.332"]
"ixaxaar/pytorch-dnc" ["l"="27.383,34.347"]
"deepmind/rc-data" ["l"="27.103,34.315"]
"bioinf-jku/SNNs" ["l"="33.874,32.628"]
"AmazingAng/deep-RL-elements" ["l"="25.327,36.193"]
"BY571/DQN-Atari-Agents" ["l"="25.236,36.072"]
"jmichaux/dqn-pytorch" ["l"="25.374,36.278"]
"openai/vime" ["l"="24.981,35.841"]
"andrewliao11/gail-tf" ["l"="25.049,35.914"]
"YunzhuLi/InfoGAIL" ["l"="24.977,35.919"]
"openai/iaf" ["l"="33.825,32.526"]
"openai/InfoGAN" ["l"="33.769,32.565"]
"TheAbhiKumar/tensorflow-value-iteration-networks" ["l"="24.947,35.835"]
"uidilr/gail_ppo_tf" ["l"="25.016,35.935"]
"MatthewJA/Inverse-Reinforcement-Learning" ["l"="25.088,35.941"]
"sisl/gail-driver" ["l"="24.994,35.918"]
"openai/evolution-strategies-starter" ["l"="24.657,38.361"]
"itaicaspi/mgail" ["l"="24.951,35.956"]
"stormmax/irl-imitation" ["l"="25.017,35.945"]
"yrlu/irl-imitation" ["l"="25.095,35.957"]
"ahq1993/inverse_rl" ["l"="25.067,35.956"]
"jangirrishabh/toyCarIRL" ["l"="25.04,35.955"]
"hoangminhle/hierarchical_IL_RL" ["l"="25.046,35.799"]
"tianheyu927/mil" ["l"="25.174,35.82"]
"haarnoja/softqlearning" ["l"="25.111,35.845"]
"junhyukoh/value-prediction-network" ["l"="25.016,35.785"]
"cbfinn/maml_rl" ["l"="23.827,35.463"]
"dmakian/feudal_networks" ["l"="24.977,35.763"]
"andrew-j-levy/Hierarchical-Actor-Critc-HAC-" ["l"="25.079,35.778"]
"EthanMacdonald/h-DQN" ["l"="24.95,35.764"]
"mrkulk/hierarchical-deep-RL" ["l"="24.972,35.746"]
"skumar9876/Hierarchical-DQN" ["l"="24.993,35.752"]
"florensacc/snn4hrl" ["l"="25.029,35.739"]
"farizrahman4u/seq2seq" ["l"="31.273,31.598"]
"ml-jku/baselines-rudder" ["l"="25.063,35.768"]
"yadrimz/option-critic" ["l"="25.012,35.735"]
"zuoxingdong/lagom" ["l"="25.087,35.763"]
"jeanharb/option_critic" ["l"="24.993,35.735"]
"anyscale/academy" ["l"="25.41,35.939"]
"ray-project/tutorial" ["l"="25.279,35.917"]
"sven1977/rllib_tutorials" ["l"="25.505,35.968"]
"DerwenAI/rllib_tutorials" ["l"="25.488,35.964"]
"DerwenAI/ray_tutorial" ["l"="25.496,35.98"]
"NVIDIA-Merlin/publications" ["l"="22.621,36.872"]
"deepmind/lab2d" ["l"="25.369,35.886"]
"RobertTLange/gymnax" ["l"="25.393,35.784"]
"kandouss/marlgrid" ["l"="25.36,35.876"]
"Farama-Foundation/MAgent2" ["l"="25.477,35.951"]
"datamllab/rlcard" ["l"="23.884,38.492"]
"facebookresearch/ReAgent" ["l"="25.226,35.878"]
"google-research/batch_rl" ["l"="25.305,35.786"]
"google-research/seed_rl" ["l"="25.265,35.845"]
"david-cortes/contextualbandits" ["l"="24.014,36.392"]
"opendilab/awesome-model-based-RL" ["l"="22.254,27.091"]
"jannerm/mbpo" ["l"="25.334,35.702"]
"rail-berkeley/d4rl_evaluations" ["l"="25.302,35.77"]
"aviralkumar2907/BEAR" ["l"="25.277,35.773"]
"sfujim/TD3_BC" ["l"="25.345,35.79"]
"jannerm/trajectory-transformer" ["l"="25.386,35.727"]
"tianheyu927/mopo" ["l"="25.309,35.747"]
"facebookresearch/torchbeast" ["l"="25.279,35.791"]
"ikostrikov/implicit_q_learning" ["l"="25.386,35.757"]
"kaixin96/rl-generalization-paper" ["l"="25.301,35.617"]
"rraileanu/idaac" ["l"="25.279,35.665"]
"nicklashansen/dmcontrol-generalization-benchmark" ["l"="25.306,35.697"]
"opendilab/awesome-decision-transformer" ["l"="22.241,27.07"]
"nikhilbarhate99/min-decision-transformer" ["l"="25.441,35.687"]
"openai/atari-py" ["l"="24.886,35.735"]
"nicklashansen/svea-vit" ["l"="25.314,35.652"]
"nicklashansen/policy-adaptation-during-deployment" ["l"="25.299,35.671"]
"facebookresearch/deep_bisim4control" ["l"="25.314,35.725"]
"rraileanu/auto-drac" ["l"="25.298,35.652"]
"denisyarats/dmc2gym" ["l"="25.303,35.738"]
"denisyarats/drq" ["l"="25.29,35.736"]
"jangirrishabh/look-closer" ["l"="25.313,35.629"]
"alex-petrenko/sample-factory" ["l"="25.38,35.829"]
"takuseno/minerva" ["l"="25.413,35.783"]
"Farama-Foundation/D4RL" ["l"="25.412,35.738"]
"takuseno/d4rl-pybullet" ["l"="25.393,35.772"]
"twni2016/pomdp-baselines" ["l"="25.351,35.748"]
"ikostrikov/rlpd" ["l"="25.401,35.733"]
"HorizonRobotics/alf" ["l"="25.321,35.697"]
"araffin/sbx" ["l"="25.418,35.774"]
"deepmind/launchpad" ["l"="23.539,33.727"]
"deepmind/dm_env" ["l"="23.578,33.67"]
"NVlabs/cule" ["l"="25.232,35.674"]
"Nat-D/FeatureControlHRL" ["l"="24.935,35.722"]
"ray-project/rl-experiments" ["l"="25.331,35.903"]
"inoryy/tensorflow2-deep-reinforcement-learning" ["l"="25.151,36.127"]
"ymd-h/cpprb" ["l"="25.139,36.084"]
"xuanlinli17/CS285_Fa19_Deep_Reinforcement_Learning" ["l"="25.127,36.205"]
"openai/mujoco-worldgen" ["l"="25.329,35.887"]
"ray-project/xgboost_ray" ["l"="25.824,34.047"]
"JBLanier/pipeline-psro" ["l"="25.58,35.993"]
"uber-research/poet" ["l"="24.6,38.426"]
"google-research/episodic-curiosity" ["l"="25.077,35.743"]
"uber-research/deep-neuroevolution" ["l"="24.628,38.387"]
"BYU-PCCL/holodeck" ["l"="25.14,35.798"]
"BYU-PCCL/holodeck-engine" ["l"="25.092,35.733"]
"deepdrive/deepdrive" ["l"="29.06,42.915"]
"zfw1226/gym-unrealcv" ["l"="-13.492,41.174"]
"MagNet-DL/magnet" ["l"="25.586,34"]
"locuslab/mpc.pytorch" ["l"="27.938,42.082"]
"stanfordnmbl/osim-rl" ["l"="25.059,35.854"]
"erlerobot/gym-gazebo" ["l"="25.254,35.913"]
"chainer/chainer" ["l"="34.121,35.439"]
"chainer/chainermn" ["l"="34.038,30.344"]
"chainer/chainercv" ["l"="34.376,35.309"]
"Kaixhin/imitation-learning" ["l"="25.232,35.942"]
"alexis-jacq/LOLA_DiCE" ["l"="25.125,36.116"]
"hhexiy/opponent" ["l"="25.127,36.103"]
"deepmind/mathematics_dataset" ["l"="25.067,35.684"]
"hendrycks/math" ["l"="22.919,34.461"]
"fchollet/ARC" ["l"="25.095,35.711"]
"openai/grade-school-math" ["l"="22.908,34.446"]
"facebookresearch/SymbolicMathematics" ["l"="22.649,34.321"]
"facebookresearch/pythia" ["l"="29.848,32.531"]
"deepmind/AQuA" ["l"="22.879,34.47"]
"facebookresearch/XLM" ["l"="29.881,32.471"]
"facebookresearch/nevergrad" ["l"="25.543,33.689"]
"harvardnlp/pytorch-struct" ["l"="29.82,32.33"]
"kimiyoung/transformer-xl" ["l"="29.94,32.481"]
"facebookresearch/PyTorch-BigGraph" ["l"="21.734,38.689"]
"facebookresearch/pyrobot" ["l"="24.621,36.597"]
"IntelLabs/coach" ["l"="25.244,35.893"]
"HumanCompatibleAI/overcooked_ai" ["l"="25.356,35.933"]
"Stanford-ILIAD/PantheonRL" ["l"="25.449,35.956"]
"HumanCompatibleAI/overcooked-demo" ["l"="25.43,35.949"]
"rosewang2008/gym-cooking" ["l"="25.434,35.929"]
"HumanCompatibleAI/imitation" ["l"="25.219,35.915"]
"MineDojo/MineDojo" ["l"="25.366,35.757"]
"openai/Video-Pre-Training" ["l"="25.315,35.713"]
"MineDojo/MineCLIP" ["l"="25.35,35.694"]
"vimalabs/VIMA" ["l"="25.527,35.787"]
"jannerm/diffuser" ["l"="25.436,35.749"]
"GT-RIPL/Awesome-LLM-Robotics" ["l"="25.503,35.794"]
"brynhayder/reinforcement_learning_an_introduction" ["l"="25.234,36.174"]
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" ["l"="25.186,36.067"]
"iamhectorotero/rlai-exercises" ["l"="25.244,36.188"]
"vojtamolda/reinforcement-learning-an-introduction" ["l"="25.214,36.154"]
"diegoalejogm/Reinforcement-Learning" ["l"="25.251,36.232"]
"mharbuz/rlbook-exercises" ["l"="25.251,36.208"]
"mtrazzi/rl-book-challenge" ["l"="28.151,32.7"]
"JKCooper2/rlai-exercises" ["l"="25.261,36.218"]
"Skylark0924/Machine-Learning-is-ALL-You-Need" ["l"="25.307,36.128"]
"Skylark0924/Reinforcement-Learning-in-Robotics" ["l"="25.347,36.206"]
"Skylark0924/Rofunc" ["l"="25.365,36.243"]
"iassael/learning-to-communicate" ["l"="25.343,36.057"]
"facebookresearch/CommNet" ["l"="25.397,36.109"]
"rhoowd/sched_net" ["l"="25.409,36.088"]
"carpedm20/NAF-tensorflow" ["l"="25.006,35.914"]
"facebookarchive/CommNet" ["l"="25.388,36.077"]
"ludc/rltorch" ["l"="26.903,34.347"]
"TakuyaHiraoka/Multi-Agent-Reinforcement-Learning-in-Stochastic-Games" ["l"="25.385,36.124"]
"wwxFromTju/deepmind_MAS_enviroment" ["l"="25.402,36.076"]
"facebook/MazeBase" ["l"="26.846,34.476"]
"srl-freiburg/pedsim_ros" ["l"="25.661,36.209"]
"onlytailei/gym_ped_sim" ["l"="25.647,36.231"]
"vita-epfl/CrowdNav" ["l"="25.6,36.182"]
"mit-acl/cadrl_ros" ["l"="25.606,36.171"]
"yuxiang-gao/PySocialForce" ["l"="25.693,36.217"]
"sybrenstuvel/Python-RVO2" ["l"="25.646,36.211"]
"spencer-project/spencer_people_tracking" ["l"="29.425,41.91"]
"svenkreiss/socialforce" ["l"="25.711,36.233"]
"ml-lab-cuny/menge_ros" ["l"="25.673,36.22"]
"marinaKollmitz/human_aware_navigation" ["l"="29.474,41.904"]
"ChanganVR/RelationalGraphLearning" ["l"="25.628,36.208"]
"mfe7/cadrl_ros" ["l"="25.63,36.219"]
"mit-acl/gym-collision-avoidance" ["l"="25.638,36.198"]
"MengeCrowdSim/Menge" ["l"="25.718,36.22"]
"RGring/drl_local_planner_ros_stable_baselines" ["l"="25.654,36.191"]
"dslwz2008/SocialForceModel" ["l"="25.741,36.249"]
"lc6chang/Social_Force_Model" ["l"="25.751,36.242"]
"fawwazbmn/SocialForceModel" ["l"="25.764,36.261"]
"CMU-TBD/SocNavBench" ["l"="25.738,36.226"]
"yandexdataschool/Practical_DL" ["l"="-9.271,20.171"]
"yandexdataschool/nlp_course" ["l"="29.871,32.784"]
"esokolov/ml-course-hse" ["l"="-9.27,20.19"]
"mpatacchiola/dissecting-reinforcement-learning" ["l"="25.021,35.909"]
"algorithmdog/Reinforcement_Learning_Blog" ["l"="24.812,35.923"]
"steveKapturowski/tensorflow-rl" ["l"="24.905,35.847"]
"yukezhu/tensorflow-reinforce" ["l"="24.925,35.9"]
"Kaixhin/NoisyNet-A3C" ["l"="24.858,35.91"]
"songrotek/Meta-Learning-Papers" ["l"="23.901,35.391"]
"jsg71/Deep-Hedging" ["l"="24.72,36.381"]
"YuMan-Tam/deep-hedging" ["l"="24.72,36.356"]
"leegao/readme2tex" ["l"="24.708,35.814"]
"agurodriguez/github-texify" ["l"="24.633,35.791"]
"tokestermw/tensorflow-shakespeare" ["l"="30.081,32.262"]
"leegao/float-hacks" ["l"="28.238,-18.848"]
"carpedm20/awesome-torch" ["l"="26.911,34.308"]
"microsoft/malmo" ["l"="25.22,35.739"]
"crowdAI/marLo" ["l"="25.19,35.759"]
"facebookresearch/craftassist" ["l"="25.206,35.668"]
"facebookresearch/nle" ["l"="25.262,35.756"]
"erwincoumans/pybullet_robots" ["l"="25.449,35.883"]
"araffin/robotics-rl-srl" ["l"="25.339,35.87"]
"openai/EPG" ["l"="25.024,35.802"]
"omerbsezer/LSTM_RNN_Tutorials_with_Demo" ["l"="25.127,36.153"]
"eleurent/rl-agents" ["l"="29.872,45.19"]
"eleurent/phd-bibliography" ["l"="28.045,42.054"]
"jaungiers/LSTM-Neural-Network-for-Time-Series-Prediction" ["l"="21.752,32.498"]
"NourozR/Stock-Price-Prediction-LSTM" ["l"="21.859,32.486"]
"buomsoo-kim/Easy-deep-learning-with-Keras" ["l"="22.647,29.399"]
"google-research/robotics_transformer" ["l"="25.544,35.806"]
"lucidrains/robotic-transformer-pytorch" ["l"="25.537,35.78"]
"peract/peract" ["l"="25.524,35.795"]
"google-research/language-table" ["l"="25.564,35.781"]
"facebookresearch/r3m" ["l"="25.543,35.794"]
"microsoft/PromptCraft-Robotics" ["l"="25.567,35.812"]
"cliport/cliport" ["l"="25.509,35.802"]
"vimalabs/VIMABench" ["l"="25.548,35.785"]
"stepjam/ARM" ["l"="25.523,35.821"]
"ikostrikov/pytorch-trpo" ["l"="25.064,35.913"]
"TianhongDai/hindsight-experience-replay" ["l"="25.243,35.788"]
"Kchu/DeepRL_PyTorch" ["l"="25.014,36.165"]
"cjy1992/gym-carla" ["l"="29.873,45.236"]
"google-research/rlds" ["l"="25.236,35.69"]
"ugurkanates/awesome-real-world-rl" ["l"="25.341,35.854"]
"shakenes/vizdoomgym" ["l"="25.021,35.757"]
"minerllabs/competition_submission_starter_template" ["l"="25.247,35.652"]
"alex-petrenko/megaverse" ["l"="25.568,35.867"]
"instadeepai/jumanji" ["l"="25.453,35.782"]
"RajGhugare19/dreamerv2" ["l"="25.327,35.684"]
"facebookresearch/drqv2" ["l"="25.321,35.74"]
"sjchoi86/irl_rocks" ["l"="25.038,35.967"]
"neka-nat/inv_rl" ["l"="25.068,35.975"]
"qzed/irl-maxent" ["l"="25.069,36.006"]
"MCZhi/Driving-IRL-NGSIM" ["l"="29.745,44.474"]
"yfzhang/vehicle-motion-forecasting" ["l"="25.017,35.987"]
"nav74neet/gail_gym" ["l"="24.989,35.962"]
"ellisk42/ec" ["l"="31.507,34.325"]
"google-research/disentanglement_lib" ["l"="25.047,32.577"]
"google/BIG-bench" ["l"="27.557,31.185"]
"arogozhnikov/einops" ["l"="34.25,35.847"]
"tomgoldstein/loss-landscape" ["l"="34.319,35.741"]
"locuslab/SATNet" ["l"="27.77,42.129"]
"google/trax" ["l"="30.039,32.448"]
"joyiswu/UCL-Deep-learning-ans-Reinforcement-learning" ["l"="25.044,36.22"]
"mikezhang95/ML_Assignment" ["l"="25.046,36.205"]
"RylanSchaeffer/ucl-adv-dl-rl" ["l"="25.029,36.212"]
"keon/policy-gradient" ["l"="24.774,36.021"]
"kh-kim/stock_market_reinforcement_learning" ["l"="21.76,32.422"]
"jsuarez5341/neural-mmo-client" ["l"="25.327,36.076"]
"jsuarez5341/neural-mmo" ["l"="25.309,36.067"]
"NeuralMMO/environment" ["l"="25.374,36.164"]
"berkeleydeeprlcourse/homework_fall2020" ["l"="25.129,36.234"]
"HaveIBeenPwned/PwnedPasswordsAzureFunction" ["l"="25.452,35.524"]
"HaveIBeenPwned/3DModels" ["l"="25.444,35.549"]
"facebookresearch/minihack" ["l"="25.346,35.71"]
"facebookresearch/impact-driven-exploration" ["l"="25.29,35.632"]
"ngoodger/nle-language-wrapper" ["l"="25.252,35.686"]
"microsoft/TextWorld" ["l"="24.945,36.51"]
"Bam4d/Griddly" ["l"="25.343,35.728"]
"RobertTLange/evosax" ["l"="25.468,35.752"]
"luchris429/purejaxrl" ["l"="25.421,35.762"]
"deepmind/distrax" ["l"="23.491,33.798"]
"google/evojax" ["l"="25.445,35.763"]
"RobertTLange/gymnax-blines" ["l"="25.446,35.722"]
"lcswillems/torch-ac" ["l"="25.215,35.644"]
"mila-iqia/atari-representation-learning" ["l"="25.266,35.712"]
"tkipf/c-swm" ["l"="31.535,34.182"]
"aravindsrinivas/curl_rainbow" ["l"="25.281,35.706"]
"alexlee-gk/slac" ["l"="25.266,35.664"]
"adaptive-intelligent-robotics/QDax" ["l"="25.491,35.778"]
"nnaisense/evotorch" ["l"="25.45,35.739"]
"deepmind/chex" ["l"="23.486,33.757"]
"icaros-usc/pyribs" ["l"="25.528,35.738"]
"n2cholas/awesome-jax" ["l"="23.466,33.755"]
"google/CommonLoopUtils" ["l"="23.465,33.729"]
"google/jaxtyping" ["l"="23.447,33.756"]
"deepmind/optax" ["l"="23.497,33.767"]
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" ["l"="25.08,36.147"]
"Zhenye-Na/advanced-deep-learning-and-reinforcement-learning-deepmind" ["l"="25.061,36.223"]
"YidingYu/UCL-DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" ["l"="25.065,36.201"]
"aamini/introtodeeplearning_labs" ["l"="23.712,30.98"]
"Zhenye-Na/reinforcement-learning-stanford" ["l"="25.091,36.345"]
"glouppe/info8010-deep-learning" ["l"="28.252,32.618"]
"beyretb/AnimalAI-Olympics" ["l"="25.206,35.817"]
"mdcrosby/animal-ai" ["l"="25.167,35.755"]
"Unity-Technologies/obstacle-tower-env" ["l"="25.144,35.758"]
"facebookresearch/rl" ["l"="25.37,35.775"]
"facebookresearch/rlmeta" ["l"="25.377,35.705"]
"facebookresearch/moolib" ["l"="25.378,35.718"]
"jurgisp/memory-maze" ["l"="25.37,35.665"]
"henry-prior/jax-rl" ["l"="25.537,35.692"]
"google/brain-tokyo-workshop" ["l"="25.248,35.774"]
"weightagnostic/weightagnostic.github.io" ["l"="25.235,35.625"]
"uber-research/PyTorch-NEAT" ["l"="24.588,38.39"]
"CodeReclaimers/neat-python" ["l"="24.614,38.364"]
"facebookresearch/theseus" ["l"="33.343,43.338"]
"samuela/git-re-basin" ["l"="25.638,35.581"]
"EvolutionGym/evogym" ["l"="25.58,35.646"]
"google/learned_optimization" ["l"="23.497,33.689"]
"Kaixhin/ACER" ["l"="24.982,35.951"]
"dchetelat/acer" ["l"="24.927,35.993"]
"mjacar/pytorch-trpo" ["l"="24.992,35.929"]
"MG2033/A2C" ["l"="24.816,35.968"]
"pemami4911/deep-rl" ["l"="24.893,35.946"]
"dgriff777/rl_a3c_pytorch" ["l"="24.993,35.906"]
"awjuliani/Meta-RL" ["l"="25.004,35.821"]
"Alfredvc/paac" ["l"="24.836,35.915"]
"aleju/mario-ai" ["l"="24.902,35.821"]
"ehrenbrav/DeepQNetwork" ["l"="24.921,35.698"]
"songrotek/DRL-FlappyBird" ["l"="24.744,35.917"]
"rameshvarun/NeuralKart" ["l"="29.116,42.755"]
"xbpeng/DeepTerrainRL" ["l"="24.873,35.812"]
"aleju/self-driving-truck" ["l"="29.036,42.924"]
"navneet-nmk/pytorch-rl" ["l"="25.119,35.833"]
"avivt/VIN" ["l"="24.962,35.825"]
"kentsommer/pytorch-value-iteration-networks" ["l"="24.964,35.794"]
"zuoxingdong/VIN_PyTorch_Visdom" ["l"="24.982,35.814"]
"zhongwen/predictron" ["l"="24.95,35.784"]
"zuoxingdong/VIN_TensorFlow" ["l"="24.961,35.776"]
"songrotek/DDPG" ["l"="24.903,35.915"]
"achao2013/Learning-To-Reinforcement-Learn" ["l"="15.52,-35.981"]
"mtrazzi/two-step-task" ["l"="24.93,35.737"]
"jonasrothfuss/ProMP" ["l"="25.129,35.77"]
"mwufi/meta-rl-bandits" ["l"="24.992,35.793"]
"deepmind/ai-safety-gridworlds" ["l"="25.257,35.794"]
"openai/safety-starter-agents" ["l"="25.413,35.86"]
"befelix/safe_learning" ["l"="25.436,35.793"]
"quanvuong/handful-of-trials-pytorch" ["l"="25.23,35.749"]
"deepmind/dqn" ["l"="24.981,35.829"]
"transedward/pytorch-dqn" ["l"="25.032,35.938"]
"deepmind/xitari" ["l"="24.838,35.722"]
"gliese581gg/DQN_tensorflow" ["l"="24.716,35.875"]
"greydanus/visualize_atari" ["l"="24.8,36.042"]
"nikaashpuri/sarfa-saliency" ["l"="24.757,36.07"]
"greydanus/baby-a3c" ["l"="24.867,35.997"]
"jingweiz/pytorch-dnc" ["l"="27.373,34.365"]
"onlytailei/A3C-PyTorch" ["l"="24.851,36.062"]
"ghliu/pytorch-ddpg" ["l"="25.054,35.957"]
"kengz/openai_lab" ["l"="24.756,35.902"]
"higgsfield/Imagination-Augmented-Agents" ["l"="25.117,35.714"]
"MillionIntegrals/vel" ["l"="25.065,35.793"]
"araffin/learning-to-drive-in-5-minutes" ["l"="29.227,42.809"]
"fedingo/Hierarchical-DQN" ["l"="24.96,35.718"]
"yandexdataschool/AgentNet" ["l"="24.855,35.978"]
"yandexdataschool/YSDA_deeplearning17" ["l"="-9.422,20.07"]
"yandexdataschool/MLatImperial2017" ["l"="24.747,36.041"]
"tensorlayer/chinese-book" ["l"="23.431,31.44"]
"PKU-MARL/Safe-Policy-Optimization" ["l"="25.432,35.884"]
"PKU-MARL/omnisafe" ["l"="21.955,27.598"]
"PKU-MARL/safety-gymnasium" ["l"="25.495,35.909"]
"anuragajay/decision-diffuser" ["l"="25.463,35.712"]
"ZhengyaoJiang/latentplan" ["l"="25.497,35.692"]
"conglu1997/v-d4rl" ["l"="25.394,35.718"]
"google-research/ibc" ["l"="25.543,35.763"]
"cyoon1729/Policy-Gradient-Methods" ["l"="25.093,36.176"]
"cyoon1729/Reinforcement-learning" ["l"="25.077,36.18"]
"dxyang/DQN_pytorch" ["l"="25.128,36.011"]
"pat-coady/trpo" ["l"="24.968,35.898"]
"wojzaremba/trpo" ["l"="24.957,35.924"]
"Zeta36/muzero" ["l"="25.424,35.68"]
"Xingyu-Lin/mbpo_pytorch" ["l"="25.302,35.726"]
"mimoralea/applied-reinforcement-learning" ["l"="24.997,36.071"]
"JuliaReinforcementLearning/ReinforcementLearningAnIntroduction.jl" ["l"="17.284,39.049"]
"TikhonJelvis/RL-book" ["l"="22.034,32.53"]
"rlcode/per" ["l"="25.054,36.017"]
"PacktPublishing/Mastering-Reinforcement-Learning-with-Python" ["l"="25.073,36.122"]
"JuliaReinforcementLearning/ReinforcementLearning.jl" ["l"="17.227,39.037"]
"ucaiado/QLearning_Trading" ["l"="21.742,32.406"]
"karpathy/reinforcejs" ["l"="24.794,35.86"]
"karpathy/recurrentjs" ["l"="24.661,35.835"]
"karpathy/convnetjs" ["l"="28.16,27.617"]
"janhuenermann/neurojs" ["l"="28.215,27.559"]
"SirTificate/gekko-neuralnet" ["l"="21.201,31.014"]
"karpathy/svmjs" ["l"="28.139,27.47"]
"rlpy/rlpy" ["l"="28.661,43.274"]
"yenchenlin1994/DeepLearningFlappyBird" ["l"="24.737,35.888"]
"harvitronix/reinforcement-learning-car" ["l"="24.885,35.923"]
"deepmind/alewrap" ["l"="24.77,35.742"]
"deepmind/plplot-ffi" ["l"="24.807,35.689"]
"BarisYazici/deep-rl-grasping" ["l"="25.511,35.881"]
"mahyaret/kuka_rl" ["l"="25.521,35.914"]
"AndrejOrsula/drl_grasping" ["l"="25.539,35.901"]
"hsp-iit/pybullet-robot-envs" ["l"="25.594,35.901"]
"IanYangChina/pybullet_multigoal_gym" ["l"="25.527,35.873"]
"eleramp/pybullet-object-models" ["l"="25.616,35.871"]
"ikostrikov/pytorch-ddpg-naf" ["l"="25.01,35.926"]
"vy007vikas/PyTorch-ActorCriticRL" ["l"="25.048,35.946"]
"ChenglongChen/pytorch-madrl" ["l"="25.084,35.995"]
"jimkon/Deep-Reinforcement-Learning-in-Large-Discrete-Action-Spaces" ["l"="24.895,36.013"]
"rlchina/RLCN" ["l"="25.409,36.026"]
"thesouther/MARL" ["l"="25.425,36.066"]
"manaski/MARL" ["l"="25.489,36.111"]
"verystrongjoe/qmix" ["l"="25.467,36.097"]
"befelix/SafeOpt" ["l"="25.488,35.748"]
"befelix/lyapunov-learning" ["l"="25.48,35.761"]
"YaChienChang/Neural-Lyapunov-Control" ["l"="25.505,35.735"]
"rcheng805/RL-CBF" ["l"="25.44,35.821"]
"hari-sikchi/safeRL" ["l"="25.446,35.814"]
"befelix/safe-exploration" ["l"="25.491,35.726"]
"AliBaheri/Safe-Reinforcement-Learning" ["l"="25.488,35.738"]
"befelix/Safe-RL-Benchmark" ["l"="25.471,35.768"]
"utiasDSL/safe-control-gym" ["l"="25.396,35.824"]
"befelix/SafeMDP" ["l"="25.497,35.757"]
"jachiam/cpo" ["l"="25.38,35.871"]
"Unity-Technologies/obstacle-tower-challenge" ["l"="25.1,35.682"]
"unixpickle/obs-tower2" ["l"="25.083,35.652"]
"Unity-Technologies/obstacle-tower-source" ["l"="25.1,35.647"]
"Unity-Technologies/marathon-envs" ["l"="-10.733,28.006"]
"lweitkamp/option-critic-pytorch" ["l"="25.005,35.704"]
"alversafa/option-critic-arch" ["l"="24.976,35.689"]
"mklissa/PPOC" ["l"="24.988,35.705"]
"veronicachelu/temporal_abstraction" ["l"="24.99,35.663"]
"ugo-nama-kun/gym_torcs" ["l"="24.851,35.951"]
"YurongYou/rlTORCS" ["l"="24.756,35.992"]
"abhisheknaik96/MultiAgentTORCS" ["l"="24.727,36.003"]
"jastfkjg/DDPG_Torcs_PyTorch" ["l"="24.771,35.975"]
"lanquarden/pyScrcClient" ["l"="24.783,35.985"]
"zsdonghao/Imitation-Learning-Dagger-Torcs" ["l"="24.772,35.997"]
"giuse/vtorcs" ["l"="24.79,35.967"]
"dosssman/GymTorcs" ["l"="24.795,35.977"]
"kennethyu2017/ddpg" ["l"="24.89,35.998"]
"tgangwani/GA3C-DeepNavigation" ["l"="24.851,35.896"]
"oxwhirl/treeqn" ["l"="28.18,32.658"]
"cuhkrlcourse/ierg6130-assignment" ["l"="25.196,36.168"]
"datawhalechina/competition-baseline" ["l"="24.256,31.189"]
"datawhalechina/leeml-notes" ["l"="24.114,31.094"]
"Sakura-gh/ML-notes" ["l"="24.248,31.085"]
"Rochester-NRT/AlphaGo" ["l"="27.036,33.829"]
"pavelgonchar/colornet" ["l"="33.622,32.36"]
"awentzonline/image-analogies" ["l"="33.551,32.404"]
"torch/tutorials" ["l"="26.922,34.268"]
"alexjc/neural-doodle" ["l"="33.595,32.401"]
"yenchenlin1994/awesome-watchos" ["l"="24.632,35.888"]
"zer0n/deepframeworks" ["l"="27.115,34.137"]
"soumith/convnet-benchmarks" ["l"="27.037,34.104"]
"nrontsis/PILCO" ["l"="25.159,35.711"]
"andyzeng/visual-pushing-grasping" ["l"="31.598,42.365"]
"tychovdo/PacmanDQN" ["l"="24.768,35.859"]
"mrkulk/deepQN_tensorflow" ["l"="24.691,35.863"]
"nuno-faria/tetris-ai" ["l"="25.14,36.18"]
"uvipen/Tetris-deep-Q-learning-pytorch" ["l"="25.148,36.098"]
"michiel-cox/Tetris-DQN" ["l"="25.145,36.218"]
"stepjam/PyRep" ["l"="25.385,35.86"]
"StanfordVL/robosuite" ["l"="25.372,35.848"]
"ARISE-Initiative/robomimic" ["l"="25.469,35.824"]
"denisyarats/pytorch_sac_ae" ["l"="25.275,35.738"]
"nicklashansen/tdmpc" ["l"="25.406,35.684"]
"PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook" ["l"="25.174,36.164"]
"abhisheksuran/Reinforcement_Learning" ["l"="25.158,36.11"]
"Huixxi/TensorFlow2.0-for-Deep-Reinforcement-Learning" ["l"="25.164,36.147"]
"ku2482/fqf-iqn-qrdqn.pytorch" ["l"="24.97,36.231"]
"sungyubkim/Deep_RL_with_pytorch" ["l"="24.966,36.214"]
"Kchu/LifelongRL" ["l"="24.99,36.189"]
"BY571/FQF-and-Extensions" ["l"="24.992,36.201"]
"xavierpuigf/watch_and_help" ["l"="24.78,36.493"]
"ChangyWen/wolpertinger_ddpg" ["l"="24.847,36.042"]
"nikhil3456/Deep-Reinforcement-Learning-in-Large-Discrete-Action-Spaces" ["l"="24.831,36.054"]
"chenhaokun/TPGR" ["l"="24.144,36.509"]
"ZhiqingXiao/pytorch-book" ["l"="25.257,36.151"]
"stevenpjg/ddpg-aigym" ["l"="24.903,35.963"]
"xinshi-chen/GenerativeAdversarialUserModel" ["l"="24.124,36.481"]
"PaulDanielML/MuJoCo_RL_UR5" ["l"="25.48,35.858"]
"StanfordVL/iGibson" ["l"="24.711,36.534"]
"BazkieBumpercar/Blunted2" ["l"="25.562,36.055"]
"vi3itor/GameplayFootball" ["l"="25.584,36.06"]
"HumanCompatibleAI/adversarial-policies" ["l"="24.953,36.073"]
"chenhongge/StateAdvDRL" ["l"="24.85,36.152"]
"huanzhang12/ATLA_robust_RL" ["l"="24.871,36.138"]
"araffin/srl-zoo" ["l"="25.36,35.86"]
"mingfeisun/DeepMimic_mujoco" ["l"="35.595,35.222"]
"SvenGronauer/Bullet-Safety-Gym" ["l"="25.401,35.853"]
"jsikyoon/dreamer-torch" ["l"="25.295,35.682"]
"hardmaru/WorldModelsExperiments" ["l"="25.166,35.726"]
"polixir/NeoRL" ["l"="25.425,35.662"]
"polixir/OfflineRL" ["l"="25.396,35.706"]
"m5823779/MotionPlannerUsingDDPG" ["l"="25.489,36.191"]
"m5823779/DDPG" ["l"="25.404,36.161"]
"m5823779/PoseEstimation" ["l"="25.453,36.193"]
"hanruihua/rl_rvo_nav" ["l"="25.677,36.259"]
"hanruihua/intelligent-robot-simulator" ["l"="25.703,36.289"]
"oxwhirl/facmac" ["l"="25.481,36.01"]
"oxwhirl/smacv2" ["l"="25.446,36.014"]
"wendelinboehmer/dcg" ["l"="25.483,36.026"]
"TonghanWang/RODE" ["l"="25.463,36.023"]
"karpathy/tsnejs" ["l"="22.955,37.66"]
"wojciechz/learning_to_execute" ["l"="26.989,34.28"]
"facebook/eyescream" ["l"="33.666,32.473"]
"kaishengtai/torch-ntm" ["l"="26.987,34.313"]
"karpathy/neuraltalk" ["l"="27.152,34.059"]
"karpathy/forestjs" ["l"="28.113,27.449"]
"jcjohnson/cnn-vis" ["l"="33.602,32.314"]
"cazala/synaptic" ["l"="28.16,27.555"]
"waylonflinn/weblas" ["l"="28.361,27.561"]
"facebook/fbcunn" ["l"="26.965,34.197"]
"Element-Research/rnn" ["l"="26.985,34.298"]
"nicholas-leonard/dp" ["l"="26.931,34.288"]
"rmst/ddpg" ["l"="24.877,35.942"]
"MOCR/DDPG" ["l"="24.915,35.959"]
"robotlearn/pyrobolearn" ["l"="25.441,35.866"]
"dougsm/ggcnn" ["l"="31.568,42.361"]
"dougsm/mvp_grasp" ["l"="31.556,42.363"]
"GeorgeDu/vision-based-robotic-grasping" ["l"="31.625,42.351"]
"robotology-playground/pybullet-robot-envs" ["l"="25.475,35.892"]
"borninfreedom/DeepLearning" ["l"="25.315,36.26"]
"eyounx/VirtualTaobao" ["l"="24.107,36.482"]
"GAOYANGAU/DRLPytorch" ["l"="25.27,36.298"]
"ikostrikov/walk_in_the_park" ["l"="25.472,35.727"]
"coax-dev/coax" ["l"="25.465,35.674"]
"young-geng/CQL" ["l"="25.386,35.745"]
"Kojoley/atari-py" ["l"="24.85,35.688"]
"takuseno/ppo" ["l"="24.93,36.173"]
"uidilr/ppo_tf" ["l"="24.895,36.21"]
"shareeff/PPO" ["l"="24.905,36.198"]
"imai-laboratory/rlsaber" ["l"="24.918,36.196"]
"YJLAugus/Reinforcement-Learning-Notes" ["l"="24.397,31.284"]
"naderAsadi/Optimal-Path-Planning-Deep-Reinforcement-Learning" ["l"="25.631,36.156"]
"yxBeginner/RL-and-Robot" ["l"="25.562,36.193"]
"sichkar-valentyn/Reinforcement_Learning_in_Python" ["l"="25.685,36.16"]
"TimeBreaker/MARL-resources-collection" ["l"="25.505,36.095"]
"TonghanWang/DOP" ["l"="25.479,36.055"]
"facebookresearch/CollaQ" ["l"="25.496,36.055"]
"wjh720/QPLEX" ["l"="25.481,36.04"]
"TonghanWang/NDQ" ["l"="25.509,36.069"]
"Sonkyunghwan/QTRAN" ["l"="25.525,36.059"]
"WorldDbs/specs-actors" ["l"="16.232,-7.114"]
"yl-yue/yue-library" ["l"="16.236,-7.096"]
"PercyJon/PercyJon.github.io" ["l"="16.213,-7.124"]
"v2ray-links/v2ray-free" ["l"="16.192,-7.088"]
"springmonster/RestfulTool-Retrofit" ["l"="16.204,-7.098"]
"HeisenbergEmpire/studynote" ["l"="16.205,-7.129"]
"fanyuan/MyMp3Convert" ["l"="16.316,-7.092"]
"Zaxblog/MinerProxy" ["l"="22.035,27.515"]
"TanaStudy/Java-Study" ["l"="16.214,-7.112"]
"gatewayorg/blue" ["l"="16.216,-7.134"]
"PiggyCh/RL_arm_under_sparse_reward" ["l"="25.558,35.938"]
"RunxinXu/Make-Information-Extraction-Great-Again" ["l"="26.078,35.245"]
"PKUnlp-icler/SCL-RAI" ["l"="26.05,35.255"]
"dbsxdbsx/rl-intro-book-chinese" ["l"="25.203,36.324"]
"rl-cn/rl-cn" ["l"="25.223,36.326"]
"gouxiangchen/dueling-DQN-pytorch" ["l"="25.092,36.101"]
"araffin/rl-tutorial-jnrr19" ["l"="25.31,35.891"]
"WilsonWangTHU/POPLIN" ["l"="25.2,35.726"]
"thanard/me-trpo" ["l"="25.182,35.732"]
"nagaban2/nn_dynamics" ["l"="25.147,35.746"]
"aravindr93/mjrl" ["l"="25.251,35.72"]
"mcgillmrl/prob_mbrl" ["l"="25.203,35.713"]
"Stable-Baselines-Team/stable-baselines" ["l"="25.356,35.917"]
"Stable-Baselines-Team/rl-colab-notebooks" ["l"="25.357,35.902"]
"aravindr93/hand_dapg" ["l"="25.295,35.713"]
"vikashplus/mj_envs" ["l"="25.257,35.698"]
"aravindr93/trajopt" ["l"="25.262,35.613"]
"SwapnilPande/MOReL" ["l"="25.286,35.689"]
"ku2482/sac-discrete.pytorch" ["l"="25.219,35.755"]
"st-tech/zr-obp" ["l"="24.063,36.423"]
"apexrl/Batch-Offline--RL-Paper-Lists" ["l"="25.413,35.751"]
"MorvanZhou/pytorch-A3C" ["l"="25.103,35.939"]
"watchernyu/REDQ" ["l"="25.401,35.762"]
"snu-mllab/EDAC" ["l"="25.398,35.747"]
"openrlbenchmark/openrlbenchmark" ["l"="25.433,35.803"]
"juanblak/Deep-Reinforcement-Learning" ["l"="25.715,36.157"]
"mehdimo/path_planning_GAN" ["l"="25.73,36.149"]
"balcilar/Multi-Robot-Path-Planning-on-Graphs" ["l"="25.743,36.161"]
"Strange-AI/frenet_path_planning" ["l"="28.812,42.98"]
"atb033/multi_agent_path_planning" ["l"="28.561,42.929"]
"kavehkamali/RobotPath" ["l"="25.71,36.146"]
"a7b23/Autonomous-MazePathFinder-using-DQN" ["l"="25.761,36.175"]
"lok-i/DRLPathPlanner" ["l"="25.722,36.167"]
"marcino239/pilco" ["l"="25.099,35.572"]
"zuoxingdong/DeepPILCO" ["l"="25.112,35.602"]
"kanakkabara/Autonomous-Drifting" ["l"="24.784,35.95"]
"harvitronix/rl-rc-car" ["l"="24.802,35.953"]
"MLJejuCamp2017/DRL_based_SelfDrivingCarControl" ["l"="29.016,42.837"]
"thibo73800/metacar" ["l"="28.966,42.703"]
"kaihuchen/DRL-AutonomousVehicles" ["l"="24.764,35.956"]
"microsoft/maro" ["l"="25.385,35.956"]
"microsoft/FOST" ["l"="25.532,36.006"]
"hubbs5/or-gym" ["l"="19.109,23.572"]
"ds4dm/ecole" ["l"="19.048,23.556"]
"PettingZoo-Team/MAgent" ["l"="25.436,36.147"]
"liampetti/DDPG" ["l"="24.909,35.988"]
"facebookresearch/rela" ["l"="25.194,35.682"]
"marooncn/learning_note" ["l"="25.602,36.294"]
"marooncn/navbot" ["l"="25.586,36.234"]
"sumitsk/marl_transfer" ["l"="25.418,36.045"]
"amidos2006/gym-pcgrl" ["l"="25.234,35.546"]
"ROBOTIS-GIT/turtlebot3_machine_learning" ["l"="25.723,36.373"]
"dranaju/project" ["l"="25.721,36.388"]
"jr-robotics/robo-gym" ["l"="25.46,35.907"]
"robotology/gym-ignition" ["l"="25.58,35.932"]
"jr-robotics/robo-gym-robot-servers" ["l"="25.513,35.926"]
"ku2482/slac.pytorch" ["l"="25.273,35.633"]
"facebookresearch/digit-design" ["l"="25.697,35.876"]
"facebookresearch/digit-interface" ["l"="25.702,35.86"]
"facebookresearch/tacto" ["l"="25.644,35.857"]
"mcubelab/gelslim" ["l"="25.746,35.896"]
"itorr/homo" ["l"="-32.663,-15.338"]
"li-haoran/DRL-FlappyBird" ["l"="24.59,35.914"]
"zhaw/neural_style" ["l"="24.523,35.914"]
"YuhangSong/DEHRL" ["l"="25.028,35.709"]
"watakandai/hiro_pytorch" ["l"="25.034,35.698"]
"jcwleo/curiosity-driven-exploration-pytorch" ["l"="25.02,35.825"]
"google/prettytensor" ["l"="27.107,34.226"]
"osh/kerlym" ["l"="24.803,35.837"]
"Marqt/ViZDoom" ["l"="24.842,35.835"]
"twitter/torch-twrl" ["l"="26.915,34.336"]
"joschu/cgt" ["l"="27.002,34.183"]
"Kaixhin/rlenvs" ["l"="26.891,34.384"]
"spiglerg/DQN_DDQN_Dueling_and_DDPG_Tensorflow" ["l"="24.808,36.018"]
"zubair-irshad/Awesome-Implicit-NeRF-Robotics" ["l"="33.375,43.27"]
"dusty-nv/jetson-reinforcement" ["l"="29.764,38.104"]
"mila-iqia/babyai" ["l"="25.214,35.725"]
"huangwl18/language-planner" ["l"="24.807,36.542"]
"askforalfred/alfred" ["l"="24.765,36.55"]
"google-research/clevr_robot_env" ["l"="31.532,34.351"]
"jacobandreas/psketch" ["l"="31.693,34.356"]
"mit-acl/rl_collision_avoidance" ["l"="25.623,36.191"]
"Acmece/rl-collision-avoidance" ["l"="25.628,36.18"]
"ethz-asl/navrep" ["l"="25.642,36.187"]
"Shuijing725/CrowdNav_DSRNN" ["l"="25.61,36.211"]
"ignc-research/arena-rosnav" ["l"="25.678,36.187"]
"RoblabWh/RobLearn" ["l"="25.592,36.212"]
"NithishkumarS/DWA-RL" ["l"="25.652,36.154"]
"AndyYue1893/Deep-reinforcement-learning-with-pytorch" ["l"="25.314,36.103"]
"apachecn/ucb-cs294-112-notes-zh" ["l"="25.166,36.336"]
"jangirrishabh/Overcoming-exploration-from-demos" ["l"="25.347,35.808"]
"kevinzakka/dexterity" ["l"="25.533,35.798"]
"yzqin/dexmv-sim" ["l"="25.562,35.831"]
"Healthcare-Robotics/assistive-gym" ["l"="25.599,35.814"]
"caelan/pybullet-planning" ["l"="25.567,35.847"]
"google-research/relay-policy-learning" ["l"="25.485,35.827"]
"Xingyu-Lin/softgym" ["l"="25.667,35.743"]
"ChanganVR/CADRL" ["l"="25.616,36.201"]
"daenny/collvoid" ["l"="25.689,36.229"]
"gsartoretti/distributedRL_MAPF" ["l"="28.49,42.898"]
"MengGuo/RVO_Py_MAS" ["l"="25.665,36.234"]
"Huixxi/CS234-Reinforcement-Learning-Winter-2019" ["l"="25.084,36.393"]
"zlpure/CS234" ["l"="25.057,36.407"]
"arowdy98/Stanford-CS234" ["l"="25.096,36.377"]
"changebo/CS234-2020" ["l"="25.081,36.38"]
"ksang/cs234-assignments" ["l"="25.085,36.407"]
"tallamjr/stanford-cs234" ["l"="25.084,36.429"]
"Howuhh/faster-trajectory-transformer" ["l"="25.441,35.668"]
"frt03/generalized_dt" ["l"="25.438,35.652"]
"dhruvramani/Transformers-RL" ["l"="25.496,35.567"]
"MorvanZhou/train-robot-arm-from-scratch" ["l"="25.541,35.918"]
"zenetio/DeepRL-Robotic" ["l"="25.603,35.939"]
"kindredresearch/SenseAct" ["l"="25.416,35.874"]
"abr/abr_control" ["l"="25.485,35.881"]
"yao62995/AS_6Dof_Arm" ["l"="25.632,35.945"]
"MorvanZhou/train-classifier-from-scratch" ["l"="15.581,6.303"]
"Shawntl/Kuka_Robotics_Arms" ["l"="25.599,35.925"]
"rtv/Stage" ["l"="25.689,36.245"]
"playerproject/player" ["l"="25.727,36.276"]
"jennyhasahat/Player-Stage-Manual" ["l"="25.711,36.267"]
"dgriff777/a3c_continuous" ["l"="24.977,35.932"]
"uvipen/Street-fighter-A3C-ICM-pytorch" ["l"="25.052,35.783"]
"uvipen/Flappy-bird-deep-Q-learning-pytorch" ["l"="25.151,36.005"]
"chagmgang/distributed_reinforcement_learning" ["l"="25.202,35.91"]
"chagmgang/pytorch_ppo_rl" ["l"="24.966,35.955"]
"rlcode/reinforcement-learning-kr" ["l"="44.622,-14.87"]
"younggyoseo/CaDM" ["l"="25.141,35.648"]
"Knoxantropicen/model-based-meta-rl" ["l"="25.148,35.731"]
"dannysdeng/dqn-pytorch" ["l"="24.939,36.242"]
"SurrealAI/surreal" ["l"="25.318,35.859"]
"SudeepDasari/visual_foresight" ["l"="25.457,35.854"]
"PSVL/DoorGym" ["l"="25.507,35.865"]
"montrealrobotics/active-domainrand" ["l"="25.545,35.877"]
"qiwihui/reinforcement-learning-an-introduction-chinese" ["l"="25.206,36.267"]
"qqiang00/ReinforcemengLearningPractice" ["l"="25.203,36.345"]
"KaleabTessera/DQN-Atari" ["l"="25.389,36.305"]
"iassael/torch-bootstrapped-dqn" ["l"="24.733,35.782"]
"yobibyte/atarigrandchallenge" ["l"="24.741,35.796"]
"vivanov879/draw" ["l"="26.935,34.352"]
"fmassa/optimize-net" ["l"="26.887,34.316"]
"vvanirudh/IRL-Toolkit" ["l"="24.969,35.945"]
"ermongroup/MetaIRL" ["l"="25.054,35.971"]
"Div99/IQ-Learn" ["l"="25.083,35.97"]
"sotetsuk/pgx" ["l"="-14.995,39.087"]
"facebookresearch/online-dt" ["l"="25.484,35.661"]
"facebookresearch/droidlet" ["l"="-7.453,-42.144"]
"Tigermouthbear/Theia" ["l"="-43.669,9.437"]
"SoulLights/SLPlayer" ["l"="9.663,19.455"]
"jlubars/RL-MPC-LaneMerging" ["l"="25.512,36.146"]
"avisingh599/reward-learning-rl" ["l"="25.339,35.829"]
"Farama-Foundation/SuperSuit" ["l"="25.393,35.936"]
"koulanurag/minimal-marl" ["l"="25.433,36.003"]
"AI4Finance-Foundation/FinRL" ["l"="21.651,32.593"]
"AI4Finance-Foundation/FinRL-Meta" ["l"="21.61,32.514"]
"berkeleydeeprlcourse/homework_fall2021" ["l"="25.121,36.312"]
"vamsianumula/cs285-deeprl-ucberkeley-2021" ["l"="25.121,36.339"]
"gwthomas/IQL-PyTorch" ["l"="25.433,35.713"]
"young-geng/JaxCQL" ["l"="25.421,35.722"]
"google-research/realworldrl_suite" ["l"="25.29,35.78"]
"vita-epfl/social-nce" ["l"="25.659,36.256"]
"bbitmaster/ale_python_interface" ["l"="24.827,35.664"]
"openai/doom-py" ["l"="24.951,35.742"]
"tdmdal/rl-hedge-2019" ["l"="24.802,36.273"]
"enhuiz/flappybird-ql" ["l"="24.621,35.937"]
"mrspeaker/Omega500" ["l"="24.636,35.92"]
"Enhuiz/flappybird-ql" ["l"="24.658,35.928"]
"pascanur/GroundHog" ["l"="27.054,34.23"]
"jdeng/rbm-mnist" ["l"="26.895,34.041"]
"CETC-TFAI/MaCA" ["l"="25.401,36.014"]
"sujiongming/starcraftAI" ["l"="25.446,36.043"]
"wangcongrobot/dual_ur5_husky_mujoco" ["l"="25.592,35.87"]
"ElectronicElephant/pybullet_ur5_robotiq" ["l"="35.71,35.349"]
"XinJingHao/PPO-Discrete-Pytorch" ["l"="25.362,36.138"]
"ac-93/tactile_gym" ["l"="25.714,35.87"]
"CMURoboTouch/Taxim" ["l"="25.746,35.865"]
"danfergo/gelsight_simulation" ["l"="25.757,35.876"]
"zixichen007115/Tacchi" ["l"="25.735,35.871"]
"yikaiw/EIP" ["l"="25.717,35.858"]
"zlr20/saferl_kit" ["l"="29.794,45.172"]
"gwthomas/Safe-MBPO" ["l"="25.476,35.905"]
"AgrawalAmey/safe-explorer" ["l"="25.449,35.896"]
"facebookresearch/PyTouch" ["l"="25.737,35.85"]
"facebookresearch/pybulletX" ["l"="25.715,35.842"]
"sea-bass/ycb-tools" ["l"="25.693,35.848"]
"stanford-iprl-lab/multimodal_representation" ["l"="25.727,35.886"]
"Farama-Foundation/Gym-Robotics" ["l"="25.473,35.777"]
"facebookresearch/differentiable-robot-model" ["l"="27.945,42.236"]
"mahyaret/gym-panda" ["l"="25.497,35.893"]
"studywolf/pydmps" ["l"="27.79,42.326"]
"nikhilbarhate99/Actor-Critic-PyTorch" ["l"="24.972,35.997"]
"paarthneekhara/Weather-From-Map" ["l"="31.627,34.388"]
"pathak22/exploration-by-disagreement" ["l"="25.05,35.717"]
"ignitionrobotics/ign-gazebo" ["l"="25.698,35.962"]
"ignitionrobotics/ign-sensors" ["l"="25.725,35.983"]
"osrf/sdformat" ["l"="29.337,41.831"]
"ignitionrobotics/ign-rendering" ["l"="25.735,35.97"]
"ignitionrobotics/ign-physics" ["l"="25.659,35.953"]
"ignitionrobotics/ign-math" ["l"="25.754,35.971"]
"ignitionrobotics/ros_ign" ["l"="25.743,35.985"]
"ros/sdformat_urdf" ["l"="25.727,35.959"]
"ignitionrobotics/ign-gui" ["l"="25.743,35.958"]
"onlytailei/gym_style_gazebo" ["l"="25.639,36.264"]
"kootenpv/neural_complete" ["l"="24.815,35.755"]
"jostmey/rwa" ["l"="26.99,34.478"]
"uclmr/pycodesuggest" ["l"="24.748,35.697"]
"silicon-valley-data-science/RNN-Tutorial" ["l"="23.282,31.076"]
"joeddav/devol" ["l"="25.627,33.732"]
"chiphuyen/tf-stanford-tutorials" ["l"="23.364,31.06"]
"JulianGaal/python-cheat-sheet" ["l"="23.127,31.178"]
"openai/generating-reviews-discovering-sentiment" ["l"="30.074,32.59"]
"Babylonpartners/fastText_multilingual" ["l"="29.966,32.578"]
"thomasj02/DeepLearningProjectWorkflow" ["l"="23.151,31.196"]
"microsoft/ChatGPT-Robot-Manipulation-Prompts" ["l"="25.681,35.793"]
"microsoft/MM-REACT" ["l"="27.381,31.26"]
"vlmaps/vlmaps" ["l"="25.657,35.81"]
"google-research/tiny-differentiable-simulator" ["l"="28.075,42.148"]
"henry8527/HCOT" ["l"="22.907,-25.123"]
"IpsumDominum/super-brain-weight-agnostic-neural-networks" ["l"="25.231,35.585"]
"SudeepDasari/RoboNet" ["l"="25.469,35.872"]
"pokaxpoka/netrand" ["l"="25.145,35.78"]
"deepmind/mujoco_mpc" ["l"="25.524,35.756"]
"zalo/mujoco_wasm" ["l"="25.607,35.711"]
"ToruOwO/minimal-stable-PPO" ["l"="25.614,35.73"]
"catziyan/DRLPytorch-" ["l"="25.281,36.33"]
"Teacher-Guo/RL_code" ["l"="25.289,36.352"]
"PKU-AI-Edge/I2C" ["l"="25.441,36.096"]
"saizhang0218/VBC" ["l"="25.446,36.11"]
"Coac/CommNet-BiCnet" ["l"="25.472,36.128"]
"apsdehal/ic3net-envs" ["l"="25.429,36.088"]
"jiechuanjiang/pytorch_DGN" ["l"="25.44,36.056"]
"tegg89/magnet" ["l"="25.418,36.002"]
"YuhangSong/Arena-BuildingToolkit" ["l"="25.508,36.014"]
"PKU-RL/DGN" ["l"="25.487,36.094"]
"tuladhay/ATOC_COMA_PyTorch" ["l"="25.448,36.125"]
"mihaibivol/Q-learning-tic-tac-toe" ["l"="24.606,35.967"]
"kyokin78/rl-flappybird" ["l"="24.598,35.955"]
"YangRui2015/Modular_HER" ["l"="25.402,36.179"]
"yao62995/A3C" ["l"="24.827,35.84"]
"deepmind/multi_object_datasets" ["l"="31.563,34.239"]
"jcoreyes/OP3" ["l"="31.542,34.16"]
"vitchyr/viskit" ["l"="25.192,35.65"]
"mengf1/DHER" ["l"="25.21,35.788"]
"pytorch-labs/tensordict" ["l"="25.511,35.705"]
"rohan-sawhney/multi-agent-rl" ["l"="24.984,36.091"]
"andrewliao11/NoisyNet-DQN" ["l"="24.782,35.929"]
"tambetm/pommerman-baselines" ["l"="25.396,36.091"]
"VinF/deer" ["l"="24.835,35.813"]
"ADGEfficiency/energy-py" ["l"="24.522,31.999"]
"Matrixeigs/energy_management_system" ["l"="24.526,32.025"]
"bulletphysics/pybullet_robots" ["l"="25.513,35.9"]
"google-research/motion_imitation" ["l"="28.178,42.168"]
"nicrusso7/rex-gym" ["l"="28.261,42.182"]
"oscar-lima/pybullet_ros" ["l"="25.559,35.901"]
"Derek-TH-Wang/quadruped_ctrl" ["l"="28.203,42.167"]
"OpenQuadruped/spot_mini_mini" ["l"="28.251,42.193"]
"go2sea/C51DQN" ["l"="24.661,35.488"]
"Kiwoo/distributional_perspective_on_RL" ["l"="24.641,35.477"]
"gemst1/IRL" ["l"="24.984,35.995"]
"sawcordwell/pymdptoolbox" ["l"="28.695,43.192"]
"awjuliani/oreilly-rl-tutorial" ["l"="33.948,32.602"]
"lusob/gym-ple" ["l"="24.741,35.835"]
"sherjilozair/dqn" ["l"="24.788,35.776"]
"datalogai/recurrentshop" ["l"="31.181,31.592"]
"opensim-org/opensim-core" ["l"="7.991,16.03"]
"bhanuvikasr/Deep-RL-TORCS" ["l"="24.716,36.018"]
"popovicidaniela/Master-Thesis" ["l"="24.7,36.005"]
"xinleipan/VirtualtoReal-RL" ["l"="24.696,36.021"]
"madras-simulator/MADRaS" ["l"="24.66,36.031"]
"PKU-TANGENT/nlp-tutorial" ["l"="26.04,35.243"]
"Zce1112zslx/ChID_baseline" ["l"="26.012,35.257"]
"dqxiu/CaliNet" ["l"="26.004,35.28"]
"dqxiu/ICL_PaperList" ["l"="27.571,31.334"]
"chenllliang/MLS" ["l"="26.059,35.236"]
"OpenMindClub/awesome-scholarly-productivity" ["l"="5.247,16.663"]
"lancopku/clip-openness" ["l"="26.031,35.263"]
"PKU-TANGENT/ConFiguRe" ["l"="26.028,35.226"]
"wzh9969/HPT" ["l"="30.223,30.454"]
"reorx/cht-colors" ["l"="26.044,35.207"]
"RunxinXu/ChildTuning" ["l"="26.075,35.231"]
"kristery/Imitation-Learning-from-Imperfect-Demonstration" ["l"="25.166,36.058"]
"carla-simulator/imitation-learning" ["l"="29.887,45.261"]
"KamyarGh/rl_swiss" ["l"="25.232,36.021"]
"ermongroup/multiagent-gail" ["l"="25.088,36.028"]
"HorizonRobotics/SocialRobot" ["l"="25.338,35.647"]
"rll-research/cic" ["l"="25.366,35.654"]
"vietnguyen91/Super-mario-bros-A3C-pytorch" ["l"="25.508,35.943"]
"vietnguyen91/Flappy-bird-deep-Q-learning-pytorch" ["l"="25.596,35.967"]
"AdeelMufti/WorldModels" ["l"="25.137,35.674"]
"dylandjian/retro-contest-sonic" ["l"="25.153,35.675"]
"ctallec/pyvarinf" ["l"="24.097,34.62"]
"Deepest-Project/WorldModels-A3C" ["l"="25.164,35.649"]
"zacwellmer/WorldModels" ["l"="25.67,36.911"]
"hardmaru/pytorch_notebooks" ["l"="24.703,38.434"]
"lucidrains/phasic-policy-gradient" ["l"="25.178,35.746"]
"ethz-asl/rl-navigation" ["l"="25.611,36.257"]
"MattChanTK/gym-maze" ["l"="25.215,35.695"]
"MattChanTK/ai-gym" ["l"="25.187,35.633"]
"zuoxingdong/mazelab" ["l"="25.198,35.618"]
"Mehdi0xC/PathFinding-Agent-with-Deep-Reinforcement-Learning" ["l"="25.794,36.181"]
"minerllabs/basalt-2022-behavioural-cloning-baseline" ["l"="25.33,35.657"]
"TeaPearce/Counter-Strike_Behavioural_Cloning" ["l"="25.35,35.59"]
"NVlabs/contact_graspnet" ["l"="31.567,42.322"]
"liruiw/GA-DDPG" ["l"="31.586,42.276"]
"jhu-lcsr/good_robot" ["l"="25.626,35.923"]
"skumra/robotic-grasping" ["l"="31.559,42.349"]
"harvard-microrobotics/object2urdf" ["l"="25.663,35.879"]
"ehrenbrav/FCEUX_Learning_Environment" ["l"="24.887,35.647"]
"xushsh163/A3CSuperMario_Windows" ["l"="24.887,35.662"]
"llSourcell/deep_q_learning" ["l"="24.962,35.669"]
"ppaquette/gym-doom" ["l"="24.953,35.642"]
"robmsylvester/Super-Mario-Bros-DQN" ["l"="24.985,35.682"]
"ppaquette/gym-pull" ["l"="24.976,35.656"]
"caelan/pddlstream" ["l"="25.694,35.824"]
"caelan/SS-Replan" ["l"="25.738,35.804"]
"caelan/motion-planners" ["l"="25.761,35.811"]
"caelan/LTAMP" ["l"="25.657,35.835"]
"tomsilver/pddlgym" ["l"="29.688,41.822"]
"MarcToussaint/18-RSS-PhysicalManipulation" ["l"="25.801,35.808"]
"yijiangh/pybullet_planning" ["l"="25.635,35.837"]
"zi-w/Kitchen2D" ["l"="25.741,35.82"]
"aibasel/pyperplan" ["l"="29.717,41.801"]
"mees/calvin" ["l"="25.521,35.807"]
"allenai/manipulathor" ["l"="24.768,36.532"]
"DanielTakeshi/deformable-ravens" ["l"="25.604,35.775"]
"gkahn13/gcg" ["l"="25.636,36.251"]
"bennylp/RL-Taxonomy" ["l"="25.428,35.815"]
"Observerspy/CS294" ["l"="25.013,36.405"]
"Observerspy/CS234" ["l"="25.021,36.43"]
"xuwd11/cs294-112_hws" ["l"="25.033,36.334"]
"BY571/CQL" ["l"="25.37,35.688"]
"instadeepai/catx" ["l"="25.479,35.798"]
"twitter/torch-autograd" ["l"="26.939,34.298"]
"yueatsprograms/Stochastic_Depth" ["l"="26.918,34.293"]
"inspirai/TimeChamber" ["l"="25.594,35.837"]
"ucl-dark/paired" ["l"="25.406,35.667"]
"duckietown/gym-duckietown" ["l"="25.202,35.768"]
"duckietown/Software" ["l"="25.16,35.627"]
"nplan/gym-line-follower" ["l"="25.16,35.662"]
"praveen-palanisamy/macad-gym" ["l"="29.861,45.215"]
"ignc-research/arena-fsm-ego-planner" ["l"="25.714,36.2"]
"ignc-research/arena-tools" ["l"="25.711,36.182"]
"ignc-research/navsafe-arena" ["l"="25.698,36.187"]
"ignc-research/all-in-one-DRL-planner" ["l"="25.713,36.19"]
"ignc-research/arena-rosnav-3D" ["l"="25.699,36.195"]
"instadeepai/fastpbrl" ["l"="25.562,35.742"]
"icaros-usc/dqd" ["l"="25.551,35.732"]
"ollenilsson19/QDgym" ["l"="25.564,35.721"]
"instadeepai/poppy" ["l"="25.505,35.723"]
"instadeepai/manyfold" ["l"="25.52,35.724"]
"instadeepai/nucleotide-transformer" ["l"="25.511,35.748"]
"princeton-nlp/CoFiPruning" ["l"="26.134,35.184"]
"RunxinXu/ContrastivePruning" ["l"="26.109,35.209"]
"princeton-nlp/TRIME" ["l"="25.841,29.312"]
"sai-prasanna/bert-experiments" ["l"="26.15,35.169"]
"WoosukKwon/retraining-free-pruning" ["l"="27.735,31.168"]
"openai/requests-for-research" ["l"="24.798,35.8"]
"openai/improved-gan" ["l"="33.713,32.627"]
"harvardnlp/seq2seq-attn" ["l"="27.07,34.332"]
"vincentberaud/Minecraft-Reinforcement-Learning" ["l"="25.099,35.663"]
"pathak22/modular-assemblies" ["l"="25.057,35.735"]
"ilyasu123/trpo" ["l"="24.854,36.006"]
"chagmgang/tf2.0_reinforcement_learning" ["l"="25.242,36.142"]
"kvfrans/parallel-trpo" ["l"="24.783,36.057"]
"berkeleydeeprlcourse/homework_fall2019" ["l"="25.087,36.21"]
"mdeib/berkeley-deep-RL-pytorch-solutions" ["l"="25.115,36.262"]
"erfanMhi/Deep-Reinforcement-Learning-CS285-Pytorch" ["l"="25.135,36.261"]
"mdeib/berkeley-deep-RL-pytorch-starter" ["l"="25.12,36.283"]
"sharadmv/parasol" ["l"="25.408,35.811"]
"JanMatas/Rainbow_ddpg" ["l"="25.575,35.755"]
"haosulab/ManiSkill-Learn" ["l"="25.63,35.78"]
"haosulab/ManiSkill" ["l"="25.648,35.774"]
"haosulab/ManiSkill2-Learn" ["l"="25.612,35.787"]
"yusukeurakami/plan2explore-pytorch" ["l"="25.283,35.646"]
"amoudgl/short-jokes-dataset" ["l"="24.561,35.705"]
"amoudgl/funnybot" ["l"="24.573,35.718"]
"taivop/joke-dataset" ["l"="24.606,35.721"]
"CrowdTruth/Short-Text-Corpus-For-Humor-Detection" ["l"="24.58,35.697"]
"AndersonJo/dqn-pytorch" ["l"="24.931,35.982"]
"daggertye/CS294_homework" ["l"="25.014,36.362"]
"AnujMahajanOxf/MAVEN" ["l"="25.604,36.085"]
"IouJenLiu/CMAE" ["l"="25.643,36.095"]
"flyyufelix/C51-DDQN-Keras" ["l"="24.709,35.51"]
"Silvicek/distributional-dqn" ["l"="24.683,35.486"]
"floringogianu/categorical-dqn" ["l"="24.7,35.475"]
"wizdom13/RND-Pytorch" ["l"="24.981,35.715"]
"orrivlin/MountainCar_DQN_RND" ["l"="24.977,35.726"]
"google-research/pddm" ["l"="25.228,35.657"]
"cts198859/deeprl_signal_control" ["l"="29.913,45.074"]
"LucasAlegre/sumo-rl" ["l"="29.899,45.082"]
"docwza/sumolights" ["l"="29.924,45.069"]
"wingsweihua/colight" ["l"="29.948,45.089"]
"stormmax/reinforcement_learning" ["l"="24.931,35.955"]
"Hunter-DDM/knowledge-neurons" ["l"="25.911,35.356"]
"EleutherAI/knowledge-neurons" ["l"="25.929,35.342"]
"resibots/pymap_elites" ["l"="24.585,38.418"]
"cross32768/Dreamer_PyTorch" ["l"="25.27,35.675"]
"zhaoyi11/dreamer-pytorch" ["l"="25.263,35.682"]
"dojo-sim/Dojo.jl" ["l"="27.962,42.166"]
"Simple-Robotics/proxsuite" ["l"="28.067,42.13"]
"tasts-robots/vulp" ["l"="25.578,35.712"]
"loco-3d/crocoddyl" ["l"="28.092,42.123"]
"qiayuanliao/legged_control" ["l"="28.126,42.154"]
"stephane-caron/robot_descriptions.py" ["l"="27.998,42.148"]
"mayataka/robotoc" ["l"="28.088,42.15"]
"typoverflow/UtilsRL" ["l"="25.58,35.73"]
"typoverflow/OfflineRL-Lib" ["l"="25.579,35.689"]
"shkrwnd/Deep-Reinforcement-Learning-for-Dynamic-Spectrum-Access" ["l"="3.356,39.266"]
"cyoon1729/Multi-agent-reinforcement-learning" ["l"="25.426,36.106"]
"Crawford-fang/ROS_pytorch_RL" ["l"="25.737,36.41"]
"Shuijing725/CrowdNav_Prediction_AttnGraph" ["l"="25.607,36.243"]
"jerrodparker20/adaptive-transformers-in-rl" ["l"="25.525,35.54"]
"alantess/gtrxl-torch" ["l"="25.501,35.534"]
"kevslinger/DTQN" ["l"="25.52,35.52"]
"yashbonde/Transformer-RL" ["l"="25.511,35.544"]
"ignc-research/arena-bench" ["l"="25.723,36.193"]
"ajlangley/cpo-pytorch" ["l"="25.433,35.9"]
"zbzhu99/Constrained-Decision-Making-Paper-List" ["l"="25.454,35.871"]
"SapanaChaudhary/PyTorch-CPO" ["l"="25.424,35.891"]
"AlgTUDelft/WCSAC" ["l"="25.429,35.874"]
"liuzuxin/safe-mbrl" ["l"="25.495,35.87"]
"UT-Austin-RPL/deoxys_control" ["l"="25.543,35.835"]
"clvrai/spirl" ["l"="25.42,35.909"]
"jaybutera/tetrisRL" ["l"="25.101,36.244"]
"lusob/gym-tetris" ["l"="25.117,36.188"]
"lukashermann/hulc" ["l"="25.553,35.776"]
"florensacc/rllab-curriculum" ["l"="25.116,35.67"]
"ruizhaogit/EnergyBasedPrioritization" ["l"="25.192,35.734"]
"mengf1/CHER" ["l"="25.172,35.706"]
"google-research/dads" ["l"="25.317,35.642"]
"paulorauber/hpg" ["l"="25.218,35.681"]
"ben-eysenbach/sac" ["l"="25.329,35.571"]
"alirezakazemipour/DIAYN-PyTorch" ["l"="25.322,35.586"]
"Baichenjia/PBRL" ["l"="25.444,35.705"]
"borgwang/reinforce_py" ["l"="25.315,36.296"]
"wangshusen/deep-rl" ["l"="25.338,36.29"]
"silencial/DeepRL" ["l"="25.063,36.252"]
"rohanpsingh/mujoco-python-viewer" ["l"="25.54,35.72"]
"brian473/neural_rl" ["l"="24.712,35.857"]
"TorontoDeepLearning/convnet" ["l"="26.891,34.077"]
"jbornschein/draw" ["l"="27.036,34.234"]
"facebook/iTorch" ["l"="26.95,34.241"]
"benanne/kaggle-ndsb" ["l"="27.053,34.115"]
"benanne/kaggle-galaxies" ["l"="27.001,34.122"]
"facebookresearch/salina" ["l"="25.405,35.649"]
"mle-infrastructure/mle-hyperopt" ["l"="25.453,35.583"]
"mlfoundations/task_vectors" ["l"="25.701,35.53"]
"mlfoundations/patching" ["l"="25.729,35.506"]
"naver-ai/coco-annotation-tool" ["l"="44.199,-15.166"]
"yihaosun1124/OfflineRL-Kit" ["l"="25.529,35.657"]
"yihaosun1124/pytorch-mopo" ["l"="25.428,35.69"]
"sisl/ngsim_env" ["l"="29.731,44.503"]
"MiguelARD/DoorDetect-Dataset" ["l"="25.572,35.883"]
"facebookresearch/modem" ["l"="25.449,35.624"]
"facebookresearch/dcd" ["l"="25.385,35.675"]
"ArnaudFickinger/gym-multigrid" ["l"="25.394,35.88"]
"vwxyzjn/gym-microrts" ["l"="24.346,37.564"]
"schaul/py-vgdl" ["l"="25.235,35.257"]
"rubenvereecken/py-vgdl" ["l"="25.236,35.23"]
"GAIGResearch/GVGAI" ["l"="25.235,35.295"]
"EssexUniversityMCTS/gvgai" ["l"="25.235,35.278"]
"lucidrains/dreamerv3-pytorch" ["l"="25.385,35.662"]
"NM512/dreamerv3-torch" ["l"="25.401,35.602"]
"chenhongge/SA_DQN" ["l"="24.83,36.181"]
"tuomaso/radial_rl" ["l"="24.811,36.183"]
"lan-lc/adversarial_example_of_Go" ["l"="24.816,36.169"]
"tuomaso/radial_rl_v2" ["l"="24.843,36.165"]
"WuTheFWasThat/hanabi.rs" ["l"="25.42,36.175"]
"alirezamika/evostra" ["l"="24.651,38.326"]
"cardwing/Codes-for-RL-PER" ["l"="24.968,36.091"]
"ku2482/soft-actor-critic.pytorch" ["l"="24.972,36.107"]
"kevinzakka/ibc" ["l"="25.598,35.724"]
"Farama-Foundation/D4RL-Evaluations" ["l"="25.457,35.69"]
"dmksjfl/MCQ" ["l"="25.464,35.699"]
"Farama-Foundation/Metaworld" ["l"="25.489,35.675"]
"Farama-Foundation/Minari" ["l"="25.476,35.689"]
"denisyarats/exorl" ["l"="25.393,35.69"]
"heyuanYao-pku/Control-VAE" ["l"="35.523,35.108"]
"Improbable-AI/walk-these-ways" ["l"="28.215,42.144"]
"facebookresearch/pytext" ["l"="29.887,32.573"]
"xbpeng/DeepLoco" ["l"="24.772,35.76"]
"akanazawa/motion_reconstruction" ["l"="35.684,35.165"]
"UBCMOCCA/TerrainRLSim" ["l"="24.791,35.76"]
"xbpeng/awr" ["l"="25.124,35.752"]
"stevens-cs546-cs554/CS-554" ["l"="25.244,36.354"]
"rwightman/pytorch-pommerman-rl" ["l"="25.426,36.13"]
"BorealisAI/pommerman-baseline" ["l"="25.433,36.122"]
"eugene/pommerman" ["l"="25.456,36.149"]
"isp1tze/MAProj" ["l"="25.562,36.091"]
"shacklettbp/bps-nav" ["l"="25.64,35.885"]
"5vision/deep-reinforcement-learning-networks" ["l"="24.767,35.78"]
"5vision/DARQN" ["l"="24.709,35.769"]
"AIRLab-POLIMI/mushroom" ["l"="25.016,35.639"]
"MushroomRL/mushroom-rl-benchmark" ["l"="24.997,35.608"]
"abr/abr_jaco2" ["l"="25.535,35.888"]
"pfnet-research/pfhedge" ["l"="24.696,36.377"]
"pfnet-research/NoTransactionBandNetwork" ["l"="24.697,36.362"]
"hansbuehler/deephedging" ["l"="24.681,36.367"]
"StanfordVL/OmniGibson" ["l"="25.669,35.768"]
"devendrachaplot/Object-Goal-Navigation" ["l"="24.716,36.523"]
"showlab/EgoVLP" ["l"="31.645,33.711"]
"facebookresearch/habitat-challenge" ["l"="24.699,36.562"]
"microsoft/cohesion-based-robot-teaching-interface" ["l"="25.709,35.787"]
"cxy1997/Robotiq-UR5" ["l"="25.578,35.906"]
"j96w/MuJoCo_Unity_UR5" ["l"="25.642,35.913"]
"roboticsleeds/mujoco_ur5_model" ["l"="25.618,35.91"]
"VincentYu68/SymmetryCurriculumLocomotion" ["l"="24.717,35.732"]
"M-J-Murray/MAMEToolkit" ["l"="25.099,35.802"]
"alito/mamele" ["l"="25.069,35.756"]
"M-J-Murray/SFAgents" ["l"="25.057,35.755"]
"TorchCraft/TorchCraftAI" ["l"="24.238,37.5"]
"m5823779/stereo_image_generator_from_single_image" ["l"="25.467,36.212"]
"CherryPieSexy/imitation_learning" ["l"="25.235,36.047"]
"seolhokim/InverseRL-Pytorch" ["l"="25.25,35.99"]
"ku2482/gail-airl-ppo.pytorch" ["l"="25.143,36.113"]
"wilson1yan/rlpyt" ["l"="25.628,35.73"]
"NVlabs/DefGraspSim" ["l"="25.853,35.85"]
"NVlabs/biotac_sim" ["l"="25.808,35.85"]
"martius-lab/SMORL" ["l"="25.281,35.679"]
"mcgillmrl/kusanagi" ["l"="25.171,35.676"]
"montrealrobotics/domain-randomizer" ["l"="25.594,35.887"]
"facebookresearch/MIXER" ["l"="26.965,34.382"]
"jingranburangyongzhongwen/torchMARL" ["l"="25.527,36.076"]
"dayekuaipao/ierg6130-assignment" ["l"="25.188,36.205"]
"Aguin/cuhkrlcourse-ierg6130" ["l"="25.194,36.215"]
"NeuralMMO/baselines" ["l"="25.396,36.202"]
"PufferAI/PufferLib" ["l"="25.413,36.206"]
"Netease-Games-AI-Lab-Guangzhou/realikun" ["l"="25.389,36.19"]
"mansicer/MAIC" ["l"="25.536,36.028"]
"themrzmaster/git-re-basin-pytorch" ["l"="25.682,35.569"]
"stanislavfort/dissect-git-re-basin" ["l"="25.662,35.564"]
"ogkalu2/Merge-Stable-Diffusion-models-without-distortion" ["l"="34.578,29.315"]
"ethancaballero/broken_neural_scaling_laws" ["l"="25.648,35.55"]
"kmeng01/rome" ["l"="25.818,35.429"]
"sidak/otfusion" ["l"="25.67,35.547"]
"synpon/prog_nn" ["l"="24.745,35.815"]
"seann999/progressive_a3c" ["l"="24.766,35.824"]
"jeanharb/a2oc_delib" ["l"="24.972,35.705"]
"kkhetarpal/ioc" ["l"="24.954,35.694"]
"shivaverma/OpenAIGym" ["l"="25.112,36.218"]
"shivaverma/Orbit" ["l"="25.091,36.275"]
"pythonlessons/RL-Bitcoin-trading-bot" ["l"="21.614,32.421"]
"stevens-cs546-cs554/CS-546" ["l"="25.25,36.4"]
"aronsar/hoad" ["l"="25.408,36.126"]
"facebookresearch/jps" ["l"="25.417,36.149"]
"facebookresearch/off-belief-learning" ["l"="25.409,36.139"]
"utiasDSL/gym-pybullet-drones" ["l"="27.9,43.677"]
"uzh-rpg/high_mpc" ["l"="27.921,43.619"]
"TUM-AAS/ml-casadi" ["l"="28.059,41.984"]
"UM-ARM-Lab/pytorch_mppi" ["l"="27.917,42.118"]
"Shunichi09/PythonLinearNonlinearControl" ["l"="28.01,42.074"]
"HybridRobotics/cbf" ["l"="28.194,41.845"]
"EderSantana/seya" ["l"="31.11,31.571"]
"google-research/dice_rl" ["l"="25.294,35.699"]
"sparkmxy/my-offlinerl" ["l"="25.314,35.68"]
"victorcampos7/edl" ["l"="25.335,35.53"]
"chauncygu/Safe-Multi-Agent-Mujoco" ["l"="25.531,35.988"]
"chauncygu/Safe-Multi-Agent-Isaac-Gym" ["l"="25.52,35.991"]
"chauncygu/Safe-Multi-Agent-Robosuite" ["l"="25.566,36.006"]
"DrZero0/MACC" ["l"="25.572,36.037"]
"belepi93/Ape-X" ["l"="25.064,35.705"]
"jingweiz/pytorch-distributed" ["l"="25.041,35.624"]
"sisl/DICG" ["l"="25.541,36.047"]
"mega002/lm-debugger" ["l"="25.844,35.399"]
"mega002/ff-layers" ["l"="25.85,35.375"]
"aviclu/ffn-values" ["l"="25.844,35.417"]
"EvolutionGym/evogym-design-tool" ["l"="25.62,35.622"]
"EvolutionGym/evolutiongym.github.io" ["l"="25.651,35.608"]
"songrotek/DQN-Atari-Tensorflow" ["l"="24.685,35.893"]
"floodsung/DQN-Atari-Tensorflow" ["l"="24.664,35.872"]
"gtoubassi/dqn-atari" ["l"="24.641,35.866"]
"haosulab/SAPIEN-Release" ["l"="25.71,35.76"]
"arex18/rocket-lander" ["l"="24.951,35.592"]
"ermongroup/InfoGAIL" ["l"="24.917,35.974"]
"sisl/hgail" ["l"="24.933,35.966"]
"tatsuyaokubo/dqn" ["l"="24.737,35.748"]
"nivwusquorum/tf-adversarial" ["l"="31.717,34.339"]
"rubenrtorrado/GVGAI_GYM" ["l"="25.235,35.326"]
"adik993/ppo-pytorch" ["l"="24.863,36.022"]
"amazon-research/meta-q-learning" ["l"="25.117,35.74"]
"Farama-Foundation/Shimmy" ["l"="25.489,35.766"]
"x35f/unstable_baselines" ["l"="25.544,35.817"]
"jiangsy/mbpo_pytorch" ["l"="25.467,35.738"]
"danfeiX/model-based-papers" ["l"="25.125,35.576"]
"ferreirafabio/mppi_pendulum" ["l"="25.111,35.524"]
"gasparramoa/DeepDoors2" ["l"="25.622,35.892"]
"PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python" ["l"="25.004,36.256"]
"sudharsan13296/Word2vec-from-scratch" ["l"="25.021,36.235"]
"stepjam/YARR" ["l"="25.618,35.82"]
"Henry1iu/ierg5350_rl_course_project" ["l"="25.77,35.896"]
"xmfbit/DQN-FlappyBird" ["l"="25.171,36.206"]
"danijar/director" ["l"="25.356,35.644"]
"danijar/crafter-baselines" ["l"="25.35,35.66"]
"orybkin/lexa" ["l"="25.364,35.608"]
"artberryx/CSD-public" ["l"="25.382,35.612"]
"facebookresearch/denoised_mdp" ["l"="25.381,35.627"]
"denisyarats/proto" ["l"="25.384,35.644"]
"tshrjn/env-zoo" ["l"="25.24,35.825"]
"haosulab/SAPIEN" ["l"="25.642,35.793"]
"yzqin/isaacgym-stubs" ["l"="25.628,35.746"]
"TianhongDai/self-imitation-learning-pytorch" ["l"="25.04,35.748"]
"pathak22/zeroshot-imitation" ["l"="25.113,35.778"]
"openai/atari-reset" ["l"="25.033,35.724"]
"snape/HRVO" ["l"="-12.971,39.765"]
"suraj2596/RVO_rospy" ["l"="25.69,36.266"]
"ferherranz/multi_robot" ["l"="25.725,36.255"]
"yilundu/imagination_augmented_agents" ["l"="25.087,35.669"]
"arraiy/torchgeometry" ["l"="26.776,34.449"]
"nsavinov/SPTM" ["l"="24.621,36.488"]
"ICL-SML/pilco-matlab" ["l"="25.127,35.635"]
"jaztsong/PILCO-gpytorch" ["l"="25.128,35.654"]
"aidanscannell/pilco-tensorflow" ["l"="25.141,35.662"]
"helgeanl/GP-MPC" ["l"="28.028,42.01"]
"david-abel/simple_rl" ["l"="24.929,35.673"]
"pbontrager/GenerativePlayingNetworks" ["l"="25.234,35.427"]
"smearle/control-pcgrl" ["l"="25.231,35.509"]
"marooncn/RL" ["l"="25.582,36.267"]
"jkulhanek/visual-navigation-agent-pytorch" ["l"="24.72,36.463"]
"yushu-liu/icra2017-visual-navigation" ["l"="24.709,36.474"]
"daniellawson9999/online-decision-transformer" ["l"="25.484,35.645"]
"rldm/rldm_tutorials" ["l"="24.949,36.117"]
"pushkar/ABAGAIL" ["l"="28.565,43.317"]
"MIT-REALM/neural_clbf" ["l"="25.56,35.694"]
"russellmendonca/maesn_suite" ["l"="25.105,35.728"]
"lhao499/taming-maml" ["l"="25.077,35.726"]
"openai/atari-demo" ["l"="25.016,35.67"]
"danijar/daydreamer" ["l"="25.36,35.673"]
"vincent-thevenin/DreamerV2-Pytorch" ["l"="25.345,35.617"]
"mhauskn/dqn" ["l"="24.684,35.797"]
"matteocasolari/reinforcement-learning-an-introduction-solutions" ["l"="25.27,36.234"]
"younggyoseo/Ape-X" ["l"="25.025,35.592"]
"anxie/meta_classifier" ["l"="25.521,35.861"]
"google-research/batch-ppo" ["l"="24.859,36.251"]
"google-research/policy-learning-landscape" ["l"="24.834,36.284"]
"hiwonjoon/ICML2019-TREX" ["l"="25.063,36.293"]
"dsbrown1331/CoRL2019-DREX" ["l"="25.083,36.241"]
"ac-93/soft-actor-critic" ["l"="25.199,35.695"]
"yining043/SAC-discrete" ["l"="25.212,35.705"]
"alirezakazemipour/Discrete-SAC-PyTorch" ["l"="25.202,35.705"]
"lmzintgraf/hyperx" ["l"="25.187,35.743"]
"LanqingLi1993/FOCAL-ICLR" ["l"="25.191,35.705"]
"chenllliang/ATP" ["l"="26.066,35.22"]
"justinjfu/doodad" ["l"="25.178,35.583"]
"tianjunz/NovelD" ["l"="25.295,35.585"]
"facebookresearch/adversarially-motivated-intrinsic-goals" ["l"="25.295,35.563"]
"yfletberliac/adversarially-guided-actor-critic" ["l"="25.282,35.576"]
"JohanSamir/revisiting_rainbow" ["l"="25.338,35.669"]
"AgileRL/AgileRL" ["l"="25.476,35.613"]
"vwxyzjn/cleanba" ["l"="25.531,35.562"]
"google-research/reincarnating_rl" ["l"="25.563,35.524"]
"microsoft/FQF" ["l"="24.953,36.271"]
"xtma/dsac" ["l"="24.937,36.264"]
"IvLabs/Natural-Language-Processing" ["l"="31.172,-22.162"]
"IvLabs/Sahayak-v3" ["l"="31.177,-22.134"]
"HiPatil/Policy-based-RL" ["l"="31.157,-22.168"]
"IvLabs/stagewise-knowledge-distillation" ["l"="31.167,-22.15"]
"HiPatil/Autonomous-Delivery-Robot" ["l"="31.151,-22.185"]
"HiPatil/Value-based-RL" ["l"="31.144,-22.17"]
"IvLabs/ResearchPaperNotes" ["l"="31.208,-22.128"]
"RajGhugare19/gym-OctaKing" ["l"="25.337,35.635"]
"HiPatil/number-plate-detection-system" ["l"="31.143,-22.18"]
"IvLabs/Quantum-Machine-Learning" ["l"="31.205,-22.172"]
"RajGhugare19/Classical-RL" ["l"="25.33,35.626"]
"robotics-upo/lightsfm" ["l"="25.789,36.275"]
"deepmind/dm_hard_eight" ["l"="25.317,35.667"]
"unstable-zeros/learning-cbfs" ["l"="28.175,41.836"]
"yemam3/SAC-RCBF" ["l"="25.503,35.772"]
"yemam3/Mod-RL-RCBF" ["l"="25.543,35.746"]
"SliceOfBread/Hanabi" ["l"="25.394,36.134"]
"MarcToussaint/rai" ["l"="25.891,35.797"]
"MarcToussaint/rai-python" ["l"="25.863,35.8"]
"resibots/blackdrops" ["l"="25.833,33.426"]
"shacklettbp/bps3D" ["l"="25.674,35.896"]
"wyndwarrior/imitation_from_observation" ["l"="25.153,35.769"]
"pathak22/hierarchical-imitation" ["l"="25.115,35.762"]
"YicongHong/Discrete-Continuous-VLN" ["l"="24.752,36.599"]
"ikostrikov/pytorch-naf" ["l"="23.952,35.357"]
"aiworld/dqn" ["l"="24.693,35.842"]
"ugo-nama-kun/DQN-chainer" ["l"="34.081,30.427"]
"alrojo/lasagne_residual_network" ["l"="26.981,34.083"]
"yandexdataschool/MLatGradDays" ["l"="24.7,36.07"]
"akashdeepjassal/VREP-RL-bot" ["l"="25.579,36.293"]
"Zamiell/hanabi-live" ["l"="25.468,36.263"]
"Zamiell/hanabi-conventions" ["l"="25.483,36.292"]
"WilsonWangTHU/mbbl-metrpo" ["l"="25.151,35.69"]
"dingyiming0427/goalgail" ["l"="25.084,35.602"]
"snu-mllab/EMI" ["l"="25.089,35.619"]
"deepmind/envlogger" ["l"="25.235,35.643"]
"Xingyu-Lin/softagent" ["l"="25.697,35.723"]
"YunzhuLi/PyFleX" ["l"="25.794,35.702"]
"Xingyu-Lin/VCD" ["l"="25.713,35.733"]
"columbia-ai-robotics/flingbot" ["l"="25.737,35.713"]
"NVlabs/DiSECt" ["l"="25.711,35.713"]
"jidiai/Competition_3v3snakes" ["l"="25.459,36.087"]
"CarlossShi/Competition_3v3snakes" ["l"="25.445,36.072"]
"jidiai/SummerCourse2021" ["l"="25.444,36.083"]
"ignc-research/arena-evaluation" ["l"="25.75,36.198"]
"vita-epfl/causalmotion" ["l"="25.681,36.293"]
"facebookresearch/vip" ["l"="25.582,35.771"]
"siddk/voltron-robotics" ["l"="25.564,35.768"]
"flyyufelix/VizDoom-Keras-RL" ["l"="24.771,35.564"]
"deepmind/dm_memorytasks" ["l"="25.404,35.562"]
"jiangsy/slbo_pytorch" ["l"="25.492,35.707"]
"xionghuichen/MAPLE" ["l"="25.532,35.936"]
"YuriCat/MuZeroJupyterExample" ["l"="25.461,35.643"]
"wulfebw/muzero" ["l"="25.454,35.659"]
"fidel-schaposnik/muzero" ["l"="25.448,35.676"]
"amiranas/minerl_imitation_learning" ["l"="25.244,35.606"]
"takuseno/d4rl-atari" ["l"="25.43,35.732"]
"qbx2/PAAC.pytorch" ["l"="24.77,35.933"]
"guhur/hiveformer" ["l"="25.588,35.799"]
"siddk/voltron-evaluation" ["l"="25.6,35.743"]
"ReinholdM/Offline-Pre-trained-Multi-Agent-Decision-Transformer" ["l"="25.544,35.601"]
"YiqinYang/ICQ" ["l"="25.575,35.572"]
"HumanCompatibleAI/overcooked-hAI-exp" ["l"="25.473,35.981"]
"ruizhaogit/maximum_entropy_population_based_training" ["l"="25.48,35.99"]
"lauramsmith/fine-tuning-locomotion" ["l"="28.258,42.12"]
"TakuyaHiraoka/Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning" ["l"="25.519,35.682"]
"ethanluoyc/magi" ["l"="25.545,35.672"]
"Improbable-AI/rapid-locomotion-rl" ["l"="28.233,42.132"]
"higgsfield/interaction_network_pytorch" ["l"="25.98,35.647"]
"jsikyoon/Interaction-networks_tensorflow" ["l"="26.004,35.657"]
"jaesik817/Interaction-networks_tensorflow" ["l"="26.019,35.64"]
"ToruOwO/InteractionNetwork-pytorch" ["l"="25.99,35.631"]
"YunzhuLi/PropNet" ["l"="25.943,35.658"]
"YunzhuLi/DPI-Net" ["l"="25.891,35.673"]
"MrGemy95/visual-interaction-networks-pytorch" ["l"="26.016,35.623"]
"indylab/nxdo" ["l"="25.618,36.009"]
"diversepsro/diverse_psro" ["l"="25.622,35.998"]
"kmeng01/memit" ["l"="25.817,35.41"]
"eric-mitchell/mend" ["l"="25.879,35.388"]
"krasheninnikov/max-causal-ent-irl" ["l"="24.997,36.084"]
"YunzhuLi/VGPL" ["l"="25.837,35.687"]
"PettingZoo-Team/AutoROM" ["l"="25.468,36.174"]
"PettingZoo-Team/Multi-Agent-ALE" ["l"="25.454,36.178"]
"PettingZoo-Team/hanabi-learning-environment" ["l"="25.456,36.167"]
"zjunlp/Kformer" ["l"="25.903,35.38"]
"ollenilsson19/PGA-MAP-Elites" ["l"="25.592,35.699"]
"senya-ashukha/quantile-regression-dqn-pytorch" ["l"="24.663,35.46"]
"ikostrikov/dmcgym" ["l"="25.426,35.701"]
"facebookresearch/3D-Vision-and-Touch" ["l"="25.788,35.846"]
"pantor/speedfolding" ["l"="25.765,35.698"]
"cyoon1729/distributedRL" ["l"="25.095,36.123"]
"yzqin/dexpoint_sim" ["l"="25.643,35.72"]
"orybkin/lexa-benchmark" ["l"="25.372,35.562"]
"TobiasLee/Awesome-Efficient-PLM" ["l"="26.115,35.228"]
"lancopku/MUKI" ["l"="26.135,35.219"]
"lancopku/well-classified-examples-are-underestimated" ["l"="26.138,35.234"]
"lyf44/pybullet_ompl" ["l"="25.619,35.851"]
"twni2016/f-IRL" ["l"="25.077,36.044"]
"RITCHIEHuang/MAGAIL" ["l"="25.044,36.121"]
"Sohojoe/ppo-dash" ["l"="25.063,35.614"]
"deepmind/torch-totem" ["l"="24.727,35.714"]
"mtrazzi/harlow" ["l"="24.898,35.702"]
"KornbergFresnel/CommNet" ["l"="25.501,36.161"]
"CORE-Robotics-Lab/SSRR" ["l"="25.065,36.269"]
"jaesik817/visual-interaction-networks_tensorflow" ["l"="26.043,35.627"]
"onlytailei/pytorch-rl" ["l"="24.812,36.093"]
"onlytailei/Value-Iteration-Networks-PyTorch" ["l"="24.792,36.104"]
"syzhang092218-source/Confidence-Aware-Imitation-Learning" ["l"="25.048,36.287"]
"jetd1/kuafu" ["l"="25.749,35.75"]
"dragonlong/articulated-pose" ["l"="31.851,42.285"]
"chauncygu/Safe-Reinforcement-Learning-Baseline" ["l"="25.61,36.026"]
"YunzhuLi/VGPL-Dynamics-Prior" ["l"="25.917,35.675"]
"cschenck/SmoothParticleNets" ["l"="25.915,35.655"]
"younggyoseo/trajectory_mcl" ["l"="25.129,35.615"]
"zhlstone/DWA-RL" ["l"="25.678,36.141"]
"wsjeon/multiagent-gail" ["l"="25.022,36.15"]
"psodhi/tactile-in-hand" ["l"="25.798,35.874"]
"RVSATHU/GelSight-Sim2Real" ["l"="25.778,35.868"]
"jimimvp/torch_rl" ["l"="25.2,35.639"]
"carolinahiguera/NCF" ["l"="25.832,35.878"]
"Zce1112zslx/AGED" ["l"="25.995,35.254"]
"Zce1112zslx/KID" ["l"="26.003,35.243"]
"MineDojo/TaskCreationUI" ["l"="25.37,35.639"]
"saizhang0218/TMC" ["l"="25.484,36.141"]
"indylab/tabular_xdo" ["l"="25.648,36.019"]
"ToruOwO/VGPL-Visual-Prior" ["l"="25.86,35.687"]
"comphyreasoning/compositional_physics_learner" ["l"="25.855,35.674"]
"darkcanuck/rumbleserver" ["l"="24.487,36.106"]
}