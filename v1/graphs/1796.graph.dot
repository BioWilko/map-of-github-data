digraph G {
"pharmapsychotic/clip-interrogator-ext" -> "alemelis/sd-webui-ar" ["e"=1]
"google/sg2im" -> "jwyang/graph-rcnn.pytorch"
"google/sg2im" -> "rowanz/neural-motifs"
"google/sg2im" -> "danfeiX/scene-graph-TF-release"
"google/sg2im" -> "taoxugit/AttnGAN" ["e"=1]
"google/sg2im" -> "yikang-li/MSDN"
"google/sg2im" -> "hanzhanggit/StackGAN-Pytorch" ["e"=1]
"google/sg2im" -> "KaihuaTang/Scene-Graph-Benchmark.pytorch"
"google/sg2im" -> "yikang-li/FactorizableNet"
"google/sg2im" -> "jiasenlu/NeuralBabyTalk"
"google/sg2im" -> "lichengunc/MAttNet" ["e"=1]
"google/sg2im" -> "peteanderson80/bottom-up-attention"
"google/sg2im" -> "hanzhanggit/StackGAN-v2" ["e"=1]
"google/sg2im" -> "openai/glow" ["e"=1]
"google/sg2im" -> "hengyuan-hu/bottom-up-attention-vqa"
"google/sg2im" -> "hanzhanggit/StackGAN" ["e"=1]
"hengyuan-hu/bottom-up-attention-vqa" -> "peteanderson80/bottom-up-attention"
"hengyuan-hu/bottom-up-attention-vqa" -> "Cadene/vqa.pytorch"
"hengyuan-hu/bottom-up-attention-vqa" -> "jnhwkim/ban-vqa"
"hengyuan-hu/bottom-up-attention-vqa" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"hengyuan-hu/bottom-up-attention-vqa" -> "MILVLG/mcan-vqa"
"hengyuan-hu/bottom-up-attention-vqa" -> "peteanderson80/Up-Down-Captioner"
"hengyuan-hu/bottom-up-attention-vqa" -> "Cyanogenoid/pytorch-vqa"
"hengyuan-hu/bottom-up-attention-vqa" -> "Cyanogenoid/vqa-counting"
"hengyuan-hu/bottom-up-attention-vqa" -> "GT-Vision-Lab/VQA"
"hengyuan-hu/bottom-up-attention-vqa" -> "markdtw/vqa-winner-cvprw-2017"
"hengyuan-hu/bottom-up-attention-vqa" -> "Cadene/block.bootstrap.pytorch"
"hengyuan-hu/bottom-up-attention-vqa" -> "stanfordnlp/mac-network"
"hengyuan-hu/bottom-up-attention-vqa" -> "ruotianluo/self-critical.pytorch"
"hengyuan-hu/bottom-up-attention-vqa" -> "yuzcccc/vqa-mfb"
"hengyuan-hu/bottom-up-attention-vqa" -> "airsplay/lxmert"
"jnhwkim/ban-vqa" -> "hengyuan-hu/bottom-up-attention-vqa"
"jnhwkim/ban-vqa" -> "MILVLG/mcan-vqa"
"jnhwkim/ban-vqa" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"jnhwkim/ban-vqa" -> "Cadene/vqa.pytorch"
"jnhwkim/ban-vqa" -> "Cyanogenoid/vqa-counting"
"jnhwkim/ban-vqa" -> "yuzcccc/vqa-mfb"
"jnhwkim/ban-vqa" -> "MILVLG/openvqa"
"jnhwkim/ban-vqa" -> "Cadene/block.bootstrap.pytorch"
"jnhwkim/ban-vqa" -> "aimbrain/vqa-project"
"jnhwkim/ban-vqa" -> "Cadene/murel.bootstrap.pytorch"
"jnhwkim/ban-vqa" -> "peteanderson80/bottom-up-attention"
"jnhwkim/ban-vqa" -> "cvlab-tohoku/Dense-CoAttention-Network"
"jnhwkim/ban-vqa" -> "SinghJasdeep/Attention-on-Attention-for-VQA"
"jnhwkim/ban-vqa" -> "stanfordnlp/mac-network"
"jnhwkim/ban-vqa" -> "linjieli222/VQA_ReGAT"
"jwyang/graph-rcnn.pytorch" -> "rowanz/neural-motifs"
"jwyang/graph-rcnn.pytorch" -> "danfeiX/scene-graph-TF-release"
"jwyang/graph-rcnn.pytorch" -> "KaihuaTang/Scene-Graph-Benchmark.pytorch"
"jwyang/graph-rcnn.pytorch" -> "yikang-li/FactorizableNet"
"jwyang/graph-rcnn.pytorch" -> "yikang-li/MSDN"
"jwyang/graph-rcnn.pytorch" -> "yangxuntu/SGAE"
"jwyang/graph-rcnn.pytorch" -> "google/sg2im"
"jwyang/graph-rcnn.pytorch" -> "NVIDIA/ContrastiveLosses4VRD"
"jwyang/graph-rcnn.pytorch" -> "vacancy/SceneGraphParser"
"jwyang/graph-rcnn.pytorch" -> "microsoft/scene_graph_benchmark"
"jwyang/graph-rcnn.pytorch" -> "peteanderson80/bottom-up-attention"
"jwyang/graph-rcnn.pytorch" -> "KaihuaTang/VCTree-Scene-Graph-Generation"
"jwyang/graph-rcnn.pytorch" -> "ranjaykrishna/visual_genome_python_driver"
"jwyang/graph-rcnn.pytorch" -> "ruotianluo/self-critical.pytorch"
"jwyang/graph-rcnn.pytorch" -> "jz462/Large-Scale-VRD.pytorch"
"DeepRNN/image_captioning" -> "yunjey/show-attend-and-tell"
"DeepRNN/image_captioning" -> "jazzsaxmafia/show_attend_and_tell.tensorflow"
"DeepRNN/image_captioning" -> "ruotianluo/ImageCaptioning.pytorch"
"DeepRNN/image_captioning" -> "kelvinxu/arctic-captions"
"DeepRNN/image_captioning" -> "yashk2810/Image-Captioning"
"DeepRNN/image_captioning" -> "mosessoh/CNN-LSTM-Caption-Generator"
"DeepRNN/image_captioning" -> "tylin/coco-caption"
"DeepRNN/image_captioning" -> "anuragmishracse/caption_generator"
"DeepRNN/image_captioning" -> "jcjohnson/densecap"
"DeepRNN/image_captioning" -> "jiasenlu/AdaptiveAttention"
"DeepRNN/image_captioning" -> "zhjohnchan/awesome-image-captioning"
"DeepRNN/image_captioning" -> "ruotianluo/self-critical.pytorch"
"DeepRNN/image_captioning" -> "jiasenlu/NeuralBabyTalk"
"DeepRNN/image_captioning" -> "sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning"
"DeepRNN/image_captioning" -> "peteanderson80/bottom-up-attention"
"kakaobrain/coyo-dataset" -> "rom1504/img2dataset"
"kakaobrain/coyo-dataset" -> "kakaobrain/kogpt" ["e"=1]
"kakaobrain/coyo-dataset" -> "kakaobrain/minDALL-E" ["e"=1]
"kakaobrain/coyo-dataset" -> "facebookresearch/SLIP"
"kakaobrain/coyo-dataset" -> "microsoft/GLIP"
"kakaobrain/coyo-dataset" -> "microsoft/X-Decoder"
"kakaobrain/coyo-dataset" -> "kakaobrain/rq-vae-transformer" ["e"=1]
"kakaobrain/coyo-dataset" -> "lucidrains/flamingo-pytorch"
"kakaobrain/coyo-dataset" -> "rom1504/clip-retrieval"
"kakaobrain/coyo-dataset" -> "jungwoo-ha/WeeklyArxivTalk" ["e"=1]
"kakaobrain/coyo-dataset" -> "tunib-ai/large-scale-lm-tutorials" ["e"=1]
"kakaobrain/coyo-dataset" -> "mlfoundations/open_clip"
"kakaobrain/coyo-dataset" -> "kakaobrain/karlo" ["e"=1]
"kakaobrain/coyo-dataset" -> "KLUE-benchmark/KLUE" ["e"=1]
"kakaobrain/coyo-dataset" -> "salesforce/BLIP"
"mlfoundations/open_clip" -> "openai/CLIP" ["e"=1]
"mlfoundations/open_clip" -> "salesforce/LAVIS"
"mlfoundations/open_clip" -> "rom1504/img2dataset"
"mlfoundations/open_clip" -> "salesforce/BLIP"
"mlfoundations/open_clip" -> "rom1504/clip-retrieval"
"mlfoundations/open_clip" -> "OFA-Sys/OFA"
"mlfoundations/open_clip" -> "mlfoundations/open_flamingo" ["e"=1]
"mlfoundations/open_clip" -> "openai/guided-diffusion" ["e"=1]
"mlfoundations/open_clip" -> "lucidrains/DALLE2-pytorch" ["e"=1]
"mlfoundations/open_clip" -> "huggingface/diffusers" ["e"=1]
"mlfoundations/open_clip" -> "facebookresearch/mae" ["e"=1]
"mlfoundations/open_clip" -> "baaivision/EVA"
"mlfoundations/open_clip" -> "CompVis/latent-diffusion" ["e"=1]
"mlfoundations/open_clip" -> "CompVis/taming-transformers" ["e"=1]
"mlfoundations/open_clip" -> "microsoft/GLIP"
"facebookresearch/Detic" -> "microsoft/GLIP"
"facebookresearch/Detic" -> "ashkamath/mdetr"
"facebookresearch/Detic" -> "facebookresearch/Mask2Former" ["e"=1]
"facebookresearch/Detic" -> "facebookresearch/SLIP"
"facebookresearch/Detic" -> "microsoft/X-Decoder"
"facebookresearch/Detic" -> "facebookresearch/ConvNeXt" ["e"=1]
"facebookresearch/Detic" -> "NVlabs/GroupViT"
"facebookresearch/Detic" -> "xingyizhou/UniDet" ["e"=1]
"facebookresearch/Detic" -> "xingyizhou/CenterNet2" ["e"=1]
"facebookresearch/Detic" -> "fundamentalvision/Deformable-DETR" ["e"=1]
"facebookresearch/Detic" -> "isl-org/lang-seg"
"facebookresearch/Detic" -> "ShoufaChen/DiffusionDet"
"facebookresearch/Detic" -> "IDEACVR/DINO"
"facebookresearch/Detic" -> "baaivision/EVA"
"facebookresearch/Detic" -> "KaiyangZhou/CoOp"
"ellisk42/ec" -> "vacancy/NSCL-PyTorch-Release"
"ellisk42/ec" -> "probcomp/Gen.jl" ["e"=1]
"ellisk42/ec" -> "RichardEvans/apperception"
"ellisk42/ec" -> "top-quarks/ARC-solution"
"ellisk42/ec" -> "piantado/LOTlib3"
"ellisk42/ec" -> "fchollet/ARC" ["e"=1]
"ellisk42/ec" -> "samacqua/LARC"
"ellisk42/ec" -> "YeWR/EfficientZero" ["e"=1]
"ellisk42/ec" -> "kexinyi/ns-vqa"
"ellisk42/ec" -> "tech-srl/RASP" ["e"=1]
"facebookresearch/mmf" -> "microsoft/Oscar"
"facebookresearch/mmf" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"facebookresearch/mmf" -> "facebookresearch/vilbert-multi-task"
"facebookresearch/mmf" -> "pliang279/awesome-multimodal-ml"
"facebookresearch/mmf" -> "airsplay/lxmert"
"facebookresearch/mmf" -> "ChenRocks/UNITER"
"facebookresearch/mmf" -> "peteanderson80/bottom-up-attention"
"facebookresearch/mmf" -> "dandelin/ViLT"
"facebookresearch/mmf" -> "jackroos/VL-BERT"
"facebookresearch/mmf" -> "facebookresearch/grid-feats-vqa"
"facebookresearch/mmf" -> "facebookresearch/multimodal"
"facebookresearch/mmf" -> "OFA-Sys/OFA"
"facebookresearch/mmf" -> "uclanlp/visualbert"
"facebookresearch/mmf" -> "jokieleung/awesome-visual-question-answering"
"facebookresearch/mmf" -> "salesforce/LAVIS"
"IDEA-Research/DINO" -> "IDEA-Research/MaskDINO"
"IDEA-Research/DINO" -> "IDEA-Research/detrex"
"IDEA-Research/DINO" -> "IDEA-Research/DN-DETR"
"IDEA-Research/DINO" -> "IDEA-Research/GroundingDINO" ["e"=1]
"IDEA-Research/DINO" -> "IDEA-Research/DAB-DETR"
"IDEA-Research/DINO" -> "OpenGVLab/InternImage"
"IDEA-Research/DINO" -> "IDEA-Research/awesome-detection-transformer"
"IDEA-Research/DINO" -> "baaivision/EVA"
"IDEA-Research/DINO" -> "fundamentalvision/Deformable-DETR" ["e"=1]
"IDEA-Research/DINO" -> "jozhang97/DETA"
"IDEA-Research/DINO" -> "ShoufaChen/DiffusionDet"
"IDEA-Research/DINO" -> "microsoft/FocalNet"
"IDEA-Research/DINO" -> "ZhangGongjie/SAM-DETR" ["e"=1]
"IDEA-Research/DINO" -> "open-mmlab/playground" ["e"=1]
"IDEA-Research/DINO" -> "czczup/ViT-Adapter"
"Computer-Vision-in-the-Wild/CVinW_Readings" -> "Computer-Vision-in-the-Wild/Elevater_Toolkit_IC"
"Computer-Vision-in-the-Wild/CVinW_Readings" -> "microsoft/RegionCLIP"
"Computer-Vision-in-the-Wild/CVinW_Readings" -> "ZhangYuanhan-AI/OmniBenchmark" ["e"=1]
"Computer-Vision-in-the-Wild/CVinW_Readings" -> "clin1223/VLDet"
"Computer-Vision-in-the-Wild/CVinW_Readings" -> "rshaojimmy/MultiModal-DeepFake" ["e"=1]
"Computer-Vision-in-the-Wild/CVinW_Readings" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision"
"Computer-Vision-in-the-Wild/CVinW_Readings" -> "muzairkhattak/multimodal-prompt-learning"
"Computer-Vision-in-the-Wild/CVinW_Readings" -> "microsoft/GLIP"
"OFA-Sys/OFA" -> "salesforce/BLIP"
"OFA-Sys/OFA" -> "salesforce/LAVIS"
"OFA-Sys/OFA" -> "microsoft/GLIP"
"OFA-Sys/OFA" -> "mlfoundations/open_clip"
"OFA-Sys/OFA" -> "salesforce/ALBEF"
"OFA-Sys/OFA" -> "microsoft/Oscar"
"OFA-Sys/OFA" -> "KaiyangZhou/CoOp"
"OFA-Sys/OFA" -> "lucidrains/flamingo-pytorch"
"OFA-Sys/OFA" -> "alibaba/AliceMind" ["e"=1]
"OFA-Sys/OFA" -> "dandelin/ViLT"
"OFA-Sys/OFA" -> "ashkamath/mdetr"
"OFA-Sys/OFA" -> "OFA-Sys/Chinese-CLIP"
"OFA-Sys/OFA" -> "rmokady/CLIP_prefix_caption"
"OFA-Sys/OFA" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"OFA-Sys/OFA" -> "zengyan-97/X-VLM"
"facebookresearch/clevr-dataset-gen" -> "facebookresearch/clevr-iep"
"facebookresearch/clevr-dataset-gen" -> "kexinyi/ns-vqa"
"facebookresearch/clevr-dataset-gen" -> "stanfordnlp/mac-network"
"facebookresearch/clevr-dataset-gen" -> "vacancy/NSCL-PyTorch-Release"
"facebookresearch/clevr-dataset-gen" -> "deepmind/multi_object_datasets"
"facebookresearch/clevr-dataset-gen" -> "ronghanghu/n2nmn"
"facebookresearch/clevr-dataset-gen" -> "lucidrains/slot-attention"
"facebookresearch/clevr-dataset-gen" -> "deepmind/dsprites-dataset" ["e"=1]
"facebookresearch/clevr-dataset-gen" -> "hengyuan-hu/bottom-up-attention-vqa"
"facebookresearch/clevr-dataset-gen" -> "google-research/clevr_robot_env"
"facebookresearch/clevr-dataset-gen" -> "davidmascharka/tbd-nets"
"facebookresearch/clevr-dataset-gen" -> "facebookresearch/phyre"
"facebookresearch/clevr-dataset-gen" -> "rowanz/neural-motifs"
"facebookresearch/clevr-dataset-gen" -> "chuangg/CLEVRER"
"facebookresearch/clevr-dataset-gen" -> "HarshTrivedi/nmn-pytorch"
"yunjey/show-attend-and-tell" -> "jazzsaxmafia/show_attend_and_tell.tensorflow"
"yunjey/show-attend-and-tell" -> "DeepRNN/image_captioning"
"yunjey/show-attend-and-tell" -> "kelvinxu/arctic-captions"
"yunjey/show-attend-and-tell" -> "tylin/coco-caption"
"yunjey/show-attend-and-tell" -> "jiasenlu/AdaptiveAttention"
"yunjey/show-attend-and-tell" -> "ruotianluo/self-critical.pytorch"
"yunjey/show-attend-and-tell" -> "ruotianluo/ImageCaptioning.pytorch"
"yunjey/show-attend-and-tell" -> "peteanderson80/bottom-up-attention"
"yunjey/show-attend-and-tell" -> "jiasenlu/NeuralBabyTalk"
"yunjey/show-attend-and-tell" -> "zhjohnchan/awesome-image-captioning"
"yunjey/show-attend-and-tell" -> "sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning"
"yunjey/show-attend-and-tell" -> "jazzsaxmafia/show_and_tell.tensorflow"
"yunjey/show-attend-and-tell" -> "tsenghungchen/show-adapt-and-tell"
"yunjey/show-attend-and-tell" -> "mosessoh/CNN-LSTM-Caption-Generator"
"yunjey/show-attend-and-tell" -> "anuragmishracse/caption_generator"
"ruotianluo/ImageCaptioning.pytorch" -> "ruotianluo/self-critical.pytorch"
"ruotianluo/ImageCaptioning.pytorch" -> "zhjohnchan/awesome-image-captioning"
"ruotianluo/ImageCaptioning.pytorch" -> "sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning"
"ruotianluo/ImageCaptioning.pytorch" -> "peteanderson80/bottom-up-attention"
"ruotianluo/ImageCaptioning.pytorch" -> "tylin/coco-caption"
"ruotianluo/ImageCaptioning.pytorch" -> "husthuaan/AoANet"
"ruotianluo/ImageCaptioning.pytorch" -> "jiasenlu/NeuralBabyTalk"
"ruotianluo/ImageCaptioning.pytorch" -> "aimagelab/meshed-memory-transformer"
"ruotianluo/ImageCaptioning.pytorch" -> "DeepRNN/image_captioning"
"ruotianluo/ImageCaptioning.pytorch" -> "yangxuntu/SGAE"
"ruotianluo/ImageCaptioning.pytorch" -> "JDAI-CV/image-captioning"
"ruotianluo/ImageCaptioning.pytorch" -> "fengyang0317/unsupervised_captioning"
"ruotianluo/ImageCaptioning.pytorch" -> "yahoo/object_relation_transformer"
"ruotianluo/ImageCaptioning.pytorch" -> "jiasenlu/AdaptiveAttention"
"ruotianluo/ImageCaptioning.pytorch" -> "peteanderson80/Up-Down-Captioner"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "ruotianluo/ImageCaptioning.pytorch"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "zhjohnchan/awesome-image-captioning"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "ruotianluo/self-critical.pytorch"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "sgrvinod/Deep-Tutorials-for-PyTorch"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "peteanderson80/bottom-up-attention"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "tylin/coco-caption"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "kelvinxu/arctic-captions"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "aimagelab/meshed-memory-transformer"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "husthuaan/AoANet"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "DeepRNN/image_captioning"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "microsoft/Oscar"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "jiasenlu/NeuralBabyTalk"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "yunjey/show-attend-and-tell"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "krasserm/fairseq-image-captioning"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "sgrvinod/a-PyTorch-Tutorial-to-Object-Detection" ["e"=1]
"yahoo/object_relation_transformer" -> "husthuaan/AoANet"
"yahoo/object_relation_transformer" -> "aimagelab/meshed-memory-transformer"
"yahoo/object_relation_transformer" -> "yangxuntu/SGAE"
"yahoo/object_relation_transformer" -> "YiwuZhong/Sub-GC"
"yahoo/object_relation_transformer" -> "zhangxuying1004/RSTNet"
"yahoo/object_relation_transformer" -> "JDAI-CV/image-captioning"
"yahoo/object_relation_transformer" -> "cshizhe/asg2cap"
"yahoo/object_relation_transformer" -> "luo3300612/image-captioning-DLCT"
"yahoo/object_relation_transformer" -> "krasserm/fairseq-image-captioning"
"yahoo/object_relation_transformer" -> "husthuaan/AAT"
"yahoo/object_relation_transformer" -> "aimagelab/show-control-and-tell"
"yahoo/object_relation_transformer" -> "fenglinliu98/MIA"
"yahoo/object_relation_transformer" -> "poojahira/image-captioning-bottom-up-top-down"
"yahoo/object_relation_transformer" -> "gujiuxiang/Stack-Captioning"
"yahoo/object_relation_transformer" -> "fawazsammani/show-edit-tell"
"zhjohnchan/awesome-image-captioning" -> "ruotianluo/self-critical.pytorch"
"zhjohnchan/awesome-image-captioning" -> "ruotianluo/ImageCaptioning.pytorch"
"zhjohnchan/awesome-image-captioning" -> "forence/Awesome-Visual-Captioning"
"zhjohnchan/awesome-image-captioning" -> "husthuaan/AoANet"
"zhjohnchan/awesome-image-captioning" -> "sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning"
"zhjohnchan/awesome-image-captioning" -> "peteanderson80/bottom-up-attention"
"zhjohnchan/awesome-image-captioning" -> "fengyang0317/unsupervised_captioning"
"zhjohnchan/awesome-image-captioning" -> "jiasenlu/NeuralBabyTalk"
"zhjohnchan/awesome-image-captioning" -> "aimagelab/meshed-memory-transformer"
"zhjohnchan/awesome-image-captioning" -> "JDAI-CV/image-captioning"
"zhjohnchan/awesome-image-captioning" -> "tylin/coco-caption"
"zhjohnchan/awesome-image-captioning" -> "yangxuntu/SGAE"
"zhjohnchan/awesome-image-captioning" -> "aimagelab/show-control-and-tell"
"zhjohnchan/awesome-image-captioning" -> "yahoo/object_relation_transformer"
"zhjohnchan/awesome-image-captioning" -> "peteanderson80/Up-Down-Captioner"
"facebookresearch/ConvNeXt-V2" -> "facebookresearch/ConvNeXt" ["e"=1]
"facebookresearch/ConvNeXt-V2" -> "baaivision/EVA"
"facebookresearch/ConvNeXt-V2" -> "DingXiaoH/RepLKNet-pytorch"
"facebookresearch/ConvNeXt-V2" -> "snap-research/EfficientFormer" ["e"=1]
"facebookresearch/ConvNeXt-V2" -> "facebookresearch/DiT" ["e"=1]
"facebookresearch/ConvNeXt-V2" -> "ShoufaChen/DiffusionDet"
"facebookresearch/ConvNeXt-V2" -> "sail-sg/metaformer"
"facebookresearch/ConvNeXt-V2" -> "Visual-Attention-Network/SegNeXt"
"facebookresearch/ConvNeXt-V2" -> "keyu-tian/SparK" ["e"=1]
"facebookresearch/ConvNeXt-V2" -> "facebookresearch/dropout"
"facebookresearch/ConvNeXt-V2" -> "microsoft/SimMIM" ["e"=1]
"facebookresearch/ConvNeXt-V2" -> "raoyongming/HorNet"
"facebookresearch/ConvNeXt-V2" -> "microsoft/X-Decoder"
"facebookresearch/ConvNeXt-V2" -> "OpenGVLab/InternImage"
"facebookresearch/ConvNeXt-V2" -> "facebookresearch/ToMe"
"facebookresearch/ToMe" -> "dbolya/tomesd" ["e"=1]
"facebookresearch/ToMe" -> "facebookresearch/DiT" ["e"=1]
"facebookresearch/ToMe" -> "facebookresearch/CutLER"
"facebookresearch/ToMe" -> "baaivision/EVA"
"facebookresearch/ToMe" -> "raoyongming/DynamicViT" ["e"=1]
"facebookresearch/ToMe" -> "facebookresearch/ConvNeXt-V2"
"facebookresearch/ToMe" -> "ziplab/SN-Net"
"facebookresearch/ToMe" -> "ShoufaChen/DiffusionDet"
"facebookresearch/ToMe" -> "ma-xu/Context-Cluster"
"facebookresearch/ToMe" -> "google-research/big_vision"
"facebookresearch/ToMe" -> "xxxnell/how-do-vits-work" ["e"=1]
"facebookresearch/ToMe" -> "NVlabs/GCVit"
"facebookresearch/ToMe" -> "microsoft/GLIP"
"facebookresearch/ToMe" -> "facebookresearch/msn" ["e"=1]
"facebookresearch/ToMe" -> "snap-research/EfficientFormer" ["e"=1]
"uta-smile/TCL" -> "zengyan-97/X-VLM"
"uta-smile/TCL" -> "salesforce/ALBEF"
"uta-smile/TCL" -> "Cuberick-Orion/CIRR" ["e"=1]
"uta-smile/TCL" -> "researchmm/soho"
"uta-smile/TCL" -> "ioanacroi/qb-norm" ["e"=1]
"uta-smile/TCL" -> "zdou0830/METER"
"uta-smile/TCL" -> "LgQu/DIME" ["e"=1]
"IDEA-Research/detrex" -> "IDEA-Research/awesome-detection-transformer"
"IDEA-Research/detrex" -> "IDEACVR/DINO"
"IDEA-Research/detrex" -> "IDEA-Research/DINO"
"IDEA-Research/detrex" -> "IDEACVR/awesome-detection-transformer"
"IDEA-Research/detrex" -> "ShoufaChen/DiffusionDet"
"IDEA-Research/detrex" -> "fundamentalvision/Deformable-DETR" ["e"=1]
"IDEA-Research/detrex" -> "open-mmlab/mmyolo" ["e"=1]
"IDEA-Research/detrex" -> "OpenGVLab/InternImage"
"IDEA-Research/detrex" -> "jozhang97/DETA"
"IDEA-Research/detrex" -> "baaivision/EVA"
"IDEA-Research/detrex" -> "alibaba/EasyCV" ["e"=1]
"IDEA-Research/detrex" -> "IDEA-opensource/DN-DETR"
"IDEA-Research/detrex" -> "IDEA-Research/GroundingDINO" ["e"=1]
"IDEA-Research/detrex" -> "Atten4Vis/ConditionalDETR"
"IDEA-Research/detrex" -> "microsoft/GLIP"
"kelvinxu/arctic-captions" -> "jazzsaxmafia/show_attend_and_tell.tensorflow"
"kelvinxu/arctic-captions" -> "tylin/coco-caption"
"kelvinxu/arctic-captions" -> "yunjey/show-attend-and-tell"
"kelvinxu/arctic-captions" -> "DeepRNN/image_captioning"
"kelvinxu/arctic-captions" -> "jcjohnson/densecap"
"kelvinxu/arctic-captions" -> "ryankiros/visual-semantic-embedding" ["e"=1]
"kelvinxu/arctic-captions" -> "ruotianluo/self-critical.pytorch"
"kelvinxu/arctic-captions" -> "jiasenlu/NeuralBabyTalk"
"kelvinxu/arctic-captions" -> "jiasenlu/AdaptiveAttention"
"kelvinxu/arctic-captions" -> "Element-Research/rnn" ["e"=1]
"kelvinxu/arctic-captions" -> "peteanderson80/bottom-up-attention"
"kelvinxu/arctic-captions" -> "sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning"
"kelvinxu/arctic-captions" -> "karpathy/neuraltalk2" ["e"=1]
"kelvinxu/arctic-captions" -> "jazzsaxmafia/show_and_tell.tensorflow"
"kelvinxu/arctic-captions" -> "ruotianluo/ImageCaptioning.pytorch"
"salesforce/BLIP" -> "salesforce/LAVIS"
"salesforce/BLIP" -> "salesforce/ALBEF"
"salesforce/BLIP" -> "OFA-Sys/OFA"
"salesforce/BLIP" -> "mlfoundations/open_clip"
"salesforce/BLIP" -> "microsoft/GLIP"
"salesforce/BLIP" -> "openai/CLIP" ["e"=1]
"salesforce/BLIP" -> "dandelin/ViLT"
"salesforce/BLIP" -> "rom1504/img2dataset"
"salesforce/BLIP" -> "KaiyangZhou/CoOp"
"salesforce/BLIP" -> "microsoft/Oscar"
"salesforce/BLIP" -> "lucidrains/flamingo-pytorch"
"salesforce/BLIP" -> "rom1504/clip-retrieval"
"salesforce/BLIP" -> "pharmapsychotic/clip-interrogator"
"salesforce/BLIP" -> "OFA-Sys/Chinese-CLIP"
"salesforce/BLIP" -> "microsoft/unilm" ["e"=1]
"salesforce/LAVIS" -> "salesforce/BLIP"
"salesforce/LAVIS" -> "mlfoundations/open_clip"
"salesforce/LAVIS" -> "OFA-Sys/OFA"
"salesforce/LAVIS" -> "mlfoundations/open_flamingo" ["e"=1]
"salesforce/LAVIS" -> "microsoft/GLIP"
"salesforce/LAVIS" -> "microsoft/unilm" ["e"=1]
"salesforce/LAVIS" -> "salesforce/ALBEF"
"salesforce/LAVIS" -> "openai/CLIP" ["e"=1]
"salesforce/LAVIS" -> "huggingface/peft" ["e"=1]
"salesforce/LAVIS" -> "amazon-science/mm-cot" ["e"=1]
"salesforce/LAVIS" -> "rom1504/img2dataset"
"salesforce/LAVIS" -> "huggingface/diffusers" ["e"=1]
"salesforce/LAVIS" -> "baaivision/EVA"
"salesforce/LAVIS" -> "ZrrSkywalker/LLaMA-Adapter" ["e"=1]
"salesforce/LAVIS" -> "IDEA-Research/Grounded-Segment-Anything" ["e"=1]
"Cloud-CV/Origami" -> "Cloud-CV/CloudCV"
"Cloud-CV/Origami" -> "Cloud-CV/visual-chatbot"
"dabasajay/Image-Caption-Generator" -> "damminhtien/deep-learning-image-caption-generator"
"dabasajay/Image-Caption-Generator" -> "Shobhit20/Image-Captioning"
"dabasajay/Image-Caption-Generator" -> "anuragmishracse/caption_generator"
"dabasajay/Image-Caption-Generator" -> "neural-nuts/image-caption-generator"
"dabasajay/Image-Caption-Generator" -> "yashk2810/Image-Captioning"
"JamesChuanggg/awesome-vqa" -> "jiasenlu/HieCoAttenVQA"
"JamesChuanggg/awesome-vqa" -> "Cadene/vqa.pytorch"
"JamesChuanggg/awesome-vqa" -> "JamesChuanggg/VQA-tensorflow"
"JamesChuanggg/awesome-vqa" -> "akirafukui/vqa-mcb"
"JamesChuanggg/awesome-vqa" -> "hengyuan-hu/bottom-up-attention-vqa"
"JamesChuanggg/awesome-vqa" -> "iamaaditya/VQA_Demo"
"JamesChuanggg/awesome-vqa" -> "metalbubble/VQAbaseline"
"JamesChuanggg/awesome-vqa" -> "Cyanogenoid/vqa-counting"
"JamesChuanggg/awesome-vqa" -> "markdtw/vqa-winner-cvprw-2017"
"JamesChuanggg/awesome-vqa" -> "zcyang/imageqa-san"
"JamesChuanggg/awesome-vqa" -> "VT-vision-lab/VQA_LSTM_CNN"
"JamesChuanggg/awesome-vqa" -> "paarthneekhara/neural-vqa-tensorflow"
"JamesChuanggg/awesome-vqa" -> "abhshkdz/neural-vqa-attention"
"JamesChuanggg/awesome-vqa" -> "GT-Vision-Lab/VQA"
"JamesChuanggg/awesome-vqa" -> "ronghanghu/n2nmn"
"IDEACVR/MaskDINO" -> "IDEACVR/DINO"
"IDEACVR/MaskDINO" -> "IDEA-opensource/DN-DETR"
"IDEACVR/MaskDINO" -> "IDEACVR/awesome-detection-transformer"
"IDEACVR/MaskDINO" -> "IDEA-opensource/DAB-DETR"
"IDEACVR/MaskDINO" -> "facebookresearch/Mask2Former" ["e"=1]
"IDEACVR/MaskDINO" -> "HDETR/H-Deformable-DETR"
"IDEACVR/MaskDINO" -> "Picsart-AI-Research/SeMask-Segmentation" ["e"=1]
"IDEACVR/MaskDINO" -> "czczup/ViT-Adapter"
"IDEACVR/MaskDINO" -> "zhiqi-li/Panoptic-SegFormer" ["e"=1]
"IDEACVR/MaskDINO" -> "jshilong/DDQ"
"IDEACVR/MaskDINO" -> "ZwwWayne/K-Net" ["e"=1]
"IDEACVR/MaskDINO" -> "wjf5203/SeqFormer" ["e"=1]
"IDEACVR/MaskDINO" -> "hustvl/MIMDet" ["e"=1]
"IDEACVR/MaskDINO" -> "IDEA-Research/detrex"
"IDEACVR/MaskDINO" -> "implus/UM-MAE" ["e"=1]
"jiasenlu/NeuralBabyTalk" -> "ruotianluo/self-critical.pytorch"
"jiasenlu/NeuralBabyTalk" -> "peteanderson80/Up-Down-Captioner"
"jiasenlu/NeuralBabyTalk" -> "zhjohnchan/awesome-image-captioning"
"jiasenlu/NeuralBabyTalk" -> "jiasenlu/AdaptiveAttention"
"jiasenlu/NeuralBabyTalk" -> "ruotianluo/ImageCaptioning.pytorch"
"jiasenlu/NeuralBabyTalk" -> "aimagelab/show-control-and-tell"
"jiasenlu/NeuralBabyTalk" -> "fengyang0317/unsupervised_captioning"
"jiasenlu/NeuralBabyTalk" -> "aditya12agd5/convcap"
"jiasenlu/NeuralBabyTalk" -> "husthuaan/AoANet"
"jiasenlu/NeuralBabyTalk" -> "peteanderson80/bottom-up-attention"
"jiasenlu/NeuralBabyTalk" -> "aimagelab/meshed-memory-transformer"
"jiasenlu/NeuralBabyTalk" -> "yangxuntu/SGAE"
"jiasenlu/NeuralBabyTalk" -> "ruotianluo/Transformer_Captioning"
"jiasenlu/NeuralBabyTalk" -> "gujiuxiang/Stack-Captioning"
"jiasenlu/NeuralBabyTalk" -> "tylin/coco-caption"
"Cadene/vqa.pytorch" -> "hengyuan-hu/bottom-up-attention-vqa"
"Cadene/vqa.pytorch" -> "Cadene/block.bootstrap.pytorch"
"Cadene/vqa.pytorch" -> "Cyanogenoid/pytorch-vqa"
"Cadene/vqa.pytorch" -> "jnhwkim/ban-vqa"
"Cadene/vqa.pytorch" -> "JamesChuanggg/awesome-vqa"
"Cadene/vqa.pytorch" -> "peteanderson80/bottom-up-attention"
"Cadene/vqa.pytorch" -> "GT-Vision-Lab/VQA"
"Cadene/vqa.pytorch" -> "markdtw/vqa-winner-cvprw-2017"
"Cadene/vqa.pytorch" -> "jiasenlu/HieCoAttenVQA"
"Cadene/vqa.pytorch" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"Cadene/vqa.pytorch" -> "Cyanogenoid/vqa-counting"
"Cadene/vqa.pytorch" -> "MILVLG/mcan-vqa"
"Cadene/vqa.pytorch" -> "akirafukui/vqa-mcb"
"Cadene/vqa.pytorch" -> "jokieleung/awesome-visual-question-answering"
"Cadene/vqa.pytorch" -> "yuzcccc/vqa-mfb"
"JosephKJ/OWOD" -> "akshitac8/OW-DETR"
"JosephKJ/OWOD" -> "megvii-model/YOLOF" ["e"=1]
"JosephKJ/OWOD" -> "mcahny/object_localization_network"
"JosephKJ/OWOD" -> "mmaaz60/mvits_for_class_agnostic_od"
"JosephKJ/OWOD" -> "ucbdrive/few-shot-object-detection" ["e"=1]
"JosephKJ/OWOD" -> "xingyizhou/CenterNet2" ["e"=1]
"JosephKJ/OWOD" -> "implus/GFocalV2" ["e"=1]
"JosephKJ/OWOD" -> "PeizeSun/SparseR-CNN" ["e"=1]
"JosephKJ/OWOD" -> "xieenze/DetCo"
"JosephKJ/OWOD" -> "fundamentalvision/Deformable-DETR" ["e"=1]
"JosephKJ/OWOD" -> "amirbar/DETReg" ["e"=1]
"JosephKJ/OWOD" -> "dddzg/up-detr"
"JosephKJ/OWOD" -> "PeizeSun/OneNet" ["e"=1]
"JosephKJ/OWOD" -> "facebookresearch/unbiased-teacher" ["e"=1]
"JosephKJ/OWOD" -> "iCGY96/awesome_OpenSetRecognition_list" ["e"=1]
"KaiyangZhou/CoOp" -> "KMnP/vpt"
"KaiyangZhou/CoOp" -> "gaopengcuhk/Tip-Adapter"
"KaiyangZhou/CoOp" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision"
"KaiyangZhou/CoOp" -> "raoyongming/DenseCLIP"
"KaiyangZhou/CoOp" -> "gaopengcuhk/CLIP-Adapter"
"KaiyangZhou/CoOp" -> "KaiyangZhou/Dassl.pytorch" ["e"=1]
"KaiyangZhou/CoOp" -> "muzairkhattak/multimodal-prompt-learning"
"KaiyangZhou/CoOp" -> "salesforce/ALBEF"
"KaiyangZhou/CoOp" -> "microsoft/GLIP"
"KaiyangZhou/CoOp" -> "sallymmx/ActionCLIP" ["e"=1]
"KaiyangZhou/CoOp" -> "facebookresearch/SLIP"
"KaiyangZhou/CoOp" -> "OFA-Sys/OFA"
"KaiyangZhou/CoOp" -> "Sense-GVT/DeCLIP"
"KaiyangZhou/CoOp" -> "isl-org/lang-seg"
"KaiyangZhou/CoOp" -> "dandelin/ViLT"
"alisure-ml/awesome-visual-relationship-detection" -> "GriffinLiang/vrd-dsr"
"alisure-ml/awesome-visual-relationship-detection" -> "Prof-Lu-Cewu/Visual-Relationship-Detection"
"alisure-ml/awesome-visual-relationship-detection" -> "rowanz/neural-motifs"
"alisure-ml/awesome-visual-relationship-detection" -> "jz462/Large-Scale-VRD.pytorch"
"alisure-ml/awesome-visual-relationship-detection" -> "NVIDIA/ContrastiveLosses4VRD"
"alisure-ml/awesome-visual-relationship-detection" -> "yangxuntu/vrd"
"alisure-ml/awesome-visual-relationship-detection" -> "yikang-li/MSDN"
"alisure-ml/awesome-visual-relationship-detection" -> "facebookresearch/Large-Scale-VRD"
"alisure-ml/awesome-visual-relationship-detection" -> "yikang-li/FactorizableNet"
"alisure-ml/awesome-visual-relationship-detection" -> "yuweihao/KERN"
"alisure-ml/awesome-visual-relationship-detection" -> "zawlin/cvpr17_vtranse"
"alisure-ml/awesome-visual-relationship-detection" -> "KaihuaTang/VCTree-Scene-Graph-Generation"
"alisure-ml/awesome-visual-relationship-detection" -> "nexusapoorvacus/DeepVariationStructuredRL"
"ezeli/BUTD_model" -> "ezeli/bottom_up_features_extract"
"microsoft/Oscar" -> "pzzhang/VinVL"
"microsoft/Oscar" -> "ChenRocks/UNITER"
"microsoft/Oscar" -> "peteanderson80/bottom-up-attention"
"microsoft/Oscar" -> "facebookresearch/vilbert-multi-task"
"microsoft/Oscar" -> "airsplay/lxmert"
"microsoft/Oscar" -> "LuoweiZhou/VLP"
"microsoft/Oscar" -> "microsoft/scene_graph_benchmark"
"microsoft/Oscar" -> "salesforce/ALBEF"
"microsoft/Oscar" -> "facebookresearch/grid-feats-vqa"
"microsoft/Oscar" -> "dandelin/ViLT"
"microsoft/Oscar" -> "jackroos/VL-BERT"
"microsoft/Oscar" -> "ruotianluo/self-critical.pytorch"
"microsoft/Oscar" -> "aimagelab/meshed-memory-transformer"
"microsoft/Oscar" -> "jayleicn/ClipBERT"
"microsoft/Oscar" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"matsui528/faiss_tips" -> "facebookresearch/distributed-faiss"
"matsui528/faiss_tips" -> "criteo/autofaiss"
"matsui528/faiss_tips" -> "UKPLab/beir" ["e"=1]
"matsui528/faiss_tips" -> "matsui528/nanopq" ["e"=1]
"matsui528/faiss_tips" -> "matsui528/annbench" ["e"=1]
"doc-doc/vRGV" -> "xdshang/VidVRD-helper"
"Alan-Lee123/relation-network" -> "kimhc6028/relational-networks"
"kimhc6028/relational-networks" -> "facebookresearch/clevr-iep"
"kimhc6028/relational-networks" -> "Alan-Lee123/relation-network"
"kimhc6028/relational-networks" -> "gitlimlab/Relation-Network-Tensorflow"
"kimhc6028/relational-networks" -> "Cadene/vqa.pytorch"
"kimhc6028/relational-networks" -> "bioinf-jku/SNNs" ["e"=1]
"kimhc6028/relational-networks" -> "ikostrikov/pytorch-a2c-ppo-acktr" ["e"=1]
"kimhc6028/relational-networks" -> "mesnico/RelationNetworks-CLEVR"
"kimhc6028/relational-networks" -> "szagoruyko/diracnets" ["e"=1]
"kimhc6028/relational-networks" -> "eladhoffer/seq2seq.pytorch" ["e"=1]
"kimhc6028/relational-networks" -> "Kaixhin/Rainbow" ["e"=1]
"kimhc6028/relational-networks" -> "ethanfetaya/NRI" ["e"=1]
"kimhc6028/relational-networks" -> "jacobandreas/nmn2"
"kimhc6028/relational-networks" -> "hengyuan-hu/bottom-up-attention-vqa"
"kimhc6028/relational-networks" -> "facebookresearch/poincare-embeddings" ["e"=1]
"kimhc6028/relational-networks" -> "fyu/drn" ["e"=1]
"KMnP/vpt" -> "hjbahng/visual_prompting"
"KMnP/vpt" -> "KaiyangZhou/CoOp"
"KMnP/vpt" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision"
"KMnP/vpt" -> "muzairkhattak/multimodal-prompt-learning"
"KMnP/vpt" -> "gaopengcuhk/Tip-Adapter"
"KMnP/vpt" -> "amirbar/visual_prompting"
"KMnP/vpt" -> "ZhangYuanhan-AI/NOAH"
"KMnP/vpt" -> "JieShibo/PETL-ViT"
"KMnP/vpt" -> "dongzelian/SSF"
"KMnP/vpt" -> "sagizty/VPT"
"KMnP/vpt" -> "ShoufaChen/AdaptFormer"
"KMnP/vpt" -> "gaopengcuhk/CLIP-Adapter"
"KMnP/vpt" -> "microsoft/GLIP"
"KMnP/vpt" -> "raoyongming/DenseCLIP"
"KMnP/vpt" -> "yuhangzang/UPT"
"sagizty/Multi-Stage-Hybrid-Transformer" -> "sagizty/MAE"
"sagizty/Multi-Stage-Hybrid-Transformer" -> "sagizty/NTUS_application"
"sagizty/Multi-Stage-Hybrid-Transformer" -> "sagizty/Parallel-Hybrid-Transformer"
"sagizty/Multi-Stage-Hybrid-Transformer" -> "sagizty/sagizty"
"sagizty/Multi-Stage-Hybrid-Transformer" -> "sagizty/Insight"
"ruotianluo/Image_Captioning_AI_Challenger" -> "ruotianluo/bottom-up-attention-ai-challenger"
"ruotianluo/Image_Captioning_AI_Challenger" -> "showkeyjar/chinese_im2text.pytorch"
"ruotianluo/Image_Captioning_AI_Challenger" -> "peteanderson80/Up-Down-Captioner"
"ruotianluo/Image_Captioning_AI_Challenger" -> "jiasenlu/NeuralBabyTalk"
"ruotianluo/Image_Captioning_AI_Challenger" -> "foamliu/Image-Captioning"
"ruotianluo/Image_Captioning_AI_Challenger" -> "Wind-Ward/Image_Caption_Competition"
"ruotianluo/Image_Captioning_AI_Challenger" -> "ruotianluo/ImageCaptioning.pytorch"
"ruotianluo/Image_Captioning_AI_Challenger" -> "yufengm/Adaptive"
"ruotianluo/Image_Captioning_AI_Challenger" -> "ruotianluo/self-critical.pytorch"
"ruotianluo/Image_Captioning_AI_Challenger" -> "gujiuxiang/Stack-Captioning"
"ruotianluo/Image_Captioning_AI_Challenger" -> "husthuaan/AoANet"
"ruotianluo/Image_Captioning_AI_Challenger" -> "HughChi/Image-Caption"
"ruotianluo/Image_Captioning_AI_Challenger" -> "tsenghungchen/show-adapt-and-tell"
"ruotianluo/Image_Captioning_AI_Challenger" -> "lxtGH/AI_challenger_Chinese_Caption"
"ruotianluo/Image_Captioning_AI_Challenger" -> "aimagelab/show-control-and-tell"
"timojl/clipseg" -> "raoyongming/DenseCLIP"
"timojl/clipseg" -> "isl-org/lang-seg"
"timojl/clipseg" -> "amrrs/stable-diffusion-prompt-inpainting" ["e"=1]
"timojl/clipseg" -> "ThereforeGames/txt2mask" ["e"=1]
"timojl/clipseg" -> "chongzhou96/MaskCLIP"
"timojl/clipseg" -> "NVlabs/GroupViT"
"timojl/clipseg" -> "microsoft/X-Decoder"
"timojl/clipseg" -> "salesforce/BLIP"
"timojl/clipseg" -> "google/prompt-to-prompt" ["e"=1]
"timojl/clipseg" -> "KaiyangZhou/CoOp"
"timojl/clipseg" -> "omriav/blended-diffusion" ["e"=1]
"timojl/clipseg" -> "microsoft/RegionCLIP"
"timojl/clipseg" -> "microsoft/GLIP"
"timojl/clipseg" -> "UX-Decoder/Segment-Everything-Everywhere-All-At-Once" ["e"=1]
"timojl/clipseg" -> "dingjiansw101/ZegFormer"
"airsplay/lxmert" -> "facebookresearch/vilbert-multi-task"
"airsplay/lxmert" -> "jackroos/VL-BERT"
"airsplay/lxmert" -> "uclanlp/visualbert"
"airsplay/lxmert" -> "jiasenlu/vilbert_beta"
"airsplay/lxmert" -> "ChenRocks/UNITER"
"airsplay/lxmert" -> "peteanderson80/bottom-up-attention"
"airsplay/lxmert" -> "MILVLG/mcan-vqa"
"airsplay/lxmert" -> "microsoft/Oscar"
"airsplay/lxmert" -> "LuoweiZhou/VLP"
"airsplay/lxmert" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"airsplay/lxmert" -> "facebookresearch/grid-feats-vqa"
"airsplay/lxmert" -> "hengyuan-hu/bottom-up-attention-vqa"
"airsplay/lxmert" -> "MILVLG/openvqa"
"airsplay/lxmert" -> "airsplay/py-bottom-up-attention"
"airsplay/lxmert" -> "dandelin/ViLT"
"SHI-Labs/OneFormer" -> "microsoft/X-Decoder"
"SHI-Labs/OneFormer" -> "baaivision/EVA"
"SHI-Labs/OneFormer" -> "facebookresearch/Mask2Former" ["e"=1]
"SHI-Labs/OneFormer" -> "NVlabs/ODISE" ["e"=1]
"SHI-Labs/OneFormer" -> "czczup/ViT-Adapter"
"SHI-Labs/OneFormer" -> "baaivision/Painter" ["e"=1]
"SHI-Labs/OneFormer" -> "SHI-Labs/Neighborhood-Attention-Transformer"
"SHI-Labs/OneFormer" -> "facebookresearch/MaskFormer" ["e"=1]
"SHI-Labs/OneFormer" -> "facebookresearch/CutLER"
"SHI-Labs/OneFormer" -> "Visual-Attention-Network/SegNeXt"
"SHI-Labs/OneFormer" -> "OpenGVLab/InternImage"
"SHI-Labs/OneFormer" -> "ShoufaChen/DiffusionDet"
"SHI-Labs/OneFormer" -> "IDEA-Research/detrex"
"SHI-Labs/OneFormer" -> "gligen/GLIGEN" ["e"=1]
"SHI-Labs/OneFormer" -> "isl-org/lang-seg"
"facebookresearch/paco" -> "facebookresearch/diht"
"facebookresearch/paco" -> "TACJu/PartImageNet"
"google-research/deeplab2" -> "facebookresearch/Mask2Former" ["e"=1]
"google-research/deeplab2" -> "facebookresearch/MaskFormer" ["e"=1]
"google-research/deeplab2" -> "csrhddlam/axial-deeplab" ["e"=1]
"google-research/deeplab2" -> "bowenc0221/panoptic-deeplab" ["e"=1]
"google-research/deeplab2" -> "SHI-Labs/OneFormer"
"google-research/deeplab2" -> "google-research/pix2seq"
"google-research/deeplab2" -> "czczup/ViT-Adapter"
"google-research/deeplab2" -> "ShoufaChen/DiffusionDet"
"google-research/deeplab2" -> "facebookresearch/Detic"
"google-research/deeplab2" -> "joe-siyuan-qiao/ViP-DeepLab" ["e"=1]
"google-research/deeplab2" -> "baaivision/EVA"
"google-research/deeplab2" -> "cocodataset/panopticapi" ["e"=1]
"google-research/deeplab2" -> "UX-Decoder/Segment-Everything-Everywhere-All-At-Once" ["e"=1]
"google-research/deeplab2" -> "openseg-group/openseg.pytorch" ["e"=1]
"google-research/deeplab2" -> "facebookresearch/ConvNeXt-V2"
"KaihuaTang/Scene-Graph-Benchmark.pytorch" -> "jwyang/graph-rcnn.pytorch"
"KaihuaTang/Scene-Graph-Benchmark.pytorch" -> "rowanz/neural-motifs"
"KaihuaTang/Scene-Graph-Benchmark.pytorch" -> "microsoft/scene_graph_benchmark"
"KaihuaTang/Scene-Graph-Benchmark.pytorch" -> "danfeiX/scene-graph-TF-release"
"KaihuaTang/Scene-Graph-Benchmark.pytorch" -> "vacancy/SceneGraphParser"
"KaihuaTang/Scene-Graph-Benchmark.pytorch" -> "yangxuntu/SGAE"
"KaihuaTang/Scene-Graph-Benchmark.pytorch" -> "microsoft/Oscar"
"KaihuaTang/Scene-Graph-Benchmark.pytorch" -> "peteanderson80/bottom-up-attention"
"KaihuaTang/Scene-Graph-Benchmark.pytorch" -> "Wangt-CN/VC-R-CNN" ["e"=1]
"KaihuaTang/Scene-Graph-Benchmark.pytorch" -> "NVIDIA/ContrastiveLosses4VRD"
"KaihuaTang/Scene-Graph-Benchmark.pytorch" -> "yuweihao/KERN"
"KaihuaTang/Scene-Graph-Benchmark.pytorch" -> "Jingkang50/OpenPSG" ["e"=1]
"KaihuaTang/Scene-Graph-Benchmark.pytorch" -> "ruotianluo/self-critical.pytorch"
"KaihuaTang/Scene-Graph-Benchmark.pytorch" -> "KaihuaTang/VCTree-Scene-Graph-Generation"
"KaihuaTang/Scene-Graph-Benchmark.pytorch" -> "cshizhe/asg2cap"
"mods333/energy-based-scene-graph" -> "SHTUPLUS/PySGG"
"mods333/energy-based-scene-graph" -> "huoxingmeishi/Awesome-Scene-Graphs"
"mods333/energy-based-scene-graph" -> "coldmanck/recovering-unbiased-scene-graphs"
"mods333/energy-based-scene-graph" -> "MCG-NJU/Structured-Sparse-RCNN"
"IDEACVR/DINO" -> "IDEACVR/awesome-detection-transformer"
"IDEACVR/DINO" -> "IDEA-opensource/DAB-DETR"
"IDEACVR/DINO" -> "IDEACVR/MaskDINO"
"IDEACVR/DINO" -> "IDEA-opensource/DN-DETR"
"IDEACVR/DINO" -> "IDEA-Research/detrex"
"IDEACVR/DINO" -> "microsoft/SoftTeacher" ["e"=1]
"IDEACVR/DINO" -> "fundamentalvision/Deformable-DETR" ["e"=1]
"IDEACVR/DINO" -> "FengLi-ust/DN-DETR"
"IDEACVR/DINO" -> "czczup/ViT-Adapter"
"IDEACVR/DINO" -> "facebookresearch/Mask2Former" ["e"=1]
"IDEACVR/DINO" -> "baaivision/EVA"
"IDEACVR/DINO" -> "microsoft/GLIP"
"IDEACVR/DINO" -> "ShoufaChen/DiffusionDet"
"IDEACVR/DINO" -> "Atten4Vis/ConditionalDETR"
"IDEACVR/DINO" -> "microsoft/DynamicHead" ["e"=1]
"peteanderson80/bottom-up-attention" -> "hengyuan-hu/bottom-up-attention-vqa"
"peteanderson80/bottom-up-attention" -> "ruotianluo/self-critical.pytorch"
"peteanderson80/bottom-up-attention" -> "peteanderson80/Up-Down-Captioner"
"peteanderson80/bottom-up-attention" -> "airsplay/lxmert"
"peteanderson80/bottom-up-attention" -> "microsoft/Oscar"
"peteanderson80/bottom-up-attention" -> "ruotianluo/ImageCaptioning.pytorch"
"peteanderson80/bottom-up-attention" -> "kuanghuei/SCAN" ["e"=1]
"peteanderson80/bottom-up-attention" -> "zhjohnchan/awesome-image-captioning"
"peteanderson80/bottom-up-attention" -> "MILVLG/bottom-up-attention.pytorch"
"peteanderson80/bottom-up-attention" -> "tylin/coco-caption"
"peteanderson80/bottom-up-attention" -> "MILVLG/mcan-vqa"
"peteanderson80/bottom-up-attention" -> "ChenRocks/UNITER"
"peteanderson80/bottom-up-attention" -> "husthuaan/AoANet"
"peteanderson80/bottom-up-attention" -> "Cadene/vqa.pytorch"
"peteanderson80/bottom-up-attention" -> "jiasenlu/vilbert_beta"
"jiasenlu/visDial.pytorch" -> "batra-mlp-lab/visdial-challenge-starter-pytorch"
"jiasenlu/visDial.pytorch" -> "batra-mlp-lab/visdial-rl"
"jiasenlu/visDial.pytorch" -> "facebookresearch/corefnmn"
"jiasenlu/visDial.pytorch" -> "zilongzheng/visdial-gnn"
"jiasenlu/visDial.pytorch" -> "yuleiniu/rva"
"jiasenlu/visDial.pytorch" -> "batra-mlp-lab/visdial"
"jiasenlu/visDial.pytorch" -> "naver/aqm-plus"
"jiasenlu/visDial.pytorch" -> "vmurahari3/visdial-bert"
"jiasenlu/visDial.pytorch" -> "satwikkottur/clevr-dialog"
"aditya12agd5/convcap" -> "chenxinpeng/ARNet"
"aditya12agd5/convcap" -> "gujiuxiang/Stack-Captioning"
"aditya12agd5/convcap" -> "daqingliu/CAVP"
"aditya12agd5/convcap" -> "s-gupta/visual-concepts"
"aditya12agd5/convcap" -> "aimagelab/show-control-and-tell"
"aditya12agd5/convcap" -> "andyweizhao/Multitask_Image_Captioning"
"aditya12agd5/convcap" -> "cswhjiang/Recurrent_Fusion_Network"
"aditya12agd5/convcap" -> "jiasenlu/NeuralBabyTalk"
"aditya12agd5/convcap" -> "ruotianluo/Transformer_Captioning"
"aditya12agd5/convcap" -> "rakshithShetty/captionGAN"
"aditya12agd5/convcap" -> "lukemelas/image-paragraph-captioning"
"chenxinpeng/ARNet" -> "aditya12agd5/convcap"
"chenxinpeng/ARNet" -> "gujiuxiang/Stack-Captioning"
"facebookresearch/clevr-iep" -> "facebookresearch/clevr-dataset-gen"
"facebookresearch/clevr-iep" -> "kimhc6028/relational-networks"
"facebookresearch/clevr-iep" -> "stanfordnlp/mac-network"
"facebookresearch/clevr-iep" -> "ronghanghu/n2nmn"
"facebookresearch/clevr-iep" -> "kexinyi/ns-vqa"
"facebookresearch/clevr-iep" -> "Cadene/vqa.pytorch"
"facebookresearch/clevr-iep" -> "jacobandreas/nmn2"
"facebookresearch/clevr-iep" -> "facebookresearch/EmbodiedQA" ["e"=1]
"facebookresearch/clevr-iep" -> "lichengunc/MAttNet" ["e"=1]
"facebookresearch/clevr-iep" -> "ethanjperez/film"
"facebookresearch/clevr-iep" -> "davidmascharka/tbd-nets"
"facebookresearch/clevr-iep" -> "szagoruyko/diracnets" ["e"=1]
"facebookresearch/clevr-iep" -> "JamesChuanggg/awesome-vqa"
"facebookresearch/clevr-iep" -> "hengyuan-hu/bottom-up-attention-vqa"
"facebookresearch/clevr-iep" -> "ronghanghu/cmn"
"akshitac8/OW-DETR" -> "RE-OWOD/RE-OWOD"
"akshitac8/OW-DETR" -> "csuhan/opendet2"
"akshitac8/OW-DETR" -> "mmaaz60/mvits_for_class_agnostic_od"
"akshitac8/OW-DETR" -> "JosephKJ/iOD"
"akshitac8/OW-DETR" -> "JosephKJ/OWOD"
"akshitac8/OW-DETR" -> "akshitac8/BiAM" ["e"=1]
"akshitac8/OW-DETR" -> "alirezazareian/ovr-cnn"
"gaopengcuhk/CLIP-Adapter" -> "gaopengcuhk/Tip-Adapter"
"gaopengcuhk/CLIP-Adapter" -> "KaiyangZhou/CoOp"
"gaopengcuhk/CLIP-Adapter" -> "ylsung/VL_adapter"
"gaopengcuhk/CLIP-Adapter" -> "gaopengcuhk/BALLAD"
"gaopengcuhk/CLIP-Adapter" -> "ZrrSkywalker/CaFo"
"gaopengcuhk/CLIP-Adapter" -> "raoyongming/DenseCLIP"
"gaopengcuhk/CLIP-Adapter" -> "muzairkhattak/multimodal-prompt-learning"
"gaopengcuhk/CLIP-Adapter" -> "ZhangYuanhan-AI/NOAH"
"FlagAI-Open/FlagAI" -> "adobe-research/custom-diffusion" ["e"=1]
"FlagAI-Open/FlagAI" -> "baaivision/EVA"
"FlagAI-Open/FlagAI" -> "microsoft/torchscale" ["e"=1]
"FlagAI-Open/FlagAI" -> "salesforce/LAVIS"
"FlagAI-Open/FlagAI" -> "comet-ml/kangas" ["e"=1]
"FlagAI-Open/FlagAI" -> "IDEA-CCNL/Fengshenbang-LM" ["e"=1]
"FlagAI-Open/FlagAI" -> "kakaobrain/coyo-dataset"
"FlagAI-Open/FlagAI" -> "ShoufaChen/DiffusionDet"
"FlagAI-Open/FlagAI" -> "thu-ml/unidiffuser" ["e"=1]
"FlagAI-Open/FlagAI" -> "code-kern-ai/bricks" ["e"=1]
"FlagAI-Open/FlagAI" -> "OFA-Sys/Chinese-CLIP"
"FlagAI-Open/FlagAI" -> "SHI-Labs/Versatile-Diffusion" ["e"=1]
"FlagAI-Open/FlagAI" -> "thunlp/OpenDelta" ["e"=1]
"FlagAI-Open/FlagAI" -> "THUDM/GLM-130B" ["e"=1]
"FlagAI-Open/FlagAI" -> "stanford-crfm/helm" ["e"=1]
"rom1504/clip-retrieval" -> "rom1504/img2dataset"
"rom1504/clip-retrieval" -> "mlfoundations/open_clip"
"rom1504/clip-retrieval" -> "criteo/autofaiss"
"rom1504/clip-retrieval" -> "salesforce/BLIP"
"rom1504/clip-retrieval" -> "crowsonkb/k-diffusion" ["e"=1]
"rom1504/clip-retrieval" -> "FreddeFrallan/Multilingual-CLIP"
"rom1504/clip-retrieval" -> "kakaobrain/coyo-dataset"
"rom1504/clip-retrieval" -> "rinongal/textual_inversion" ["e"=1]
"rom1504/clip-retrieval" -> "microsoft/VQ-Diffusion" ["e"=1]
"rom1504/clip-retrieval" -> "OFA-Sys/OFA"
"rom1504/clip-retrieval" -> "mlfoundations/wise-ft"
"rom1504/clip-retrieval" -> "salesforce/LAVIS"
"rom1504/clip-retrieval" -> "Sense-GVT/DeCLIP"
"rom1504/clip-retrieval" -> "pharmapsychotic/clip-interrogator"
"rom1504/clip-retrieval" -> "JD-P/simulacra-aesthetic-captions" ["e"=1]
"rom1504/img2dataset" -> "rom1504/clip-retrieval"
"rom1504/img2dataset" -> "mlfoundations/open_clip"
"rom1504/img2dataset" -> "kakaobrain/coyo-dataset"
"rom1504/img2dataset" -> "webdataset/webdataset"
"rom1504/img2dataset" -> "salesforce/BLIP"
"rom1504/img2dataset" -> "salesforce/LAVIS"
"rom1504/img2dataset" -> "facebookresearch/SLIP"
"rom1504/img2dataset" -> "rinongal/textual_inversion" ["e"=1]
"rom1504/img2dataset" -> "mlfoundations/open_flamingo" ["e"=1]
"rom1504/img2dataset" -> "FreddeFrallan/Multilingual-CLIP"
"rom1504/img2dataset" -> "OFA-Sys/OFA"
"rom1504/img2dataset" -> "lucidrains/x-clip"
"rom1504/img2dataset" -> "microsoft/VQ-Diffusion" ["e"=1]
"rom1504/img2dataset" -> "openai/glide-text2im" ["e"=1]
"rom1504/img2dataset" -> "facebookresearch/vissl" ["e"=1]
"Cloud-CV/EvalAI" -> "Cloud-CV/Fabrik"
"Cloud-CV/EvalAI" -> "Cloud-CV/Origami"
"Cloud-CV/EvalAI" -> "Cloud-CV/GSoC-Ideas"
"Cloud-CV/EvalAI" -> "Cloud-CV/CloudCV"
"Cloud-CV/EvalAI" -> "Cloud-CV/EvalAI-Starters"
"Cloud-CV/EvalAI" -> "Cloud-CV/visual-chatbot"
"Cloud-CV/EvalAI" -> "Cloud-CV/EvalAI-ngx"
"Cloud-CV/EvalAI" -> "Cloud-CV/evalai-cli"
"Cloud-CV/EvalAI" -> "chiphuyen/sotawhat" ["e"=1]
"Cloud-CV/EvalAI" -> "uber/pyro" ["e"=1]
"Cloud-CV/EvalAI" -> "facebookresearch/fairseq-py" ["e"=1]
"Cloud-CV/EvalAI" -> "codalab/codalab-competitions"
"Cloud-CV/EvalAI" -> "beyretb/AnimalAI-Olympics" ["e"=1]
"Cloud-CV/EvalAI" -> "polyaxon/polyaxon" ["e"=1]
"Cloud-CV/EvalAI" -> "abhshkdz/ai-deadlines" ["e"=1]
"deepmind/multi_object_datasets" -> "applied-ai-lab/genesis"
"deepmind/multi_object_datasets" -> "baudm/MONet-pytorch"
"deepmind/multi_object_datasets" -> "lucidrains/slot-attention"
"deepmind/multi_object_datasets" -> "zhixuan-lin/SPACE"
"deepmind/multi_object_datasets" -> "stelzner/monet"
"deepmind/multi_object_datasets" -> "singhgautam/slate"
"deepmind/multi_object_datasets" -> "zhixuan-lin/IODINE"
"deepmind/multi_object_datasets" -> "ecker-lab/object-centric-representation-benchmark"
"deepmind/multi_object_datasets" -> "JindongJiang/SCALOR"
"deepmind/multi_object_datasets" -> "deepmind/spriteworld" ["e"=1]
"deepmind/multi_object_datasets" -> "facebookresearch/clevr-dataset-gen"
"SHI-Labs/Neighborhood-Attention-Transformer" -> "SHI-Labs/OneFormer"
"SHI-Labs/Neighborhood-Attention-Transformer" -> "sail-sg/poolformer" ["e"=1]
"SHI-Labs/Neighborhood-Attention-Transformer" -> "LeapLabTHU/DAT" ["e"=1]
"SHI-Labs/Neighborhood-Attention-Transformer" -> "SHI-Labs/NATTEN"
"SHI-Labs/Neighborhood-Attention-Transformer" -> "raoyongming/HorNet"
"SHI-Labs/Neighborhood-Attention-Transformer" -> "dingmyu/davit"
"SHI-Labs/Neighborhood-Attention-Transformer" -> "Sense-X/UniFormer" ["e"=1]
"SHI-Labs/Neighborhood-Attention-Transformer" -> "NVlabs/FAN"
"SHI-Labs/Neighborhood-Attention-Transformer" -> "ShoufaChen/DiffusionDet"
"SHI-Labs/Neighborhood-Attention-Transformer" -> "facebookresearch/Mask2Former" ["e"=1]
"SHI-Labs/Neighborhood-Attention-Transformer" -> "microsoft/Focal-Transformer" ["e"=1]
"SHI-Labs/Neighborhood-Attention-Transformer" -> "microsoft/SimMIM" ["e"=1]
"SHI-Labs/Neighborhood-Attention-Transformer" -> "google-research/maxim" ["e"=1]
"SHI-Labs/Neighborhood-Attention-Transformer" -> "facebookresearch/MaskFormer" ["e"=1]
"SHI-Labs/Neighborhood-Attention-Transformer" -> "baaivision/EVA"
"OpenGVLab/InternImage" -> "baaivision/EVA"
"OpenGVLab/InternImage" -> "czczup/ViT-Adapter"
"OpenGVLab/InternImage" -> "ShoufaChen/DiffusionDet"
"OpenGVLab/InternImage" -> "IDEA-Research/detrex"
"OpenGVLab/InternImage" -> "baaivision/Painter" ["e"=1]
"OpenGVLab/InternImage" -> "whai362/PVT" ["e"=1]
"OpenGVLab/InternImage" -> "microsoft/FocalNet"
"OpenGVLab/InternImage" -> "IDEA-Research/GroundingDINO" ["e"=1]
"OpenGVLab/InternImage" -> "DingXiaoH/RepLKNet-pytorch"
"OpenGVLab/InternImage" -> "IDEA-Research/DINO"
"OpenGVLab/InternImage" -> "SHI-Labs/OneFormer"
"OpenGVLab/InternImage" -> "facebookresearch/ConvNeXt-V2"
"OpenGVLab/InternImage" -> "microsoft/GLIP"
"OpenGVLab/InternImage" -> "IDEACVR/DINO"
"OpenGVLab/InternImage" -> "VainF/Awesome-Anything" ["e"=1]
"NVIDIA/aistore" -> "tmbdev/webdataset"
"NVIDIA/aistore" -> "NVIDIA/ais-k8s"
"NVIDIA/aistore" -> "webdataset/webdataset"
"NVIDIA/aistore" -> "NVlabs/tensorcom"
"NVIDIA/aistore" -> "pytorch/elastic" ["e"=1]
"NVIDIA/aistore" -> "petuum/adaptdl" ["e"=1]
"NVIDIA/aistore" -> "pytorch/torchx" ["e"=1]
"NVIDIA/aistore" -> "NVIDIA/deepops" ["e"=1]
"NVIDIA/aistore" -> "facebookresearch/fairscale" ["e"=1]
"NVIDIA/aistore" -> "pytorch/data" ["e"=1]
"NVIDIA/aistore" -> "BaguaSys/bagua" ["e"=1]
"NVIDIA/aistore" -> "uber/petastorm" ["e"=1]
"NVIDIA/aistore" -> "pytorch/benchmark" ["e"=1]
"NVIDIA/aistore" -> "tkestack/gpu-manager" ["e"=1]
"NVIDIA/aistore" -> "NVIDIA/DALI" ["e"=1]
"OFA-Sys/Chinese-CLIP" -> "IDEA-CCNL/Fengshenbang-LM" ["e"=1]
"OFA-Sys/Chinese-CLIP" -> "mlfoundations/open_clip"
"OFA-Sys/Chinese-CLIP" -> "OFA-Sys/OFA"
"OFA-Sys/Chinese-CLIP" -> "salesforce/BLIP"
"OFA-Sys/Chinese-CLIP" -> "salesforce/LAVIS"
"OFA-Sys/Chinese-CLIP" -> "rom1504/clip-retrieval"
"OFA-Sys/Chinese-CLIP" -> "salesforce/ALBEF"
"OFA-Sys/Chinese-CLIP" -> "billjie1/Chinese-CLIP"
"OFA-Sys/Chinese-CLIP" -> "alibaba/EasyNLP" ["e"=1]
"OFA-Sys/Chinese-CLIP" -> "yuxie11/R2D2"
"OFA-Sys/Chinese-CLIP" -> "LianjiaTech/BELLE" ["e"=1]
"OFA-Sys/Chinese-CLIP" -> "lonePatient/awesome-pretrained-chinese-nlp-models" ["e"=1]
"OFA-Sys/Chinese-CLIP" -> "alibaba/AliceMind" ["e"=1]
"OFA-Sys/Chinese-CLIP" -> "mymusise/ChatGLM-Tuning" ["e"=1]
"OFA-Sys/Chinese-CLIP" -> "FreddeFrallan/Multilingual-CLIP"
"pharmapsychotic/clip-interrogator" -> "rinongal/textual_inversion" ["e"=1]
"pharmapsychotic/clip-interrogator" -> "salesforce/BLIP"
"pharmapsychotic/clip-interrogator" -> "pharmapsychotic/clip-interrogator-ext"
"pharmapsychotic/clip-interrogator" -> "mlfoundations/open_clip"
"pharmapsychotic/clip-interrogator" -> "TencentARC/T2I-Adapter" ["e"=1]
"pharmapsychotic/clip-interrogator" -> "crowsonkb/k-diffusion" ["e"=1]
"pharmapsychotic/clip-interrogator" -> "cloneofsimo/lora" ["e"=1]
"pharmapsychotic/clip-interrogator" -> "rom1504/clip-retrieval"
"pharmapsychotic/clip-interrogator" -> "kohya-ss/sd-scripts" ["e"=1]
"pharmapsychotic/clip-interrogator" -> "salesforce/LAVIS"
"pharmapsychotic/clip-interrogator" -> "justinpinkney/stable-diffusion" ["e"=1]
"pharmapsychotic/clip-interrogator" -> "lkwq007/stablediffusion-infinity" ["e"=1]
"pharmapsychotic/clip-interrogator" -> "poloclub/diffusiondb" ["e"=1]
"pharmapsychotic/clip-interrogator" -> "google/prompt-to-prompt" ["e"=1]
"pharmapsychotic/clip-interrogator" -> "ShivamShrirao/diffusers" ["e"=1]
"currentslab/awesome-vector-search" -> "criteo/autofaiss"
"currentslab/awesome-vector-search" -> "facebookresearch/distributed-faiss"
"isl-org/lang-seg" -> "NVlabs/GroupViT"
"isl-org/lang-seg" -> "raoyongming/DenseCLIP"
"isl-org/lang-seg" -> "chongzhou96/MaskCLIP"
"isl-org/lang-seg" -> "timojl/clipseg"
"isl-org/lang-seg" -> "microsoft/GLIP"
"isl-org/lang-seg" -> "MendelXu/zsseg.baseline"
"isl-org/lang-seg" -> "KaiyangZhou/CoOp"
"isl-org/lang-seg" -> "ashkamath/mdetr"
"isl-org/lang-seg" -> "MarkMoHR/Awesome-Referring-Image-Segmentation" ["e"=1]
"isl-org/lang-seg" -> "NVlabs/ODISE" ["e"=1]
"isl-org/lang-seg" -> "lukemelas/deep-spectral-segmentation"
"isl-org/lang-seg" -> "facebookresearch/MaskFormer" ["e"=1]
"isl-org/lang-seg" -> "DerrickWang005/CRIS.pytorch" ["e"=1]
"isl-org/lang-seg" -> "facebookresearch/ov-seg"
"isl-org/lang-seg" -> "microsoft/X-Decoder"
"jayleicn/ClipBERT" -> "ArrowLuo/CLIP4Clip" ["e"=1]
"jayleicn/ClipBERT" -> "gabeur/mmt" ["e"=1]
"jayleicn/ClipBERT" -> "danieljf24/awesome-video-text-retrieval" ["e"=1]
"jayleicn/ClipBERT" -> "microsoft/UniVL" ["e"=1]
"jayleicn/ClipBERT" -> "linjieli222/HERO" ["e"=1]
"jayleicn/ClipBERT" -> "m-bain/frozen-in-time" ["e"=1]
"jayleicn/ClipBERT" -> "CryhanFang/CLIP2Video" ["e"=1]
"jayleicn/ClipBERT" -> "jayleicn/moment_detr" ["e"=1]
"jayleicn/ClipBERT" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"jayleicn/ClipBERT" -> "salesforce/ALBEF"
"jayleicn/ClipBERT" -> "microsoft/Oscar"
"jayleicn/ClipBERT" -> "jackroos/VL-BERT"
"jayleicn/ClipBERT" -> "rowanz/merlot" ["e"=1]
"jayleicn/ClipBERT" -> "antoine77340/MIL-NCE_HowTo100M" ["e"=1]
"jayleicn/ClipBERT" -> "ChenRocks/UNITER"
"ShoufaChen/DiffusionDet" -> "fundamentalvision/Deformable-DETR" ["e"=1]
"ShoufaChen/DiffusionDet" -> "baaivision/EVA"
"ShoufaChen/DiffusionDet" -> "IDEA-Research/detrex"
"ShoufaChen/DiffusionDet" -> "microsoft/GLIP"
"ShoufaChen/DiffusionDet" -> "facebookresearch/DiT" ["e"=1]
"ShoufaChen/DiffusionDet" -> "OpenGVLab/InternImage"
"ShoufaChen/DiffusionDet" -> "facebookresearch/Mask2Former" ["e"=1]
"ShoufaChen/DiffusionDet" -> "IDEACVR/DINO"
"ShoufaChen/DiffusionDet" -> "UX-Decoder/Segment-Everything-Everywhere-All-At-Once" ["e"=1]
"ShoufaChen/DiffusionDet" -> "IDEACVR/awesome-detection-transformer"
"ShoufaChen/DiffusionDet" -> "facebookresearch/ConvNeXt-V2"
"ShoufaChen/DiffusionDet" -> "microsoft/X-Decoder"
"ShoufaChen/DiffusionDet" -> "NVlabs/GroupViT"
"ShoufaChen/DiffusionDet" -> "czczup/ViT-Adapter"
"ShoufaChen/DiffusionDet" -> "facebookresearch/mae" ["e"=1]
"microsoft/scene_graph_benchmark" -> "pzzhang/VinVL"
"microsoft/scene_graph_benchmark" -> "KaihuaTang/Scene-Graph-Benchmark.pytorch"
"microsoft/scene_graph_benchmark" -> "microsoft/Oscar"
"microsoft/scene_graph_benchmark" -> "vacancy/SceneGraphParser"
"microsoft/scene_graph_benchmark" -> "jwyang/graph-rcnn.pytorch"
"microsoft/scene_graph_benchmark" -> "Jingkang50/OpenPSG" ["e"=1]
"microsoft/scene_graph_benchmark" -> "rowanz/neural-motifs"
"microsoft/scene_graph_benchmark" -> "bknyaz/sgg"
"microsoft/scene_graph_benchmark" -> "MILVLG/bottom-up-attention.pytorch"
"microsoft/scene_graph_benchmark" -> "danfeiX/scene-graph-TF-release"
"microsoft/scene_graph_benchmark" -> "yangxuntu/SGAE"
"microsoft/scene_graph_benchmark" -> "facebookresearch/grid-feats-vqa"
"microsoft/scene_graph_benchmark" -> "airsplay/py-bottom-up-attention"
"microsoft/scene_graph_benchmark" -> "SHTUPLUS/PySGG"
"microsoft/scene_graph_benchmark" -> "yikang-li/MSDN"
"mert-kurttutan/torchview" -> "iejMac/video2numpy"
"mert-kurttutan/torchview" -> "pytorch-labs/tensordict" ["e"=1]
"czczup/ViT-Adapter" -> "facebookresearch/Mask2Former" ["e"=1]
"czczup/ViT-Adapter" -> "baaivision/EVA"
"czczup/ViT-Adapter" -> "OpenGVLab/InternImage"
"czczup/ViT-Adapter" -> "microsoft/X-Decoder"
"czczup/ViT-Adapter" -> "IDEACVR/DINO"
"czczup/ViT-Adapter" -> "chongzhou96/MaskCLIP"
"czczup/ViT-Adapter" -> "NVlabs/SegFormer" ["e"=1]
"czczup/ViT-Adapter" -> "Visual-Attention-Network/SegNeXt"
"czczup/ViT-Adapter" -> "facebookresearch/MaskFormer" ["e"=1]
"czczup/ViT-Adapter" -> "SHI-Labs/OneFormer"
"czczup/ViT-Adapter" -> "raoyongming/DenseCLIP"
"czczup/ViT-Adapter" -> "IDEACVR/MaskDINO"
"czczup/ViT-Adapter" -> "Picsart-AI-Research/SeMask-Segmentation" ["e"=1]
"czczup/ViT-Adapter" -> "ShoufaChen/DiffusionDet"
"czczup/ViT-Adapter" -> "ZwwWayne/K-Net" ["e"=1]
"Cloud-CV/Fabrik" -> "Cloud-CV/EvalAI"
"Cloud-CV/Fabrik" -> "Cloud-CV/Origami"
"Cloud-CV/Fabrik" -> "Cloud-CV/visual-chatbot"
"Cloud-CV/Fabrik" -> "Cloud-CV/GSoC-Ideas"
"Cloud-CV/Fabrik" -> "Cloud-CV/CloudCV"
"Cloud-CV/Fabrik" -> "nmhkahn/deep_learning_tutorial" ["e"=1]
"Cloud-CV/Fabrik" -> "dnouri/skorch" ["e"=1]
"Cloud-CV/Fabrik" -> "GunhoChoi/PyTorch-FastCampus" ["e"=1]
"Cloud-CV/Fabrik" -> "uber/horovod" ["e"=1]
"Cloud-CV/Fabrik" -> "MrNothing/AI-Blocks" ["e"=1]
"Cloud-CV/Fabrik" -> "joeddav/devol" ["e"=1]
"Cloud-CV/Fabrik" -> "reiinakano/xcessiv" ["e"=1]
"Cloud-CV/Fabrik" -> "facebookresearch/loop" ["e"=1]
"Cloud-CV/Fabrik" -> "uber/pyro" ["e"=1]
"Cloud-CV/Fabrik" -> "Prodicode/ann-visualizer" ["e"=1]
"google-research/big_vision" -> "google-research/pix2seq"
"google-research/big_vision" -> "baaivision/EVA"
"google-research/big_vision" -> "lucidrains/CoCa-pytorch"
"google-research/big_vision" -> "google-research/scenic" ["e"=1]
"google-research/big_vision" -> "facebookresearch/ToMe"
"google-research/big_vision" -> "google/ml_collections" ["e"=1]
"google-research/big_vision" -> "facebookresearch/SLIP"
"google-research/big_vision" -> "mlfoundations/open_clip"
"google-research/big_vision" -> "czczup/ViT-Adapter"
"google-research/big_vision" -> "microsoft/X-Decoder"
"google-research/big_vision" -> "google/fiddle" ["e"=1]
"google-research/big_vision" -> "facebookresearch/ConvNeXt-V2"
"google-research/big_vision" -> "EPFL-VILAB/MultiMAE" ["e"=1]
"google-research/big_vision" -> "rom1504/clip-retrieval"
"google-research/big_vision" -> "microsoft/GLIP"
"haltakov/natural-language-image-search" -> "haltakov/natural-language-youtube-search"
"haltakov/natural-language-image-search" -> "haofanwang/natural-language-joint-query-search"
"haltakov/natural-language-image-search" -> "Zasder3/train-CLIP"
"haltakov/natural-language-image-search" -> "FreddeFrallan/Multilingual-CLIP"
"haltakov/natural-language-image-search" -> "rkouye/es-clip-image-search"
"haltakov/natural-language-image-search" -> "rmokady/CLIP_prefix_caption"
"haltakov/natural-language-image-search" -> "rom1504/clip-retrieval"
"haltakov/natural-language-image-search" -> "nerdyrodent/CLIP-Guided-Diffusion" ["e"=1]
"haltakov/natural-language-image-search" -> "lucidrains/DALLE-pytorch" ["e"=1]
"haltakov/natural-language-image-search" -> "eps696/stylegan2" ["e"=1]
"haltakov/natural-language-image-search" -> "kingyiusuen/clip-image-search"
"haltakov/natural-language-image-search" -> "rom1504/img2dataset"
"haltakov/natural-language-image-search" -> "PaddlePaddle/VIMER" ["e"=1]
"haltakov/natural-language-image-search" -> "AndreyGuzhov/AudioCLIP" ["e"=1]
"haltakov/natural-language-image-search" -> "orpatashnik/StyleCLIP" ["e"=1]
"LuoweiZhou/VLP" -> "microsoft/Oscar"
"LuoweiZhou/VLP" -> "jackroos/VL-BERT"
"LuoweiZhou/VLP" -> "ChenRocks/UNITER"
"LuoweiZhou/VLP" -> "airsplay/lxmert"
"LuoweiZhou/VLP" -> "jiasenlu/vilbert_beta"
"LuoweiZhou/VLP" -> "facebookresearch/vilbert-multi-task"
"LuoweiZhou/VLP" -> "husthuaan/AoANet"
"LuoweiZhou/VLP" -> "facebookresearch/grid-feats-vqa"
"LuoweiZhou/VLP" -> "cshizhe/asg2cap"
"LuoweiZhou/VLP" -> "aimagelab/meshed-memory-transformer"
"LuoweiZhou/VLP" -> "yangxuntu/SGAE"
"LuoweiZhou/VLP" -> "pzzhang/VinVL"
"LuoweiZhou/VLP" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"LuoweiZhou/VLP" -> "ruotianluo/self-critical.pytorch"
"LuoweiZhou/VLP" -> "yahoo/object_relation_transformer"
"Vision-CAIR/VisualGPT" -> "omar-mohamed/GPT2-Chest-X-Ray-Report-Generation" ["e"=1]
"Vision-CAIR/VisualGPT" -> "zhangxuying1004/RSTNet"
"Vision-CAIR/VisualGPT" -> "luo3300612/image-captioning-DLCT"
"Vision-CAIR/VisualGPT" -> "aimagelab/meshed-memory-transformer"
"Vision-CAIR/VisualGPT" -> "yahoo/object_relation_transformer"
"dandelin/ViLT" -> "salesforce/ALBEF"
"dandelin/ViLT" -> "zdou0830/METER"
"dandelin/ViLT" -> "microsoft/Oscar"
"dandelin/ViLT" -> "ChenRocks/UNITER"
"dandelin/ViLT" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"dandelin/ViLT" -> "facebookresearch/vilbert-multi-task"
"dandelin/ViLT" -> "airsplay/lxmert"
"dandelin/ViLT" -> "salesforce/BLIP"
"dandelin/ViLT" -> "KaiyangZhou/CoOp"
"dandelin/ViLT" -> "OFA-Sys/OFA"
"dandelin/ViLT" -> "ashkamath/mdetr"
"dandelin/ViLT" -> "researchmm/soho"
"dandelin/ViLT" -> "jayleicn/ClipBERT"
"dandelin/ViLT" -> "facebookresearch/mmf"
"dandelin/ViLT" -> "jackroos/VL-BERT"
"j-min/VL-T5" -> "ylsung/VL_adapter"
"j-min/VL-T5" -> "microsoft/PICa"
"j-min/VL-T5" -> "salesforce/ALPRO" ["e"=1]
"j-min/VL-T5" -> "salesforce/ALBEF"
"j-min/VL-T5" -> "clip-vil/CLIP-ViL"
"j-min/VL-T5" -> "microsoft/UniVL" ["e"=1]
"j-min/VL-T5" -> "microsoft/FIBER"
"j-min/VL-T5" -> "ChenRocks/UNITER"
"j-min/VL-T5" -> "MikeWangWZHL/VidIL" ["e"=1]
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "sgrvinod/a-PyTorch-Tutorial-to-Object-Detection" ["e"=1]
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning"
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "sgrvinod/a-PyTorch-Tutorial-to-Sequence-Labeling" ["e"=1]
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "sgrvinod/a-PyTorch-Tutorial-to-Super-Resolution" ["e"=1]
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "ruotianluo/ImageCaptioning.pytorch"
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "bentrevett/pytorch-seq2seq" ["e"=1]
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "zhjohnchan/awesome-image-captioning"
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "dsgiitr/d2l-pytorch" ["e"=1]
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "sgrvinod/a-PyTorch-Tutorial-to-Text-Classification" ["e"=1]
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "ritchieng/the-incredible-pytorch" ["e"=1]
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "yhenon/pytorch-retinanet" ["e"=1]
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "ahkarami/Deep-Learning-in-Production" ["e"=1]
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "bentrevett/pytorch-sentiment-analysis" ["e"=1]
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "victoresque/pytorch-template" ["e"=1]
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "ruotianluo/self-critical.pytorch"
"foamliu/Image-Captioning-PyTorch" -> "HughChi/Image-Caption"
"foamliu/Image-Captioning-PyTorch" -> "foamliu/Image-Captioning"
"yangxuntu/SGAE" -> "husthuaan/AoANet"
"yangxuntu/SGAE" -> "YiwuZhong/Sub-GC"
"yangxuntu/SGAE" -> "rowanz/neural-motifs"
"yangxuntu/SGAE" -> "cshizhe/asg2cap"
"yangxuntu/SGAE" -> "yahoo/object_relation_transformer"
"yangxuntu/SGAE" -> "vacancy/SceneGraphParser"
"yangxuntu/SGAE" -> "aimagelab/show-control-and-tell"
"yangxuntu/SGAE" -> "ruotianluo/self-critical.pytorch"
"yangxuntu/SGAE" -> "JDAI-CV/image-captioning"
"yangxuntu/SGAE" -> "forence/Awesome-Visual-Captioning"
"yangxuntu/SGAE" -> "husthuaan/AAT"
"yangxuntu/SGAE" -> "fengyang0317/unsupervised_captioning"
"yangxuntu/SGAE" -> "aimagelab/meshed-memory-transformer"
"yangxuntu/SGAE" -> "Dong-JinKim/DenseRelationalCaptioning"
"yangxuntu/SGAE" -> "ruotianluo/Transformer_Captioning"
"lizekang/DSTC10-MOD" -> "gsh199449/stickerchat"
"sail-sg/Adan" -> "lucidrains/Adan-pytorch"
"sail-sg/Adan" -> "sail-sg/metaformer"
"sail-sg/Adan" -> "baaivision/EVA"
"sail-sg/Adan" -> "facebookresearch/ConvNeXt-V2"
"sail-sg/Adan" -> "raoyongming/HorNet"
"sail-sg/Adan" -> "sail-sg/poolformer" ["e"=1]
"sail-sg/Adan" -> "megvii-research/RevCol"
"sail-sg/Adan" -> "SHI-Labs/Neighborhood-Attention-Transformer"
"sail-sg/Adan" -> "facebookresearch/DiT" ["e"=1]
"sail-sg/Adan" -> "google-research/sam" ["e"=1]
"sail-sg/Adan" -> "Visual-Attention-Network/VAN-Classification" ["e"=1]
"sail-sg/Adan" -> "LayneH/GreenMIM" ["e"=1]
"sail-sg/Adan" -> "ShoufaChen/DiffusionDet"
"sail-sg/Adan" -> "nnaisense/evotorch" ["e"=1]
"sail-sg/Adan" -> "DingXiaoH/RepLKNet-pytorch"
"zengyan-97/X-VLM" -> "uta-smile/TCL"
"zengyan-97/X-VLM" -> "salesforce/ALBEF"
"zengyan-97/X-VLM" -> "zdou0830/METER"
"zengyan-97/X-VLM" -> "zengyan-97/CCLM"
"zengyan-97/X-VLM" -> "OFA-Sys/OFA"
"zengyan-97/X-VLM" -> "Paranioar/SGRAF" ["e"=1]
"zengyan-97/X-VLM" -> "sail-sg/ptp"
"zengyan-97/X-VLM" -> "MichaelZhouwang/VLUE"
"zengyan-97/X-VLM" -> "microsoft/XPretrain" ["e"=1]
"zengyan-97/X-VLM" -> "zengyan-97/X2-VLM"
"zengyan-97/X-VLM" -> "lucidrains/CoCa-pytorch"
"zengyan-97/X-VLM" -> "dandelin/ViLT"
"zengyan-97/X-VLM" -> "TencentARC/MCQ" ["e"=1]
"zengyan-97/X-VLM" -> "salesforce/BLIP"
"zengyan-97/X-VLM" -> "Paranioar/Cross-modal_Retrieval_Tutorial" ["e"=1]
"MILVLG/mcan-vqa" -> "MILVLG/openvqa"
"MILVLG/mcan-vqa" -> "jnhwkim/ban-vqa"
"MILVLG/mcan-vqa" -> "facebookresearch/grid-feats-vqa"
"MILVLG/mcan-vqa" -> "hengyuan-hu/bottom-up-attention-vqa"
"MILVLG/mcan-vqa" -> "airsplay/lxmert"
"MILVLG/mcan-vqa" -> "jokieleung/awesome-visual-question-answering"
"MILVLG/mcan-vqa" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"MILVLG/mcan-vqa" -> "linjieli222/VQA_ReGAT"
"MILVLG/mcan-vqa" -> "Cadene/murel.bootstrap.pytorch"
"MILVLG/mcan-vqa" -> "peteanderson80/bottom-up-attention"
"MILVLG/mcan-vqa" -> "yuzcccc/vqa-mfb"
"MILVLG/mcan-vqa" -> "Cadene/block.bootstrap.pytorch"
"MILVLG/mcan-vqa" -> "ChenRocks/UNITER"
"MILVLG/mcan-vqa" -> "jiasenlu/vilbert_beta"
"MILVLG/mcan-vqa" -> "yanxinzju/CSS-VQA"
"gaopengcuhk/SMCA-DETR" -> "kakaobrain/sparse-detr"
"gaopengcuhk/SMCA-DETR" -> "Atten4Vis/ConditionalDETR"
"gaopengcuhk/SMCA-DETR" -> "zhechen/Deformable-DETR-REGO"
"gaopengcuhk/SMCA-DETR" -> "megvii-research/AnchorDETR"
"gaopengcuhk/SMCA-DETR" -> "gaopengcuhk/Container"
"gaopengcuhk/SMCA-DETR" -> "abc403/SMCA-replication"
"gaopengcuhk/SMCA-DETR" -> "twangnh/pnp-detr"
"gaopengcuhk/SMCA-DETR" -> "megvii-research/SOLQ" ["e"=1]
"jiasenlu/HieCoAttenVQA" -> "GT-Vision-Lab/VQA_LSTM_CNN"
"jiasenlu/HieCoAttenVQA" -> "akirafukui/vqa-mcb"
"jiasenlu/HieCoAttenVQA" -> "JamesChuanggg/awesome-vqa"
"jiasenlu/HieCoAttenVQA" -> "VT-vision-lab/VQA_LSTM_CNN"
"jiasenlu/HieCoAttenVQA" -> "GT-Vision-Lab/VQA"
"jiasenlu/HieCoAttenVQA" -> "iamaaditya/VQA_Demo"
"jiasenlu/HieCoAttenVQA" -> "metalbubble/VQAbaseline"
"jiasenlu/HieCoAttenVQA" -> "zcyang/imageqa-san"
"jiasenlu/HieCoAttenVQA" -> "Cadene/vqa.pytorch"
"jiasenlu/HieCoAttenVQA" -> "JamesChuanggg/VQA-tensorflow"
"jiasenlu/HieCoAttenVQA" -> "paarthneekhara/neural-vqa-tensorflow"
"jiasenlu/HieCoAttenVQA" -> "hengyuan-hu/bottom-up-attention-vqa"
"jiasenlu/HieCoAttenVQA" -> "anantzoid/VQA-Keras-Visual-Question-Answering"
"jiasenlu/HieCoAttenVQA" -> "jacobandreas/nmn2"
"jiasenlu/HieCoAttenVQA" -> "ronghanghu/n2nmn"
"luo3300612/image-captioning-DLCT" -> "zhangxuying1004/RSTNet"
"luo3300612/image-captioning-DLCT" -> "aimagelab/meshed-memory-transformer"
"luo3300612/image-captioning-DLCT" -> "facebookresearch/grid-feats-vqa"
"luo3300612/image-captioning-DLCT" -> "JDAI-CV/image-captioning"
"luo3300612/image-captioning-DLCT" -> "yahoo/object_relation_transformer"
"luo3300612/image-captioning-DLCT" -> "husthuaan/AoANet"
"luo3300612/image-captioning-DLCT" -> "cshizhe/asg2cap"
"luo3300612/image-captioning-DLCT" -> "forence/Awesome-Visual-Captioning"
"luo3300612/image-captioning-DLCT" -> "232525/PureT"
"luo3300612/image-captioning-DLCT" -> "terry-r123/Awesome-Captioning"
"luo3300612/image-captioning-DLCT" -> "luo3300612/Transformer-Captioning"
"luo3300612/image-captioning-DLCT" -> "yangxuntu/SGAE"
"luo3300612/image-captioning-DLCT" -> "davidnvq/grit"
"luo3300612/image-captioning-DLCT" -> "YiwuZhong/Sub-GC"
"luo3300612/image-captioning-DLCT" -> "Gitsamshi/WeakVRD-Captioning"
"xinke-wang/Awesome-Text-VQA" -> "yashkant/sam-textvqa"
"xinke-wang/Awesome-Text-VQA" -> "microsoft/TAP"
"xinke-wang/Awesome-Text-VQA" -> "ZephyrZhuQi/ssbaseline"
"xinke-wang/Awesome-Text-VQA" -> "uakarsh/latr"
"xinke-wang/Awesome-Text-VQA" -> "ronghanghu/pythia"
"xinke-wang/Awesome-Text-VQA" -> "ZeningLin/ViBERTgrid-PyTorch"
"xinke-wang/Awesome-Text-VQA" -> "jokieleung/awesome-visual-question-answering"
"MendelXu/zsseg.baseline" -> "dingjiansw101/ZegFormer"
"MendelXu/zsseg.baseline" -> "chongzhou96/MaskCLIP"
"jnhwkim/cbp" -> "jnhwkim/MulLowBiVQA"
"vacancy/SceneGraphParser" -> "yangxuntu/SGAE"
"vacancy/SceneGraphParser" -> "KaihuaTang/Scene-Graph-Benchmark.pytorch"
"vacancy/SceneGraphParser" -> "peteanderson80/SPICE"
"vacancy/SceneGraphParser" -> "rowanz/neural-motifs"
"vacancy/SceneGraphParser" -> "microsoft/scene_graph_benchmark"
"vacancy/SceneGraphParser" -> "cshizhe/asg2cap"
"vacancy/SceneGraphParser" -> "jwyang/graph-rcnn.pytorch"
"vacancy/SceneGraphParser" -> "JDAI-CV/image-captioning"
"vacancy/SceneGraphParser" -> "danfeiX/scene-graph-TF-release"
"vacancy/SceneGraphParser" -> "YiwuZhong/Sub-GC"
"vacancy/SceneGraphParser" -> "shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome"
"vacancy/SceneGraphParser" -> "yikang-li/MSDN"
"vacancy/SceneGraphParser" -> "sibeiyang/sgmn" ["e"=1]
"vacancy/SceneGraphParser" -> "lichengunc/MAttNet" ["e"=1]
"vacancy/SceneGraphParser" -> "ruotianluo/self-critical.pytorch"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "ChenRocks/UNITER"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "sangminwoo/awesome-vision-and-language"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "jackroos/VL-BERT"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "jayleicn/ClipBERT"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "salesforce/ALBEF"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "airsplay/lxmert"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "microsoft/Oscar"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "jiasenlu/vilbert_beta"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "pliang279/awesome-multimodal-ml"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "dandelin/ViLT"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "facebookresearch/vilbert-multi-task"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "TheShadow29/awesome-grounding" ["e"=1]
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "forence/Awesome-Visual-Captioning"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "facebookresearch/mmf"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "jokieleung/awesome-visual-question-answering"
"naver-ai/vidt" -> "kakaobrain/sparse-detr"
"naver-ai/vidt" -> "FengLi-ust/DN-DETR"
"naver-ai/vidt" -> "kaist-dmlab/RecencyBias" ["e"=1]
"naver-ai/vidt" -> "kaist-dmlab/TRAP" ["e"=1]
"naver-ai/vidt" -> "naver-ai/eccv-caption" ["e"=1]
"naver-ai/vidt" -> "hustvl/MIMDet" ["e"=1]
"naver-ai/vidt" -> "zhechen/Deformable-DETR-REGO"
"naver-ai/vidt" -> "kaist-dmlab/Hi-COVIDNet" ["e"=1]
"naver-ai/vidt" -> "megvii-research/AnchorDETR"
"naver-ai/vidt" -> "kaist-dmlab/Ada-Boundary" ["e"=1]
"naver-ai/vidt" -> "kaist-dmlab/NETS" ["e"=1]
"naver-ai/vidt" -> "kaist-dmlab/STARE" ["e"=1]
"ZephyrZhuQi/ssbaseline" -> "ronghanghu/mmf"
"ZephyrZhuQi/ssbaseline" -> "yashkant/sam-textvqa"
"ZephyrZhuQi/ssbaseline" -> "guanghuixu/CRN_tvqa"
"gaopengcuhk/Stable-Pix2Seq" -> "gaopengcuhk/Pretrained-Pix2Seq"
"gaopengcuhk/Stable-Pix2Seq" -> "gaopengcuhk/Unofficial-Pix2Seq"
"gaopengcuhk/Stable-Pix2Seq" -> "google-research/pix2seq"
"vacancy/NSCL-PyTorch-Release" -> "kexinyi/ns-vqa"
"vacancy/NSCL-PyTorch-Release" -> "google/neural-logic-machines"
"vacancy/NSCL-PyTorch-Release" -> "stanfordnlp/mac-network"
"vacancy/NSCL-PyTorch-Release" -> "ccclyu/awesome-deeplogic"
"vacancy/NSCL-PyTorch-Release" -> "facebookresearch/clevr-dataset-gen"
"vacancy/NSCL-PyTorch-Release" -> "Glaciohound/VCML"
"vacancy/NSCL-PyTorch-Release" -> "vacancy/Jacinle"
"vacancy/NSCL-PyTorch-Release" -> "crazydonkey200/neural-symbolic-machines" ["e"=1]
"vacancy/NSCL-PyTorch-Release" -> "ExplorerFreda/VGNSL"
"vacancy/NSCL-PyTorch-Release" -> "deepmind/multi_object_datasets"
"vacancy/NSCL-PyTorch-Release" -> "ellisk42/ec"
"vacancy/NSCL-PyTorch-Release" -> "chuangg/CLEVRER"
"vacancy/NSCL-PyTorch-Release" -> "Cadene/murel.bootstrap.pytorch"
"vacancy/NSCL-PyTorch-Release" -> "WellyZhang/RAVEN"
"vacancy/NSCL-PyTorch-Release" -> "facebookresearch/clevr-iep"
"Cyanogenoid/vqa-counting" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"Cyanogenoid/vqa-counting" -> "Cyanogenoid/pytorch-vqa"
"Cyanogenoid/vqa-counting" -> "jnhwkim/ban-vqa"
"Cyanogenoid/vqa-counting" -> "aimbrain/vqa-project"
"Cyanogenoid/vqa-counting" -> "hengyuan-hu/bottom-up-attention-vqa"
"Cyanogenoid/vqa-counting" -> "SinghJasdeep/Attention-on-Attention-for-VQA"
"Cyanogenoid/vqa-counting" -> "yuzcccc/vqa-mfb"
"Cyanogenoid/vqa-counting" -> "Cadene/murel.bootstrap.pytorch"
"Cyanogenoid/vqa-counting" -> "yanxinzju/CSS-VQA"
"Cyanogenoid/vqa-counting" -> "Cadene/block.bootstrap.pytorch"
"Cyanogenoid/vqa-counting" -> "Cadene/vqa.pytorch"
"Cyanogenoid/vqa-counting" -> "batra-mlp-lab/visdial-rl"
"Cyanogenoid/vqa-counting" -> "asdf0982/vqa-mfb.pytorch"
"Cyanogenoid/vqa-counting" -> "JamesChuanggg/awesome-vqa"
"Cyanogenoid/vqa-counting" -> "linjieli222/VQA_ReGAT"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "hengyuan-hu/bottom-up-attention-vqa"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "Cyanogenoid/vqa-counting"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "jnhwkim/ban-vqa"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "MILVLG/mcan-vqa"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "MILVLG/openvqa"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "Cadene/murel.bootstrap.pytorch"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "jokieleung/awesome-visual-question-answering"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "aimbrain/vqa-project"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "Cadene/block.bootstrap.pytorch"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "Cadene/vqa.pytorch"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "Cyanogenoid/pytorch-vqa"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "peteanderson80/bottom-up-attention"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "ronghanghu/lcgn"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "yuzcccc/vqa-mfb"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "linjieli222/VQA_ReGAT"
"doubledaibo/drnet_cvpr2017" -> "GriffinLiang/vrd-dsr"
"doubledaibo/drnet_cvpr2017" -> "Prof-Lu-Cewu/Visual-Relationship-Detection"
"doubledaibo/drnet_cvpr2017" -> "zawlin/cvpr17_vtranse"
"doubledaibo/drnet_cvpr2017" -> "yikang-li/MSDN"
"doubledaibo/drnet_cvpr2017" -> "StanfordVL/ReferringRelationships"
"doubledaibo/drnet_cvpr2017" -> "nexusapoorvacus/DeepVariationStructuredRL"
"doubledaibo/drnet_cvpr2017" -> "yangxuntu/vrd"
"doubledaibo/drnet_cvpr2017" -> "rowanz/neural-motifs"
"doubledaibo/drnet_cvpr2017" -> "facebookresearch/Large-Scale-VRD"
"doubledaibo/drnet_cvpr2017" -> "danfeiX/scene-graph-TF-release"
"jokieleung/awesome-visual-question-answering" -> "MILVLG/mcan-vqa"
"jokieleung/awesome-visual-question-answering" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"jokieleung/awesome-visual-question-answering" -> "MILVLG/openvqa"
"jokieleung/awesome-visual-question-answering" -> "TheShadow29/awesome-grounding" ["e"=1]
"jokieleung/awesome-visual-question-answering" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"jokieleung/awesome-visual-question-answering" -> "xinke-wang/Awesome-Text-VQA"
"jokieleung/awesome-visual-question-answering" -> "forence/Awesome-Visual-Captioning"
"jokieleung/awesome-visual-question-answering" -> "peteanderson80/bottom-up-attention"
"jokieleung/awesome-visual-question-answering" -> "airsplay/lxmert"
"jokieleung/awesome-visual-question-answering" -> "linjieli222/VQA_ReGAT"
"jokieleung/awesome-visual-question-answering" -> "facebookresearch/grid-feats-vqa"
"jokieleung/awesome-visual-question-answering" -> "Cadene/vqa.pytorch"
"jokieleung/awesome-visual-question-answering" -> "chingyaoc/awesome-vqa"
"jokieleung/awesome-visual-question-answering" -> "hengyuan-hu/bottom-up-attention-vqa"
"jokieleung/awesome-visual-question-answering" -> "Cadene/block.bootstrap.pytorch"
"HughChi/Image-Caption" -> "foamliu/Image-Captioning"
"HughChi/Image-Caption" -> "foamliu/Image-Captioning-PyTorch"
"HughChi/Image-Caption" -> "li-xirong/cross-lingual-cap"
"HughChi/Image-Caption" -> "weiyuk/fluent-cap"
"HughChi/Image-Caption" -> "showkeyjar/chinese_im2text.pytorch"
"Zasder3/train-CLIP" -> "FreddeFrallan/Multilingual-CLIP"
"Zasder3/train-CLIP" -> "mlfoundations/open_clip"
"Zasder3/train-CLIP" -> "lucidrains/x-clip"
"Zasder3/train-CLIP" -> "Zasder3/train-CLIP-FT"
"Zasder3/train-CLIP" -> "KaiyangZhou/CoOp"
"Zasder3/train-CLIP" -> "Sense-GVT/DeCLIP"
"Zasder3/train-CLIP" -> "facebookresearch/SLIP"
"Zasder3/train-CLIP" -> "rmokady/CLIP_prefix_caption"
"Zasder3/train-CLIP" -> "revantteotia/clip-training"
"Zasder3/train-CLIP" -> "gaopengcuhk/CLIP-Adapter"
"Zasder3/train-CLIP" -> "salesforce/ALBEF"
"Zasder3/train-CLIP" -> "haofanwang/natural-language-joint-query-search"
"Zasder3/train-CLIP" -> "mlfoundations/wise-ft"
"Zasder3/train-CLIP" -> "ArrowLuo/CLIP4Clip" ["e"=1]
"Zasder3/train-CLIP" -> "gaopengcuhk/Tip-Adapter"
"billjie1/Chinese-CLIP" -> "yuxie11/R2D2"
"billjie1/Chinese-CLIP" -> "yangjianxin1/ClipCap-Chinese"
"billjie1/Chinese-CLIP" -> "DtYXs/Chinese-CLIP"
"foamliu/Image-Captioning" -> "HughChi/Image-Caption"
"foamliu/Image-Captioning" -> "foamliu/Image-Captioning-PyTorch"
"foamliu/Image-Captioning" -> "showkeyjar/chinese_im2text.pytorch"
"foamliu/Image-Captioning" -> "foamliu/Image-Captioning-v2"
"foamliu/Image-Captioning" -> "li-xirong/cross-lingual-cap"
"foamliu/Image-Captioning" -> "TimoYoung/im2txt_Chinese"
"lucidrains/CoCa-pytorch" -> "salesforce/ALBEF"
"lucidrains/CoCa-pytorch" -> "lucidrains/flamingo-pytorch"
"lucidrains/CoCa-pytorch" -> "facebookresearch/multimodal"
"lucidrains/CoCa-pytorch" -> "facebookresearch/SLIP"
"lucidrains/CoCa-pytorch" -> "zengyan-97/X-VLM"
"lucidrains/CoCa-pytorch" -> "OFA-Sys/OFA"
"lucidrains/CoCa-pytorch" -> "salesforce/BLIP"
"lucidrains/CoCa-pytorch" -> "baaivision/EVA"
"lucidrains/CoCa-pytorch" -> "mlfoundations/open_clip"
"lucidrains/CoCa-pytorch" -> "Sense-GVT/DeCLIP"
"lucidrains/CoCa-pytorch" -> "salesforce/LAVIS"
"lucidrains/CoCa-pytorch" -> "microsoft/GLIP"
"lucidrains/CoCa-pytorch" -> "google-research/big_vision"
"lucidrains/CoCa-pytorch" -> "dandelin/ViLT"
"lucidrains/CoCa-pytorch" -> "KaiyangZhou/CoOp"
"microsoft/GenerativeImage2Text" -> "microsoft/TAP"
"microsoft/GenerativeImage2Text" -> "microsoft/SwinBERT" ["e"=1]
"microsoft/GenerativeImage2Text" -> "JialianW/GRiT" ["e"=1]
"microsoft/GenerativeImage2Text" -> "microsoft/UniTAB"
"microsoft/GenerativeImage2Text" -> "davidnvq/grit"
"microsoft/GenerativeImage2Text" -> "microsoft/azfuse"
"microsoft/GenerativeImage2Text" -> "microsoft/UniVL" ["e"=1]
"microsoft/GenerativeImage2Text" -> "MikeWangWZHL/VidIL" ["e"=1]
"microsoft/GenerativeImage2Text" -> "zengyan-97/X-VLM"
"ronghanghu/mmf" -> "ZephyrZhuQi/ssbaseline"
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "facebookresearch/grid-feats-vqa"
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "yahoo/object_relation_transformer"
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "vacancy/SceneGraphParser"
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "yangxuntu/SGAE"
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "KunpengLi1994/VSRN" ["e"=1]
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "peteanderson80/bottom-up-attention"
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "forence/Awesome-Visual-Captioning"
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "airsplay/py-bottom-up-attention"
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "peteanderson80/Up-Down-Captioner"
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "ranjaykrishna/visual_genome_python_driver"
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "MILVLG/bottom-up-attention.pytorch"
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "JDAI-CV/image-captioning"
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "BryanPlummer/flickr30k_entities" ["e"=1]
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "jackroos/VL-BERT"
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "husthuaan/AoANet"
"raoyongming/DenseCLIP" -> "chongzhou96/MaskCLIP"
"raoyongming/DenseCLIP" -> "isl-org/lang-seg"
"raoyongming/DenseCLIP" -> "KaiyangZhou/CoOp"
"raoyongming/DenseCLIP" -> "Sense-GVT/DeCLIP"
"raoyongming/DenseCLIP" -> "timojl/clipseg"
"raoyongming/DenseCLIP" -> "gaopengcuhk/Tip-Adapter"
"raoyongming/DenseCLIP" -> "MendelXu/zsseg.baseline"
"raoyongming/DenseCLIP" -> "gaopengcuhk/CLIP-Adapter"
"raoyongming/DenseCLIP" -> "sallymmx/ActionCLIP" ["e"=1]
"raoyongming/DenseCLIP" -> "facebookresearch/SLIP"
"raoyongming/DenseCLIP" -> "NVlabs/GroupViT"
"raoyongming/DenseCLIP" -> "microsoft/GLIP"
"raoyongming/DenseCLIP" -> "ashkamath/mdetr"
"raoyongming/DenseCLIP" -> "clip-vil/CLIP-ViL"
"raoyongming/DenseCLIP" -> "alirezazareian/ovr-cnn"
"DingXiaoH/RepLKNet-pytorch" -> "megvii-research/RepLKNet"
"DingXiaoH/RepLKNet-pytorch" -> "MegEngine/RepLKNet"
"DingXiaoH/RepLKNet-pytorch" -> "facebookresearch/ConvNeXt-V2"
"DingXiaoH/RepLKNet-pytorch" -> "Visual-Attention-Network/VAN-Classification" ["e"=1]
"DingXiaoH/RepLKNet-pytorch" -> "VITA-Group/SLaK"
"DingXiaoH/RepLKNet-pytorch" -> "Visual-Attention-Network/SegNeXt"
"DingXiaoH/RepLKNet-pytorch" -> "raoyongming/HorNet"
"DingXiaoH/RepLKNet-pytorch" -> "facebookresearch/ConvNeXt" ["e"=1]
"DingXiaoH/RepLKNet-pytorch" -> "DingXiaoH/RepVGG" ["e"=1]
"DingXiaoH/RepLKNet-pytorch" -> "OpenGVLab/InternImage"
"DingXiaoH/RepLKNet-pytorch" -> "swz30/Restormer" ["e"=1]
"DingXiaoH/RepLKNet-pytorch" -> "OliverRensu/Shunted-Transformer" ["e"=1]
"DingXiaoH/RepLKNet-pytorch" -> "Sense-X/UniFormer" ["e"=1]
"DingXiaoH/RepLKNet-pytorch" -> "xxxnell/how-do-vits-work" ["e"=1]
"DingXiaoH/RepLKNet-pytorch" -> "megvii-research/RevCol"
"FreddeFrallan/Multilingual-CLIP" -> "Zasder3/train-CLIP"
"FreddeFrallan/Multilingual-CLIP" -> "rom1504/clip-retrieval"
"FreddeFrallan/Multilingual-CLIP" -> "rom1504/img2dataset"
"FreddeFrallan/Multilingual-CLIP" -> "lucidrains/x-clip"
"FreddeFrallan/Multilingual-CLIP" -> "mlfoundations/open_clip"
"FreddeFrallan/Multilingual-CLIP" -> "BAAI-WuDao/BriVL"
"FreddeFrallan/Multilingual-CLIP" -> "kakaobrain/coyo-dataset"
"FreddeFrallan/Multilingual-CLIP" -> "rmokady/CLIP_prefix_caption"
"FreddeFrallan/Multilingual-CLIP" -> "facebookresearch/SLIP"
"FreddeFrallan/Multilingual-CLIP" -> "google-research-datasets/wit"
"FreddeFrallan/Multilingual-CLIP" -> "moein-shariatnia/OpenAI-CLIP"
"FreddeFrallan/Multilingual-CLIP" -> "salesforce/ALBEF"
"FreddeFrallan/Multilingual-CLIP" -> "OFA-Sys/OFA"
"FreddeFrallan/Multilingual-CLIP" -> "criteo/autofaiss"
"FreddeFrallan/Multilingual-CLIP" -> "webdataset/webdataset"
"google-research/pix2seq" -> "gaopengcuhk/Stable-Pix2Seq"
"google-research/pix2seq" -> "microsoft/X-Decoder"
"google-research/pix2seq" -> "gaopengcuhk/Pretrained-Pix2Seq"
"google-research/pix2seq" -> "microsoft/GLIP"
"google-research/pix2seq" -> "OFA-Sys/OFA"
"google-research/pix2seq" -> "hustvl/MIMDet" ["e"=1]
"google-research/pix2seq" -> "NVlabs/GroupViT"
"google-research/pix2seq" -> "gaopengcuhk/Unofficial-Pix2Seq"
"google-research/pix2seq" -> "fundamentalvision/Uni-Perceiver"
"google-research/pix2seq" -> "google-research/big_vision"
"google-research/pix2seq" -> "facebookresearch/Detic"
"google-research/pix2seq" -> "raoyongming/DenseCLIP"
"google-research/pix2seq" -> "KaiyangZhou/CoOp"
"google-research/pix2seq" -> "IDEACVR/awesome-detection-transformer"
"google-research/pix2seq" -> "facebookresearch/omnivore" ["e"=1]
"IDEA-opensource/DN-DETR" -> "IDEA-opensource/DAB-DETR"
"IDEA-opensource/DN-DETR" -> "Atten4Vis/ConditionalDETR"
"IDEA-opensource/DN-DETR" -> "IDEACVR/MaskDINO"
"IDEA-opensource/DN-DETR" -> "MCG-NJU/AdaMixer"
"IDEA-opensource/DN-DETR" -> "IDEACVR/DINO"
"IDEA-opensource/DN-DETR" -> "IDEACVR/awesome-detection-transformer"
"IDEA-opensource/DN-DETR" -> "jshilong/DDQ"
"IDEA-opensource/DN-DETR" -> "jozhang97/DETA"
"IDEA-opensource/DN-DETR" -> "HDETR/H-Deformable-DETR"
"raoyongming/HorNet" -> "dingmyu/davit"
"raoyongming/HorNet" -> "Visual-Attention-Network/SegNeXt"
"raoyongming/HorNet" -> "yuhuan-wu/P2T"
"raoyongming/HorNet" -> "VITA-Group/SLaK"
"raoyongming/HorNet" -> "megvii-research/RevCol"
"raoyongming/HorNet" -> "ziplab/LITv2" ["e"=1]
"raoyongming/HorNet" -> "DingXiaoH/RepLKNet-pytorch"
"raoyongming/HorNet" -> "Atten4Vis/DemystifyLocalViT"
"raoyongming/HorNet" -> "microsoft/Focal-Transformer" ["e"=1]
"raoyongming/HorNet" -> "SHI-Labs/Neighborhood-Attention-Transformer"
"raoyongming/HorNet" -> "facebookresearch/ConvNeXt-V2"
"raoyongming/HorNet" -> "okojoalg/sequencer"
"raoyongming/HorNet" -> "fcjian/TOOD" ["e"=1]
"raoyongming/HorNet" -> "hkzhang91/ParC-Net" ["e"=1]
"ashkamath/mdetr" -> "microsoft/GLIP"
"ashkamath/mdetr" -> "TheShadow29/awesome-grounding" ["e"=1]
"ashkamath/mdetr" -> "microsoft/Oscar"
"ashkamath/mdetr" -> "salesforce/ALBEF"
"ashkamath/mdetr" -> "facebookresearch/Detic"
"ashkamath/mdetr" -> "raoyongming/DenseCLIP"
"ashkamath/mdetr" -> "clip-vil/CLIP-ViL"
"ashkamath/mdetr" -> "microsoft/RegionCLIP"
"ashkamath/mdetr" -> "isl-org/lang-seg"
"ashkamath/mdetr" -> "jayleicn/ClipBERT"
"ashkamath/mdetr" -> "facebookresearch/SLIP"
"ashkamath/mdetr" -> "Sense-GVT/DeCLIP"
"ashkamath/mdetr" -> "dandelin/ViLT"
"ashkamath/mdetr" -> "mmaaz60/mvits_for_class_agnostic_od"
"ashkamath/mdetr" -> "lichengunc/refer" ["e"=1]
"facebookresearch/SLIP" -> "Sense-GVT/DeCLIP"
"facebookresearch/SLIP" -> "lucidrains/x-clip"
"facebookresearch/SLIP" -> "raoyongming/DenseCLIP"
"facebookresearch/SLIP" -> "microsoft/GLIP"
"facebookresearch/SLIP" -> "KaiyangZhou/CoOp"
"facebookresearch/SLIP" -> "kakaobrain/coyo-dataset"
"facebookresearch/SLIP" -> "salesforce/ALBEF"
"facebookresearch/SLIP" -> "ashkamath/mdetr"
"facebookresearch/SLIP" -> "facebookresearch/Detic"
"facebookresearch/SLIP" -> "facebookresearch/moco-v3" ["e"=1]
"facebookresearch/SLIP" -> "baaivision/EVA"
"facebookresearch/SLIP" -> "lucidrains/CoCa-pytorch"
"facebookresearch/SLIP" -> "bytedance/ibot" ["e"=1]
"facebookresearch/SLIP" -> "rom1504/img2dataset"
"facebookresearch/SLIP" -> "microsoft/UniCL"
"pliang279/awesome-multimodal-ml" -> "Eurus-Holmes/Awesome-Multimodal-Research"
"pliang279/awesome-multimodal-ml" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"pliang279/awesome-multimodal-ml" -> "facebookresearch/mmf"
"pliang279/awesome-multimodal-ml" -> "jason718/awesome-self-supervised-learning" ["e"=1]
"pliang279/awesome-multimodal-ml" -> "A2Zadeh/CMU-MultimodalSDK" ["e"=1]
"pliang279/awesome-multimodal-ml" -> "salesforce/ALBEF"
"pliang279/awesome-multimodal-ml" -> "openai/CLIP" ["e"=1]
"pliang279/awesome-multimodal-ml" -> "TheShadow29/awesome-grounding" ["e"=1]
"pliang279/awesome-multimodal-ml" -> "thunlp/PromptPapers" ["e"=1]
"pliang279/awesome-multimodal-ml" -> "airsplay/lxmert"
"pliang279/awesome-multimodal-ml" -> "dk-liang/Awesome-Visual-Transformer" ["e"=1]
"pliang279/awesome-multimodal-ml" -> "yaohungt/Multimodal-Transformer" ["e"=1]
"pliang279/awesome-multimodal-ml" -> "microsoft/unilm" ["e"=1]
"pliang279/awesome-multimodal-ml" -> "facebookresearch/SlowFast" ["e"=1]
"pliang279/awesome-multimodal-ml" -> "facebookresearch/moco" ["e"=1]
"IDEA-Research/awesome-detection-transformer" -> "IDEA-Research/detrex"
"IDEA-Research/awesome-detection-transformer" -> "HDETR/H-Deformable-DETR"
"IDEA-Research/awesome-detection-transformer" -> "IDEA-Research/DINO"
"IDEA-Research/awesome-detection-transformer" -> "IDEA-Research/MaskDINO"
"IDEA-Research/awesome-detection-transformer" -> "jozhang97/DETA"
"IDEA-Research/awesome-detection-transformer" -> "IDEA-Research/DN-DETR"
"GT-Vision-Lab/VQA" -> "GT-Vision-Lab/VQA_LSTM_CNN"
"GT-Vision-Lab/VQA" -> "jiasenlu/HieCoAttenVQA"
"GT-Vision-Lab/VQA" -> "hengyuan-hu/bottom-up-attention-vqa"
"GT-Vision-Lab/VQA" -> "Cadene/vqa.pytorch"
"GT-Vision-Lab/VQA" -> "Cadene/block.bootstrap.pytorch"
"GT-Vision-Lab/VQA" -> "jnhwkim/ban-vqa"
"GT-Vision-Lab/VQA" -> "facebookresearch/grid-feats-vqa"
"GT-Vision-Lab/VQA" -> "Cyanogenoid/pytorch-vqa"
"GT-Vision-Lab/VQA" -> "iamaaditya/VQA_Demo"
"GT-Vision-Lab/VQA" -> "MILVLG/mcan-vqa"
"GT-Vision-Lab/VQA" -> "yuzcccc/vqa-mfb"
"GT-Vision-Lab/VQA" -> "JamesChuanggg/awesome-vqa"
"GT-Vision-Lab/VQA" -> "JamesChuanggg/VQA-tensorflow"
"GT-Vision-Lab/VQA" -> "AishwaryaAgrawal/GVQA"
"GT-Vision-Lab/VQA" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"metalbubble/VQAbaseline" -> "VT-vision-lab/VQA_LSTM_CNN"
"metalbubble/VQAbaseline" -> "JamesChuanggg/VQA-tensorflow"
"metalbubble/VQAbaseline" -> "VT-vision-lab/VQA"
"metalbubble/VQAbaseline" -> "jiasenlu/HieCoAttenVQA"
"metalbubble/VQAbaseline" -> "HyeonwooNoh/DPPnet"
"metalbubble/VQAbaseline" -> "akirafukui/vqa-mcb"
"metalbubble/VQAbaseline" -> "JamesChuanggg/awesome-vqa"
"metalbubble/VQAbaseline" -> "yukezhu/visual7w-qa-models"
"metalbubble/VQAbaseline" -> "iamaaditya/VQA_Demo"
"criteo/autofaiss" -> "rom1504/clip-retrieval"
"criteo/autofaiss" -> "facebookresearch/distributed-faiss"
"criteo/autofaiss" -> "SeanNaren/minGPT" ["e"=1]
"criteo/autofaiss" -> "facebookresearch/contriever" ["e"=1]
"criteo/autofaiss" -> "lucidrains/RETRO-pytorch" ["e"=1]
"criteo/autofaiss" -> "matsui528/faiss_tips"
"criteo/autofaiss" -> "FreddeFrallan/Multilingual-CLIP"
"criteo/autofaiss" -> "rom1504/img2dataset"
"criteo/autofaiss" -> "webdataset/webdataset"
"criteo/autofaiss" -> "currentslab/awesome-vector-search"
"Cyanogenoid/pytorch-vqa" -> "Cadene/vqa.pytorch"
"Cyanogenoid/pytorch-vqa" -> "Cyanogenoid/vqa-counting"
"Cyanogenoid/pytorch-vqa" -> "hengyuan-hu/bottom-up-attention-vqa"
"Cyanogenoid/pytorch-vqa" -> "markdtw/vqa-winner-cvprw-2017"
"Cyanogenoid/pytorch-vqa" -> "aimbrain/vqa-project"
"Cyanogenoid/pytorch-vqa" -> "Cadene/block.bootstrap.pytorch"
"Cyanogenoid/pytorch-vqa" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"Cyanogenoid/pytorch-vqa" -> "jnhwkim/ban-vqa"
"Cyanogenoid/pytorch-vqa" -> "Shivanshu-Gupta/Visual-Question-Answering"
"Cyanogenoid/pytorch-vqa" -> "tbmoon/basic_vqa" ["e"=1]
"Cyanogenoid/pytorch-vqa" -> "GT-Vision-Lab/VQA_LSTM_CNN"
"Cyanogenoid/pytorch-vqa" -> "DenisDsh/VizWiz-VQA-PyTorch"
"Cyanogenoid/pytorch-vqa" -> "GT-Vision-Lab/VQA"
"Cyanogenoid/pytorch-vqa" -> "SinghJasdeep/Attention-on-Attention-for-VQA"
"Cyanogenoid/pytorch-vqa" -> "yuzcccc/vqa-mfb"
"clip-vil/CLIP-ViL" -> "YicongHong/Recurrent-VLN-BERT" ["e"=1]
"clip-vil/CLIP-ViL" -> "airsplay/R2R-EnvDrop" ["e"=1]
"clip-vil/CLIP-ViL" -> "cshizhe/VLN-DUET" ["e"=1]
"clip-vil/CLIP-ViL" -> "cshizhe/VLN-HAMT" ["e"=1]
"clip-vil/CLIP-ViL" -> "pzzhang/VinVL"
"clip-vil/CLIP-ViL" -> "weituo12321/PREVALENT" ["e"=1]
"clip-vil/CLIP-ViL" -> "microsoft/Oscar"
"clip-vil/CLIP-ViL" -> "facebookresearch/grid-feats-vqa"
"clip-vil/CLIP-ViL" -> "jialuli-luka/EnvEdit" ["e"=1]
"clip-vil/CLIP-ViL" -> "zdou0830/METER"
"clip-vil/CLIP-ViL" -> "salesforce/ALBEF"
"clip-vil/CLIP-ViL" -> "ashkamath/mdetr"
"clip-vil/CLIP-ViL" -> "jayleicn/ClipBERT"
"clip-vil/CLIP-ViL" -> "researchmm/soho"
"clip-vil/CLIP-ViL" -> "raoyongming/DenseCLIP"
"peteanderson80/Up-Down-Captioner" -> "peteanderson80/bottom-up-attention"
"peteanderson80/Up-Down-Captioner" -> "poojahira/image-captioning-bottom-up-top-down"
"peteanderson80/Up-Down-Captioner" -> "hengyuan-hu/bottom-up-attention-vqa"
"peteanderson80/Up-Down-Captioner" -> "ruotianluo/self-critical.pytorch"
"peteanderson80/Up-Down-Captioner" -> "jiasenlu/NeuralBabyTalk"
"peteanderson80/Up-Down-Captioner" -> "jiasenlu/AdaptiveAttention"
"peteanderson80/Up-Down-Captioner" -> "aimagelab/show-control-and-tell"
"peteanderson80/Up-Down-Captioner" -> "yangxuntu/SGAE"
"peteanderson80/Up-Down-Captioner" -> "fengyang0317/unsupervised_captioning"
"peteanderson80/Up-Down-Captioner" -> "husthuaan/AoANet"
"peteanderson80/Up-Down-Captioner" -> "ruotianluo/DiscCaptioning"
"peteanderson80/Up-Down-Captioner" -> "richardaecn/cvpr18-caption-eval"
"peteanderson80/Up-Down-Captioner" -> "s-gupta/visual-concepts"
"peteanderson80/Up-Down-Captioner" -> "violetteshev/bottom-up-features"
"peteanderson80/Up-Down-Captioner" -> "ruotianluo/Image_Captioning_AI_Challenger"
"salesforce/ALBEF" -> "dandelin/ViLT"
"salesforce/ALBEF" -> "salesforce/BLIP"
"salesforce/ALBEF" -> "microsoft/Oscar"
"salesforce/ALBEF" -> "ChenRocks/UNITER"
"salesforce/ALBEF" -> "zdou0830/METER"
"salesforce/ALBEF" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"salesforce/ALBEF" -> "zengyan-97/X-VLM"
"salesforce/ALBEF" -> "jayleicn/ClipBERT"
"salesforce/ALBEF" -> "uta-smile/TCL"
"salesforce/ALBEF" -> "KaiyangZhou/CoOp"
"salesforce/ALBEF" -> "OFA-Sys/OFA"
"salesforce/ALBEF" -> "facebookresearch/multimodal"
"salesforce/ALBEF" -> "ArrowLuo/CLIP4Clip" ["e"=1]
"salesforce/ALBEF" -> "ashkamath/mdetr"
"salesforce/ALBEF" -> "airsplay/lxmert"
"lucidrains/slot-attention" -> "evelinehong/slot-attention-pytorch"
"lucidrains/slot-attention" -> "deepmind/multi_object_datasets"
"lucidrains/slot-attention" -> "untitled-ai/slot_attention"
"lucidrains/slot-attention" -> "singhgautam/slate"
"lucidrains/slot-attention" -> "applied-ai-lab/genesis"
"lucidrains/slot-attention" -> "tkipf/c-swm"
"lucidrains/slot-attention" -> "addtt/object-centric-library"
"lucidrains/slot-attention" -> "baudm/MONet-pytorch"
"lucidrains/slot-attention" -> "google-research/slot-attention-video"
"lucidrains/slot-attention" -> "charigyang/motiongrouping"
"lucidrains/slot-attention" -> "zhixuan-lin/IODINE"
"lucidrains/slot-attention" -> "zhixuan-lin/SPACE"
"lucidrains/slot-attention" -> "wbw520/scouter"
"lucidrains/slot-attention" -> "ajabri/videowalk" ["e"=1]
"lucidrains/slot-attention" -> "zhixuan-lin/G-SWM"
"researchmm/soho" -> "facebookresearch/grid-feats-vqa"
"researchmm/soho" -> "zdou0830/METER"
"researchmm/soho" -> "ylsung/VL_adapter"
"researchmm/soho" -> "microsoft/Oscar"
"researchmm/soho" -> "clip-vil/CLIP-ViL"
"researchmm/soho" -> "pzzhang/VinVL"
"researchmm/soho" -> "salesforce/ALBEF"
"rmokady/CLIP_prefix_caption" -> "OFA-Sys/OFA"
"rmokady/CLIP_prefix_caption" -> "YoadTew/zero-shot-image-to-text" ["e"=1]
"rmokady/CLIP_prefix_caption" -> "KaiyangZhou/CoOp"
"rmokady/CLIP_prefix_caption" -> "clip-vil/CLIP-ViL"
"rmokady/CLIP_prefix_caption" -> "microsoft/GLIP"
"rmokady/CLIP_prefix_caption" -> "microsoft/Oscar"
"rmokady/CLIP_prefix_caption" -> "salesforce/ALBEF"
"rmokady/CLIP_prefix_caption" -> "salesforce/BLIP"
"rmokady/CLIP_prefix_caption" -> "j-min/CLIP-Caption-Reward"
"rmokady/CLIP_prefix_caption" -> "Zasder3/train-CLIP"
"rmokady/CLIP_prefix_caption" -> "mlfoundations/open_clip"
"rmokady/CLIP_prefix_caption" -> "facebookresearch/SLIP"
"rmokady/CLIP_prefix_caption" -> "microsoft/RegionCLIP"
"rmokady/CLIP_prefix_caption" -> "FreddeFrallan/Multilingual-CLIP"
"rmokady/CLIP_prefix_caption" -> "galatolofederico/clip-glass"
"RE-OWOD/RE-OWOD" -> "csuhan/opendet2"
"RE-OWOD/RE-OWOD" -> "akshitac8/OW-DETR"
"ZrrSkywalker/CaFo" -> "gaopengcuhk/Tip-Adapter"
"ZrrSkywalker/CaFo" -> "wl-zhao/VPD" ["e"=1]
"ZrrSkywalker/CaFo" -> "gaopengcuhk/CLIP-Adapter"
"microsoft/GLIP" -> "ashkamath/mdetr"
"microsoft/GLIP" -> "microsoft/RegionCLIP"
"microsoft/GLIP" -> "facebookresearch/Detic"
"microsoft/GLIP" -> "baaivision/EVA"
"microsoft/GLIP" -> "OFA-Sys/OFA"
"microsoft/GLIP" -> "salesforce/BLIP"
"microsoft/GLIP" -> "KaiyangZhou/CoOp"
"microsoft/GLIP" -> "isl-org/lang-seg"
"microsoft/GLIP" -> "NVlabs/GroupViT"
"microsoft/GLIP" -> "microsoft/X-Decoder"
"microsoft/GLIP" -> "raoyongming/DenseCLIP"
"microsoft/GLIP" -> "facebookresearch/SLIP"
"microsoft/GLIP" -> "salesforce/LAVIS"
"microsoft/GLIP" -> "salesforce/ALBEF"
"microsoft/GLIP" -> "ShoufaChen/DiffusionDet"
"sail-sg/iFormer" -> "sail-sg/metaformer"
"sail-sg/iFormer" -> "ziplab/LITv2" ["e"=1]
"MILVLG/bottom-up-attention.pytorch" -> "airsplay/py-bottom-up-attention"
"MILVLG/bottom-up-attention.pytorch" -> "peteanderson80/bottom-up-attention"
"MILVLG/bottom-up-attention.pytorch" -> "cshizhe/asg2cap"
"MILVLG/bottom-up-attention.pytorch" -> "facebookresearch/grid-feats-vqa"
"MILVLG/bottom-up-attention.pytorch" -> "yangxuntu/SGAE"
"MILVLG/bottom-up-attention.pytorch" -> "MILVLG/mcan-vqa"
"MILVLG/bottom-up-attention.pytorch" -> "MILVLG/rosita"
"MILVLG/bottom-up-attention.pytorch" -> "violetteshev/bottom-up-features"
"MILVLG/bottom-up-attention.pytorch" -> "forence/Awesome-Visual-Captioning"
"MILVLG/bottom-up-attention.pytorch" -> "poojahira/image-captioning-bottom-up-top-down"
"MILVLG/bottom-up-attention.pytorch" -> "microsoft/scene_graph_benchmark"
"MILVLG/bottom-up-attention.pytorch" -> "yahoo/object_relation_transformer"
"MILVLG/bottom-up-attention.pytorch" -> "hengyuan-hu/bottom-up-attention-vqa"
"MILVLG/bottom-up-attention.pytorch" -> "ChenRocks/UNITER"
"MILVLG/bottom-up-attention.pytorch" -> "microsoft/Oscar"
"YIKUAN8/Transformers-VQA" -> "airsplay/py-bottom-up-attention"
"YIKUAN8/Transformers-VQA" -> "abachaa/VQA-Med-2019"
"YIKUAN8/Transformers-VQA" -> "uclanlp/visualbert"
"YIKUAN8/Transformers-VQA" -> "UCSD-AI4H/PathVQA"
"jackroos/VL-BERT" -> "facebookresearch/vilbert-multi-task"
"jackroos/VL-BERT" -> "airsplay/lxmert"
"jackroos/VL-BERT" -> "ChenRocks/UNITER"
"jackroos/VL-BERT" -> "jiasenlu/vilbert_beta"
"jackroos/VL-BERT" -> "uclanlp/visualbert"
"jackroos/VL-BERT" -> "LuoweiZhou/VLP"
"jackroos/VL-BERT" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"jackroos/VL-BERT" -> "microsoft/Oscar"
"jackroos/VL-BERT" -> "peteanderson80/bottom-up-attention"
"jackroos/VL-BERT" -> "jayleicn/ClipBERT"
"jackroos/VL-BERT" -> "ruotianluo/self-critical.pytorch"
"jackroos/VL-BERT" -> "facebookresearch/grid-feats-vqa"
"jackroos/VL-BERT" -> "KaihuaTang/Scene-Graph-Benchmark.pytorch"
"jackroos/VL-BERT" -> "salesforce/ALBEF"
"jackroos/VL-BERT" -> "MILVLG/mcan-vqa"
"pzzhang/VinVL" -> "microsoft/scene_graph_benchmark"
"pzzhang/VinVL" -> "microsoft/Oscar"
"pzzhang/VinVL" -> "facebookresearch/grid-feats-vqa"
"pzzhang/VinVL" -> "airsplay/py-bottom-up-attention"
"pzzhang/VinVL" -> "clip-vil/CLIP-ViL"
"pzzhang/VinVL" -> "ChenRocks/UNITER"
"pzzhang/VinVL" -> "LuoweiZhou/VLP"
"pzzhang/VinVL" -> "aimagelab/meshed-memory-transformer"
"pzzhang/VinVL" -> "researchmm/soho"
"pzzhang/VinVL" -> "salesforce/ALBEF"
"pzzhang/VinVL" -> "microsoft/TAP"
"pzzhang/VinVL" -> "KaihuaTang/Scene-Graph-Benchmark.pytorch"
"pzzhang/VinVL" -> "jokieleung/awesome-visual-question-answering"
"pzzhang/VinVL" -> "linjieli222/VQA_ReGAT"
"pzzhang/VinVL" -> "peteanderson80/bottom-up-attention"
"saahiluppal/catr" -> "aimagelab/meshed-memory-transformer"
"saahiluppal/catr" -> "krasserm/fairseq-image-captioning"
"saahiluppal/catr" -> "RoyalSkye/Image-Caption"
"saahiluppal/catr" -> "luo3300612/image-captioning-DLCT"
"saahiluppal/catr" -> "yahoo/object_relation_transformer"
"saahiluppal/catr" -> "zarzouram/image_captioning_with_transformers"
"saahiluppal/catr" -> "YiwuZhong/Sub-GC"
"saahiluppal/catr" -> "zhangxuying1004/RSTNet"
"forence/Awesome-Visual-Captioning" -> "tgc1997/Awesome-Video-Captioning" ["e"=1]
"forence/Awesome-Visual-Captioning" -> "JDAI-CV/image-captioning"
"forence/Awesome-Visual-Captioning" -> "yangxuntu/SGAE"
"forence/Awesome-Visual-Captioning" -> "jayleicn/recurrent-transformer" ["e"=1]
"forence/Awesome-Visual-Captioning" -> "zhjohnchan/awesome-image-captioning"
"forence/Awesome-Visual-Captioning" -> "tgc1997/RMN" ["e"=1]
"forence/Awesome-Visual-Captioning" -> "aimagelab/meshed-memory-transformer"
"forence/Awesome-Visual-Captioning" -> "yangbang18/Non-Autoregressive-Video-Captioning" ["e"=1]
"forence/Awesome-Visual-Captioning" -> "husthuaan/AoANet"
"forence/Awesome-Visual-Captioning" -> "facebookresearch/grounded-video-description" ["e"=1]
"forence/Awesome-Visual-Captioning" -> "luo3300612/image-captioning-DLCT"
"forence/Awesome-Visual-Captioning" -> "cshizhe/asg2cap"
"forence/Awesome-Visual-Captioning" -> "terry-r123/Awesome-Captioning"
"forence/Awesome-Visual-Captioning" -> "SydCaption/SAAT" ["e"=1]
"forence/Awesome-Visual-Captioning" -> "xiadingZ/video-caption.pytorch" ["e"=1]
"krasserm/fairseq-image-captioning" -> "yahoo/object_relation_transformer"
"krasserm/fairseq-image-captioning" -> "aimagelab/meshed-memory-transformer"
"krasserm/fairseq-image-captioning" -> "ruotianluo/self-critical.pytorch"
"krasserm/fairseq-image-captioning" -> "JDAI-CV/image-captioning"
"krasserm/fairseq-image-captioning" -> "husthuaan/AoANet"
"krasserm/fairseq-image-captioning" -> "saahiluppal/catr"
"krasserm/fairseq-image-captioning" -> "wtliao/ImageTransformer"
"krasserm/fairseq-image-captioning" -> "yangxuntu/SGAE"
"krasserm/fairseq-image-captioning" -> "ruotianluo/ImageCaptioning.pytorch"
"krasserm/fairseq-image-captioning" -> "cshizhe/asg2cap"
"krasserm/fairseq-image-captioning" -> "LuoweiZhou/VLP"
"krasserm/fairseq-image-captioning" -> "RoyalSkye/Image-Caption"
"krasserm/fairseq-image-captioning" -> "zhjohnchan/awesome-image-captioning"
"krasserm/fairseq-image-captioning" -> "fawazsammani/show-edit-tell"
"krasserm/fairseq-image-captioning" -> "luo3300612/image-captioning-DLCT"
"ShannonAI/OpenViDial" -> "gsh199449/stickerchat"
"ShannonAI/OpenViDial" -> "jokieleung/Maria"
"ShannonAI/OpenViDial" -> "vmurahari3/visdial-bert"
"ShannonAI/OpenViDial" -> "henryhungle/MTN"
"google-research/slot-attention-video" -> "evelinehong/slot-attention-pytorch"
"google-research/slot-attention-video" -> "YuLiu-LY/BO-QSA"
"google-research/slot-attention-video" -> "junkeun-yi/SAVi-pytorch"
"google-research/slot-attention-video" -> "singhgautam/steve"
"facebookresearch/dropout" -> "facebookresearch/ConvNeXt-V2"
"facebookresearch/vilbert-multi-task" -> "jiasenlu/vilbert_beta"
"facebookresearch/vilbert-multi-task" -> "jackroos/VL-BERT"
"facebookresearch/vilbert-multi-task" -> "airsplay/lxmert"
"facebookresearch/vilbert-multi-task" -> "ChenRocks/UNITER"
"facebookresearch/vilbert-multi-task" -> "uclanlp/visualbert"
"facebookresearch/vilbert-multi-task" -> "microsoft/Oscar"
"facebookresearch/vilbert-multi-task" -> "LuoweiZhou/VLP"
"facebookresearch/vilbert-multi-task" -> "peteanderson80/bottom-up-attention"
"facebookresearch/vilbert-multi-task" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"facebookresearch/vilbert-multi-task" -> "dandelin/ViLT"
"facebookresearch/vilbert-multi-task" -> "facebookresearch/mmf"
"facebookresearch/vilbert-multi-task" -> "jayleicn/ClipBERT"
"facebookresearch/vilbert-multi-task" -> "airsplay/py-bottom-up-attention"
"facebookresearch/vilbert-multi-task" -> "pzzhang/VinVL"
"facebookresearch/vilbert-multi-task" -> "ashkamath/mdetr"
"jiasenlu/vilbert_beta" -> "facebookresearch/vilbert-multi-task"
"jiasenlu/vilbert_beta" -> "uclanlp/visualbert"
"jiasenlu/vilbert_beta" -> "airsplay/lxmert"
"jiasenlu/vilbert_beta" -> "jackroos/VL-BERT"
"jiasenlu/vilbert_beta" -> "ChenRocks/UNITER"
"jiasenlu/vilbert_beta" -> "peteanderson80/bottom-up-attention"
"jiasenlu/vilbert_beta" -> "LuoweiZhou/VLP"
"jiasenlu/vilbert_beta" -> "MILVLG/mcan-vqa"
"jiasenlu/vilbert_beta" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"jiasenlu/vilbert_beta" -> "microsoft/Oscar"
"jiasenlu/vilbert_beta" -> "vmurahari3/visdial-bert"
"jiasenlu/vilbert_beta" -> "kuanghuei/SCAN" ["e"=1]
"jiasenlu/vilbert_beta" -> "KunpengLi1994/VSRN" ["e"=1]
"jiasenlu/vilbert_beta" -> "hengyuan-hu/bottom-up-attention-vqa"
"jiasenlu/vilbert_beta" -> "jnhwkim/ban-vqa"
"amazon-science/prompt-pretraining" -> "xmed-lab/CLIP_Surgery"
"ma-xu/Context-Cluster" -> "fudan-zvg/GSS"
"ma-xu/Context-Cluster" -> "microsoft/X-Decoder"
"ma-xu/Context-Cluster" -> "Vision-CAIR/ChatCaptioner" ["e"=1]
"ma-xu/Context-Cluster" -> "NVlabs/ODISE" ["e"=1]
"ma-xu/Context-Cluster" -> "facebookresearch/CutLER"
"ma-xu/Context-Cluster" -> "amirbar/visual_prompting"
"ma-xu/Context-Cluster" -> "snap-research/R2L"
"ma-xu/Context-Cluster" -> "wl-zhao/VPD" ["e"=1]
"ma-xu/Context-Cluster" -> "facebookresearch/ConvNeXt-V2"
"ma-xu/Context-Cluster" -> "IDEA-Research/OpenSeeD"
"ma-xu/Context-Cluster" -> "facebookresearch/ToMe"
"ma-xu/Context-Cluster" -> "VainF/Awesome-Anything" ["e"=1]
"ma-xu/Context-Cluster" -> "ShoufaChen/DiffusionDet"
"ziplab/SN-Net" -> "ziplab/EcoFormer" ["e"=1]
"baaivision/EVA" -> "microsoft/X-Decoder"
"baaivision/EVA" -> "OpenGVLab/InternImage"
"baaivision/EVA" -> "microsoft/GLIP"
"baaivision/EVA" -> "czczup/ViT-Adapter"
"baaivision/EVA" -> "baaivision/Painter" ["e"=1]
"baaivision/EVA" -> "ShoufaChen/DiffusionDet"
"baaivision/EVA" -> "facebookresearch/ConvNeXt-V2"
"baaivision/EVA" -> "mlfoundations/open_clip"
"baaivision/EVA" -> "UX-Decoder/Segment-Everything-Everywhere-All-At-Once" ["e"=1]
"baaivision/EVA" -> "SHI-Labs/OneFormer"
"baaivision/EVA" -> "IDEACVR/DINO"
"baaivision/EVA" -> "hustvl/MIMDet" ["e"=1]
"baaivision/EVA" -> "IDEA-Research/detrex"
"baaivision/EVA" -> "facebookresearch/Mask2Former" ["e"=1]
"baaivision/EVA" -> "microsoft/SimMIM" ["e"=1]
"muzairkhattak/multimodal-prompt-learning" -> "muzairkhattak/ViFi-CLIP"
"muzairkhattak/multimodal-prompt-learning" -> "yuhangzang/UPT"
"muzairkhattak/multimodal-prompt-learning" -> "KMnP/vpt"
"muzairkhattak/multimodal-prompt-learning" -> "CHENGY12/PLOT"
"muzairkhattak/multimodal-prompt-learning" -> "hanoonaR/object-centric-ovd"
"muzairkhattak/multimodal-prompt-learning" -> "abdohelmy/D-3Former"
"muzairkhattak/multimodal-prompt-learning" -> "hjbahng/visual_prompting"
"muzairkhattak/multimodal-prompt-learning" -> "KaiyangZhou/CoOp"
"muzairkhattak/multimodal-prompt-learning" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision"
"muzairkhattak/multimodal-prompt-learning" -> "azshue/TPT"
"muzairkhattak/multimodal-prompt-learning" -> "mmaaz60/mvits_for_class_agnostic_od"
"muzairkhattak/multimodal-prompt-learning" -> "ZhangYuanhan-AI/NOAH"
"muzairkhattak/multimodal-prompt-learning" -> "ylsung/VL_adapter"
"Amshaker/unetr_plus_plus" -> "Amshaker/SwiftFormer"
"Amshaker/unetr_plus_plus" -> "mindflow-institue/DAEFormer" ["e"=1]
"ruotianluo/Transformer_Captioning" -> "zhegan27/Semantic_Compositional_Nets"
"ruotianluo/Transformer_Captioning" -> "ruotianluo/DiscCaptioning"
"ruotianluo/Transformer_Captioning" -> "rakshithShetty/captionGAN"
"ruotianluo/Transformer_Captioning" -> "yangxuntu/SGAE"
"ruotianluo/Transformer_Captioning" -> "husthuaan/AoANet"
"ruotianluo/Transformer_Captioning" -> "chenxinpeng/im2p"
"ruotianluo/Transformer_Captioning" -> "ruotianluo/self-critical.pytorch"
"ruotianluo/Transformer_Captioning" -> "doubledaibo/gancaption_iccv2017"
"ruotianluo/Transformer_Captioning" -> "andyweizhao/Multitask_Image_Captioning"
"ruotianluo/Transformer_Captioning" -> "daqingliu/CAVP"
"ruotianluo/Transformer_Captioning" -> "yahoo/object_relation_transformer"
"ruotianluo/Transformer_Captioning" -> "aditya12agd5/convcap"
"ruotianluo/Transformer_Captioning" -> "s-gupta/visual-concepts"
"ruotianluo/self-critical.pytorch" -> "ruotianluo/ImageCaptioning.pytorch"
"ruotianluo/self-critical.pytorch" -> "peteanderson80/bottom-up-attention"
"ruotianluo/self-critical.pytorch" -> "zhjohnchan/awesome-image-captioning"
"ruotianluo/self-critical.pytorch" -> "husthuaan/AoANet"
"ruotianluo/self-critical.pytorch" -> "aimagelab/meshed-memory-transformer"
"ruotianluo/self-critical.pytorch" -> "yangxuntu/SGAE"
"ruotianluo/self-critical.pytorch" -> "tylin/coco-caption"
"ruotianluo/self-critical.pytorch" -> "jiasenlu/NeuralBabyTalk"
"ruotianluo/self-critical.pytorch" -> "JDAI-CV/image-captioning"
"ruotianluo/self-critical.pytorch" -> "peteanderson80/Up-Down-Captioner"
"ruotianluo/self-critical.pytorch" -> "jiasenlu/AdaptiveAttention"
"ruotianluo/self-critical.pytorch" -> "sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning"
"ruotianluo/self-critical.pytorch" -> "yahoo/object_relation_transformer"
"ruotianluo/self-critical.pytorch" -> "ruotianluo/Transformer_Captioning"
"ruotianluo/self-critical.pytorch" -> "aimagelab/show-control-and-tell"
"yuleiniu/cfvqa" -> "yanxinzju/CSS-VQA"
"yuleiniu/cfvqa" -> "yuleiniu/introd"
"yuleiniu/cfvqa" -> "cdancette/rubi.bootstrap.pytorch"
"yuleiniu/cfvqa" -> "yangxuntu/lxmertcatt" ["e"=1]
"yuleiniu/cfvqa" -> "cdancette/vqa-cp-leaderboard"
"yuleiniu/cfvqa" -> "chrisc36/debias"
"Eurus-Holmes/Awesome-Multimodal-Research" -> "pliang279/awesome-multimodal-ml"
"Eurus-Holmes/Awesome-Multimodal-Research" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"Eurus-Holmes/Awesome-Multimodal-Research" -> "yaohungt/Multimodal-Transformer" ["e"=1]
"Eurus-Holmes/Awesome-Multimodal-Research" -> "salesforce/ALBEF"
"Eurus-Holmes/Awesome-Multimodal-Research" -> "microsoft/Oscar"
"Eurus-Holmes/Awesome-Multimodal-Research" -> "jayleicn/ClipBERT"
"Eurus-Holmes/Awesome-Multimodal-Research" -> "facebookresearch/mmf"
"Eurus-Holmes/Awesome-Multimodal-Research" -> "forence/Awesome-Visual-Captioning"
"Eurus-Holmes/Awesome-Multimodal-Research" -> "A2Zadeh/CMU-MultimodalSDK" ["e"=1]
"Eurus-Holmes/Awesome-Multimodal-Research" -> "jackroos/VL-BERT"
"Eurus-Holmes/Awesome-Multimodal-Research" -> "thunlp/PromptPapers" ["e"=1]
"Eurus-Holmes/Awesome-Multimodal-Research" -> "jokieleung/awesome-visual-question-answering"
"Eurus-Holmes/Awesome-Multimodal-Research" -> "danieljf24/awesome-video-text-retrieval" ["e"=1]
"Eurus-Holmes/Awesome-Multimodal-Research" -> "soujanyaporia/multimodal-sentiment-analysis" ["e"=1]
"Eurus-Holmes/Awesome-Multimodal-Research" -> "airsplay/lxmert"
"IDEA-opensource/DAB-DETR" -> "IDEA-opensource/DN-DETR"
"IDEA-opensource/DAB-DETR" -> "IDEACVR/DINO"
"IDEA-opensource/DAB-DETR" -> "IDEACVR/awesome-detection-transformer"
"IDEA-opensource/DAB-DETR" -> "Atten4Vis/ConditionalDETR"
"IDEA-opensource/DAB-DETR" -> "IDEACVR/MaskDINO"
"IDEA-opensource/DAB-DETR" -> "MCG-NJU/AdaMixer"
"IDEA-opensource/DAB-DETR" -> "megvii-research/AnchorDETR"
"IDEA-opensource/DAB-DETR" -> "kakaobrain/sparse-detr"
"IDEA-opensource/DAB-DETR" -> "jshilong/DDQ"
"IDEA-opensource/DAB-DETR" -> "naver-ai/vidt"
"IDEACVR/awesome-detection-transformer" -> "IDEACVR/DINO"
"IDEACVR/awesome-detection-transformer" -> "IDEA-opensource/DN-DETR"
"IDEACVR/awesome-detection-transformer" -> "IDEACVR/MaskDINO"
"IDEACVR/awesome-detection-transformer" -> "IDEA-opensource/DAB-DETR"
"IDEACVR/awesome-detection-transformer" -> "Atten4Vis/ConditionalDETR"
"IDEACVR/awesome-detection-transformer" -> "MCG-NJU/AdaMixer"
"IDEACVR/awesome-detection-transformer" -> "IDEA-Research/detrex"
"IDEACVR/awesome-detection-transformer" -> "facebookresearch/Mask2Former" ["e"=1]
"IDEACVR/awesome-detection-transformer" -> "megvii-research/PETR" ["e"=1]
"IDEACVR/awesome-detection-transformer" -> "fundamentalvision/Deformable-DETR" ["e"=1]
"IDEACVR/awesome-detection-transformer" -> "ShoufaChen/DiffusionDet"
"IDEACVR/awesome-detection-transformer" -> "raoyongming/DenseCLIP"
"IDEACVR/awesome-detection-transformer" -> "hustvl/MIMDet" ["e"=1]
"IDEACVR/awesome-detection-transformer" -> "chaytonmin/Awesome-BEV-Perception-Multi-Cameras" ["e"=1]
"IDEACVR/awesome-detection-transformer" -> "HDETR/H-Deformable-DETR"
"microsoft/X-Decoder" -> "UX-Decoder/Segment-Everything-Everywhere-All-At-Once" ["e"=1]
"microsoft/X-Decoder" -> "baaivision/EVA"
"microsoft/X-Decoder" -> "NVlabs/ODISE" ["e"=1]
"microsoft/X-Decoder" -> "SHI-Labs/OneFormer"
"microsoft/X-Decoder" -> "czczup/ViT-Adapter"
"microsoft/X-Decoder" -> "microsoft/GLIP"
"microsoft/X-Decoder" -> "baaivision/Painter" ["e"=1]
"microsoft/X-Decoder" -> "google-research/pix2seq"
"microsoft/X-Decoder" -> "facebookresearch/ov-seg"
"microsoft/X-Decoder" -> "facebookresearch/CutLER"
"microsoft/X-Decoder" -> "clin1223/VLDet"
"microsoft/X-Decoder" -> "mlfoundations/open_flamingo" ["e"=1]
"microsoft/X-Decoder" -> "ma-xu/Context-Cluster"
"microsoft/X-Decoder" -> "facebookresearch/Detic"
"microsoft/X-Decoder" -> "IDEA-Research/OpenSeeD"
"mmaaz60/mvits_for_class_agnostic_od" -> "hanoonaR/object-centric-ovd"
"mmaaz60/mvits_for_class_agnostic_od" -> "akshitac8/OW-DETR"
"mmaaz60/mvits_for_class_agnostic_od" -> "abdohelmy/D-3Former"
"mmaaz60/mvits_for_class_agnostic_od" -> "mmaaz60/EdgeNeXt" ["e"=1]
"mmaaz60/mvits_for_class_agnostic_od" -> "amirbar/DETReg" ["e"=1]
"mmaaz60/mvits_for_class_agnostic_od" -> "muzairkhattak/multimodal-prompt-learning"
"mmaaz60/mvits_for_class_agnostic_od" -> "mcahny/object_localization_network"
"mmaaz60/mvits_for_class_agnostic_od" -> "muzairkhattak/ViFi-CLIP"
"mmaaz60/mvits_for_class_agnostic_od" -> "RE-OWOD/RE-OWOD"
"mmaaz60/mvits_for_class_agnostic_od" -> "ashkamath/mdetr"
"mmaaz60/mvits_for_class_agnostic_od" -> "Amshaker/SwiftFormer"
"mmaaz60/mvits_for_class_agnostic_od" -> "JosephKJ/OWOD"
"mmaaz60/mvits_for_class_agnostic_od" -> "clin1223/VLDet"
"uakarsh/latr" -> "microsoft/TAP"
"zdou0830/METER" -> "salesforce/ALBEF"
"zdou0830/METER" -> "dandelin/ViLT"
"zdou0830/METER" -> "zengyan-97/X-VLM"
"zdou0830/METER" -> "researchmm/soho"
"zdou0830/METER" -> "clip-vil/CLIP-ViL"
"zdou0830/METER" -> "microsoft/FIBER"
"zdou0830/METER" -> "uta-smile/TCL"
"zdou0830/METER" -> "ylsung/VL_adapter"
"zdou0830/METER" -> "djiajunustc/TransVG" ["e"=1]
"zdou0830/METER" -> "pzzhang/VinVL"
"zdou0830/METER" -> "microsoft/BridgeTower"
"zdou0830/METER" -> "OFA-Sys/OFA"
"Visual-Attention-Network/SegNeXt" -> "Jittor/JSeg"
"Visual-Attention-Network/SegNeXt" -> "czczup/ViT-Adapter"
"Visual-Attention-Network/SegNeXt" -> "Gsunshine/Enjoy-Hamburger"
"Visual-Attention-Network/SegNeXt" -> "raoyongming/HorNet"
"Visual-Attention-Network/SegNeXt" -> "Visual-Attention-Network/VAN-Segmentation"
"Visual-Attention-Network/SegNeXt" -> "DingXiaoH/RepLKNet-pytorch"
"Visual-Attention-Network/SegNeXt" -> "Visual-Attention-Network/VAN-Classification" ["e"=1]
"Visual-Attention-Network/SegNeXt" -> "facebookresearch/ConvNeXt-V2"
"Visual-Attention-Network/SegNeXt" -> "NVlabs/SegFormer" ["e"=1]
"Visual-Attention-Network/SegNeXt" -> "SHI-Labs/OneFormer"
"Visual-Attention-Network/SegNeXt" -> "hustvl/TopFormer" ["e"=1]
"Visual-Attention-Network/SegNeXt" -> "Picsart-AI-Research/SeMask-Segmentation" ["e"=1]
"Visual-Attention-Network/SegNeXt" -> "tfzhou/ProtoSeg"
"Visual-Attention-Network/SegNeXt" -> "hustvl/SparseInst" ["e"=1]
"Visual-Attention-Network/SegNeXt" -> "facebookresearch/MaskFormer" ["e"=1]
"rowanz/swagaf" -> "jonathanherzig/commonsenseqa" ["e"=1]
"Cloud-CV/GSoC-Ideas" -> "Cloud-CV/EvalAI-ngx"
"Cloud-CV/GSoC-Ideas" -> "Cloud-CV/CloudCV"
"Cloud-CV/GSoC-Ideas" -> "Cloud-CV/Origami"
"Cloud-CV/GSoC-Ideas" -> "Cloud-CV/visual-chatbot"
"yuxie11/R2D2" -> "BAAI-WuDao/BriVL"
"yuxie11/R2D2" -> "billjie1/Chinese-CLIP"
"microsoft/RegionCLIP" -> "microsoft/GLIP"
"microsoft/RegionCLIP" -> "hanoonaR/object-centric-ovd"
"microsoft/RegionCLIP" -> "microsoft/UniCL"
"microsoft/RegionCLIP" -> "ashkamath/mdetr"
"microsoft/RegionCLIP" -> "xiaofeng94/VL-PLM"
"microsoft/RegionCLIP" -> "dyabel/detpro"
"microsoft/RegionCLIP" -> "alirezazareian/ovr-cnn"
"microsoft/RegionCLIP" -> "yuhangzang/OV-DETR"
"microsoft/RegionCLIP" -> "zhenyuw16/UniDetector"
"microsoft/RegionCLIP" -> "Computer-Vision-in-the-Wild/CVinW_Readings"
"microsoft/RegionCLIP" -> "clin1223/VLDet"
"microsoft/RegionCLIP" -> "fcjian/PromptDet"
"microsoft/RegionCLIP" -> "microsoft/X-Decoder"
"microsoft/RegionCLIP" -> "raoyongming/DenseCLIP"
"microsoft/RegionCLIP" -> "IDEA-Research/OpenSeeD"
"yiyang92/vae_captioning" -> "rakshithShetty/captionGAN"
"chongzhou96/MaskCLIP" -> "raoyongming/DenseCLIP"
"chongzhou96/MaskCLIP" -> "MendelXu/zsseg.baseline"
"chongzhou96/MaskCLIP" -> "isl-org/lang-seg"
"chongzhou96/MaskCLIP" -> "NoelShin/reco"
"chongzhou96/MaskCLIP" -> "NVlabs/GroupViT"
"chongzhou96/MaskCLIP" -> "ZiqinZhou66/ZegCLIP"
"chongzhou96/MaskCLIP" -> "tfzhou/ProtoSeg"
"chongzhou96/MaskCLIP" -> "yuhangzang/OV-DETR"
"chongzhou96/MaskCLIP" -> "dingjiansw101/ZegFormer"
"chongzhou96/MaskCLIP" -> "facebookresearch/Generic-Grouping"
"chongzhou96/MaskCLIP" -> "lukemelas/deep-spectral-segmentation"
"chongzhou96/MaskCLIP" -> "DerrickWang005/CRIS.pytorch" ["e"=1]
"GT-Vision-Lab/VQA_LSTM_CNN" -> "jiasenlu/HieCoAttenVQA"
"GT-Vision-Lab/VQA_LSTM_CNN" -> "GT-Vision-Lab/VQA"
"GT-Vision-Lab/VQA_LSTM_CNN" -> "iamaaditya/VQA_Demo"
"GT-Vision-Lab/VQA_LSTM_CNN" -> "anantzoid/VQA-Keras-Visual-Question-Answering"
"GT-Vision-Lab/VQA_LSTM_CNN" -> "JamesChuanggg/VQA-tensorflow"
"GT-Vision-Lab/VQA_LSTM_CNN" -> "Cyanogenoid/pytorch-vqa"
"GT-Vision-Lab/VQA_LSTM_CNN" -> "akirafukui/vqa-mcb"
"GT-Vision-Lab/VQA_LSTM_CNN" -> "iamaaditya/VQA_Keras"
"GT-Vision-Lab/VQA_LSTM_CNN" -> "Cadene/vqa.pytorch"
"GT-Vision-Lab/VQA_LSTM_CNN" -> "SinghJasdeep/Attention-on-Attention-for-VQA"
"SinghJasdeep/Attention-on-Attention-for-VQA" -> "aimbrain/vqa-project"
"SinghJasdeep/Attention-on-Attention-for-VQA" -> "markdtw/vqa-winner-cvprw-2017"
"SinghJasdeep/Attention-on-Attention-for-VQA" -> "Cyanogenoid/vqa-counting"
"SinghJasdeep/Attention-on-Attention-for-VQA" -> "abhshkdz/neural-vqa-attention"
"aimbrain/vqa-project" -> "Cadene/murel.bootstrap.pytorch"
"aimbrain/vqa-project" -> "linjieli222/VQA_ReGAT"
"aimbrain/vqa-project" -> "SinghJasdeep/Attention-on-Attention-for-VQA"
"aimbrain/vqa-project" -> "Cyanogenoid/vqa-counting"
"aimbrain/vqa-project" -> "jnhwkim/ban-vqa"
"aimbrain/vqa-project" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"aimbrain/vqa-project" -> "Cyanogenoid/pytorch-vqa"
"aimbrain/vqa-project" -> "cdancette/rubi.bootstrap.pytorch"
"aimbrain/vqa-project" -> "hengyuan-hu/bottom-up-attention-vqa"
"jacobandreas/nmn2" -> "ronghanghu/n2nmn"
"jacobandreas/nmn2" -> "VT-vision-lab/VQA_LSTM_CNN"
"jacobandreas/nmn2" -> "jiasenlu/HieCoAttenVQA"
"jacobandreas/nmn2" -> "akirafukui/vqa-mcb"
"jacobandreas/nmn2" -> "facebookresearch/clevr-iep"
"jacobandreas/nmn2" -> "metalbubble/VQAbaseline"
"jacobandreas/nmn2" -> "facebookresearch/MIXER" ["e"=1]
"jacobandreas/nmn2" -> "JamesChuanggg/awesome-vqa"
"jacobandreas/nmn2" -> "ilyasu123/rlntm" ["e"=1]
"jacobandreas/nmn2" -> "facebook/MemNN" ["e"=1]
"jacobandreas/nmn2" -> "twitter/torch-twrl" ["e"=1]
"jacobandreas/nmn2" -> "jacobandreas/psketch"
"jacobandreas/nmn2" -> "kaishengtai/torch-ntm" ["e"=1]
"jacobandreas/nmn2" -> "donglixp/lang2logic" ["e"=1]
"ronghanghu/n2nmn" -> "jacobandreas/nmn2"
"ronghanghu/n2nmn" -> "YuJiang01/n2nmn_pytorch"
"ronghanghu/n2nmn" -> "facebookresearch/clevr-iep"
"ronghanghu/n2nmn" -> "stanfordnlp/mac-network"
"ronghanghu/n2nmn" -> "ronghanghu/snmn"
"ronghanghu/n2nmn" -> "HarshTrivedi/nmn-pytorch"
"ronghanghu/n2nmn" -> "ronghanghu/cmn"
"ronghanghu/n2nmn" -> "jiasenlu/HieCoAttenVQA"
"ronghanghu/n2nmn" -> "jnhwkim/ban-vqa"
"ronghanghu/n2nmn" -> "aimbrain/vqa-project"
"ronghanghu/n2nmn" -> "ronghanghu/speaker_follower" ["e"=1]
"ronghanghu/n2nmn" -> "akirafukui/vqa-mcb"
"ronghanghu/n2nmn" -> "JamesChuanggg/awesome-vqa"
"ronghanghu/n2nmn" -> "hengyuan-hu/bottom-up-attention-vqa"
"ronghanghu/n2nmn" -> "metalbubble/VQAbaseline"
"zcyang/imageqa-san" -> "HyeonwooNoh/DPPnet"
"zcyang/imageqa-san" -> "jiasenlu/HieCoAttenVQA"
"zcyang/imageqa-san" -> "akirafukui/vqa-mcb"
"zcyang/imageqa-san" -> "abhshkdz/neural-vqa-attention"
"zcyang/imageqa-san" -> "JamesChuanggg/san-torch"
"zcyang/imageqa-san" -> "yukezhu/visual7w-qa-models"
"zcyang/imageqa-san" -> "JamesChuanggg/VQA-tensorflow"
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "yunjey/show-attend-and-tell"
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "kelvinxu/arctic-captions"
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "jazzsaxmafia/show_and_tell.tensorflow"
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "DeepRNN/image_captioning"
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "mosessoh/CNN-LSTM-Caption-Generator"
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "jazzsaxmafia/video_to_sequence" ["e"=1]
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "ruotianluo/neuraltalk2.pytorch"
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "paarthneekhara/neural-vqa-tensorflow"
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "ry/tensorflow-vgg16" ["e"=1]
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "emansim/text2image" ["e"=1]
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "tylin/coco-caption"
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "facebookresearch/MIXER" ["e"=1]
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "carpedm20/pixel-rnn-tensorflow" ["e"=1]
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "kimiyoung/review_net"
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "carpedm20/NTM-tensorflow" ["e"=1]
"jcjohnson/densecap" -> "karpathy/neuraltalk2" ["e"=1]
"jcjohnson/densecap" -> "kelvinxu/arctic-captions"
"jcjohnson/densecap" -> "ruotianluo/ImageCaptioning.pytorch"
"jcjohnson/densecap" -> "DeepRNN/image_captioning"
"jcjohnson/densecap" -> "tylin/coco-caption"
"jcjohnson/densecap" -> "peteanderson80/bottom-up-attention"
"jcjohnson/densecap" -> "facebook/fb.resnet.torch" ["e"=1]
"jcjohnson/densecap" -> "yueatsprograms/Stochastic_Depth" ["e"=1]
"jcjohnson/densecap" -> "ruotianluo/self-critical.pytorch"
"jcjohnson/densecap" -> "torchnet/torchnet" ["e"=1]
"jcjohnson/densecap" -> "hengyuan-hu/bottom-up-attention-vqa"
"jcjohnson/densecap" -> "harvardnlp/seq2seq-attn" ["e"=1]
"jcjohnson/densecap" -> "jcjohnson/torch-rnn" ["e"=1]
"jcjohnson/densecap" -> "Element-Research/rnn" ["e"=1]
"jcjohnson/densecap" -> "facebookresearch/deepmask" ["e"=1]
"UCSD-AI4H/PathVQA" -> "aioz-ai/MICCAI21_MMQ"
"UCSD-AI4H/PathVQA" -> "abachaa/VQA-Med-2019"
"chrisc36/bottom-up-attention-vqa" -> "yanxinzju/CSS-VQA"
"jiasenlu/AdaptiveAttention" -> "yufengm/Adaptive"
"jiasenlu/AdaptiveAttention" -> "ruotianluo/self-critical.pytorch"
"jiasenlu/AdaptiveAttention" -> "fawazsammani/knowing-when-to-look-adaptive-attention"
"jiasenlu/AdaptiveAttention" -> "ruotianluo/DiscCaptioning"
"jiasenlu/AdaptiveAttention" -> "jiasenlu/NeuralBabyTalk"
"jiasenlu/AdaptiveAttention" -> "peteanderson80/Up-Down-Captioner"
"jiasenlu/AdaptiveAttention" -> "gujiuxiang/Stack-Captioning"
"jiasenlu/AdaptiveAttention" -> "zjuchenlong/sca-cnn.cvpr17"
"jiasenlu/AdaptiveAttention" -> "yunjey/show-attend-and-tell"
"jiasenlu/AdaptiveAttention" -> "peteanderson80/bottom-up-attention"
"jiasenlu/AdaptiveAttention" -> "LuoweiZhou/e2e-gLSTM-sc"
"jiasenlu/AdaptiveAttention" -> "aimagelab/show-control-and-tell"
"jiasenlu/AdaptiveAttention" -> "aditya12agd5/convcap"
"jiasenlu/AdaptiveAttention" -> "husthuaan/AoANet"
"jiasenlu/AdaptiveAttention" -> "cesc-park/attend2u"
"poojahira/image-captioning-bottom-up-top-down" -> "peteanderson80/Up-Down-Captioner"
"poojahira/image-captioning-bottom-up-top-down" -> "njchoma/transformer_image_caption"
"poojahira/image-captioning-bottom-up-top-down" -> "fawazsammani/knowing-when-to-look-adaptive-attention"
"poojahira/image-captioning-bottom-up-top-down" -> "yahoo/object_relation_transformer"
"poojahira/image-captioning-bottom-up-top-down" -> "violetteshev/bottom-up-features"
"poojahira/image-captioning-bottom-up-top-down" -> "husthuaan/AoANet"
"poojahira/image-captioning-bottom-up-top-down" -> "aimagelab/show-control-and-tell"
"poojahira/image-captioning-bottom-up-top-down" -> "yangxuntu/SGAE"
"poojahira/image-captioning-bottom-up-top-down" -> "YiwuZhong/Sub-GC"
"poojahira/image-captioning-bottom-up-top-down" -> "ruotianluo/coco-caption"
"poojahira/image-captioning-bottom-up-top-down" -> "cshizhe/asg2cap"
"poojahira/image-captioning-bottom-up-top-down" -> "ezeli/BUTD_model"
"poojahira/image-captioning-bottom-up-top-down" -> "aimagelab/meshed-memory-transformer"
"poojahira/image-captioning-bottom-up-top-down" -> "airsplay/py-bottom-up-attention"
"richardaecn/cvpr18-caption-eval" -> "ruotianluo/DiscCaptioning"
"richardaecn/cvpr18-caption-eval" -> "JonghwanMun/TextguidedATT"
"ruotianluo/DiscCaptioning" -> "gujiuxiang/Stack-Captioning"
"ruotianluo/DiscCaptioning" -> "cswhjiang/Recurrent_Fusion_Network"
"ruotianluo/DiscCaptioning" -> "richardaecn/cvpr18-caption-eval"
"ruotianluo/DiscCaptioning" -> "doubledaibo/gancaption_iccv2017"
"ruotianluo/DiscCaptioning" -> "ruotianluo/Transformer_Captioning"
"ruotianluo/DiscCaptioning" -> "fkxssaa/Deliberate-Attention-Networks-for-Image-Captioning"
"salaniz/pycocoevalcap" -> "tylin/coco-caption"
"salaniz/pycocoevalcap" -> "yahoo/object_relation_transformer"
"salaniz/pycocoevalcap" -> "wangleihitcs/CaptionMetrics"
"salaniz/pycocoevalcap" -> "ruotianluo/self-critical.pytorch"
"salaniz/pycocoevalcap" -> "forence/Awesome-Visual-Captioning"
"salaniz/pycocoevalcap" -> "salesforce/densecap" ["e"=1]
"salaniz/pycocoevalcap" -> "cshizhe/asg2cap"
"salaniz/pycocoevalcap" -> "jiasenlu/NeuralBabyTalk"
"salaniz/pycocoevalcap" -> "fawazsammani/knowing-when-to-look-adaptive-attention"
"salaniz/pycocoevalcap" -> "ruotianluo/DiscCaptioning"
"salaniz/pycocoevalcap" -> "zhjohnchan/awesome-image-captioning"
"salaniz/pycocoevalcap" -> "husthuaan/AoANet"
"salaniz/pycocoevalcap" -> "krasserm/fairseq-image-captioning"
"salaniz/pycocoevalcap" -> "yangxuntu/SGAE"
"salaniz/pycocoevalcap" -> "tgc1997/Awesome-Video-Captioning" ["e"=1]
"tylin/coco-caption" -> "ruotianluo/self-critical.pytorch"
"tylin/coco-caption" -> "ruotianluo/ImageCaptioning.pytorch"
"tylin/coco-caption" -> "peteanderson80/bottom-up-attention"
"tylin/coco-caption" -> "kelvinxu/arctic-captions"
"tylin/coco-caption" -> "zhjohnchan/awesome-image-captioning"
"tylin/coco-caption" -> "yunjey/show-attend-and-tell"
"tylin/coco-caption" -> "salaniz/pycocoevalcap"
"tylin/coco-caption" -> "jiasenlu/NeuralBabyTalk"
"tylin/coco-caption" -> "aimagelab/meshed-memory-transformer"
"tylin/coco-caption" -> "husthuaan/AoANet"
"tylin/coco-caption" -> "peteanderson80/Up-Down-Captioner"
"tylin/coco-caption" -> "yangxuntu/SGAE"
"tylin/coco-caption" -> "jiasenlu/AdaptiveAttention"
"tylin/coco-caption" -> "xiadingZ/video-caption.pytorch" ["e"=1]
"tylin/coco-caption" -> "DeepRNN/image_captioning"
"yanxinzju/CSS-VQA" -> "chrisc36/bottom-up-attention-vqa"
"yanxinzju/CSS-VQA" -> "jialinwu17/self_critical_vqa"
"yanxinzju/CSS-VQA" -> "cdancette/rubi.bootstrap.pytorch"
"yufengm/Adaptive" -> "fawazsammani/knowing-when-to-look-adaptive-attention"
"yufengm/Adaptive" -> "jiasenlu/AdaptiveAttention"
"yufengm/Adaptive" -> "ruotianluo/DiscCaptioning"
"IDEA-Research/Lite-DETR" -> "IDEA-Research/DQ-DETR"
"IDEA-Research/Lite-DETR" -> "IDEA-Research/Stable-DINO"
"JDAI-CV/image-captioning" -> "husthuaan/AoANet"
"JDAI-CV/image-captioning" -> "cshizhe/asg2cap"
"JDAI-CV/image-captioning" -> "aimagelab/meshed-memory-transformer"
"JDAI-CV/image-captioning" -> "yahoo/object_relation_transformer"
"JDAI-CV/image-captioning" -> "luo3300612/image-captioning-DLCT"
"JDAI-CV/image-captioning" -> "forence/Awesome-Visual-Captioning"
"JDAI-CV/image-captioning" -> "facebookresearch/grid-feats-vqa"
"JDAI-CV/image-captioning" -> "yangxuntu/SGAE"
"JDAI-CV/image-captioning" -> "ruotianluo/self-critical.pytorch"
"JDAI-CV/image-captioning" -> "YuanEZhou/Grounded-Image-Captioning"
"JDAI-CV/image-captioning" -> "fawazsammani/show-edit-tell"
"JDAI-CV/image-captioning" -> "aimagelab/show-control-and-tell"
"JDAI-CV/image-captioning" -> "ruotianluo/coco-caption"
"JDAI-CV/image-captioning" -> "zhjohnchan/awesome-image-captioning"
"JDAI-CV/image-captioning" -> "YiwuZhong/Sub-GC"
"kexinyi/ns-vqa" -> "vacancy/NSCL-PyTorch-Release"
"kexinyi/ns-vqa" -> "facebookresearch/clevr-dataset-gen"
"kexinyi/ns-vqa" -> "stanfordnlp/mac-network"
"kexinyi/ns-vqa" -> "chuangg/CLEVRER"
"kexinyi/ns-vqa" -> "facebookresearch/clevr-iep"
"kexinyi/ns-vqa" -> "kdexd/probnmn-clevr"
"kexinyi/ns-vqa" -> "vacancy/Jacinle"
"kexinyi/ns-vqa" -> "nerdimite/neuro-symbolic-ai-soc"
"kexinyi/ns-vqa" -> "google-research/clevr_robot_env"
"kexinyi/ns-vqa" -> "ceyzaguirre4/NSM"
"kexinyi/ns-vqa" -> "Glaciohound/VCML"
"kexinyi/ns-vqa" -> "satwikkottur/clevr-dialog"
"kexinyi/ns-vqa" -> "ronghanghu/snmn"
"kexinyi/ns-vqa" -> "ronghanghu/n2nmn"
"kexinyi/ns-vqa" -> "crazydonkey200/neural-symbolic-machines" ["e"=1]
"Sense-GVT/DeCLIP" -> "facebookresearch/SLIP"
"Sense-GVT/DeCLIP" -> "raoyongming/DenseCLIP"
"Sense-GVT/DeCLIP" -> "lucidrains/x-clip"
"Sense-GVT/DeCLIP" -> "KaiyangZhou/CoOp"
"Sense-GVT/DeCLIP" -> "ashkamath/mdetr"
"Sense-GVT/DeCLIP" -> "microsoft/GLIP"
"Sense-GVT/DeCLIP" -> "salesforce/ALBEF"
"Sense-GVT/DeCLIP" -> "gaopengcuhk/CLIP-Adapter"
"Sense-GVT/DeCLIP" -> "EPFL-VILAB/MultiMAE" ["e"=1]
"Sense-GVT/DeCLIP" -> "OpenGVLab/gv-benchmark" ["e"=1]
"Sense-GVT/DeCLIP" -> "uta-smile/TCL"
"Sense-GVT/DeCLIP" -> "jayleicn/ClipBERT"
"Sense-GVT/DeCLIP" -> "mlfoundations/wise-ft"
"Sense-GVT/DeCLIP" -> "baaivision/EVA"
"Sense-GVT/DeCLIP" -> "alirezazareian/ovr-cnn"
"nexusapoorvacus/DeepVariationStructuredRL" -> "yangxuntu/vrd"
"HDETR/H-Deformable-DETR" -> "Sense-X/Co-DETR"
"HDETR/H-Deformable-DETR" -> "jozhang97/DETA"
"HDETR/H-Deformable-DETR" -> "IDEA-opensource/DN-DETR"
"facebookresearch/multimodal" -> "salesforce/ALBEF"
"facebookresearch/multimodal" -> "microsoft/GLIP"
"facebookresearch/multimodal" -> "lucidrains/CoCa-pytorch"
"facebookresearch/multimodal" -> "facebookresearch/mmf"
"facebookresearch/multimodal" -> "salesforce/LAVIS"
"facebookresearch/multimodal" -> "facebookresearch/SLIP"
"facebookresearch/multimodal" -> "salesforce/BLIP"
"facebookresearch/multimodal" -> "lucidrains/flamingo-pytorch"
"facebookresearch/multimodal" -> "OFA-Sys/OFA"
"facebookresearch/multimodal" -> "lucidrains/x-clip"
"facebookresearch/multimodal" -> "ArrowLuo/CLIP4Clip" ["e"=1]
"facebookresearch/multimodal" -> "facebookresearch/omnivore" ["e"=1]
"facebookresearch/multimodal" -> "kakaobrain/coyo-dataset"
"facebookresearch/multimodal" -> "ashkamath/mdetr"
"facebookresearch/multimodal" -> "yzhuoning/Awesome-CLIP"
"subho406/OmniNet" -> "jiasenlu/vilbert_beta"
"subho406/OmniNet" -> "chihyaoma/cyclical-visual-captioning"
"subho406/OmniNet" -> "facebookresearch/vilbert-multi-task"
"subho406/OmniNet" -> "amanchadha/iPerceive" ["e"=1]
"subho406/OmniNet" -> "jayleicn/recurrent-transformer" ["e"=1]
"hjbahng/visual_prompting" -> "KMnP/vpt"
"hjbahng/visual_prompting" -> "amirbar/visual_prompting"
"hjbahng/visual_prompting" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision"
"hjbahng/visual_prompting" -> "gaopengcuhk/Tip-Adapter"
"hjbahng/visual_prompting" -> "muzairkhattak/multimodal-prompt-learning"
"hjbahng/visual_prompting" -> "ZhangYuanhan-AI/NOAH"
"hjbahng/visual_prompting" -> "yuhangzang/UPT"
"hjbahng/visual_prompting" -> "dyabel/detpro"
"hjbahng/visual_prompting" -> "ShoufaChen/AdaptFormer"
"sagizty/VPT" -> "sagizty/MAE"
"sagizty/VPT" -> "sagizty/Multi-Stage-Hybrid-Transformer"
"sagizty/VPT" -> "sagizty/Insight"
"sagizty/VPT" -> "sagizty/NTUS_application"
"sagizty/VPT" -> "sagizty/Parallel-Hybrid-Transformer"
"sagizty/VPT" -> "sagizty/sagizty"
"facebookresearch/ov-seg" -> "microsoft/X-Decoder"
"facebookresearch/ov-seg" -> "MendelXu/zsseg.baseline"
"facebookresearch/ov-seg" -> "amazon-science/prompt-pretraining"
"facebookresearch/ov-seg" -> "isl-org/lang-seg"
"facebookresearch/ov-seg" -> "chongzhou96/MaskCLIP"
"facebookresearch/ov-seg" -> "yuhangzang/OV-DETR"
"facebookresearch/ov-seg" -> "UX-Decoder/Segment-Everything-Everywhere-All-At-Once" ["e"=1]
"facebookresearch/ov-seg" -> "NVlabs/ODISE" ["e"=1]
"facebookresearch/ov-seg" -> "IDEA-Research/OpenSeeD"
"facebookresearch/ov-seg" -> "dingjiansw101/ZegFormer"
"KaihuaTang/VCTree-Scene-Graph-Generation" -> "rowanz/neural-motifs"
"KaihuaTang/VCTree-Scene-Graph-Generation" -> "KaihuaTang/VCTree-Visual-Question-Answering"
"KaihuaTang/VCTree-Scene-Graph-Generation" -> "danfeiX/scene-graph-TF-release"
"KaihuaTang/VCTree-Scene-Graph-Generation" -> "yuweihao/KERN"
"yuweihao/KERN" -> "HCPLab-SYSU/KERN"
"yuweihao/KERN" -> "rowanz/neural-motifs"
"yuweihao/KERN" -> "yikang-li/FactorizableNet"
"yuweihao/KERN" -> "alirezazareian/gbnet"
"yuweihao/KERN" -> "taksau/GPS-Net"
"yuweihao/KERN" -> "danfeiX/scene-graph-TF-release"
"yuweihao/KERN" -> "alirezazareian/vspnet"
"cdancette/vqa-cp-leaderboard" -> "tejas-gokhale/vqa_mutant"
"wyzjack/MRMGA4VAD" -> "wyzjack/SLA2P"
"oarriaga/neural_image_captioning" -> "anuragmishracse/caption_generator"
"oarriaga/neural_image_captioning" -> "danieljl/keras-image-captioning"
"allenai/container" -> "gaopengcuhk/Container"
"BAAI-WuDao/BriVL" -> "chuhaojin/BriVL-BUA-applications"
"BAAI-WuDao/BriVL" -> "yuxie11/R2D2"
"BAAI-WuDao/BriVL" -> "chuhaojin/WenLan-api-document"
"BAAI-WuDao/BriVL" -> "neilfei/brivl-nmi"
"BAAI-WuDao/BriVL" -> "billjie1/Chinese-CLIP"
"BAAI-WuDao/BriVL" -> "FreddeFrallan/Multilingual-CLIP"
"BAAI-WuDao/BriVL" -> "li-xirong/coco-cn"
"li-xirong/coco-cn" -> "li-xirong/cross-lingual-cap"
"danfeiX/scene-graph-TF-release" -> "rowanz/neural-motifs"
"danfeiX/scene-graph-TF-release" -> "jwyang/graph-rcnn.pytorch"
"danfeiX/scene-graph-TF-release" -> "yikang-li/MSDN"
"danfeiX/scene-graph-TF-release" -> "yikang-li/FactorizableNet"
"danfeiX/scene-graph-TF-release" -> "KaihuaTang/Scene-Graph-Benchmark.pytorch"
"danfeiX/scene-graph-TF-release" -> "Prof-Lu-Cewu/Visual-Relationship-Detection"
"danfeiX/scene-graph-TF-release" -> "NVIDIA/ContrastiveLosses4VRD"
"danfeiX/scene-graph-TF-release" -> "yuweihao/KERN"
"danfeiX/scene-graph-TF-release" -> "KaihuaTang/VCTree-Scene-Graph-Generation"
"danfeiX/scene-graph-TF-release" -> "google/sg2im"
"danfeiX/scene-graph-TF-release" -> "ranjaykrishna/visual_genome_python_driver"
"danfeiX/scene-graph-TF-release" -> "vacancy/SceneGraphParser"
"danfeiX/scene-graph-TF-release" -> "yangxuntu/SGAE"
"danfeiX/scene-graph-TF-release" -> "nexusapoorvacus/DeepVariationStructuredRL"
"danfeiX/scene-graph-TF-release" -> "microsoft/scene_graph_benchmark"
"rowanz/neural-motifs" -> "danfeiX/scene-graph-TF-release"
"rowanz/neural-motifs" -> "jwyang/graph-rcnn.pytorch"
"rowanz/neural-motifs" -> "KaihuaTang/Scene-Graph-Benchmark.pytorch"
"rowanz/neural-motifs" -> "yangxuntu/SGAE"
"rowanz/neural-motifs" -> "yikang-li/FactorizableNet"
"rowanz/neural-motifs" -> "yikang-li/MSDN"
"rowanz/neural-motifs" -> "KaihuaTang/VCTree-Scene-Graph-Generation"
"rowanz/neural-motifs" -> "yuweihao/KERN"
"rowanz/neural-motifs" -> "NVIDIA/ContrastiveLosses4VRD"
"rowanz/neural-motifs" -> "vacancy/SceneGraphParser"
"rowanz/neural-motifs" -> "alisure-ml/awesome-visual-relationship-detection"
"rowanz/neural-motifs" -> "Prof-Lu-Cewu/Visual-Relationship-Detection"
"rowanz/neural-motifs" -> "google/sg2im"
"rowanz/neural-motifs" -> "peteanderson80/bottom-up-attention"
"rowanz/neural-motifs" -> "peteanderson80/SPICE"
"snap-research/MobileR2L" -> "snap-research/R2L"
"NVlabs/GroupViT" -> "isl-org/lang-seg"
"NVlabs/GroupViT" -> "chongzhou96/MaskCLIP"
"NVlabs/GroupViT" -> "raoyongming/DenseCLIP"
"NVlabs/GroupViT" -> "microsoft/GLIP"
"NVlabs/GroupViT" -> "tfzhou/ProtoSeg"
"NVlabs/GroupViT" -> "NVlabs/ODISE" ["e"=1]
"NVlabs/GroupViT" -> "KaiyangZhou/CoOp"
"NVlabs/GroupViT" -> "salesforce/ALBEF"
"NVlabs/GroupViT" -> "yandex-research/ddpm-segmentation" ["e"=1]
"NVlabs/GroupViT" -> "ashkamath/mdetr"
"NVlabs/GroupViT" -> "facebookresearch/Detic"
"NVlabs/GroupViT" -> "MendelXu/zsseg.baseline"
"NVlabs/GroupViT" -> "ShoufaChen/DiffusionDet"
"NVlabs/GroupViT" -> "google-research/pix2seq"
"NVlabs/GroupViT" -> "dandelin/ViLT"
"ShoufaChen/AdaptFormer" -> "dongzelian/SSF"
"ShoufaChen/AdaptFormer" -> "hjbahng/visual_prompting"
"ShoufaChen/AdaptFormer" -> "hustvl/MIMDet" ["e"=1]
"jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning" -> "OPTML-Group/ILM-VP"
"webdataset/webdataset" -> "rom1504/img2dataset"
"webdataset/webdataset" -> "NVIDIA/aistore"
"webdataset/webdataset" -> "pytorch/data" ["e"=1]
"webdataset/webdataset" -> "rom1504/clip-retrieval"
"webdataset/webdataset" -> "mlfoundations/open_clip"
"webdataset/webdataset" -> "facebookresearch/fairscale" ["e"=1]
"webdataset/webdataset" -> "kakaobrain/coyo-dataset"
"webdataset/webdataset" -> "libffcv/ffcv" ["e"=1]
"webdataset/webdataset" -> "FreddeFrallan/Multilingual-CLIP"
"webdataset/webdataset" -> "criteo/autofaiss"
"webdataset/webdataset" -> "facebookresearch/vissl" ["e"=1]
"webdataset/webdataset" -> "uber/petastorm" ["e"=1]
"webdataset/webdataset" -> "mlfoundations/open_flamingo" ["e"=1]
"webdataset/webdataset" -> "lucidrains/x-clip"
"webdataset/webdataset" -> "microsoft/GLIP"
"yaohungt/Gated-Spatio-Temporal-Energy-Graph" -> "xdshang/VidVRD-helper"
"yaohungt/Gated-Spatio-Temporal-Energy-Graph" -> "ronghanghu/lcgn"
"yaohungt/Gated-Spatio-Temporal-Energy-Graph" -> "JingweiJ/ActionGenome"
"yaohungt/Gated-Spatio-Temporal-Energy-Graph" -> "fabienbaradel/object_level_visual_reasoning"
"ylsung/VL_adapter" -> "ylsung/Ladder-Side-Tuning" ["e"=1]
"ylsung/VL_adapter" -> "j-min/VL-T5"
"ylsung/VL_adapter" -> "gaopengcuhk/CLIP-Adapter"
"taesunwhang/MVAN-VisDial" -> "gicheonkang/dan-visdial"
"choasup/SIN" -> "msracver/Relation-Networks-for-Object-Detection" ["e"=1]
"choasup/SIN" -> "endernewton/iter-reason"
"choasup/SIN" -> "Hwang64/MLKP"
"aimagelab/meshed-memory-transformer" -> "husthuaan/AoANet"
"aimagelab/meshed-memory-transformer" -> "yahoo/object_relation_transformer"
"aimagelab/meshed-memory-transformer" -> "JDAI-CV/image-captioning"
"aimagelab/meshed-memory-transformer" -> "luo3300612/image-captioning-DLCT"
"aimagelab/meshed-memory-transformer" -> "ruotianluo/self-critical.pytorch"
"aimagelab/meshed-memory-transformer" -> "krasserm/fairseq-image-captioning"
"aimagelab/meshed-memory-transformer" -> "yangxuntu/SGAE"
"aimagelab/meshed-memory-transformer" -> "cshizhe/asg2cap"
"aimagelab/meshed-memory-transformer" -> "forence/Awesome-Visual-Captioning"
"aimagelab/meshed-memory-transformer" -> "aimagelab/show-control-and-tell"
"aimagelab/meshed-memory-transformer" -> "zhjohnchan/awesome-image-captioning"
"aimagelab/meshed-memory-transformer" -> "facebookresearch/grid-feats-vqa"
"aimagelab/meshed-memory-transformer" -> "zhangxuying1004/RSTNet"
"aimagelab/meshed-memory-transformer" -> "ruotianluo/ImageCaptioning.pytorch"
"aimagelab/meshed-memory-transformer" -> "microsoft/Oscar"
"airsplay/py-bottom-up-attention" -> "MILVLG/bottom-up-attention.pytorch"
"airsplay/py-bottom-up-attention" -> "YIKUAN8/Transformers-VQA"
"airsplay/py-bottom-up-attention" -> "facebookresearch/grid-feats-vqa"
"airsplay/py-bottom-up-attention" -> "pzzhang/VinVL"
"airsplay/py-bottom-up-attention" -> "microsoft/Oscar"
"airsplay/py-bottom-up-attention" -> "peteanderson80/bottom-up-attention"
"airsplay/py-bottom-up-attention" -> "airsplay/lxmert"
"airsplay/py-bottom-up-attention" -> "poojahira/image-captioning-bottom-up-top-down"
"airsplay/py-bottom-up-attention" -> "YuanEZhou/Grounded-Image-Captioning"
"airsplay/py-bottom-up-attention" -> "shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome"
"airsplay/py-bottom-up-attention" -> "linjieli222/VQA_ReGAT"
"airsplay/py-bottom-up-attention" -> "violetteshev/bottom-up-features"
"airsplay/py-bottom-up-attention" -> "ChenRocks/UNITER"
"airsplay/py-bottom-up-attention" -> "JDAI-CV/image-captioning"
"airsplay/py-bottom-up-attention" -> "microsoft/scene_graph_benchmark"
"yikang-li/FactorizableNet" -> "yikang-li/MSDN"
"yikang-li/FactorizableNet" -> "rowanz/neural-motifs"
"yikang-li/FactorizableNet" -> "danfeiX/scene-graph-TF-release"
"yikang-li/FactorizableNet" -> "jwyang/graph-rcnn.pytorch"
"yikang-li/FactorizableNet" -> "yuweihao/KERN"
"yikang-li/FactorizableNet" -> "alisure-ml/awesome-visual-relationship-detection"
"yikang-li/FactorizableNet" -> "nexusapoorvacus/DeepVariationStructuredRL"
"yikang-li/FactorizableNet" -> "yikang-li/vg_cleansing"
"yikang-li/FactorizableNet" -> "NVIDIA/ContrastiveLosses4VRD"
"yikang-li/FactorizableNet" -> "KaihuaTang/Scene-Graph-Benchmark.pytorch"
"yikang-li/FactorizableNet" -> "jz462/Large-Scale-VRD.pytorch"
"yashk2810/Image-Captioning" -> "anuragmishracse/caption_generator"
"yashk2810/Image-Captioning" -> "danieljl/keras-image-captioning"
"yashk2810/Image-Captioning" -> "hlamba28/Automatic-Image-Captioning"
"yashk2810/Image-Captioning" -> "DeepRNN/image_captioning"
"yashk2810/Image-Captioning" -> "Shobhit20/Image-Captioning"
"yashk2810/Image-Captioning" -> "Div99/Image-Captioning"
"yashk2810/Image-Captioning" -> "neural-nuts/image-caption-generator"
"yashk2810/Image-Captioning" -> "jiasenlu/AdaptiveAttention"
"yashk2810/Image-Captioning" -> "fengyang0317/unsupervised_captioning"
"yashk2810/Image-Captioning" -> "tylin/coco-caption"
"yashk2810/Image-Captioning" -> "yufengm/Adaptive"
"yashk2810/Image-Captioning" -> "peteanderson80/Up-Down-Captioner"
"yashk2810/Image-Captioning" -> "yunjey/show-attend-and-tell"
"yashk2810/Image-Captioning" -> "zhjohnchan/awesome-image-captioning"
"yashk2810/Image-Captioning" -> "oarriaga/neural_image_captioning"
"ZhengyaoJiang/NLRL" -> "ai-systems/DILP-Core"
"applied-ai-lab/genesis" -> "deepmind/multi_object_datasets"
"applied-ai-lab/genesis" -> "zhixuan-lin/IODINE"
"applied-ai-lab/genesis" -> "stelzner/monet"
"applied-ai-lab/genesis" -> "hyenal/relate"
"applied-ai-lab/genesis" -> "baudm/MONet-pytorch"
"applied-ai-lab/genesis" -> "JindongJiang/SCALOR"
"applied-ai-lab/genesis" -> "zhixuan-lin/G-SWM"
"google/neural-logic-machines" -> "vacancy/NSCL-PyTorch-Release"
"google/neural-logic-machines" -> "ZhengyaoJiang/NLRL"
"google/neural-logic-machines" -> "vacancy/Jacinle"
"google/neural-logic-machines" -> "fanyangxyz/Neural-LP" ["e"=1]
"google/neural-logic-machines" -> "ai-systems/DILP-Core"
"google/neural-logic-machines" -> "logictensornetworks/logictensornetworks"
"google/neural-logic-machines" -> "locuslab/SATNet" ["e"=1]
"zhixuan-lin/SPACE" -> "zhixuan-lin/IODINE"
"zhixuan-lin/SPACE" -> "JindongJiang/SCALOR"
"InnerPeace-Wu/densecap-tensorflow" -> "linjieyangsc/densecap"
"InnerPeace-Wu/densecap-tensorflow" -> "InnerPeace-Wu/im2p-tensorflow"
"anuragmishracse/caption_generator" -> "yashk2810/Image-Captioning"
"anuragmishracse/caption_generator" -> "LemonATsu/Keras-Image-Caption"
"anuragmishracse/caption_generator" -> "neural-nuts/image-caption-generator"
"anuragmishracse/caption_generator" -> "oarriaga/neural_image_captioning"
"anuragmishracse/caption_generator" -> "DeepRNN/image_captioning"
"anuragmishracse/caption_generator" -> "Shobhit20/Image-Captioning"
"anuragmishracse/caption_generator" -> "mosessoh/CNN-LSTM-Caption-Generator"
"anuragmishracse/caption_generator" -> "amaiasalvador/imcap_keras"
"anuragmishracse/caption_generator" -> "zsdonghao/Image-Captioning"
"anuragmishracse/caption_generator" -> "Div99/Image-Captioning"
"anuragmishracse/caption_generator" -> "danieljl/keras-image-captioning"
"anuragmishracse/caption_generator" -> "boluoyu/ImageCaption"
"anuragmishracse/caption_generator" -> "dabasajay/Image-Caption-Generator"
"anuragmishracse/caption_generator" -> "apple2373/chainer-caption"
"anuragmishracse/caption_generator" -> "yunjey/show-attend-and-tell"
"lucidrains/Adan-pytorch" -> "sail-sg/Adan"
"lucidrains/Adan-pytorch" -> "facebookresearch/torchdim" ["e"=1]
"neural-nuts/Cam2Caption" -> "neural-nuts/image-caption-generator"
"singhgautam/slate" -> "addtt/object-centric-library"
"tmbdev/webdataset" -> "NVIDIA/aistore"
"tmbdev/webdataset" -> "NVlabs/tensorcom"
"tmbdev/webdataset" -> "albanie/slurm_gpustat"
"tmbdev/webdataset" -> "NVIDIA/ais-k8s"
"logictensornetworks/logictensornetworks" -> "logictensornetworks/tutorials"
"logictensornetworks/logictensornetworks" -> "ML-KULeuven/deepproblog" ["e"=1]
"logictensornetworks/logictensornetworks" -> "tommasocarraro/LTNtorch"
"logictensornetworks/logictensornetworks" -> "ccclyu/awesome-deeplogic"
"logictensornetworks/logictensornetworks" -> "google/neural-logic-machines"
"logictensornetworks/logictensornetworks" -> "jyhong0304/LTN_pytorch"
"logictensornetworks/logictensornetworks" -> "logictensornetworks/LTNtorch"
"logictensornetworks/logictensornetworks" -> "ml-research/nsfr"
"NVlabs/FAN" -> "microsoft/FocalNet"
"NVlabs/FAN" -> "dingmyu/davit"
"NVlabs/FAN" -> "SHI-Labs/Neighborhood-Attention-Transformer"
"NVlabs/FAN" -> "Sense-X/UniFormer" ["e"=1]
"NVlabs/FAN" -> "Visual-Attention-Network/VAN-Segmentation"
"NVlabs/FAN" -> "raoyongming/HorNet"
"NVlabs/FAN" -> "snap-research/EfficientFormer" ["e"=1]
"NVlabs/FAN" -> "Visual-Attention-Network/SegNeXt"
"NVlabs/FAN" -> "hustvl/MIMDet" ["e"=1]
"NVlabs/FAN" -> "OliverRensu/Shunted-Transformer" ["e"=1]
"NVlabs/FAN" -> "NVlabs/FreeSOLO" ["e"=1]
"NVlabs/FAN" -> "zhoudaquan/Refiner_ViT" ["e"=1]
"amirbar/visual_prompting" -> "hjbahng/visual_prompting"
"amirbar/visual_prompting" -> "ZhangYuanhan-AI/visual_prompt_retrieval"
"amirbar/visual_prompting" -> "KMnP/vpt"
"facebookresearch/diht" -> "facebookresearch/paco"
"facebookresearch/diht" -> "facebookresearch/CiT"
"dongzelian/SSF" -> "JieShibo/PETL-ViT"
"dongzelian/SSF" -> "yuhangzang/UPT"
"megvii-research/RepLKNet" -> "DingXiaoH/RepLKNet-pytorch"
"megvii-research/RepLKNet" -> "megvii-research/RevCol"
"shijx12/XNM-Net" -> "ronghanghu/snmn"
"shijx12/XNM-Net" -> "ronghanghu/lcgn"
"zengyan-97/CCLM" -> "zmykevin/UC2"
"endernewton/iter-reason" -> "coderSkyChen/Iterative-Visual-Reasoning.pytorch"
"endernewton/iter-reason" -> "davidmascharka/tbd-nets"
"endernewton/iter-reason" -> "aimbrain/vqa-project"
"endernewton/iter-reason" -> "shubhtuls/factored3d" ["e"=1]
"endernewton/iter-reason" -> "choasup/SIN"
"cshizhe/asg2cap" -> "JDAI-CV/image-captioning"
"cshizhe/asg2cap" -> "yangxuntu/SGAE"
"cshizhe/asg2cap" -> "YiwuZhong/Sub-GC"
"cshizhe/asg2cap" -> "yahoo/object_relation_transformer"
"cshizhe/asg2cap" -> "ruotianluo/coco-caption"
"cshizhe/asg2cap" -> "luo3300612/image-captioning-DLCT"
"cshizhe/asg2cap" -> "aimagelab/show-control-and-tell"
"cshizhe/asg2cap" -> "husthuaan/AoANet"
"cshizhe/asg2cap" -> "aimagelab/meshed-memory-transformer"
"cshizhe/asg2cap" -> "fawazsammani/show-edit-tell"
"cshizhe/asg2cap" -> "forence/Awesome-Visual-Captioning"
"cshizhe/asg2cap" -> "MILVLG/bottom-up-attention.pytorch"
"cshizhe/asg2cap" -> "jayleicn/recurrent-transformer" ["e"=1]
"cshizhe/asg2cap" -> "YuanEZhou/Grounded-Image-Captioning"
"violetteshev/bottom-up-features" -> "poojahira/image-captioning-bottom-up-top-down"
"s-gupta/visual-concepts" -> "chapternewscu/image-captioning-with-semantic-attention"
"s-gupta/visual-concepts" -> "kimiyoung/review_net"
"s-gupta/visual-concepts" -> "LuoweiZhou/e2e-gLSTM-sc"
"s-gupta/visual-concepts" -> "gujiuxiang/MIL.pytorch" ["e"=1]
"s-gupta/visual-concepts" -> "zhegan27/Semantic_Compositional_Nets"
"s-gupta/visual-concepts" -> "fenglinliu98/MIA"
"s-gupta/visual-concepts" -> "mjhucla/mRNN-CR"
"s-gupta/visual-concepts" -> "aditya12agd5/convcap"
"s-gupta/visual-concepts" -> "andyweizhao/Multitask_Image_Captioning"
"s-gupta/visual-concepts" -> "yahoo/object_relation_transformer"
"s-gupta/visual-concepts" -> "ruotianluo/Transformer_Captioning"
"s-gupta/visual-concepts" -> "doubledaibo/gancaption_iccv2017"
"s-gupta/visual-concepts" -> "gujiuxiang/Stack-Captioning"
"JingweiJ/ActionGenome" -> "yrcong/STTran"
"JingweiJ/ActionGenome" -> "joaanna/something_else" ["e"=1]
"JingweiJ/ActionGenome" -> "NVIDIA/ContrastiveLosses4VRD"
"JingweiJ/ActionGenome" -> "MCG-NJU/TRACE"
"JingweiJ/ActionGenome" -> "xdshang/VidVRD-helper"
"google-research-datasets/conceptual-12m" -> "google-research-datasets/conceptual-captions"
"google-research-datasets/conceptual-12m" -> "igorbrigadir/DownloadConceptualCaptions"
"google-research-datasets/conceptual-captions" -> "google-research-datasets/conceptual-12m"
"google-research-datasets/conceptual-captions" -> "igorbrigadir/DownloadConceptualCaptions"
"google-research-datasets/conceptual-captions" -> "fengyang0317/unsupervised_captioning"
"google-research-datasets/conceptual-captions" -> "microsoft/Oscar"
"google-research-datasets/conceptual-captions" -> "google-research-datasets/wit"
"google-research-datasets/conceptual-captions" -> "fartashf/vsepp" ["e"=1]
"google-research-datasets/conceptual-captions" -> "peteanderson80/Up-Down-Captioner"
"google-research-datasets/conceptual-captions" -> "yahoo/object_relation_transformer"
"google-research-datasets/conceptual-captions" -> "ruotianluo/GoogleConceptualCaptioning"
"google-research-datasets/conceptual-captions" -> "facebookresearch/grid-feats-vqa"
"google-research-datasets/conceptual-captions" -> "jayleicn/ClipBERT"
"google-research-datasets/conceptual-captions" -> "facebookresearch/vilbert-multi-task"
"google-research-datasets/conceptual-captions" -> "zhjohnchan/awesome-image-captioning"
"google-research-datasets/conceptual-captions" -> "lichengunc/refer" ["e"=1]
"google-research-datasets/conceptual-captions" -> "jiasenlu/vilbert_beta"
"ChenRocks/UNITER" -> "jackroos/VL-BERT"
"ChenRocks/UNITER" -> "airsplay/lxmert"
"ChenRocks/UNITER" -> "facebookresearch/vilbert-multi-task"
"ChenRocks/UNITER" -> "microsoft/Oscar"
"ChenRocks/UNITER" -> "uclanlp/visualbert"
"ChenRocks/UNITER" -> "jiasenlu/vilbert_beta"
"ChenRocks/UNITER" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"ChenRocks/UNITER" -> "LuoweiZhou/VLP"
"ChenRocks/UNITER" -> "salesforce/ALBEF"
"ChenRocks/UNITER" -> "peteanderson80/bottom-up-attention"
"ChenRocks/UNITER" -> "dandelin/ViLT"
"ChenRocks/UNITER" -> "MILVLG/mcan-vqa"
"ChenRocks/UNITER" -> "linjieli222/HERO" ["e"=1]
"ChenRocks/UNITER" -> "jayleicn/ClipBERT"
"ChenRocks/UNITER" -> "pzzhang/VinVL"
"uclanlp/visualbert" -> "jiasenlu/vilbert_beta"
"uclanlp/visualbert" -> "airsplay/lxmert"
"uclanlp/visualbert" -> "jackroos/VL-BERT"
"uclanlp/visualbert" -> "facebookresearch/vilbert-multi-task"
"uclanlp/visualbert" -> "ChenRocks/UNITER"
"uclanlp/visualbert" -> "YIKUAN8/Transformers-VQA"
"uclanlp/visualbert" -> "microsoft/Oscar"
"uclanlp/visualbert" -> "LuoweiZhou/VLP"
"uclanlp/visualbert" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"uclanlp/visualbert" -> "peteanderson80/bottom-up-attention"
"uclanlp/visualbert" -> "facebookresearch/grid-feats-vqa"
"uclanlp/visualbert" -> "husthuaan/AoANet"
"uclanlp/visualbert" -> "jnhwkim/ban-vqa"
"uclanlp/visualbert" -> "airsplay/py-bottom-up-attention"
"uclanlp/visualbert" -> "pzzhang/VinVL"
"rsokl/MyGrad" -> "rsokl/noggin"
"rsokl/MyGrad" -> "davidmascharka/MyNN"
"rsokl/MyGrad" -> "jjyang12/upgraded-octo-discocito-3"
"rsokl/MyGrad" -> "davidmascharka/tbd-nets"
"rsokl/noggin" -> "davidmascharka/MyNN"
"avisingh599/visual-qa" -> "iamaaditya/VQA_Demo"
"avisingh599/visual-qa" -> "VT-vision-lab/VQA_LSTM_CNN"
"avisingh599/visual-qa" -> "anantzoid/VQA-Keras-Visual-Question-Answering"
"avisingh599/visual-qa" -> "abhshkdz/neural-vqa"
"avisingh599/visual-qa" -> "jiasenlu/HieCoAttenVQA"
"avisingh599/visual-qa" -> "metalbubble/VQAbaseline"
"avisingh599/visual-qa" -> "GT-Vision-Lab/VQA_LSTM_CNN"
"avisingh599/visual-qa" -> "nivwusquorum/tf-adversarial"
"avisingh599/visual-qa" -> "JamesChuanggg/awesome-vqa"
"avisingh599/visual-qa" -> "akirafukui/vqa-mcb"
"avisingh599/visual-qa" -> "renmengye/imageqa-public"
"avisingh599/visual-qa" -> "dblN/stochastic_depth_keras"
"avisingh599/visual-qa" -> "codekansas/keras-language-modeling" ["e"=1]
"avisingh599/visual-qa" -> "vinhkhuc/MemN2N-babi-python" ["e"=1]
"avisingh599/visual-qa" -> "allenai/deep_qa" ["e"=1]
"tfzhou/ProtoSeg" -> "tfzhou/ContrastiveSeg" ["e"=1]
"tfzhou/ProtoSeg" -> "leonnnop/GMMSeg" ["e"=1]
"tfzhou/ProtoSeg" -> "MendelXu/zsseg.baseline"
"tfzhou/ProtoSeg" -> "dvlab-research/GFS-Seg" ["e"=1]
"tfzhou/ProtoSeg" -> "chongzhou96/MaskCLIP"
"tfzhou/ProtoSeg" -> "dingjiansw101/ZegFormer"
"tfzhou/ProtoSeg" -> "YiF-Zhang/RegionProxy"
"tfzhou/ProtoSeg" -> "CVI-SZU/CLIMS" ["e"=1]
"tfzhou/ProtoSeg" -> "LiheYoung/ST-PlusPlus" ["e"=1]
"tfzhou/ProtoSeg" -> "chunbolang/BAM" ["e"=1]
"tfzhou/ProtoSeg" -> "LiheYoung/MiningFSS" ["e"=1]
"tfzhou/ProtoSeg" -> "NVlabs/GroupViT"
"tfzhou/ProtoSeg" -> "NVlabs/FreeSOLO" ["e"=1]
"tfzhou/ProtoSeg" -> "facebookresearch/MaskFormer" ["e"=1]
"fawazsammani/knowing-when-to-look-adaptive-attention" -> "yufengm/Adaptive"
"fawazsammani/knowing-when-to-look-adaptive-attention" -> "gujiuxiang/Stack-Captioning"
"gujiuxiang/Stack-Captioning" -> "cswhjiang/Recurrent_Fusion_Network"
"gujiuxiang/Stack-Captioning" -> "andyweizhao/Multitask_Image_Captioning"
"gujiuxiang/Stack-Captioning" -> "yiyang92/vae_captioning"
"njchoma/transformer_image_caption" -> "poojahira/image-captioning-bottom-up-top-down"
"yikang-li/MSDN" -> "yikang-li/FactorizableNet"
"yikang-li/MSDN" -> "danfeiX/scene-graph-TF-release"
"yikang-li/MSDN" -> "rowanz/neural-motifs"
"yikang-li/MSDN" -> "jwyang/graph-rcnn.pytorch"
"yikang-li/MSDN" -> "doubledaibo/drnet_cvpr2017"
"yikang-li/MSDN" -> "yikang-li/vg_cleansing"
"yikang-li/MSDN" -> "shikorab/SceneGraph"
"yikang-li/MSDN" -> "yangxuntu/SGAE"
"yikang-li/MSDN" -> "Prof-Lu-Cewu/Visual-Relationship-Detection"
"yikang-li/MSDN" -> "YiwuZhong/Sub-GC"
"yikang-li/MSDN" -> "nexusapoorvacus/DeepVariationStructuredRL"
"yikang-li/MSDN" -> "alisure-ml/awesome-visual-relationship-detection"
"yikang-li/MSDN" -> "ranjaykrishna/visual_genome_python_driver"
"yikang-li/MSDN" -> "umich-vl/px2graph"
"yikang-li/MSDN" -> "peteanderson80/SPICE"
"google-research-datasets/wit" -> "microsoft/Oscar"
"google-research-datasets/wit" -> "google-research-datasets/conceptual-captions"
"google-research-datasets/wit" -> "jiasenlu/vilbert_beta"
"google-research-datasets/wit" -> "ChenRocks/UNITER"
"google-research-datasets/wit" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"google-research-datasets/wit" -> "clip-vil/CLIP-ViL"
"google-research-datasets/wit" -> "ashkamath/mdetr"
"google-research-datasets/wit" -> "salesforce/ALBEF"
"google-research-datasets/wit" -> "facebookresearch/SLIP"
"google-research-datasets/wit" -> "rom1504/img2dataset"
"google-research-datasets/wit" -> "facebookresearch/vilbert-multi-task"
"google-research-datasets/wit" -> "Zasder3/train-CLIP"
"google-research-datasets/wit" -> "pzzhang/VinVL"
"google-research-datasets/wit" -> "m-bain/frozen-in-time" ["e"=1]
"google-research-datasets/wit" -> "FreddeFrallan/Multilingual-CLIP"
"microsoft/FocalNet" -> "microsoft/Focal-Transformer" ["e"=1]
"microsoft/FocalNet" -> "IDEA-Research/detrex"
"microsoft/FocalNet" -> "OpenGVLab/InternImage"
"microsoft/FocalNet" -> "FocalNet/Networks-Beyond-Attention"
"microsoft/FocalNet" -> "NVlabs/FAN"
"microsoft/FocalNet" -> "FocalNet/FocalNet-DINO"
"microsoft/FocalNet" -> "sail-sg/metaformer"
"microsoft/FocalNet" -> "ShoufaChen/DiffusionDet"
"microsoft/FocalNet" -> "jozhang97/DETA"
"microsoft/FocalNet" -> "raoyongming/HorNet"
"microsoft/FocalNet" -> "snap-research/EfficientFormer" ["e"=1]
"microsoft/FocalNet" -> "Visual-Attention-Network/SegNeXt"
"microsoft/FocalNet" -> "SHI-Labs/OneFormer"
"microsoft/FocalNet" -> "IDEA-Research/DINO"
"microsoft/FocalNet" -> "facebookresearch/ConvNeXt-V2"
"floodsung/Deep-Reasoning-Papers" -> "WellyZhang/RAVEN"
"floodsung/Deep-Reasoning-Papers" -> "tkipf/c-swm"
"floodsung/Deep-Reasoning-Papers" -> "yue-zhongqi/ifsl" ["e"=1]
"stanfordnlp/mac-network" -> "rosinality/mac-network-pytorch"
"stanfordnlp/mac-network" -> "hengyuan-hu/bottom-up-attention-vqa"
"stanfordnlp/mac-network" -> "jnhwkim/ban-vqa"
"stanfordnlp/mac-network" -> "ceyzaguirre4/NSM"
"stanfordnlp/mac-network" -> "kexinyi/ns-vqa"
"stanfordnlp/mac-network" -> "vacancy/NSCL-PyTorch-Release"
"stanfordnlp/mac-network" -> "facebookresearch/clevr-iep"
"stanfordnlp/mac-network" -> "facebookresearch/clevr-dataset-gen"
"stanfordnlp/mac-network" -> "Cadene/murel.bootstrap.pytorch"
"stanfordnlp/mac-network" -> "MILVLG/mcan-vqa"
"stanfordnlp/mac-network" -> "ronghanghu/n2nmn"
"stanfordnlp/mac-network" -> "ethanjperez/film"
"stanfordnlp/mac-network" -> "peteanderson80/bottom-up-attention"
"stanfordnlp/mac-network" -> "davidmascharka/tbd-nets"
"stanfordnlp/mac-network" -> "Cyanogenoid/vqa-counting"
"232525/PureT" -> "davidnvq/grit"
"232525/PureT" -> "YuanEZhou/CBTrans"
"Cadene/murel.bootstrap.pytorch" -> "Cadene/block.bootstrap.pytorch"
"Cadene/murel.bootstrap.pytorch" -> "aimbrain/vqa-project"
"Cadene/murel.bootstrap.pytorch" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"Cadene/murel.bootstrap.pytorch" -> "cdancette/rubi.bootstrap.pytorch"
"Cadene/murel.bootstrap.pytorch" -> "linjieli222/VQA_ReGAT"
"Cadene/murel.bootstrap.pytorch" -> "MILVLG/mcan-vqa"
"Cadene/murel.bootstrap.pytorch" -> "jnhwkim/ban-vqa"
"Cadene/murel.bootstrap.pytorch" -> "Cyanogenoid/vqa-counting"
"Cadene/murel.bootstrap.pytorch" -> "Cadene/bootstrap.pytorch"
"Cadene/murel.bootstrap.pytorch" -> "ronghanghu/lcgn"
"Cadene/murel.bootstrap.pytorch" -> "hengyuan-hu/bottom-up-attention-vqa"
"Cadene/murel.bootstrap.pytorch" -> "yanxinzju/CSS-VQA"
"Cadene/murel.bootstrap.pytorch" -> "SinghJasdeep/Attention-on-Attention-for-VQA"
"Cadene/murel.bootstrap.pytorch" -> "MILVLG/openvqa"
"Cadene/murel.bootstrap.pytorch" -> "stanfordnlp/mac-network"
"Gitsamshi/WeakVRD-Captioning" -> "visinf/cos-cvae"
"Gitsamshi/WeakVRD-Captioning" -> "fawazsammani/look-and-modify"
"MILVLG/openvqa" -> "MILVLG/mcan-vqa"
"MILVLG/openvqa" -> "facebookresearch/grid-feats-vqa"
"MILVLG/openvqa" -> "yuzcccc/vqa-mfb"
"MILVLG/openvqa" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"MILVLG/openvqa" -> "jnhwkim/ban-vqa"
"MILVLG/openvqa" -> "airsplay/lxmert"
"MILVLG/openvqa" -> "asdf0982/vqa-mfb.pytorch"
"MILVLG/openvqa" -> "jokieleung/awesome-visual-question-answering"
"MILVLG/openvqa" -> "hengyuan-hu/bottom-up-attention-vqa"
"MILVLG/openvqa" -> "linjieli222/VQA_ReGAT"
"MILVLG/openvqa" -> "Cyanogenoid/vqa-counting"
"MILVLG/openvqa" -> "Cadene/murel.bootstrap.pytorch"
"MILVLG/openvqa" -> "yanxinzju/CSS-VQA"
"MILVLG/openvqa" -> "peteanderson80/bottom-up-attention"
"MILVLG/openvqa" -> "jiasenlu/vilbert_beta"
"davidnvq/grit" -> "zhangxuying1004/RSTNet"
"davidnvq/grit" -> "232525/PureT"
"davidnvq/grit" -> "GT-RIPL/Xmodal-Ctx"
"davidnvq/grit" -> "jchenghu/ExpansionNet_v2"
"davidnvq/grit" -> "SjokerLily/awesome-image-captioning"
"facebookresearch/grid-feats-vqa" -> "luo3300612/image-captioning-DLCT"
"facebookresearch/grid-feats-vqa" -> "MILVLG/mcan-vqa"
"facebookresearch/grid-feats-vqa" -> "MILVLG/openvqa"
"facebookresearch/grid-feats-vqa" -> "JDAI-CV/image-captioning"
"facebookresearch/grid-feats-vqa" -> "shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome"
"facebookresearch/grid-feats-vqa" -> "zhangxuying1004/RSTNet"
"facebookresearch/grid-feats-vqa" -> "microsoft/Oscar"
"facebookresearch/grid-feats-vqa" -> "pzzhang/VinVL"
"facebookresearch/grid-feats-vqa" -> "airsplay/py-bottom-up-attention"
"facebookresearch/grid-feats-vqa" -> "husthuaan/AoANet"
"facebookresearch/grid-feats-vqa" -> "aimagelab/meshed-memory-transformer"
"facebookresearch/grid-feats-vqa" -> "forence/Awesome-Visual-Captioning"
"facebookresearch/grid-feats-vqa" -> "researchmm/soho"
"facebookresearch/grid-feats-vqa" -> "MILVLG/bottom-up-attention.pytorch"
"facebookresearch/grid-feats-vqa" -> "yahoo/object_relation_transformer"
"fengyang0317/unsupervised_captioning" -> "aimagelab/show-control-and-tell"
"fengyang0317/unsupervised_captioning" -> "husthuaan/AoANet"
"fengyang0317/unsupervised_captioning" -> "yangxuntu/SGAE"
"fengyang0317/unsupervised_captioning" -> "zhjohnchan/awesome-image-captioning"
"fengyang0317/unsupervised_captioning" -> "yahoo/object_relation_transformer"
"fengyang0317/unsupervised_captioning" -> "jiasenlu/NeuralBabyTalk"
"fengyang0317/unsupervised_captioning" -> "gujiuxiang/Stack-Captioning"
"fengyang0317/unsupervised_captioning" -> "ruotianluo/DiscCaptioning"
"fengyang0317/unsupervised_captioning" -> "ruotianluo/Transformer_Captioning"
"fengyang0317/unsupervised_captioning" -> "ruotianluo/self-critical.pytorch"
"fengyang0317/unsupervised_captioning" -> "peteanderson80/Up-Down-Captioner"
"fengyang0317/unsupervised_captioning" -> "aditya12agd5/convcap"
"fengyang0317/unsupervised_captioning" -> "doubledaibo/gancaption_iccv2017"
"fengyang0317/unsupervised_captioning" -> "JDAI-CV/image-captioning"
"fengyang0317/unsupervised_captioning" -> "Dong-JinKim/DenseRelationalCaptioning"
"gaopengcuhk/Pretrained-Pix2Seq" -> "gaopengcuhk/Stable-Pix2Seq"
"jnhwkim/MulLowBiVQA" -> "jnhwkim/cbp"
"zhangxuying1004/RSTNet" -> "luo3300612/image-captioning-DLCT"
"zhangxuying1004/RSTNet" -> "yahoo/object_relation_transformer"
"zhangxuying1004/RSTNet" -> "davidnvq/grit"
"zhangxuying1004/RSTNet" -> "mrwu-mac/DIFNet"
"zhangxuying1004/RSTNet" -> "Gitsamshi/WeakVRD-Captioning"
"zhangxuying1004/RSTNet" -> "luo3300612/Transformer-Captioning"
"tkipf/c-swm" -> "applied-ai-lab/genesis"
"tkipf/c-swm" -> "jcoreyes/OP3"
"tkipf/c-swm" -> "lucidrains/slot-attention"
"tkipf/c-swm" -> "mila-iqia/atari-representation-learning" ["e"=1]
"tkipf/c-swm" -> "baudm/MONet-pytorch"
"tkipf/c-swm" -> "deepmind/multi_object_datasets"
"tkipf/c-swm" -> "danijar/dreamer" ["e"=1]
"tkipf/c-swm" -> "stelzner/monet"
"tkipf/c-swm" -> "floodsung/Deep-Reasoning-Papers"
"tkipf/c-swm" -> "MishaLaskin/curl" ["e"=1]
"tkipf/c-swm" -> "JindongJiang/SCALOR"
"tkipf/c-swm" -> "maximecb/gym-miniworld" ["e"=1]
"tkipf/c-swm" -> "ramanans1/plan2explore" ["e"=1]
"tkipf/c-swm" -> "jlko/STOVE"
"iamaaditya/VQA_Demo" -> "anantzoid/VQA-Keras-Visual-Question-Answering"
"iamaaditya/VQA_Demo" -> "GT-Vision-Lab/VQA_LSTM_CNN"
"iamaaditya/VQA_Demo" -> "jiasenlu/HieCoAttenVQA"
"iamaaditya/VQA_Demo" -> "avisingh599/visual-qa"
"iamaaditya/VQA_Demo" -> "iamaaditya/VQA_Keras"
"iamaaditya/VQA_Demo" -> "JamesChuanggg/awesome-vqa"
"iamaaditya/VQA_Demo" -> "abhshkdz/neural-vqa"
"iamaaditya/VQA_Demo" -> "metalbubble/VQAbaseline"
"iamaaditya/VQA_Demo" -> "JamesChuanggg/VQA-tensorflow"
"iamaaditya/VQA_Demo" -> "akirafukui/vqa-mcb"
"iamaaditya/VQA_Demo" -> "GT-Vision-Lab/VQA"
"iamaaditya/VQA_Demo" -> "Cyanogenoid/vqa-counting"
"iamaaditya/VQA_Demo" -> "Cadene/vqa.pytorch"
"iamaaditya/VQA_Demo" -> "anujshah1003/VQA-Demo-GUI"
"iamaaditya/VQA_Demo" -> "VedantYadav/VQA"
"rayleizhu/BiFormer" -> "OpenGVLab/STM-Evaluation"
"husthuaan/AoANet" -> "yahoo/object_relation_transformer"
"husthuaan/AoANet" -> "yangxuntu/SGAE"
"husthuaan/AoANet" -> "JDAI-CV/image-captioning"
"husthuaan/AoANet" -> "aimagelab/meshed-memory-transformer"
"husthuaan/AoANet" -> "ruotianluo/self-critical.pytorch"
"husthuaan/AoANet" -> "aimagelab/show-control-and-tell"
"husthuaan/AoANet" -> "husthuaan/AAT"
"husthuaan/AoANet" -> "fengyang0317/unsupervised_captioning"
"husthuaan/AoANet" -> "luo3300612/image-captioning-DLCT"
"husthuaan/AoANet" -> "cshizhe/asg2cap"
"husthuaan/AoANet" -> "zhjohnchan/awesome-image-captioning"
"husthuaan/AoANet" -> "ruotianluo/ImageCaptioning.pytorch"
"husthuaan/AoANet" -> "ruotianluo/Transformer_Captioning"
"husthuaan/AoANet" -> "poojahira/image-captioning-bottom-up-top-down"
"husthuaan/AoANet" -> "forence/Awesome-Visual-Captioning"
"MCG-NJU/AdaMixer" -> "IDEA-opensource/DN-DETR"
"MCG-NJU/AdaMixer" -> "jshilong/DDQ"
"MCG-NJU/AdaMixer" -> "FengLi-ust/DN-DETR"
"MCG-NJU/AdaMixer" -> "Atten4Vis/ConditionalDETR"
"MCG-NJU/AdaMixer" -> "IDEACVR/awesome-detection-transformer"
"MCG-NJU/AdaMixer" -> "Sense-X/Co-DETR"
"MCG-NJU/AdaMixer" -> "IDEA-opensource/DAB-DETR"
"yuleiniu/rva" -> "vmurahari3/visdial-bert"
"yuleiniu/rva" -> "zilongzheng/visdial-gnn"
"yuleiniu/rva" -> "batra-mlp-lab/visdial-challenge-starter-pytorch"
"yuleiniu/rva" -> "naver/aqm-plus"
"yuleiniu/rva" -> "vmurahari3/visdial-diversity"
"yuleiniu/rva" -> "jiasenlu/visDial.pytorch"
"yuleiniu/rva" -> "shubhamagarwal92/visdial_conv"
"yuleiniu/rva" -> "satwikkottur/clevr-dialog"
"megvii-research/RevCol" -> "megvii-research/RepLKNet"
"megvii-research/RevCol" -> "OpenGVLab/STM-Evaluation"
"xiaofeng94/VL-PLM" -> "samschulter/omnilabeltools"
"ranjaykrishna/visual_genome_python_driver" -> "yikang-li/MSDN"
"ranjaykrishna/visual_genome_python_driver" -> "danfeiX/scene-graph-TF-release"
"ranjaykrishna/visual_genome_python_driver" -> "rowanz/neural-motifs"
"ranjaykrishna/visual_genome_python_driver" -> "jwyang/graph-rcnn.pytorch"
"ranjaykrishna/visual_genome_python_driver" -> "shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome"
"ranjaykrishna/visual_genome_python_driver" -> "peteanderson80/SPICE"
"ranjaykrishna/visual_genome_python_driver" -> "yikang-li/vg_cleansing"
"ranjaykrishna/visual_genome_python_driver" -> "peteanderson80/bottom-up-attention"
"ranjaykrishna/visual_genome_python_driver" -> "vacancy/SceneGraphParser"
"ranjaykrishna/visual_genome_python_driver" -> "Prof-Lu-Cewu/Visual-Relationship-Detection"
"ranjaykrishna/visual_genome_python_driver" -> "KaihuaTang/Scene-Graph-Benchmark.pytorch"
"ranjaykrishna/visual_genome_python_driver" -> "microsoft/scene_graph_benchmark"
"ranjaykrishna/visual_genome_python_driver" -> "yikang-li/FactorizableNet"
"ranjaykrishna/visual_genome_python_driver" -> "doubledaibo/drnet_cvpr2017"
"ranjaykrishna/visual_genome_python_driver" -> "google-research-datasets/conceptual-captions"
"xieenze/DetCo" -> "limbo0000/InstanceLoc"
"xieenze/DetCo" -> "PerdonLiu/CSE-Autoloss"
"xieenze/DetCo" -> "WXinlong/DenseCL" ["e"=1]
"xieenze/DetCo" -> "shuuchen/DetCo.pytorch"
"xieenze/DetCo" -> "xieenze/Trans2Seg" ["e"=1]
"xieenze/DetCo" -> "Jia-Research-Lab/SA-AutoAug"
"xieenze/DetCo" -> "kkhoot/PAA" ["e"=1]
"facebookresearch/CutLER" -> "NVlabs/ODISE" ["e"=1]
"facebookresearch/CutLER" -> "microsoft/X-Decoder"
"facebookresearch/CutLER" -> "baaivision/EVA"
"facebookresearch/CutLER" -> "SHI-Labs/OneFormer"
"facebookresearch/CutLER" -> "chongzhou96/MaskCLIP"
"facebookresearch/CutLER" -> "facebookresearch/ToMe"
"facebookresearch/CutLER" -> "YangtaoWANG95/TokenCut" ["e"=1]
"facebookresearch/CutLER" -> "facebookresearch/paco"
"facebookresearch/CutLER" -> "facebookresearch/Detic"
"facebookresearch/CutLER" -> "ma-xu/Context-Cluster"
"facebookresearch/CutLER" -> "LiWentomng/BoxInstSeg" ["e"=1]
"facebookresearch/CutLER" -> "baaivision/Painter" ["e"=1]
"facebookresearch/CutLER" -> "UX-Decoder/Segment-Everything-Everywhere-All-At-Once" ["e"=1]
"facebookresearch/CutLER" -> "ShoufaChen/DiffusionDet"
"facebookresearch/CutLER" -> "czczup/ViT-Adapter"
"sail-sg/metaformer" -> "sail-sg/poolformer" ["e"=1]
"sail-sg/metaformer" -> "HVision-NKU/Conv2Former"
"sail-sg/metaformer" -> "sail-sg/iFormer"
"sail-sg/metaformer" -> "OpenGVLab/STM-Evaluation"
"sail-sg/metaformer" -> "facebookresearch/ConvNeXt-V2"
"haltakov/natural-language-youtube-search" -> "haltakov/natural-language-image-search"
"haltakov/natural-language-youtube-search" -> "thegrims/UsTaxes"
"haltakov/natural-language-youtube-search" -> "dennis-tra/pcp" ["e"=1]
"haltakov/natural-language-youtube-search" -> "baxtree/subaligner" ["e"=1]
"AIChallenger/AI_Challenger_2017" -> "foamliu/Image-Captioning-PyTorch"
"chapternewscu/image-captioning-with-semantic-attention" -> "LuoweiZhou/e2e-gLSTM-sc"
"chapternewscu/image-captioning-with-semantic-attention" -> "s-gupta/visual-concepts"
"jazzsaxmafia/show_and_tell.tensorflow" -> "jazzsaxmafia/show_attend_and_tell.tensorflow"
"jazzsaxmafia/show_and_tell.tensorflow" -> "mjhucla/mRNN-CR"
"jazzsaxmafia/show_and_tell.tensorflow" -> "mosessoh/CNN-LSTM-Caption-Generator"
"jazzsaxmafia/show_and_tell.tensorflow" -> "kelvinxu/arctic-captions"
"jazzsaxmafia/show_and_tell.tensorflow" -> "metalbubble/VQAbaseline"
"jazzsaxmafia/show_and_tell.tensorflow" -> "chapternewscu/image-captioning-with-semantic-attention"
"jazzsaxmafia/show_and_tell.tensorflow" -> "VT-vision-lab/VQA_LSTM_CNN"
"jazzsaxmafia/show_and_tell.tensorflow" -> "mjhucla/TF-mRNN"
"jazzsaxmafia/show_and_tell.tensorflow" -> "ruotianluo/neuraltalk2-tensorflow"
"jazzsaxmafia/show_and_tell.tensorflow" -> "yunjey/show-attend-and-tell"
"jazzsaxmafia/show_and_tell.tensorflow" -> "ry/tensorflow-vgg16" ["e"=1]
"jazzsaxmafia/show_and_tell.tensorflow" -> "paarthneekhara/neural-vqa-tensorflow"
"mlfoundations/model-soups" -> "LAION-AI/CLIP_benchmark"
"zhenyuw16/UniDetector" -> "IDEA-Research/OpenSeeD"
"zhenyuw16/UniDetector" -> "clin1223/VLDet"
"zhenyuw16/UniDetector" -> "orrzohar/PROB"
"zhenyuw16/UniDetector" -> "wusize/ovdet"
"zhenyuw16/UniDetector" -> "tgxs002/CORA"
"zhenyuw16/UniDetector" -> "jozhang97/DETA"
"zhenyuw16/UniDetector" -> "microsoft/RegionCLIP"
"batra-mlp-lab/visdial-rl" -> "batra-mlp-lab/visdial-challenge-starter-pytorch"
"batra-mlp-lab/visdial-rl" -> "jiasenlu/visDial.pytorch"
"batra-mlp-lab/visdial-rl" -> "satwikkottur/clevr-dialog"
"batra-mlp-lab/visdial-rl" -> "batra-mlp-lab/visdial"
"batra-mlp-lab/visdial-rl" -> "GuessWhatGame/guesswhat"
"batra-mlp-lab/visdial-rl" -> "naver/aqm-plus"
"batra-mlp-lab/visdial-rl" -> "batra-mlp-lab/visdial-amt-chat"
"batra-mlp-lab/visdial-rl" -> "Cyanogenoid/vqa-counting"
"batra-mlp-lab/visdial-rl" -> "vmurahari3/visdial-diversity"
"Atten4Vis/ConditionalDETR" -> "megvii-research/AnchorDETR"
"Atten4Vis/ConditionalDETR" -> "IDEA-opensource/DN-DETR"
"Atten4Vis/ConditionalDETR" -> "kakaobrain/sparse-detr"
"Atten4Vis/ConditionalDETR" -> "gaopengcuhk/SMCA-DETR"
"Atten4Vis/ConditionalDETR" -> "jozhang97/DETA"
"Atten4Vis/ConditionalDETR" -> "MCG-NJU/AdaMixer"
"Atten4Vis/ConditionalDETR" -> "jshilong/DDQ"
"Atten4Vis/ConditionalDETR" -> "IDEA-opensource/DAB-DETR"
"Atten4Vis/ConditionalDETR" -> "IDEACVR/awesome-detection-transformer"
"Atten4Vis/ConditionalDETR" -> "FengLi-ust/DN-DETR"
"Atten4Vis/ConditionalDETR" -> "twangnh/pnp-detr"
"Atten4Vis/ConditionalDETR" -> "dddzg/up-detr"
"Atten4Vis/ConditionalDETR" -> "facebookresearch/MaskFormer" ["e"=1]
"Atten4Vis/ConditionalDETR" -> "IDEACVR/DINO"
"jozhang97/DETA" -> "jshilong/DDQ"
"jozhang97/DETA" -> "HDETR/H-Deformable-DETR-mmdet"
"jozhang97/DETA" -> "Atten4Vis/ConditionalDETR"
"lvis-dataset/lvis-api" -> "tztztztztz/eql.detectron2" ["e"=1]
"lvis-dataset/lvis-api" -> "GothicAi/Instaboost" ["e"=1]
"lvis-dataset/lvis-api" -> "facebookresearch/phyre"
"lvis-dataset/lvis-api" -> "thangvubk/Cascade-RPN" ["e"=1]
"igorbrigadir/DownloadConceptualCaptions" -> "gchhablani/multilingual-image-captioning"
"Cloud-CV/visual-chatbot" -> "Cloud-CV/Origami"
"Cloud-CV/visual-chatbot" -> "Cloud-CV/CloudCV"
"Cloud-CV/visual-chatbot" -> "batra-mlp-lab/visdial"
"Cloud-CV/visual-chatbot" -> "Cloud-CV/VQA"
"Cloud-CV/visual-chatbot" -> "Cloud-CV/GSoC-Ideas"
"snap-research/R2L" -> "snap-research/MobileR2L"
"snap-research/R2L" -> "facebookresearch/neural-light-fields" ["e"=1]
"snap-research/R2L" -> "wyzjack/SLA2P"
"FengLi-ust/DN-DETR" -> "SlongLiu/DAB-DETR"
"moein-shariatnia/OpenAI-CLIP" -> "mlfoundations/wise-ft"
"moein-shariatnia/OpenAI-CLIP" -> "rmokady/CLIP_prefix_caption"
"moein-shariatnia/OpenAI-CLIP" -> "FreddeFrallan/Multilingual-CLIP"
"moein-shariatnia/OpenAI-CLIP" -> "weiyx16/CLIP-pytorch"
"VITA-Group/SLaK" -> "DingXiaoH/RepLKNet-pytorch"
"VITA-Group/SLaK" -> "sail-sg/inceptionnext"
"VITA-Group/SLaK" -> "raoyongming/HorNet"
"microsoft/TAP" -> "uakarsh/latr"
"microsoft/TAP" -> "yashkant/sam-textvqa"
"microsoft/TAP" -> "xinke-wang/Awesome-Text-VQA"
"microsoft/TAP" -> "ZephyrZhuQi/ssbaseline"
"microsoft/TAP" -> "ChenyuGAO-CS/SMA"
"computationalmedia/semstyle" -> "kacky24/stylenet"
"computationalmedia/semstyle" -> "yiyang92/caption-stylenet_tensorflow"
"ruotianluo/coco-caption" -> "ruotianluo/cider"
"Cheems-Seminar/segment-anything-and-name-it" -> "jshilong/DDQ"
"IDEA-Research/OpenSeeD" -> "IDEA-Research/Stable-DINO"
"abhshkdz/neural-vqa" -> "iamaaditya/VQA_Demo"
"abhshkdz/neural-vqa" -> "avisingh599/visual-qa"
"abhshkdz/neural-vqa" -> "jiasenlu/HieCoAttenVQA"
"abhshkdz/neural-vqa" -> "metalbubble/VQAbaseline"
"abhshkdz/neural-vqa" -> "paarthneekhara/neural-vqa-tensorflow"
"abhshkdz/neural-vqa" -> "kaishengtai/torch-ntm" ["e"=1]
"abhshkdz/neural-vqa" -> "VT-vision-lab/VQA_LSTM_CNN"
"abhshkdz/neural-vqa" -> "carpedm20/visual-analogy-tensorflow" ["e"=1]
"abhshkdz/neural-vqa" -> "Cadene/vqa.pytorch"
"abhshkdz/neural-vqa" -> "VT-vision-lab/VQA"
"abhshkdz/neural-vqa" -> "Element-Research/rnn" ["e"=1]
"abhshkdz/neural-vqa" -> "akirafukui/vqa-mcb"
"abhshkdz/neural-vqa" -> "JamesChuanggg/VQA-tensorflow"
"abhshkdz/neural-vqa" -> "ericjang/tdb" ["e"=1]
"abhshkdz/neural-vqa" -> "cheng6076/SNLI-attention" ["e"=1]
"batra-mlp-lab/visdial" -> "jiasenlu/visDial.pytorch"
"batra-mlp-lab/visdial" -> "batra-mlp-lab/visdial-amt-chat"
"batra-mlp-lab/visdial" -> "batra-mlp-lab/visdial-rl"
"batra-mlp-lab/visdial" -> "batra-mlp-lab/visdial-challenge-starter-pytorch"
"batra-mlp-lab/visdial" -> "Cloud-CV/visual-chatbot"
"batra-mlp-lab/visdial" -> "facebookresearch/corefnmn"
"yrcong/RelTR" -> "yrcong/STTran"
"yrcong/RelTR" -> "Scarecrow0/SGTR"
"yrcong/RelTR" -> "MCG-NJU/Structured-Sparse-RCNN"
"yrcong/RelTR" -> "SHTUPLUS/PySGG"
"yrcong/RelTR" -> "liuhengyue/fcsgg" ["e"=1]
"yrcong/RelTR" -> "layer6ai-labs/SGG-Seq2Seq"
"yrcong/RelTR" -> "waxnkw/IETrans-SGG.pytorch"
"yrcong/RelTR" -> "anoopsanka/retinal_oct"
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "KaiyangZhou/CoOp"
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "KMnP/vpt"
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "hjbahng/visual_prompting"
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "muzairkhattak/multimodal-prompt-learning"
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "gaopengcuhk/Tip-Adapter"
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "yzhuoning/Awesome-CLIP"
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "MarkMoHR/Awesome-Referring-Image-Segmentation" ["e"=1]
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "gaopengcuhk/CLIP-Adapter"
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "google-research/l2p" ["e"=1]
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "ylsung/VL_adapter"
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "sangminwoo/awesome-vision-and-language"
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning"
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "YuejiangLIU/awesome-source-free-test-time-adaptation" ["e"=1]
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "Computer-Vision-in-the-Wild/CVinW_Readings"
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "microsoft/GLIP"
"gaopengcuhk/Tip-Adapter" -> "gaopengcuhk/CLIP-Adapter"
"gaopengcuhk/Tip-Adapter" -> "ZrrSkywalker/CaFo"
"gaopengcuhk/Tip-Adapter" -> "KaiyangZhou/CoOp"
"gaopengcuhk/Tip-Adapter" -> "hjbahng/visual_prompting"
"gaopengcuhk/Tip-Adapter" -> "raoyongming/DenseCLIP"
"gaopengcuhk/Tip-Adapter" -> "KMnP/vpt"
"gaopengcuhk/Tip-Adapter" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision"
"gaopengcuhk/Tip-Adapter" -> "muzairkhattak/multimodal-prompt-learning"
"gaopengcuhk/Tip-Adapter" -> "ZrrSkywalker/PointCLIP" ["e"=1]
"gaopengcuhk/Tip-Adapter" -> "ZrrSkywalker/MonoDETR" ["e"=1]
"gaopengcuhk/Tip-Adapter" -> "dyabel/detpro"
"hanoonaR/object-centric-ovd" -> "mmaaz60/mvits_for_class_agnostic_od"
"hanoonaR/object-centric-ovd" -> "mmaaz60/EdgeNeXt" ["e"=1]
"hanoonaR/object-centric-ovd" -> "muzairkhattak/multimodal-prompt-learning"
"hanoonaR/object-centric-ovd" -> "muzairkhattak/ViFi-CLIP"
"hanoonaR/object-centric-ovd" -> "fcjian/PromptDet"
"hanoonaR/object-centric-ovd" -> "dyabel/detpro"
"hanoonaR/object-centric-ovd" -> "microsoft/RegionCLIP"
"hanoonaR/object-centric-ovd" -> "alirezazareian/ovr-cnn"
"hanoonaR/object-centric-ovd" -> "clin1223/VLDet"
"hanoonaR/object-centric-ovd" -> "yuhangzang/OV-DETR"
"hanoonaR/object-centric-ovd" -> "abdohelmy/D-3Former"
"hanoonaR/object-centric-ovd" -> "akshitac8/OW-DETR"
"hanoonaR/object-centric-ovd" -> "HashmatShadab/APR"
"hanoonaR/object-centric-ovd" -> "xiaofeng94/VL-PLM"
"hanoonaR/object-centric-ovd" -> "wusize/ovdet"
"sagizty/sagizty" -> "sagizty/NTUS_application"
"gaopengcuhk/Unofficial-Pix2Seq" -> "gaopengcuhk/Stable-Pix2Seq"
"gaopengcuhk/Unofficial-Pix2Seq" -> "gaopengcuhk/BALLAD"
"stelzner/monet" -> "baudm/MONet-pytorch"
"lucidrains/x-clip" -> "facebookresearch/SLIP"
"lucidrains/x-clip" -> "Sense-GVT/DeCLIP"
"lucidrains/x-clip" -> "lucidrains/nuwa-pytorch" ["e"=1]
"lucidrains/x-clip" -> "Zasder3/train-CLIP"
"lucidrains/x-clip" -> "FreddeFrallan/Multilingual-CLIP"
"lucidrains/x-clip" -> "rom1504/img2dataset"
"lucidrains/x-clip" -> "mlfoundations/open_clip"
"lucidrains/x-clip" -> "lucidrains/vector-quantize-pytorch" ["e"=1]
"lucidrains/x-clip" -> "crowsonkb/v-diffusion-pytorch" ["e"=1]
"lucidrains/x-clip" -> "kakaobrain/coyo-dataset"
"lucidrains/x-clip" -> "microsoft/VQ-Diffusion" ["e"=1]
"lucidrains/x-clip" -> "iejMac/video2numpy"
"lucidrains/x-clip" -> "lucidrains/CoCa-pytorch"
"lucidrains/x-clip" -> "facebookresearch/multimodal"
"lucidrains/x-clip" -> "rom1504/clip-retrieval"
"Gsunshine/Enjoy-Hamburger" -> "Visual-Attention-Network/VAN-Segmentation"
"Gsunshine/Enjoy-Hamburger" -> "Visual-Attention-Network/SegNeXt"
"Gsunshine/Enjoy-Hamburger" -> "lxtGH/PFSegNets" ["e"=1]
"gitlimlab/Relation-Network-Tensorflow" -> "kimhc6028/relational-networks"
"gitlimlab/Relation-Network-Tensorflow" -> "rasmusbergpalm/recurrent-relational-networks" ["e"=1]
"gitlimlab/Relation-Network-Tensorflow" -> "siddk/relation-network"
"lukemelas/image-paragraph-captioning" -> "arjung128/image-paragraph-captioning"
"lukemelas/image-paragraph-captioning" -> "bupt-mmai/CNN-Caption"
"lukemelas/image-paragraph-captioning" -> "daqingliu/CAVP"
"Visual-Attention-Network/VAN-Segmentation" -> "Visual-Attention-Network/VAN-Jittor"
"Visual-Attention-Network/VAN-Segmentation" -> "Visual-Attention-Network/VAN-Classification" ["e"=1]
"Visual-Attention-Network/VAN-Segmentation" -> "Jittor/JSeg"
"Visual-Attention-Network/VAN-Segmentation" -> "Gsunshine/Enjoy-Hamburger"
"Visual-Attention-Network/VAN-Segmentation" -> "Visual-Attention-Network/SegNeXt"
"phellonchen/awesome-Vision-and-Language-Pre-training" -> "sangminwoo/awesome-vision-and-language"
"chenxinpeng/im2p" -> "InnerPeace-Wu/im2p-tensorflow"
"chenxinpeng/im2p" -> "jack1yang/image-paragraph-captioning"
"chenxinpeng/im2p" -> "Wentong-DST/im2p"
"zjuchenlong/sca-cnn.cvpr17" -> "stevehuanghe/image_captioning"
"zjuchenlong/sca-cnn.cvpr17" -> "jiasenlu/AdaptiveAttention"
"zjuchenlong/sca-cnn.cvpr17" -> "HaleyPei/Implement-of-SCA-CNN"
"zjuchenlong/sca-cnn.cvpr17" -> "gujiuxiang/Stack-Captioning"
"zjuchenlong/sca-cnn.cvpr17" -> "fawazsammani/knowing-when-to-look-adaptive-attention"
"zjuchenlong/sca-cnn.cvpr17" -> "yufengm/Adaptive"
"zjuchenlong/sca-cnn.cvpr17" -> "husthuaan/AoANet"
"zjuchenlong/sca-cnn.cvpr17" -> "njchoma/transformer_image_caption"
"zjuchenlong/sca-cnn.cvpr17" -> "poojahira/image-captioning-bottom-up-top-down"
"zjuchenlong/sca-cnn.cvpr17" -> "richardaecn/cvpr18-caption-eval"
"sagizty/Insight" -> "sagizty/MAE"
"sagizty/Insight" -> "sagizty/Multi-Stage-Hybrid-Transformer"
"sagizty/Insight" -> "sagizty/NTUS_application"
"sagizty/Insight" -> "sagizty/Parallel-Hybrid-Transformer"
"sagizty/Insight" -> "sagizty/sagizty"
"sagizty/Insight" -> "sagizty/VPT"
"Awenbocc/med-vqa" -> "aioz-ai/MICCAI21_MMQ"
"Awenbocc/med-vqa" -> "haifangong/CMSA-MTPT-4-MedicalVQA" ["e"=1]
"Dawn-LX/VidSGG-BIG" -> "Dawn-LX/VidVRD-tracklets"
"kimiyoung/review_net" -> "s-gupta/visual-concepts"
"kimiyoung/review_net" -> "zhegan27/Semantic_Compositional_Nets"
"kimiyoung/review_net" -> "JonghwanMun/TextguidedATT"
"kimiyoung/review_net" -> "LuoweiZhou/e2e-gLSTM-sc"
"zsdonghao/Image-Captioning" -> "mosessoh/CNN-LSTM-Caption-Generator"
"zsdonghao/Image-Captioning" -> "tsenghungchen/show-adapt-and-tell"
"kdexd/virtex" -> "ChenRocks/UNITER"
"kdexd/virtex" -> "microsoft/Oscar"
"kdexd/virtex" -> "facebookresearch/grid-feats-vqa"
"kdexd/virtex" -> "jiasenlu/vilbert_beta"
"kdexd/virtex" -> "airsplay/lxmert"
"kdexd/virtex" -> "jayleicn/ClipBERT"
"kdexd/virtex" -> "pzzhang/VinVL"
"kdexd/virtex" -> "facebookresearch/vilbert-multi-task"
"kdexd/virtex" -> "LuoweiZhou/VLP"
"kdexd/virtex" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"kdexd/virtex" -> "kuanghuei/SCAN" ["e"=1]
"kdexd/virtex" -> "ashkamath/mdetr"
"kdexd/virtex" -> "salesforce/ALBEF"
"kdexd/virtex" -> "facebookresearch/SLIP"
"kdexd/virtex" -> "clip-vil/CLIP-ViL"
"charigyang/motiongrouping" -> "hzxie/RMNet" ["e"=1]
"charigyang/motiongrouping" -> "gengshan-y/rigidmask" ["e"=1]
"charigyang/motiongrouping" -> "tfzhou/MATNet" ["e"=1]
"charigyang/motiongrouping" -> "lucidrains/slot-attention"
"charigyang/motiongrouping" -> "GewelsJI/FSNet" ["e"=1]
"charigyang/motiongrouping" -> "zlai0/MAST" ["e"=1]
"sagizty/MAE" -> "sagizty/NTUS_application"
"sagizty/MAE" -> "sagizty/Parallel-Hybrid-Transformer"
"sagizty/MAE" -> "sagizty/sagizty"
"sagizty/MAE" -> "sagizty/Multi-Stage-Hybrid-Transformer"
"cesc-park/attend2u" -> "tsenghungchen/show-adapt-and-tell"
"cesc-park/attend2u" -> "jiasenlu/AdaptiveAttention"
"cesc-park/attend2u" -> "LuoweiZhou/e2e-gLSTM-sc"
"cesc-park/attend2u" -> "s-gupta/visual-concepts"
"cesc-park/attend2u" -> "zhegan27/Semantic_Compositional_Nets"
"cesc-park/attend2u" -> "zjuchenlong/sca-cnn.cvpr17"
"cesc-park/attend2u" -> "peteanderson80/Up-Down-Captioner"
"ronghanghu/lcgn" -> "sibeiyang/sgmn" ["e"=1]
"ronghanghu/lcgn" -> "shijx12/XNM-Net"
"zhegan27/Semantic_Compositional_Nets" -> "yiyang92/vae_captioning"
"zhegan27/Semantic_Compositional_Nets" -> "zhegan27/SCN_for_video_captioning" ["e"=1]
"wbw520/NoisyLSTM" -> "wbw520/AutoNoduleDetect"
"wbw520/NoisyLSTM" -> "wbw520/MTUNet"
"ictnlp/DSTC8-AVSD" -> "hudaAlamri/DSTC7-Audio-Visual-Scene-Aware-Dialog-AVSD-Challenge"
"Prof-Lu-Cewu/Visual-Relationship-Detection" -> "GriffinLiang/vrd-dsr"
"Prof-Lu-Cewu/Visual-Relationship-Detection" -> "zawlin/cvpr17_vtranse"
"Prof-Lu-Cewu/Visual-Relationship-Detection" -> "doubledaibo/drnet_cvpr2017"
"Prof-Lu-Cewu/Visual-Relationship-Detection" -> "alisure-ml/awesome-visual-relationship-detection"
"Prof-Lu-Cewu/Visual-Relationship-Detection" -> "yangxuntu/vrd"
"Prof-Lu-Cewu/Visual-Relationship-Detection" -> "yikang-li/MSDN"
"Prof-Lu-Cewu/Visual-Relationship-Detection" -> "jz462/Large-Scale-VRD.pytorch"
"Prof-Lu-Cewu/Visual-Relationship-Detection" -> "rowanz/neural-motifs"
"Prof-Lu-Cewu/Visual-Relationship-Detection" -> "danfeiX/scene-graph-TF-release"
"Prof-Lu-Cewu/Visual-Relationship-Detection" -> "StanfordVL/ReferringRelationships"
"Prof-Lu-Cewu/Visual-Relationship-Detection" -> "NVIDIA/ContrastiveLosses4VRD"
"Prof-Lu-Cewu/Visual-Relationship-Detection" -> "facebookresearch/Large-Scale-VRD"
"Prof-Lu-Cewu/Visual-Relationship-Detection" -> "SiyuanQi/gpnn" ["e"=1]
"Prof-Lu-Cewu/Visual-Relationship-Detection" -> "nexusapoorvacus/DeepVariationStructuredRL"
"mjhucla/TF-mRNN" -> "mjhucla/mRNN-CR"
"mjhucla/mRNN-CR" -> "mjhucla/TF-mRNN"
"twangnh/pnp-detr" -> "VODKA312/IntroToSelf-control"
"twangnh/pnp-detr" -> "kakaobrain/sparse-detr"
"GriffinLiang/vrd-dsr" -> "yangxuntu/vrd"
"GriffinLiang/vrd-dsr" -> "alisure-ml/awesome-visual-relationship-detection"
"GriffinLiang/vrd-dsr" -> "Prof-Lu-Cewu/Visual-Relationship-Detection"
"GriffinLiang/vrd-dsr" -> "doubledaibo/drnet_cvpr2017"
"GriffinLiang/vrd-dsr" -> "zawlin/cvpr17_vtranse"
"GriffinLiang/vrd-dsr" -> "nexusapoorvacus/DeepVariationStructuredRL"
"GriffinLiang/vrd-dsr" -> "jz462/Large-Scale-VRD.pytorch"
"GriffinLiang/vrd-dsr" -> "NVIDIA/ContrastiveLosses4VRD"
"GriffinLiang/vrd-dsr" -> "facebookresearch/Large-Scale-VRD"
"lukemelas/deep-spectral-segmentation" -> "MkuuWaUjinga/leopart" ["e"=1]
"lukemelas/deep-spectral-segmentation" -> "chongzhou96/MaskCLIP"
"lukemelas/deep-spectral-segmentation" -> "isl-org/lang-seg"
"dingmyu/davit" -> "OliverRensu/Shunted-Transformer" ["e"=1]
"dingmyu/davit" -> "raoyongming/HorNet"
"dingmyu/davit" -> "ShoufaChen/CycleMLP" ["e"=1]
"dingmyu/davit" -> "YehLi/ImageNetModel"
"dingmyu/davit" -> "ChristophReich1996/MaxViT" ["e"=1]
"dingmyu/davit" -> "youngwanLEE/MPViT" ["e"=1]
"dingmyu/davit" -> "SHI-Labs/Neighborhood-Attention-Transformer"
"JosephKJ/iOD" -> "Hi-FT/ERD"
"JosephKJ/iOD" -> "CanPeng123/Faster-ILOD"
"JosephKJ/iOD" -> "JosephKJ/ELI"
"lucidrains/flamingo-pytorch" -> "mlfoundations/open_flamingo" ["e"=1]
"lucidrains/flamingo-pytorch" -> "OFA-Sys/OFA"
"lucidrains/flamingo-pytorch" -> "microsoft/GLIP"
"lucidrains/flamingo-pytorch" -> "lucidrains/CoCa-pytorch"
"lucidrains/flamingo-pytorch" -> "salesforce/BLIP"
"lucidrains/flamingo-pytorch" -> "kakaobrain/coyo-dataset"
"lucidrains/flamingo-pytorch" -> "salesforce/ALBEF"
"lucidrains/flamingo-pytorch" -> "dhansmair/flamingo-mini"
"lucidrains/flamingo-pytorch" -> "KaiyangZhou/CoOp"
"lucidrains/flamingo-pytorch" -> "salesforce/LAVIS"
"lucidrains/flamingo-pytorch" -> "lucidrains/RETRO-pytorch" ["e"=1]
"lucidrains/flamingo-pytorch" -> "facebookresearch/multimodal"
"lucidrains/flamingo-pytorch" -> "microsoft/Oscar"
"lucidrains/flamingo-pytorch" -> "zengyan-97/X-VLM"
"lucidrains/flamingo-pytorch" -> "facebookresearch/SLIP"
"alirezazareian/ovr-cnn" -> "dyabel/detpro"
"alirezazareian/ovr-cnn" -> "yuhangzang/OV-DETR"
"alirezazareian/ovr-cnn" -> "fcjian/PromptDet"
"ccclyu/awesome-deeplogic" -> "logictensornetworks/logictensornetworks"
"ccclyu/awesome-deeplogic" -> "vacancy/NSCL-PyTorch-Release"
"davidmascharka/tbd-nets" -> "endernewton/iter-reason"
"davidmascharka/tbd-nets" -> "rsokl/MyGrad"
"davidmascharka/tbd-nets" -> "stanfordnlp/mac-network"
"davidmascharka/tbd-nets" -> "facebookresearch/clevr-iep"
"davidmascharka/tbd-nets" -> "Cyanogenoid/vqa-counting"
"davidmascharka/tbd-nets" -> "ronghanghu/n2nmn"
"davidmascharka/tbd-nets" -> "ethanjperez/film"
"davidmascharka/tbd-nets" -> "Cadene/murel.bootstrap.pytorch"
"davidmascharka/tbd-nets" -> "hengyuan-hu/bottom-up-attention-vqa"
"davidmascharka/tbd-nets" -> "facebookresearch/clevr-dataset-gen"
"davidmascharka/tbd-nets" -> "jnhwkim/ban-vqa"
"davidmascharka/tbd-nets" -> "jiasenlu/NeuralBabyTalk"
"davidmascharka/tbd-nets" -> "ronghanghu/snmn"
"gsh199449/stickerchat" -> "lizekang/DSTC10-MOD"
"Cadene/block.bootstrap.pytorch" -> "Cadene/vqa.pytorch"
"Cadene/block.bootstrap.pytorch" -> "Cadene/murel.bootstrap.pytorch"
"Cadene/block.bootstrap.pytorch" -> "jnhwkim/ban-vqa"
"Cadene/block.bootstrap.pytorch" -> "hengyuan-hu/bottom-up-attention-vqa"
"Cadene/block.bootstrap.pytorch" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"Cadene/block.bootstrap.pytorch" -> "MILVLG/mcan-vqa"
"Cadene/block.bootstrap.pytorch" -> "Cadene/bootstrap.pytorch"
"Cadene/block.bootstrap.pytorch" -> "linjieli222/VQA_ReGAT"
"Cadene/block.bootstrap.pytorch" -> "Cyanogenoid/vqa-counting"
"Cadene/block.bootstrap.pytorch" -> "Cyanogenoid/pytorch-vqa"
"Cadene/block.bootstrap.pytorch" -> "GT-Vision-Lab/VQA"
"Cadene/block.bootstrap.pytorch" -> "aimbrain/vqa-project"
"Cadene/block.bootstrap.pytorch" -> "jokieleung/awesome-visual-question-answering"
"Cadene/block.bootstrap.pytorch" -> "peteanderson80/bottom-up-attention"
"Cadene/block.bootstrap.pytorch" -> "jiasenlu/HieCoAttenVQA"
"LAION-AI/CLIP_benchmark" -> "mlfoundations/wise-ft"
"LAION-AI/CLIP_benchmark" -> "facebookresearch/diht"
"LAION-AI/CLIP_benchmark" -> "mlfoundations/model-soups"
"LAION-AI/CLIP_benchmark" -> "mlfoundations/patching" ["e"=1]
"LAION-AI/CLIP_benchmark" -> "facebookresearch/CiT"
"LAION-AI/CLIP_benchmark" -> "LightDXY/FT-CLIP"
"LAION-AI/CLIP_benchmark" -> "LAION-AI/scaling-laws-openclip"
"mlfoundations/wise-ft" -> "LAION-AI/CLIP_benchmark"
"mlfoundations/wise-ft" -> "Sense-GVT/DeCLIP"
"mlfoundations/wise-ft" -> "LightDXY/FT-CLIP"
"mlfoundations/wise-ft" -> "mlfoundations/open_clip"
"mlfoundations/wise-ft" -> "hjbahng/visual_prompting"
"mlfoundations/wise-ft" -> "mlfoundations/model-soups"
"mlfoundations/wise-ft" -> "gaopengcuhk/CLIP-Adapter"
"mlfoundations/wise-ft" -> "rom1504/clip-retrieval"
"mlfoundations/wise-ft" -> "facebookresearch/SLIP"
"mlfoundations/wise-ft" -> "arampacha/CLIP-rsicd" ["e"=1]
"chuhaojin/BriVL-BUA-applications" -> "chuhaojin/WenLan-api-document"
"chuhaojin/BriVL-BUA-applications" -> "BAAI-WuDao/BriVL"
"chuhaojin/WenLan-api-document" -> "chuhaojin/BriVL-BUA-applications"
"dddzg/up-detr" -> "amirbar/DETReg" ["e"=1]
"dddzg/up-detr" -> "Atten4Vis/ConditionalDETR"
"dddzg/up-detr" -> "megvii-research/AnchorDETR"
"dddzg/up-detr" -> "WXinlong/DenseCL" ["e"=1]
"dddzg/up-detr" -> "PeizeSun/SparseR-CNN" ["e"=1]
"dddzg/up-detr" -> "JosephKJ/OWOD"
"dddzg/up-detr" -> "fundamentalvision/Deformable-DETR" ["e"=1]
"dddzg/up-detr" -> "hologerry/SoCo" ["e"=1]
"dddzg/up-detr" -> "hustvl/MIMDet" ["e"=1]
"dddzg/up-detr" -> "limbo0000/InstanceLoc"
"dddzg/up-detr" -> "ashkamath/mdetr"
"dddzg/up-detr" -> "gaopengcuhk/SMCA-DETR"
"dddzg/up-detr" -> "Megvii-BaseDetection/DeFCN" ["e"=1]
"dddzg/up-detr" -> "xieenze/DetCo"
"dddzg/up-detr" -> "IDEA-opensource/DAB-DETR"
"abachaa/VQA-Med-2019" -> "UCSD-AI4H/PathVQA"
"ajamjoom/Image-Captions" -> "yahoo/object_relation_transformer"
"GuessWhatGame/guesswhat" -> "ruizhaogit/GuessWhat-TemperedPolicyGradient"
"GuessWhatGame/guesswhat" -> "satwikkottur/clevr-dialog"
"satwikkottur/clevr-dialog" -> "zilongzheng/visdial-gnn"
"yzhuoning/Awesome-CLIP" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision"
"yzhuoning/Awesome-CLIP" -> "KaiyangZhou/CoOp"
"yzhuoning/Awesome-CLIP" -> "raoyongming/DenseCLIP"
"yzhuoning/Awesome-CLIP" -> "Sense-GVT/DeCLIP"
"yzhuoning/Awesome-CLIP" -> "ArrowLuo/CLIP4Clip" ["e"=1]
"yzhuoning/Awesome-CLIP" -> "microsoft/GLIP"
"yzhuoning/Awesome-CLIP" -> "isl-org/lang-seg"
"yzhuoning/Awesome-CLIP" -> "kakaobrain/coyo-dataset"
"yzhuoning/Awesome-CLIP" -> "mlfoundations/open_clip"
"yzhuoning/Awesome-CLIP" -> "ShoufaChen/DiffusionDet"
"yzhuoning/Awesome-CLIP" -> "facebookresearch/multimodal"
"yzhuoning/Awesome-CLIP" -> "clip-vil/CLIP-ViL"
"yzhuoning/Awesome-CLIP" -> "dk-liang/Awesome-Visual-Transformer" ["e"=1]
"yzhuoning/Awesome-CLIP" -> "salesforce/ALBEF"
"yzhuoning/Awesome-CLIP" -> "ttengwang/Caption-Anything" ["e"=1]
"microsoft/UniCL" -> "microsoft/RegionCLIP"
"microsoft/UniCL" -> "researchmm/soho"
"microsoft/UniCL" -> "microsoft/GLIP"
"microsoft/UniCL" -> "facebookresearch/SLIP"
"markdtw/vqa-winner-cvprw-2017" -> "hengyuan-hu/bottom-up-attention-vqa"
"markdtw/vqa-winner-cvprw-2017" -> "SinghJasdeep/Attention-on-Attention-for-VQA"
"markdtw/vqa-winner-cvprw-2017" -> "Cadene/vqa.pytorch"
"markdtw/vqa-winner-cvprw-2017" -> "Cyanogenoid/pytorch-vqa"
"markdtw/vqa-winner-cvprw-2017" -> "aimbrain/vqa-project"
"markdtw/vqa-winner-cvprw-2017" -> "JamesChuanggg/awesome-vqa"
"markdtw/vqa-winner-cvprw-2017" -> "asdf0982/vqa-mfb.pytorch"
"markdtw/vqa-winner-cvprw-2017" -> "Cyanogenoid/vqa-counting"
"wusize/ovdet" -> "tgxs002/CORA"
"NVlabs/GCVit" -> "facebookresearch/ToMe"
"NVlabs/GCVit" -> "SHI-Labs/Neighborhood-Attention-Transformer"
"rowanz/r2c" -> "rowanz/neural-motifs"
"rowanz/r2c" -> "jiasenlu/vilbert_beta"
"rowanz/r2c" -> "jwyang/graph-rcnn.pytorch"
"rowanz/r2c" -> "hengyuan-hu/bottom-up-attention-vqa"
"rowanz/r2c" -> "rowanz/swagaf"
"rowanz/r2c" -> "jokieleung/awesome-visual-question-answering"
"rowanz/r2c" -> "jackroos/VL-BERT"
"rowanz/r2c" -> "stanfordnlp/mac-network"
"rowanz/r2c" -> "lichengunc/MAttNet" ["e"=1]
"rowanz/r2c" -> "SinghJasdeep/Attention-on-Attention-for-VQA"
"rowanz/r2c" -> "peteanderson80/bottom-up-attention"
"rowanz/r2c" -> "yikang-li/MSDN"
"rowanz/r2c" -> "MILVLG/mcan-vqa"
"rowanz/r2c" -> "rowanz/merlot" ["e"=1]
"rowanz/r2c" -> "yuweijiang/HGL-pytorch"
"StanfordVL/ReferringRelationships" -> "Prof-Lu-Cewu/Visual-Relationship-Detection"
"StanfordVL/ReferringRelationships" -> "doubledaibo/drnet_cvpr2017"
"StanfordVL/ReferringRelationships" -> "lichengunc/MAttNet" ["e"=1]
"StanfordVL/ReferringRelationships" -> "GriffinLiang/vrd-dsr"
"StanfordVL/ReferringRelationships" -> "nexusapoorvacus/DeepVariationStructuredRL"
"StanfordVL/ReferringRelationships" -> "yangxuntu/vrd"
"StanfordVL/ReferringRelationships" -> "yikang-li/MSDN"
"StanfordVL/ReferringRelationships" -> "zawlin/cvpr17_vtranse"
"StanfordVL/ReferringRelationships" -> "rowanz/neural-motifs"
"StanfordVL/ReferringRelationships" -> "SiyuanQi/gpnn" ["e"=1]
"fabienbaradel/object_level_visual_reasoning" -> "yaohungt/Gated-Spatio-Temporal-Energy-Graph"
"YiwuZhong/Sub-GC" -> "yangxuntu/SGAE"
"YiwuZhong/Sub-GC" -> "yahoo/object_relation_transformer"
"YiwuZhong/Sub-GC" -> "cshizhe/asg2cap"
"akirafukui/vqa-mcb" -> "jiasenlu/HieCoAttenVQA"
"akirafukui/vqa-mcb" -> "yuzcccc/vqa-mfb"
"akirafukui/vqa-mcb" -> "jnhwkim/MulLowBiVQA"
"akirafukui/vqa-mcb" -> "VT-vision-lab/VQA_LSTM_CNN"
"akirafukui/vqa-mcb" -> "JamesChuanggg/awesome-vqa"
"akirafukui/vqa-mcb" -> "zcyang/imageqa-san"
"akirafukui/vqa-mcb" -> "JamesChuanggg/VQA-tensorflow"
"akirafukui/vqa-mcb" -> "shmsw25/mcb-model-for-vqa"
"akirafukui/vqa-mcb" -> "metalbubble/VQAbaseline"
"akirafukui/vqa-mcb" -> "jnhwkim/cbp"
"akirafukui/vqa-mcb" -> "Cadene/vqa.pytorch"
"akirafukui/vqa-mcb" -> "iamaaditya/VQA_Demo"
"akirafukui/vqa-mcb" -> "asdf0982/vqa-mfb.pytorch"
"akirafukui/vqa-mcb" -> "markdtw/vqa-winner-cvprw-2017"
"akirafukui/vqa-mcb" -> "jnhwkim/ban-vqa"
"ethanjperez/film" -> "stanfordnlp/mac-network"
"ethanjperez/film" -> "facebookresearch/clevr-iep"
"ethanjperez/film" -> "caffeinism/FiLM-pytorch"
"ethanjperez/film" -> "devendrachaplot/DeepRL-Grounding" ["e"=1]
"ethanjperez/film" -> "davidmascharka/tbd-nets"
"ethanjperez/film" -> "Cadene/murel.bootstrap.pytorch"
"Amshaker/SwiftFormer" -> "abdohelmy/D-3Former"
"HashmatShadab/APR" -> "faresmalik/SEViT"
"abdohelmy/D-3Former" -> "faresmalik/SEViT"
"iejMac/clip-video-encode" -> "iejMac/video2numpy"
"Cloud-CV/CloudCV" -> "Cloud-CV/Origami"
"hlamba28/Automatic-Image-Captioning" -> "yashk2810/Image-Captioning"
"kacky24/stylenet" -> "yiyang92/caption-stylenet_tensorflow"
"kacky24/stylenet" -> "computationalmedia/semstyle"
"kacky24/stylenet" -> "andyweizhao/Multitask_Image_Captioning"
"ruotianluo/neuraltalk2.pytorch" -> "eladhoffer/captionGen"
"ruotianluo/neuraltalk2.pytorch" -> "gujiuxiang/chinese_im2text.pytorch"
"linjieli222/VQA_ReGAT" -> "aimbrain/vqa-project"
"linjieli222/VQA_ReGAT" -> "MILVLG/mcan-vqa"
"linjieli222/VQA_ReGAT" -> "ronghanghu/lcgn"
"linjieli222/VQA_ReGAT" -> "Cadene/murel.bootstrap.pytorch"
"linjieli222/VQA_ReGAT" -> "codexxxl/GraphVQA"
"linjieli222/VQA_ReGAT" -> "Cadene/block.bootstrap.pytorch"
"linjieli222/VQA_ReGAT" -> "astro-zihao/mucko"
"linjieli222/VQA_ReGAT" -> "CrossmodalGroup/GSMN" ["e"=1]
"linjieli222/VQA_ReGAT" -> "yangxuntu/SGAE"
"linjieli222/VQA_ReGAT" -> "KunpengLi1994/VSRN" ["e"=1]
"linjieli222/VQA_ReGAT" -> "cshizhe/asg2cap"
"linjieli222/VQA_ReGAT" -> "jnhwkim/ban-vqa"
"megvii-research/AnchorDETR" -> "Atten4Vis/ConditionalDETR"
"megvii-research/AnchorDETR" -> "kakaobrain/sparse-detr"
"megvii-research/AnchorDETR" -> "FengLi-ust/DN-DETR"
"megvii-research/AnchorDETR" -> "gaopengcuhk/SMCA-DETR"
"megvii-research/AnchorDETR" -> "dddzg/up-detr"
"megvii-research/AnchorDETR" -> "IDEA-Research/DAB-DETR"
"megvii-research/AnchorDETR" -> "IDEA-opensource/DAB-DETR"
"megvii-research/AnchorDETR" -> "naver-ai/vidt"
"megvii-research/AnchorDETR" -> "tangjiuqi097/ATCAIS"
"Fen9/WReN" -> "WellyZhang/CoPINet"
"Fen9/WReN" -> "zkcys001/distracting_feature"
"WellyZhang/CoPINet" -> "zkcys001/distracting_feature"
"WellyZhang/RAVEN" -> "Fen9/WReN"
"WellyZhang/RAVEN" -> "WellyZhang/CoPINet"
"WellyZhang/RAVEN" -> "zkcys001/distracting_feature"
"WellyZhang/RAVEN" -> "deepmind/abstract-reasoning-matrices"
"WellyZhang/RAVEN" -> "husheng12345/SRAN"
"deepmind/abstract-reasoning-matrices" -> "WellyZhang/RAVEN"
"deepmind/abstract-reasoning-matrices" -> "Fen9/WReN"
"deepmind/abstract-reasoning-matrices" -> "husheng12345/SRAN"
"deepmind/abstract-reasoning-matrices" -> "WellyZhang/CoPINet"
"zhixuan-lin/G-SWM" -> "JindongJiang/SCALOR"
"zkcys001/distracting_feature" -> "WellyZhang/CoPINet"
"zkcys001/distracting_feature" -> "husheng12345/SRAN"
"ZhangYuanhan-AI/NOAH" -> "JieShibo/PETL-ViT"
"facebookresearch/phyre" -> "WellyZhang/RAVEN"
"facebookresearch/phyre" -> "lvis-dataset/lvis-api"
"facebookresearch/phyre" -> "HaozhiQi/RPIN"
"facebookresearch/phyre" -> "facebookresearch/clevr-dataset-gen"
"facebookresearch/phyre" -> "k-r-allen/tool-games"
"facebookresearch/phyre" -> "facebookresearch/EmbodiedQA" ["e"=1]
"facebookresearch/phyre" -> "stepjam/RLBench" ["e"=1]
"astro-zihao/mucko" -> "wangpengnorman/FVQA"
"Scarecrow0/SGTR" -> "layer6ai-labs/SGG-Seq2Seq"
"Scarecrow0/SGTR" -> "SHTUPLUS/PySGG"
"jz462/Large-Scale-VRD.pytorch" -> "facebookresearch/Large-Scale-VRD"
"jz462/Large-Scale-VRD.pytorch" -> "alisure-ml/awesome-visual-relationship-detection"
"jz462/Large-Scale-VRD.pytorch" -> "Prof-Lu-Cewu/Visual-Relationship-Detection"
"jz462/Large-Scale-VRD.pytorch" -> "yangxuntu/vrd"
"jz462/Large-Scale-VRD.pytorch" -> "NVIDIA/ContrastiveLosses4VRD"
"jz462/Large-Scale-VRD.pytorch" -> "GriffinLiang/vrd-dsr"
"rakshithShetty/captionGAN" -> "doubledaibo/gancaption_iccv2017"
"rakshithShetty/captionGAN" -> "yiyang92/vae_captioning"
"rakshithShetty/captionGAN" -> "andyweizhao/Multitask_Image_Captioning"
"paarthneekhara/neural-vqa-tensorflow" -> "JamesChuanggg/VQA-tensorflow"
"paarthneekhara/neural-vqa-tensorflow" -> "jiasenlu/HieCoAttenVQA"
"paarthneekhara/neural-vqa-tensorflow" -> "paarthneekhara/convolutional-vqa"
"paarthneekhara/neural-vqa-tensorflow" -> "paarthneekhara/Weather-From-Map"
"paarthneekhara/neural-vqa-tensorflow" -> "adiitya/p2pstream"
"paarthneekhara/neural-vqa-tensorflow" -> "Kapilks/Basic-Kernel"
"paarthneekhara/neural-vqa-tensorflow" -> "Kapilks/Download-youtube-video"
"paarthneekhara/neural-vqa-tensorflow" -> "DeepRNN/visual_question_answering"
"paarthneekhara/neural-vqa-tensorflow" -> "JamesChuanggg/awesome-vqa"
"paarthneekhara/neural-vqa-tensorflow" -> "anantzoid/VQA-Keras-Visual-Question-Answering"
"VT-vision-lab/VQA_LSTM_CNN" -> "HyeonwooNoh/DPPnet"
"VT-vision-lab/VQA_LSTM_CNN" -> "VT-vision-lab/VQA"
"VT-vision-lab/VQA_LSTM_CNN" -> "jiasenlu/HieCoAttenVQA"
"VT-vision-lab/VQA_LSTM_CNN" -> "metalbubble/VQAbaseline"
"VT-vision-lab/VQA_LSTM_CNN" -> "JamesChuanggg/VQA-tensorflow"
"VT-vision-lab/VQA_LSTM_CNN" -> "akirafukui/vqa-mcb"
"VT-vision-lab/VQA_LSTM_CNN" -> "jnhwkim/MulLowBiVQA"
"VT-vision-lab/VQA_LSTM_CNN" -> "avisingh599/visual-qa"
"VT-vision-lab/VQA_LSTM_CNN" -> "zcyang/imageqa-san"
"VT-vision-lab/VQA_LSTM_CNN" -> "renmengye/imageqa-public"
"VT-vision-lab/VQA_LSTM_CNN" -> "JamesChuanggg/san-torch"
"VT-vision-lab/VQA_LSTM_CNN" -> "yukezhu/visual7w-qa-models"
"VT-vision-lab/VQA_LSTM_CNN" -> "JamesChuanggg/awesome-vqa"
"VT-vision-lab/VQA_LSTM_CNN" -> "jacobandreas/nmn2"
"muzairkhattak/ViFi-CLIP" -> "muzairkhattak/multimodal-prompt-learning"
"baudm/MONet-pytorch" -> "stelzner/monet"
"baudm/MONet-pytorch" -> "zhixuan-lin/IODINE"
"baudm/MONet-pytorch" -> "applied-ai-lab/genesis"
"baudm/MONet-pytorch" -> "jcoreyes/OP3"
"yuhangzang/OV-DETR" -> "dyabel/detpro"
"yuhangzang/OV-DETR" -> "alirezazareian/ovr-cnn"
"yuhangzang/OV-DETR" -> "xiaofeng94/VL-PLM"
"yuhangzang/OV-DETR" -> "mengqiDyangge/HierKD"
"yuhangzang/OV-DETR" -> "ZhangYuanhan-AI/OmniBenchmark" ["e"=1]
"facebookresearch/mmbt" -> "facebookresearch/vilbert-multi-task"
"facebookresearch/mmbt" -> "WasifurRahman/BERT_multimodal_transformer" ["e"=1]
"waxnkw/IETrans-SGG.pytorch" -> "SHTUPLUS/PySGG"
"waxnkw/IETrans-SGG.pytorch" -> "Scarecrow0/SGTR"
"yrcong/STTran" -> "MCG-NJU/TRACE"
"yrcong/STTran" -> "JingweiJ/ActionGenome"
"yrcong/STTran" -> "yrcong/RelTR"
"yrcong/STTran" -> "Dawn-LX/VidSGG-BIG"
"yrcong/STTran" -> "yrcong/NODIS"
"yrcong/STTran" -> "Scarecrow0/SGTR"
"yrcong/STTran" -> "SHTUPLUS/PySGG"
"aimagelab/show-control-and-tell" -> "husthuaan/AoANet"
"aimagelab/show-control-and-tell" -> "fengyang0317/unsupervised_captioning"
"aimagelab/show-control-and-tell" -> "yangxuntu/SGAE"
"aimagelab/show-control-and-tell" -> "cshizhe/asg2cap"
"aimagelab/show-control-and-tell" -> "yahoo/object_relation_transformer"
"aimagelab/show-control-and-tell" -> "ruotianluo/DiscCaptioning"
"aimagelab/show-control-and-tell" -> "JDAI-CV/image-captioning"
"aimagelab/show-control-and-tell" -> "poojahira/image-captioning-bottom-up-top-down"
"aimagelab/show-control-and-tell" -> "aimagelab/meshed-memory-transformer"
"aimagelab/show-control-and-tell" -> "peteanderson80/Up-Down-Captioner"
"aimagelab/show-control-and-tell" -> "gujiuxiang/Stack-Captioning"
"aimagelab/show-control-and-tell" -> "jiasenlu/NeuralBabyTalk"
"aimagelab/show-control-and-tell" -> "aditya12agd5/convcap"
"aimagelab/show-control-and-tell" -> "ruotianluo/self-critical.pytorch"
"aimagelab/show-control-and-tell" -> "aimagelab/speaksee"
"sangminwoo/awesome-vision-and-language" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"sangminwoo/awesome-vision-and-language" -> "phellonchen/awesome-Vision-and-Language-Pre-training"
"sangminwoo/awesome-vision-and-language" -> "danieljf24/awesome-video-text-retrieval" ["e"=1]
"sangminwoo/awesome-vision-and-language" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision"
"sangminwoo/awesome-vision-and-language" -> "clip-vil/CLIP-ViL"
"sangminwoo/awesome-vision-and-language" -> "jokieleung/awesome-visual-question-answering"
"sangminwoo/awesome-vision-and-language" -> "pzzhang/VinVL"
"IDEA-Research/MaskDINO" -> "IDEA-Research/DINO"
"IDEA-Research/MaskDINO" -> "IDEA-Research/OpenSeeD"
"IDEA-Research/MaskDINO" -> "IDEA-Research/detrex"
"IDEA-Research/MaskDINO" -> "IDEA-Research/Stable-DINO"
"IDEA-Research/MaskDINO" -> "IDEA-opensource/DN-DETR"
"IDEA-Research/MaskDINO" -> "IDEA-Research/awesome-detection-transformer"
"albanie/slurm_gpustat" -> "TengdaHan/slurm_web"
"NVIDIA/ContrastiveLosses4VRD" -> "rowanz/neural-motifs"
"NVIDIA/ContrastiveLosses4VRD" -> "alisure-ml/awesome-visual-relationship-detection"
"NVIDIA/ContrastiveLosses4VRD" -> "JingweiJ/ActionGenome"
"NVIDIA/ContrastiveLosses4VRD" -> "jz462/Large-Scale-VRD.pytorch"
"NVIDIA/ContrastiveLosses4VRD" -> "danfeiX/scene-graph-TF-release"
"NVIDIA/ContrastiveLosses4VRD" -> "jwyang/graph-rcnn.pytorch"
"NVIDIA/ContrastiveLosses4VRD" -> "SHTUPLUS/PySGG"
"NVIDIA/ContrastiveLosses4VRD" -> "yuweihao/KERN"
"NVIDIA/ContrastiveLosses4VRD" -> "KaihuaTang/VCTree-Scene-Graph-Generation"
"NVIDIA/ContrastiveLosses4VRD" -> "Prof-Lu-Cewu/Visual-Relationship-Detection"
"NVIDIA/ContrastiveLosses4VRD" -> "GriffinLiang/vrd-dsr"
"NVIDIA/ContrastiveLosses4VRD" -> "KaihuaTang/Scene-Graph-Benchmark.pytorch"
"NVIDIA/ContrastiveLosses4VRD" -> "yikang-li/FactorizableNet"
"limbo0000/InstanceLoc" -> "xieenze/DetCo"
"HyeonwooNoh/DPPnet" -> "VT-vision-lab/VQA_LSTM_CNN"
"HyeonwooNoh/DPPnet" -> "zcyang/imageqa-san"
"HyeonwooNoh/DPPnet" -> "VT-vision-lab/VQA"
"mosessoh/CNN-LSTM-Caption-Generator" -> "zsdonghao/Image-Captioning"
"mosessoh/CNN-LSTM-Caption-Generator" -> "DeepRNN/image_captioning"
"mosessoh/CNN-LSTM-Caption-Generator" -> "anuragmishracse/caption_generator"
"mosessoh/CNN-LSTM-Caption-Generator" -> "jazzsaxmafia/show_and_tell.tensorflow"
"mosessoh/CNN-LSTM-Caption-Generator" -> "jazzsaxmafia/show_attend_and_tell.tensorflow"
"mosessoh/CNN-LSTM-Caption-Generator" -> "tylin/coco-caption"
"mosessoh/CNN-LSTM-Caption-Generator" -> "tsenghungchen/show-adapt-and-tell"
"mosessoh/CNN-LSTM-Caption-Generator" -> "JamesChuanggg/VQA-tensorflow"
"mosessoh/CNN-LSTM-Caption-Generator" -> "mjhucla/TF-mRNN"
"neural-nuts/image-caption-generator" -> "neural-nuts/Cam2Caption"
"neural-nuts/image-caption-generator" -> "anuragmishracse/caption_generator"
"tsenghungchen/show-adapt-and-tell" -> "kacky24/stylenet"
"tsenghungchen/show-adapt-and-tell" -> "gujiuxiang/Stack-Captioning"
"tsenghungchen/show-adapt-and-tell" -> "ruotianluo/DiscCaptioning"
"tsenghungchen/show-adapt-and-tell" -> "LuoweiZhou/e2e-gLSTM-sc"
"tsenghungchen/show-adapt-and-tell" -> "aimagelab/show-control-and-tell"
"tsenghungchen/show-adapt-and-tell" -> "zhegan27/Semantic_Compositional_Nets"
"tsenghungchen/show-adapt-and-tell" -> "yiyang92/vae_captioning"
"tsenghungchen/show-adapt-and-tell" -> "richardaecn/cvpr18-caption-eval"
"tsenghungchen/show-adapt-and-tell" -> "cesc-park/attend2u"
"henryhungle/MTN" -> "salesforce/BiST"
"jialinwu17/self_critical_vqa" -> "erobic/negative_analysis_of_grounding"
"xdshang/VidVRD-helper" -> "yaohungt/Gated-Spatio-Temporal-Energy-Graph"
"xdshang/VidVRD-helper" -> "doc-doc/vRGV"
"ronghanghu/cmn" -> "zjuchenlong/sca-cnn"
"kakaobrain/sparse-detr" -> "gaopengcuhk/SMCA-DETR"
"kakaobrain/sparse-detr" -> "twangnh/pnp-detr"
"kakaobrain/sparse-detr" -> "Atten4Vis/ConditionalDETR"
"kakaobrain/sparse-detr" -> "megvii-research/AnchorDETR"
"kakaobrain/sparse-detr" -> "naver-ai/vidt"
"davidnvq/visdial" -> "gicheonkang/dan-visdial"
"salesforce/VD-BERT" -> "idansc/mrr-ndcg"
"salesforce/VD-BERT" -> "vmurahari3/visdial-bert"
"DenisDsh/VizWiz-VQA-PyTorch" -> "liqing-ustc/VizWiz_LSTM_CNN_Attention"
"doubledaibo/gancaption_iccv2017" -> "rakshithShetty/captionGAN"
"doubledaibo/gancaption_iccv2017" -> "doubledaibo/clcaption_nips2017"
"doubledaibo/gancaption_iccv2017" -> "yiyang92/vae_captioning"
"doubledaibo/gancaption_iccv2017" -> "ruotianluo/DiscCaptioning"
"anantzoid/VQA-Keras-Visual-Question-Answering" -> "iamaaditya/VQA_Demo"
"anantzoid/VQA-Keras-Visual-Question-Answering" -> "iamaaditya/VQA_Keras"
"anantzoid/VQA-Keras-Visual-Question-Answering" -> "avisingh599/visual-qa"
"anantzoid/VQA-Keras-Visual-Question-Answering" -> "JamesChuanggg/VQA-tensorflow"
"anantzoid/VQA-Keras-Visual-Question-Answering" -> "abhshkdz/neural-vqa-attention"
"anantzoid/VQA-Keras-Visual-Question-Answering" -> "GT-Vision-Lab/VQA_LSTM_CNN"
"anantzoid/VQA-Keras-Visual-Question-Answering" -> "jiasenlu/HieCoAttenVQA"
"Cadene/bootstrap.pytorch" -> "Cadene/murel.bootstrap.pytorch"
"Cadene/bootstrap.pytorch" -> "Cadene/block.bootstrap.pytorch"
"Cadene/bootstrap.pytorch" -> "ThomasRobertFr/gpu-monitor"
"Cadene/bootstrap.pytorch" -> "emited/gantk2"
"Cadene/bootstrap.pytorch" -> "cdancette/rubi.bootstrap.pytorch"
"Cadene/bootstrap.pytorch" -> "yanxinzju/CSS-VQA"
"asdf0982/vqa-mfb.pytorch" -> "yuzcccc/vqa-mfb"
"adiitya/p2pstream" -> "Kapilks/Basic-Kernel"
"adiitya/p2pstream" -> "Kapilks/Download-youtube-video"
"adiitya/p2pstream" -> "adiitya/frozen"
"yuzcccc/vqa-mfb" -> "asdf0982/vqa-mfb.pytorch"
"yuzcccc/vqa-mfb" -> "akirafukui/vqa-mcb"
"yuzcccc/vqa-mfb" -> "MILVLG/openvqa"
"yuzcccc/vqa-mfb" -> "jnhwkim/ban-vqa"
"yuzcccc/vqa-mfb" -> "MILVLG/mcan-vqa"
"yuzcccc/vqa-mfb" -> "Cyanogenoid/vqa-counting"
"yuzcccc/vqa-mfb" -> "hengyuan-hu/bottom-up-attention-vqa"
"yuzcccc/vqa-mfb" -> "aimbrain/vqa-project"
"yuzcccc/vqa-mfb" -> "jnhwkim/MulLowBiVQA"
"yuzcccc/vqa-mfb" -> "shtechair/vqa-sva"
"yuzcccc/vqa-mfb" -> "gicheonkang/DAN-VisDial"
"yuzcccc/vqa-mfb" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"yuzcccc/vqa-mfb" -> "zcyang/imageqa-san"
"yuzcccc/vqa-mfb" -> "Cadene/murel.bootstrap.pytorch"
"yuzcccc/vqa-mfb" -> "Cadene/vqa.pytorch"
"coderSkyChen/Iterative-Visual-Reasoning.pytorch" -> "endernewton/iter-reason"
"rosinality/mac-network-pytorch" -> "bhpfelix/Compositional-Attention-Networks-for-Machine-Reasoning-PyTorch"
"rosinality/mac-network-pytorch" -> "ronghanghu/snmn"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "jiasenlu/visDial.pytorch"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "batra-mlp-lab/visdial-rl"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "vmurahari3/visdial-bert"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "yuleiniu/rva"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "gicheonkang/DAN-VisDial"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "batra-mlp-lab/visdial"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "vmurahari3/visdial-diversity"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "hudaAlamri/DSTC7-Audio-Visual-Scene-Aware-Dialog-AVSD-Challenge"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "taesunwhang/MVAN-VisDial"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "zilongzheng/visdial-gnn"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "simpleshinobu/visdial-principles"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "naver/aqm-plus"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "jnhwkim/ban-vqa"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "batra-mlp-lab/visdial-amt-chat"
"zawlin/cvpr17_vtranse" -> "yangxuntu/vrd"
"zawlin/cvpr17_vtranse" -> "Prof-Lu-Cewu/Visual-Relationship-Detection"
"zawlin/cvpr17_vtranse" -> "yangxuntu/vtranse"
"zawlin/cvpr17_vtranse" -> "GriffinLiang/vrd-dsr"
"zawlin/cvpr17_vtranse" -> "doubledaibo/drnet_cvpr2017"
"idansc/simple-avsd" -> "dialogtekgeek/AudioVisualSceneAwareDialog"
"idansc/simple-avsd" -> "idansc/fga"
"RoyalSkye/Image-Caption" -> "aravindvarier/Image-Captioning-Pytorch"
"IDEA-Research/DQ-DETR" -> "SlongLiu/UnsupKeypointsParts"
"aioz-ai/MICCAI19-MedVQA" -> "aioz-ai/MICCAI21_MMQ"
"peteanderson80/SPICE" -> "vacancy/SceneGraphParser"
"peteanderson80/SPICE" -> "yangxuntu/SGAE"
"peteanderson80/SPICE" -> "richardaecn/cvpr18-caption-eval"
"peteanderson80/SPICE" -> "rowanz/neural-motifs"
"hudaAlamri/DSTC7-Audio-Visual-Scene-Aware-Dialog-AVSD-Challenge" -> "ictnlp/DSTC8-AVSD"
"hudaAlamri/DSTC7-Audio-Visual-Scene-Aware-Dialog-AVSD-Challenge" -> "dialogtekgeek/AudioVisualSceneAwareDialog"
"jshilong/DDQ" -> "jshilong/GroupRCNN" ["e"=1]
"jshilong/DDQ" -> "jozhang97/DETA"
"jshilong/DDQ" -> "MCG-NJU/AdaMixer"
"gaopengcuhk/Container" -> "allenai/container"
"davidmascharka/MyNN" -> "rsokl/noggin"
"IDEA-Research/DN-DETR" -> "IDEA-Research/DAB-DETR"
"wbw520/scouter" -> "wbw520/MTUNet"
"wbw520/scouter" -> "wbw520/AutoNoduleDetect"
"wbw520/scouter" -> "wbw520/NoisyLSTM"
"chuangg/CLEVRER" -> "zfchenUnique/DCL-Release"
"LightDXY/FT-CLIP" -> "facebookresearch/long_seq_mae" ["e"=1]
"yukezhu/visual7w-qa-models" -> "yukezhu/visual7w-toolkit"
"vmurahari3/visdial-bert" -> "salesforce/VD-BERT"
"vmurahari3/visdial-bert" -> "idansc/mrr-ndcg"
"vmurahari3/visdial-bert" -> "yuleiniu/rva"
"vmurahari3/visdial-bert" -> "simpleshinobu/visdial-principles"
"vmurahari3/visdial-bert" -> "shubhamagarwal92/visdial_conv"
"vmurahari3/visdial-bert" -> "batra-mlp-lab/visdial-challenge-starter-pytorch"
"vmurahari3/visdial-bert" -> "gicheonkang/DAN-VisDial"
"vmurahari3/visdial-bert" -> "JXZe/DualVD"
"vmurahari3/visdial-bert" -> "taesunwhang/MVAN-VisDial"
"vmurahari3/visdial-bert" -> "davidnvq/visdial"
"vmurahari3/visdial-bert" -> "gicheonkang/dan-visdial"
"vmurahari3/visdial-bert" -> "wh0330/CAG_VisDial"
"IDEA-Research/DAB-DETR" -> "IDEA-Research/DN-DETR"
"ChangyaoTian/VL-LTR" -> "gaopengcuhk/BALLAD"
"dingjiansw101/ZegFormer" -> "MendelXu/zsseg.baseline"
"dingjiansw101/ZegFormer" -> "ZiqinZhou66/ZegCLIP"
"facebookresearch/Large-Scale-VRD" -> "jz462/Large-Scale-VRD.pytorch"
"facebookresearch/Large-Scale-VRD" -> "yangxuntu/vrd"
"facebookresearch/Generic-Grouping" -> "ksaito-ut/openworld_ldet"
"dyabel/detpro" -> "alirezazareian/ovr-cnn"
"dyabel/detpro" -> "yuhangzang/OV-DETR"
"dyabel/detpro" -> "mengqiDyangge/HierKD"
"dyabel/detpro" -> "yuhangzang/UPT"
"dyabel/detpro" -> "mcahny/object_localization_network"
"yangxuntu/vrd" -> "GriffinLiang/vrd-dsr"
"yangxuntu/vrd" -> "zawlin/cvpr17_vtranse"
"yangxuntu/vrd" -> "nexusapoorvacus/DeepVariationStructuredRL"
"yangxuntu/vrd" -> "Prof-Lu-Cewu/Visual-Relationship-Detection"
"yangxuntu/vrd" -> "jz462/Large-Scale-VRD.pytorch"
"Dawn-LX/VidVRD-tracklets" -> "Dawn-LX/VidSGG-BIG"
"yashkant/sam-textvqa" -> "ZephyrZhuQi/ssbaseline"
"yashkant/sam-textvqa" -> "microsoft/TAP"
"yashkant/sam-textvqa" -> "guanghuixu/CRN_tvqa"
"yashkant/sam-textvqa" -> "uakarsh/latr"
"yashkant/sam-textvqa" -> "xinke-wang/Awesome-Text-VQA"
"e-bug/iglue" -> "zmykevin/UC2"
"e-bug/volta" -> "lil-lab/nlvr"
"e-bug/volta" -> "e-bug/iglue"
"e-bug/volta" -> "marvl-challenge/marvl-code"
"mcahny/object_localization_network" -> "ksaito-ut/openworld_ldet"
"mcahny/object_localization_network" -> "dyabel/detpro"
"mcahny/object_localization_network" -> "csuhan/opendet2"
"mcahny/object_localization_network" -> "xiaofeng94/VL-PLM"
"mcahny/object_localization_network" -> "RE-OWOD/RE-OWOD"
"mcahny/object_localization_network" -> "wusize/ovdet"
"mcahny/object_localization_network" -> "facebookresearch/Generic-Grouping"
"mcahny/object_localization_network" -> "mmaaz60/mvits_for_class_agnostic_od"
"simpleshinobu/visdial-principles" -> "idansc/mrr-ndcg"
"csuhan/opendet2" -> "RE-OWOD/RE-OWOD"
"csuhan/opendet2" -> "ksaito-ut/openworld_ldet"
"SlongLiu/DAB-DETR" -> "FengLi-ust/DN-DETR"
"linjieyangsc/densecap" -> "InnerPeace-Wu/densecap-tensorflow"
"wyzjack/SLA2P" -> "wyzjack/MRMGA4VAD"
"MCG-NJU/Structured-Sparse-RCNN" -> "SHTUPLUS/PySGG"
"MCG-NJU/Structured-Sparse-RCNN" -> "XinyuLyu/FGPL"
"iejMac/video2numpy" -> "iejMac/clip-video-encode"
"cdancette/rubi.bootstrap.pytorch" -> "yanxinzju/CSS-VQA"
"cdancette/rubi.bootstrap.pytorch" -> "chrisc36/bottom-up-attention-vqa"
"cdancette/rubi.bootstrap.pytorch" -> "jialinwu17/self_critical_vqa"
"cdancette/rubi.bootstrap.pytorch" -> "cdancette/vqa-cp-leaderboard"
"cdancette/rubi.bootstrap.pytorch" -> "chrisc36/debias"
"batra-mlp-lab/visdial-amt-chat" -> "btgraham/Batchwise-Dropout" ["e"=1]
"li-xirong/cross-lingual-cap" -> "weiyuk/fluent-cap"
"li-xirong/cross-lingual-cap" -> "li-xirong/coco-cn"
"li-xirong/cross-lingual-cap" -> "HughChi/Image-Caption"
"zjuchenlong/sca-cnn" -> "ronghanghu/cmn"
"bknyaz/sgg" -> "layer6ai-labs/SGG-Seq2Seq"
"bknyaz/sgg" -> "SHTUPLUS/PySGG"
"taksau/GPS-Net" -> "Vision-CAIR/RelTransformer"
"rosinality/relation-networks-pytorch" -> "aelnouby/Relational-Networks"
"sagizty/NTUS_application" -> "sagizty/Parallel-Hybrid-Transformer"
"sagizty/NTUS_application" -> "sagizty/sagizty"
"sagizty/NTUS_application" -> "sagizty/MAE"
"sagizty/Parallel-Hybrid-Transformer" -> "sagizty/NTUS_application"
"sagizty/Parallel-Hybrid-Transformer" -> "sagizty/sagizty"
"sagizty/Parallel-Hybrid-Transformer" -> "sagizty/MAE"
"JXZe/DualVD" -> "vmurahari3/visdial-bert"
"JXZe/DualVD" -> "wh0330/CAG_VisDial"
"JXZe/DualVD" -> "JXZe/Learning_DualVD"
"JXZe/DualVD" -> "taesunwhang/MVAN-VisDial"
"ThomasRobertFr/gpu-monitor" -> "emited/gantk2"
"andyweizhao/Multitask_Image_Captioning" -> "cswhjiang/Recurrent_Fusion_Network"
"JieShibo/PETL-ViT" -> "ZhangYuanhan-AI/NOAH"
"chrisc36/debias" -> "UKPLab/acl2020-confidence-regularization"
"chrisc36/debias" -> "UKPLab/emnlp2020-debiasing-unknown"
"chrisc36/debias" -> "cdancette/rubi.bootstrap.pytorch"
"SHTUPLUS/PySGG" -> "waxnkw/IETrans-SGG.pytorch"
"SHTUPLUS/PySGG" -> "Scarecrow0/BGNN-SGG"
"SHTUPLUS/PySGG" -> "XinyuLyu/FGPL"
"SHTUPLUS/PySGG" -> "muktilin/NICE"
"SHTUPLUS/PySGG" -> "Scarecrow0/SGTR"
"SHTUPLUS/PySGG" -> "dongxingning/SHA-GCL-for-SGG"
"wbw520/MTUNet" -> "wbw520/AutoNoduleDetect"
"JamesChuanggg/VQA-tensorflow" -> "paarthneekhara/neural-vqa-tensorflow"
"JamesChuanggg/VQA-tensorflow" -> "VT-vision-lab/VQA"
"JamesChuanggg/VQA-tensorflow" -> "shmsw25/mcb-model-for-vqa"
"JamesChuanggg/VQA-tensorflow" -> "metalbubble/VQAbaseline"
"JamesChuanggg/VQA-tensorflow" -> "liuzhi136/Visual-Question-Answering"
"huoxingmeishi/Awesome-Scene-Graphs" -> "mods333/energy-based-scene-graph"
"mesnico/RelationNetworks-CLEVR" -> "rosinality/relation-networks-pytorch"
"cswhjiang/Recurrent_Fusion_Network" -> "andyweizhao/Multitask_Image_Captioning"
"cswhjiang/Recurrent_Fusion_Network" -> "gujiuxiang/Stack-Captioning"
"YiwuZhong/SGG_from_NLS" -> "jshi31/WS-SGG"
"YiwuZhong/SGG_from_NLS" -> "Scarecrow0/BGNN-SGG"
"Computer-Vision-in-the-Wild/Elevater_Toolkit_IC" -> "Computer-Vision-in-the-Wild/DataDownload"
"fcjian/PromptDet" -> "alirezazareian/ovr-cnn"
"siddk/relation-network" -> "guotong1988/Relation-Network-babi"
"gaopengcuhk/BALLAD" -> "ChangyaoTian/VL-LTR"
"google-research/clevr_robot_env" -> "bhiziroglu/Language-as-an-Abstraction-for-Hierarchical-Deep-Reinforcement-Learning"
"Kapilks/Basic-Kernel" -> "Kapilks/Download-youtube-video"
"Kapilks/Download-youtube-video" -> "Kapilks/Basic-Kernel"
"shubhamagarwal92/visdial_conv" -> "wh0330/CAG_VisDial"
"facebookresearch/corefnmn" -> "shubhamagarwal92/visdial_conv"
"facebookresearch/corefnmn" -> "idansc/mrr-ndcg"
"JindongJiang/SCALOR" -> "zhixuan-lin/G-SWM"
"yukezhu/visual7w-toolkit" -> "yukezhu/visual7w-qa-models"
"pharmapsychotic/clip-interrogator-ext" ["l"="31.931,34.762"]
"alemelis/sd-webui-ar" ["l"="34.536,29.409"]
"google/sg2im" ["l"="31.475,34.529"]
"jwyang/graph-rcnn.pytorch" ["l"="31.469,34.55"]
"rowanz/neural-motifs" ["l"="31.452,34.541"]
"danfeiX/scene-graph-TF-release" ["l"="31.443,34.552"]
"taoxugit/AttnGAN" ["l"="33.483,32.601"]
"yikang-li/MSDN" ["l"="31.432,34.56"]
"hanzhanggit/StackGAN-Pytorch" ["l"="33.502,32.6"]
"KaihuaTang/Scene-Graph-Benchmark.pytorch" ["l"="31.488,34.565"]
"yikang-li/FactorizableNet" ["l"="31.427,34.541"]
"jiasenlu/NeuralBabyTalk" ["l"="31.518,34.575"]
"lichengunc/MAttNet" ["l"="31.293,33.934"]
"peteanderson80/bottom-up-attention" ["l"="31.583,34.573"]
"hanzhanggit/StackGAN-v2" ["l"="33.526,32.593"]
"openai/glow" ["l"="33.63,32.773"]
"hengyuan-hu/bottom-up-attention-vqa" ["l"="31.614,34.51"]
"hanzhanggit/StackGAN" ["l"="33.613,32.591"]
"Cadene/vqa.pytorch" ["l"="31.655,34.492"]
"jnhwkim/ban-vqa" ["l"="31.638,34.504"]
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" ["l"="31.633,34.518"]
"MILVLG/mcan-vqa" ["l"="31.631,34.554"]
"peteanderson80/Up-Down-Captioner" ["l"="31.532,34.595"]
"Cyanogenoid/pytorch-vqa" ["l"="31.626,34.483"]
"Cyanogenoid/vqa-counting" ["l"="31.657,34.477"]
"GT-Vision-Lab/VQA" ["l"="31.645,34.482"]
"markdtw/vqa-winner-cvprw-2017" ["l"="31.639,34.471"]
"Cadene/block.bootstrap.pytorch" ["l"="31.649,34.517"]
"stanfordnlp/mac-network" ["l"="31.591,34.442"]
"ruotianluo/self-critical.pytorch" ["l"="31.54,34.587"]
"yuzcccc/vqa-mfb" ["l"="31.667,34.499"]
"airsplay/lxmert" ["l"="31.645,34.628"]
"MILVLG/openvqa" ["l"="31.646,34.544"]
"aimbrain/vqa-project" ["l"="31.637,34.488"]
"Cadene/murel.bootstrap.pytorch" ["l"="31.626,34.493"]
"cvlab-tohoku/Dense-CoAttention-Network" ["l"="31.591,34.48"]
"SinghJasdeep/Attention-on-Attention-for-VQA" ["l"="31.617,34.475"]
"linjieli222/VQA_ReGAT" ["l"="31.618,34.538"]
"yangxuntu/SGAE" ["l"="31.515,34.602"]
"NVIDIA/ContrastiveLosses4VRD" ["l"="31.415,34.527"]
"vacancy/SceneGraphParser" ["l"="31.48,34.582"]
"microsoft/scene_graph_benchmark" ["l"="31.505,34.592"]
"KaihuaTang/VCTree-Scene-Graph-Generation" ["l"="31.432,34.528"]
"ranjaykrishna/visual_genome_python_driver" ["l"="31.468,34.569"]
"jz462/Large-Scale-VRD.pytorch" ["l"="31.394,34.516"]
"DeepRNN/image_captioning" ["l"="31.518,34.522"]
"yunjey/show-attend-and-tell" ["l"="31.504,34.533"]
"jazzsaxmafia/show_attend_and_tell.tensorflow" ["l"="31.5,34.5"]
"ruotianluo/ImageCaptioning.pytorch" ["l"="31.542,34.572"]
"kelvinxu/arctic-captions" ["l"="31.538,34.527"]
"yashk2810/Image-Captioning" ["l"="31.475,34.509"]
"mosessoh/CNN-LSTM-Caption-Generator" ["l"="31.511,34.49"]
"tylin/coco-caption" ["l"="31.535,34.553"]
"anuragmishracse/caption_generator" ["l"="31.456,34.476"]
"jcjohnson/densecap" ["l"="31.566,34.521"]
"jiasenlu/AdaptiveAttention" ["l"="31.496,34.581"]
"zhjohnchan/awesome-image-captioning" ["l"="31.523,34.59"]
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" ["l"="31.516,34.558"]
"kakaobrain/coyo-dataset" ["l"="31.781,34.814"]
"rom1504/img2dataset" ["l"="31.811,34.781"]
"kakaobrain/kogpt" ["l"="44.501,-15.081"]
"kakaobrain/minDALL-E" ["l"="44.483,-15.121"]
"facebookresearch/SLIP" ["l"="31.756,34.811"]
"microsoft/GLIP" ["l"="31.754,34.849"]
"microsoft/X-Decoder" ["l"="31.782,34.908"]
"kakaobrain/rq-vae-transformer" ["l"="34.477,28.967"]
"lucidrains/flamingo-pytorch" ["l"="31.729,34.772"]
"rom1504/clip-retrieval" ["l"="31.831,34.795"]
"jungwoo-ha/WeeklyArxivTalk" ["l"="44.487,-15.102"]
"tunib-ai/large-scale-lm-tutorials" ["l"="44.459,-15.148"]
"mlfoundations/open_clip" ["l"="31.811,34.814"]
"kakaobrain/karlo" ["l"="34.495,29.152"]
"KLUE-benchmark/KLUE" ["l"="44.471,-15.08"]
"salesforce/BLIP" ["l"="31.762,34.773"]
"openai/CLIP" ["l"="34.392,35.886"]
"salesforce/LAVIS" ["l"="31.777,34.792"]
"OFA-Sys/OFA" ["l"="31.744,34.759"]
"mlfoundations/open_flamingo" ["l"="27.344,31.188"]
"openai/guided-diffusion" ["l"="34.449,28.889"]
"lucidrains/DALLE2-pytorch" ["l"="34.334,28.871"]
"huggingface/diffusers" ["l"="27.252,30.798"]
"facebookresearch/mae" ["l"="34.492,35.902"]
"baaivision/EVA" ["l"="31.804,34.915"]
"CompVis/latent-diffusion" ["l"="34.407,28.925"]
"CompVis/taming-transformers" ["l"="34.389,28.843"]
"facebookresearch/Detic" ["l"="31.772,34.878"]
"ashkamath/mdetr" ["l"="31.697,34.805"]
"facebookresearch/Mask2Former" ["l"="34.733,35.856"]
"facebookresearch/ConvNeXt" ["l"="34.507,35.844"]
"NVlabs/GroupViT" ["l"="31.749,34.867"]
"xingyizhou/UniDet" ["l"="34.863,35.692"]
"xingyizhou/CenterNet2" ["l"="34.724,35.591"]
"fundamentalvision/Deformable-DETR" ["l"="34.63,35.791"]
"isl-org/lang-seg" ["l"="31.729,34.875"]
"ShoufaChen/DiffusionDet" ["l"="31.798,34.939"]
"IDEACVR/DINO" ["l"="31.756,34.972"]
"KaiyangZhou/CoOp" ["l"="31.718,34.817"]
"ellisk42/ec" ["l"="31.507,34.325"]
"vacancy/NSCL-PyTorch-Release" ["l"="31.552,34.337"]
"probcomp/Gen.jl" ["l"="17.17,39.055"]
"RichardEvans/apperception" ["l"="31.457,34.312"]
"top-quarks/ARC-solution" ["l"="31.465,34.327"]
"piantado/LOTlib3" ["l"="31.493,34.303"]
"fchollet/ARC" ["l"="25.095,35.711"]
"samacqua/LARC" ["l"="31.479,34.313"]
"YeWR/EfficientZero" ["l"="25.341,35.762"]
"kexinyi/ns-vqa" ["l"="31.584,34.364"]
"tech-srl/RASP" ["l"="27.811,31.079"]
"facebookresearch/mmf" ["l"="31.659,34.688"]
"microsoft/Oscar" ["l"="31.625,34.667"]
"yuewang-cuhk/awesome-vision-language-pretraining-papers" ["l"="31.663,34.667"]
"facebookresearch/vilbert-multi-task" ["l"="31.642,34.667"]
"pliang279/awesome-multimodal-ml" ["l"="31.618,34.721"]
"ChenRocks/UNITER" ["l"="31.649,34.65"]
"dandelin/ViLT" ["l"="31.685,34.722"]
"jackroos/VL-BERT" ["l"="31.623,34.639"]
"facebookresearch/grid-feats-vqa" ["l"="31.599,34.627"]
"facebookresearch/multimodal" ["l"="31.721,34.785"]
"uclanlp/visualbert" ["l"="31.629,34.62"]
"jokieleung/awesome-visual-question-answering" ["l"="31.647,34.581"]
"IDEA-Research/DINO" ["l"="31.8,35.002"]
"IDEA-Research/MaskDINO" ["l"="31.777,35.011"]
"IDEA-Research/detrex" ["l"="31.781,34.982"]
"IDEA-Research/DN-DETR" ["l"="31.79,35.051"]
"IDEA-Research/GroundingDINO" ["l"="27.195,31.176"]
"IDEA-Research/DAB-DETR" ["l"="31.764,35.052"]
"OpenGVLab/InternImage" ["l"="31.815,34.976"]
"IDEA-Research/awesome-detection-transformer" ["l"="31.788,35.026"]
"jozhang97/DETA" ["l"="31.757,35.02"]
"microsoft/FocalNet" ["l"="31.843,35.002"]
"ZhangGongjie/SAM-DETR" ["l"="22.098,27.769"]
"open-mmlab/playground" ["l"="34.799,35.78"]
"czczup/ViT-Adapter" ["l"="31.793,34.954"]
"Computer-Vision-in-the-Wild/CVinW_Readings" ["l"="31.669,34.875"]
"Computer-Vision-in-the-Wild/Elevater_Toolkit_IC" ["l"="31.596,34.9"]
"microsoft/RegionCLIP" ["l"="31.701,34.882"]
"ZhangYuanhan-AI/OmniBenchmark" ["l"="35.404,34.896"]
"clin1223/VLDet" ["l"="31.69,34.913"]
"rshaojimmy/MultiModal-DeepFake" ["l"="35.447,34.925"]
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" ["l"="31.671,34.825"]
"muzairkhattak/multimodal-prompt-learning" ["l"="31.65,34.866"]
"salesforce/ALBEF" ["l"="31.703,34.744"]
"alibaba/AliceMind" ["l"="32.166,30.363"]
"OFA-Sys/Chinese-CLIP" ["l"="31.821,34.747"]
"rmokady/CLIP_prefix_caption" ["l"="31.773,34.754"]
"zengyan-97/X-VLM" ["l"="31.745,34.728"]
"facebookresearch/clevr-dataset-gen" ["l"="31.56,34.365"]
"facebookresearch/clevr-iep" ["l"="31.613,34.404"]
"deepmind/multi_object_datasets" ["l"="31.563,34.239"]
"ronghanghu/n2nmn" ["l"="31.621,34.428"]
"lucidrains/slot-attention" ["l"="31.579,34.192"]
"deepmind/dsprites-dataset" ["l"="25.076,32.602"]
"google-research/clevr_robot_env" ["l"="31.532,34.351"]
"davidmascharka/tbd-nets" ["l"="31.598,34.423"]
"facebookresearch/phyre" ["l"="31.468,34.287"]
"chuangg/CLEVRER" ["l"="31.572,34.324"]
"HarshTrivedi/nmn-pytorch" ["l"="31.586,34.382"]
"jazzsaxmafia/show_and_tell.tensorflow" ["l"="31.539,34.48"]
"tsenghungchen/show-adapt-and-tell" ["l"="31.463,34.583"]
"husthuaan/AoANet" ["l"="31.532,34.609"]
"aimagelab/meshed-memory-transformer" ["l"="31.548,34.626"]
"JDAI-CV/image-captioning" ["l"="31.542,34.618"]
"fengyang0317/unsupervised_captioning" ["l"="31.499,34.603"]
"yahoo/object_relation_transformer" ["l"="31.52,34.629"]
"sgrvinod/Deep-Tutorials-for-PyTorch" ["l"="31.445,34.584"]
"krasserm/fairseq-image-captioning" ["l"="31.51,34.635"]
"sgrvinod/a-PyTorch-Tutorial-to-Object-Detection" ["l"="34.444,35.404"]
"YiwuZhong/Sub-GC" ["l"="31.499,34.625"]
"zhangxuying1004/RSTNet" ["l"="31.531,34.668"]
"cshizhe/asg2cap" ["l"="31.531,34.623"]
"luo3300612/image-captioning-DLCT" ["l"="31.536,34.649"]
"husthuaan/AAT" ["l"="31.518,34.645"]
"aimagelab/show-control-and-tell" ["l"="31.512,34.614"]
"fenglinliu98/MIA" ["l"="31.451,34.648"]
"poojahira/image-captioning-bottom-up-top-down" ["l"="31.53,34.635"]
"gujiuxiang/Stack-Captioning" ["l"="31.475,34.612"]
"fawazsammani/show-edit-tell" ["l"="31.504,34.649"]
"forence/Awesome-Visual-Captioning" ["l"="31.564,34.63"]
"facebookresearch/ConvNeXt-V2" ["l"="31.849,34.977"]
"DingXiaoH/RepLKNet-pytorch" ["l"="31.864,35.039"]
"snap-research/EfficientFormer" ["l"="34.791,35.903"]
"facebookresearch/DiT" ["l"="34.52,28.995"]
"sail-sg/metaformer" ["l"="31.91,35.039"]
"Visual-Attention-Network/SegNeXt" ["l"="31.871,34.995"]
"keyu-tian/SparK" ["l"="21.964,27.519"]
"facebookresearch/dropout" ["l"="31.835,35.028"]
"microsoft/SimMIM" ["l"="34.662,35.966"]
"raoyongming/HorNet" ["l"="31.888,35.034"]
"facebookresearch/ToMe" ["l"="31.857,34.919"]
"dbolya/tomesd" ["l"="34.539,29.223"]
"facebookresearch/CutLER" ["l"="31.829,34.917"]
"raoyongming/DynamicViT" ["l"="34.708,35.981"]
"ziplab/SN-Net" ["l"="31.927,34.922"]
"ma-xu/Context-Cluster" ["l"="31.829,34.932"]
"google-research/big_vision" ["l"="31.812,34.87"]
"xxxnell/how-do-vits-work" ["l"="34.589,35.985"]
"NVlabs/GCVit" ["l"="31.907,34.948"]
"facebookresearch/msn" ["l"="34.637,36.029"]
"uta-smile/TCL" ["l"="31.722,34.739"]
"Cuberick-Orion/CIRR" ["l"="32.661,34.464"]
"researchmm/soho" ["l"="31.658,34.719"]
"ioanacroi/qb-norm" ["l"="31.734,33.722"]
"zdou0830/METER" ["l"="31.699,34.729"]
"LgQu/DIME" ["l"="31.818,33.715"]
"IDEACVR/awesome-detection-transformer" ["l"="31.738,34.985"]
"open-mmlab/mmyolo" ["l"="34.674,35.747"]
"alibaba/EasyCV" ["l"="34.591,35.745"]
"IDEA-opensource/DN-DETR" ["l"="31.738,35.013"]
"Atten4Vis/ConditionalDETR" ["l"="31.71,35.027"]
"ryankiros/visual-semantic-embedding" ["l"="31.9,33.781"]
"Element-Research/rnn" ["l"="26.985,34.298"]
"karpathy/neuraltalk2" ["l"="27.197,34.147"]
"pharmapsychotic/clip-interrogator" ["l"="31.861,34.779"]
"microsoft/unilm" ["l"="34.387,35.927"]
"huggingface/peft" ["l"="27.406,31.119"]
"amazon-science/mm-cot" ["l"="27.362,31.144"]
"ZrrSkywalker/LLaMA-Adapter" ["l"="27.319,31.189"]
"IDEA-Research/Grounded-Segment-Anything" ["l"="27.159,31.08"]
"Cloud-CV/Origami" ["l"="31.939,34.211"]
"Cloud-CV/CloudCV" ["l"="31.951,34.2"]
"Cloud-CV/visual-chatbot" ["l"="31.911,34.228"]
"dabasajay/Image-Caption-Generator" ["l"="31.435,34.447"]
"damminhtien/deep-learning-image-caption-generator" ["l"="31.412,34.414"]
"Shobhit20/Image-Captioning" ["l"="31.447,34.462"]
"neural-nuts/image-caption-generator" ["l"="31.456,34.443"]
"JamesChuanggg/awesome-vqa" ["l"="31.651,34.453"]
"jiasenlu/HieCoAttenVQA" ["l"="31.658,34.444"]
"JamesChuanggg/VQA-tensorflow" ["l"="31.645,34.434"]
"akirafukui/vqa-mcb" ["l"="31.671,34.456"]
"iamaaditya/VQA_Demo" ["l"="31.674,34.442"]
"metalbubble/VQAbaseline" ["l"="31.662,34.431"]
"zcyang/imageqa-san" ["l"="31.689,34.449"]
"VT-vision-lab/VQA_LSTM_CNN" ["l"="31.678,34.43"]
"paarthneekhara/neural-vqa-tensorflow" ["l"="31.63,34.414"]
"abhshkdz/neural-vqa-attention" ["l"="31.631,34.441"]
"IDEACVR/MaskDINO" ["l"="31.753,35"]
"IDEA-opensource/DAB-DETR" ["l"="31.717,35.015"]
"HDETR/H-Deformable-DETR" ["l"="31.752,35.033"]
"Picsart-AI-Research/SeMask-Segmentation" ["l"="34.819,35.92"]
"zhiqi-li/Panoptic-SegFormer" ["l"="34.914,35.871"]
"jshilong/DDQ" ["l"="31.737,35.044"]
"ZwwWayne/K-Net" ["l"="34.784,35.829"]
"wjf5203/SeqFormer" ["l"="31.825,33.377"]
"hustvl/MIMDet" ["l"="34.772,35.945"]
"implus/UM-MAE" ["l"="34.723,36.047"]
"aditya12agd5/convcap" ["l"="31.455,34.623"]
"ruotianluo/Transformer_Captioning" ["l"="31.468,34.619"]
"JosephKJ/OWOD" ["l"="31.611,34.986"]
"akshitac8/OW-DETR" ["l"="31.61,34.952"]
"megvii-model/YOLOF" ["l"="34.768,35.611"]
"mcahny/object_localization_network" ["l"="31.629,34.943"]
"mmaaz60/mvits_for_class_agnostic_od" ["l"="31.629,34.916"]
"ucbdrive/few-shot-object-detection" ["l"="22.249,27.944"]
"implus/GFocalV2" ["l"="34.785,35.593"]
"PeizeSun/SparseR-CNN" ["l"="34.727,35.654"]
"xieenze/DetCo" ["l"="31.59,35.048"]
"amirbar/DETReg" ["l"="22.231,27.905"]
"dddzg/up-detr" ["l"="31.658,35.01"]
"PeizeSun/OneNet" ["l"="34.785,35.582"]
"facebookresearch/unbiased-teacher" ["l"="34.962,35.722"]
"iCGY96/awesome_OpenSetRecognition_list" ["l"="23.474,34.962"]
"KMnP/vpt" ["l"="31.661,34.852"]
"gaopengcuhk/Tip-Adapter" ["l"="31.69,34.844"]
"raoyongming/DenseCLIP" ["l"="31.722,34.855"]
"gaopengcuhk/CLIP-Adapter" ["l"="31.709,34.836"]
"KaiyangZhou/Dassl.pytorch" ["l"="30.112,34.956"]
"sallymmx/ActionCLIP" ["l"="32.407,35.037"]
"Sense-GVT/DeCLIP" ["l"="31.744,34.818"]
"alisure-ml/awesome-visual-relationship-detection" ["l"="31.401,34.528"]
"GriffinLiang/vrd-dsr" ["l"="31.382,34.525"]
"Prof-Lu-Cewu/Visual-Relationship-Detection" ["l"="31.402,34.539"]
"yangxuntu/vrd" ["l"="31.372,34.527"]
"facebookresearch/Large-Scale-VRD" ["l"="31.368,34.512"]
"yuweihao/KERN" ["l"="31.4,34.555"]
"zawlin/cvpr17_vtranse" ["l"="31.362,34.535"]
"nexusapoorvacus/DeepVariationStructuredRL" ["l"="31.388,34.537"]
"ezeli/BUTD_model" ["l"="31.477,34.686"]
"ezeli/bottom_up_features_extract" ["l"="31.449,34.712"]
"pzzhang/VinVL" ["l"="31.625,34.651"]
"LuoweiZhou/VLP" ["l"="31.599,34.641"]
"jayleicn/ClipBERT" ["l"="31.654,34.705"]
"matsui528/faiss_tips" ["l"="31.931,34.788"]
"facebookresearch/distributed-faiss" ["l"="31.956,34.79"]
"criteo/autofaiss" ["l"="31.879,34.799"]
"UKPLab/beir" ["l"="25.827,29.17"]
"matsui528/nanopq" ["l"="28.432,31.468"]
"matsui528/annbench" ["l"="28.592,31.465"]
"doc-doc/vRGV" ["l"="31.317,34.403"]
"xdshang/VidVRD-helper" ["l"="31.338,34.422"]
"Alan-Lee123/relation-network" ["l"="31.768,34.408"]
"kimhc6028/relational-networks" ["l"="31.732,34.429"]
"gitlimlab/Relation-Network-Tensorflow" ["l"="31.794,34.424"]
"bioinf-jku/SNNs" ["l"="33.874,32.628"]
"ikostrikov/pytorch-a2c-ppo-acktr" ["l"="25.045,35.899"]
"mesnico/RelationNetworks-CLEVR" ["l"="31.821,34.434"]
"szagoruyko/diracnets" ["l"="34.143,35.233"]
"eladhoffer/seq2seq.pytorch" ["l"="30.083,32.417"]
"Kaixhin/Rainbow" ["l"="25.109,35.903"]
"ethanfetaya/NRI" ["l"="23.716,33.042"]
"jacobandreas/nmn2" ["l"="31.661,34.407"]
"facebookresearch/poincare-embeddings" ["l"="22.614,33.94"]
"fyu/drn" ["l"="31.637,36.944"]
"hjbahng/visual_prompting" ["l"="31.674,34.86"]
"amirbar/visual_prompting" ["l"="31.685,34.888"]
"ZhangYuanhan-AI/NOAH" ["l"="31.649,34.84"]
"JieShibo/PETL-ViT" ["l"="31.626,34.838"]
"dongzelian/SSF" ["l"="31.613,34.851"]
"sagizty/VPT" ["l"="31.507,34.93"]
"ShoufaChen/AdaptFormer" ["l"="31.631,34.856"]
"yuhangzang/UPT" ["l"="31.638,34.875"]
"sagizty/Multi-Stage-Hybrid-Transformer" ["l"="31.473,34.954"]
"sagizty/MAE" ["l"="31.465,34.943"]
"sagizty/NTUS_application" ["l"="31.479,34.945"]
"sagizty/Parallel-Hybrid-Transformer" ["l"="31.473,34.935"]
"sagizty/sagizty" ["l"="31.485,34.937"]
"sagizty/Insight" ["l"="31.489,34.95"]
"ruotianluo/Image_Captioning_AI_Challenger" ["l"="31.578,34.592"]
"ruotianluo/bottom-up-attention-ai-challenger" ["l"="31.611,34.57"]
"showkeyjar/chinese_im2text.pytorch" ["l"="31.691,34.596"]
"foamliu/Image-Captioning" ["l"="31.737,34.595"]
"Wind-Ward/Image_Caption_Competition" ["l"="31.604,34.585"]
"yufengm/Adaptive" ["l"="31.483,34.597"]
"HughChi/Image-Caption" ["l"="31.744,34.607"]
"lxtGH/AI_challenger_Chinese_Caption" ["l"="31.618,34.582"]
"timojl/clipseg" ["l"="31.742,34.883"]
"amrrs/stable-diffusion-prompt-inpainting" ["l"="34.383,29.166"]
"ThereforeGames/txt2mask" ["l"="34.432,29.221"]
"chongzhou96/MaskCLIP" ["l"="31.734,34.909"]
"google/prompt-to-prompt" ["l"="34.451,29.095"]
"omriav/blended-diffusion" ["l"="34.504,28.998"]
"UX-Decoder/Segment-Everything-Everywhere-All-At-Once" ["l"="27.252,31.215"]
"dingjiansw101/ZegFormer" ["l"="31.734,34.926"]
"jiasenlu/vilbert_beta" ["l"="31.642,34.607"]
"airsplay/py-bottom-up-attention" ["l"="31.595,34.615"]
"SHI-Labs/OneFormer" ["l"="31.827,34.954"]
"NVlabs/ODISE" ["l"="27.245,31.231"]
"baaivision/Painter" ["l"="27.24,31.191"]
"SHI-Labs/Neighborhood-Attention-Transformer" ["l"="31.893,34.988"]
"facebookresearch/MaskFormer" ["l"="34.696,35.879"]
"gligen/GLIGEN" ["l"="34.538,29.116"]
"facebookresearch/paco" ["l"="31.955,34.907"]
"facebookresearch/diht" ["l"="31.966,34.885"]
"TACJu/PartImageNet" ["l"="32.007,34.914"]
"google-research/deeplab2" ["l"="31.848,34.94"]
"csrhddlam/axial-deeplab" ["l"="34.731,35.972"]
"bowenc0221/panoptic-deeplab" ["l"="31.859,36.972"]
"google-research/pix2seq" ["l"="31.796,34.881"]
"joe-siyuan-qiao/ViP-DeepLab" ["l"="31.922,36.945"]
"cocodataset/panopticapi" ["l"="31.836,36.936"]
"openseg-group/openseg.pytorch" ["l"="31.729,37.032"]
"Wangt-CN/VC-R-CNN" ["l"="-26.079,18.942"]
"Jingkang50/OpenPSG" ["l"="35.443,34.898"]
"mods333/energy-based-scene-graph" ["l"="31.219,34.521"]
"SHTUPLUS/PySGG" ["l"="31.299,34.531"]
"huoxingmeishi/Awesome-Scene-Graphs" ["l"="31.192,34.523"]
"coldmanck/recovering-unbiased-scene-graphs" ["l"="31.184,34.508"]
"MCG-NJU/Structured-Sparse-RCNN" ["l"="31.249,34.522"]
"microsoft/SoftTeacher" ["l"="34.81,35.714"]
"FengLi-ust/DN-DETR" ["l"="31.716,35.051"]
"microsoft/DynamicHead" ["l"="34.771,35.738"]
"kuanghuei/SCAN" ["l"="31.869,33.734"]
"MILVLG/bottom-up-attention.pytorch" ["l"="31.575,34.606"]
"jiasenlu/visDial.pytorch" ["l"="31.804,34.349"]
"batra-mlp-lab/visdial-challenge-starter-pytorch" ["l"="31.8,34.375"]
"batra-mlp-lab/visdial-rl" ["l"="31.773,34.368"]
"facebookresearch/corefnmn" ["l"="31.841,34.341"]
"zilongzheng/visdial-gnn" ["l"="31.784,34.346"]
"yuleiniu/rva" ["l"="31.806,34.362"]
"batra-mlp-lab/visdial" ["l"="31.829,34.32"]
"naver/aqm-plus" ["l"="31.817,34.355"]
"vmurahari3/visdial-bert" ["l"="31.82,34.394"]
"satwikkottur/clevr-dialog" ["l"="31.749,34.348"]
"chenxinpeng/ARNet" ["l"="31.432,34.644"]
"daqingliu/CAVP" ["l"="31.419,34.652"]
"s-gupta/visual-concepts" ["l"="31.447,34.6"]
"andyweizhao/Multitask_Image_Captioning" ["l"="31.428,34.618"]
"cswhjiang/Recurrent_Fusion_Network" ["l"="31.443,34.627"]
"rakshithShetty/captionGAN" ["l"="31.411,34.629"]
"lukemelas/image-paragraph-captioning" ["l"="31.384,34.668"]
"facebookresearch/EmbodiedQA" ["l"="24.628,36.536"]
"ethanjperez/film" ["l"="31.576,34.422"]
"ronghanghu/cmn" ["l"="31.615,34.372"]
"RE-OWOD/RE-OWOD" ["l"="31.594,34.947"]
"csuhan/opendet2" ["l"="31.594,34.964"]
"JosephKJ/iOD" ["l"="31.534,35.01"]
"akshitac8/BiAM" ["l"="23.481,35.387"]
"alirezazareian/ovr-cnn" ["l"="31.675,34.898"]
"ylsung/VL_adapter" ["l"="31.663,34.792"]
"gaopengcuhk/BALLAD" ["l"="31.827,34.858"]
"ZrrSkywalker/CaFo" ["l"="31.691,34.859"]
"FlagAI-Open/FlagAI" ["l"="31.845,34.847"]
"adobe-research/custom-diffusion" ["l"="34.494,29.118"]
"microsoft/torchscale" ["l"="27.525,31.101"]
"comet-ml/kangas" ["l"="34.78,-28.242"]
"IDEA-CCNL/Fengshenbang-LM" ["l"="32.138,30.376"]
"thu-ml/unidiffuser" ["l"="34.55,29.084"]
"code-kern-ai/bricks" ["l"="34.763,-28.248"]
"SHI-Labs/Versatile-Diffusion" ["l"="34.492,29.078"]
"thunlp/OpenDelta" ["l"="27.659,31.313"]
"THUDM/GLM-130B" ["l"="27.323,31.105"]
"stanford-crfm/helm" ["l"="27.55,31.244"]
"crowsonkb/k-diffusion" ["l"="34.459,29.027"]
"FreddeFrallan/Multilingual-CLIP" ["l"="31.808,34.763"]
"rinongal/textual_inversion" ["l"="34.377,29.081"]
"microsoft/VQ-Diffusion" ["l"="34.448,28.972"]
"mlfoundations/wise-ft" ["l"="31.829,34.826"]
"JD-P/simulacra-aesthetic-captions" ["l"="34.296,29.04"]
"webdataset/webdataset" ["l"="31.786,34.83"]
"lucidrains/x-clip" ["l"="31.8,34.796"]
"openai/glide-text2im" ["l"="34.391,28.892"]
"facebookresearch/vissl" ["l"="34.44,35.967"]
"Cloud-CV/EvalAI" ["l"="31.971,34.177"]
"Cloud-CV/Fabrik" ["l"="31.931,34.177"]
"Cloud-CV/GSoC-Ideas" ["l"="31.931,34.197"]
"Cloud-CV/EvalAI-Starters" ["l"="31.982,34.146"]
"Cloud-CV/EvalAI-ngx" ["l"="31.95,34.162"]
"Cloud-CV/evalai-cli" ["l"="32.007,34.171"]
"chiphuyen/sotawhat" ["l"="29.899,32.507"]
"uber/pyro" ["l"="25.461,33.65"]
"facebookresearch/fairseq-py" ["l"="30.093,32.49"]
"codalab/codalab-competitions" ["l"="32.017,34.138"]
"beyretb/AnimalAI-Olympics" ["l"="25.206,35.817"]
"polyaxon/polyaxon" ["l"="25.346,34.076"]
"abhshkdz/ai-deadlines" ["l"="27.677,30.131"]
"applied-ai-lab/genesis" ["l"="31.551,34.203"]
"baudm/MONet-pytorch" ["l"="31.551,34.191"]
"zhixuan-lin/SPACE" ["l"="31.583,34.215"]
"stelzner/monet" ["l"="31.535,34.204"]
"singhgautam/slate" ["l"="31.6,34.207"]
"zhixuan-lin/IODINE" ["l"="31.567,34.212"]
"ecker-lab/object-centric-representation-benchmark" ["l"="31.547,34.224"]
"JindongJiang/SCALOR" ["l"="31.564,34.2"]
"deepmind/spriteworld" ["l"="25.223,35.852"]
"sail-sg/poolformer" ["l"="34.67,35.919"]
"LeapLabTHU/DAT" ["l"="34.768,35.93"]
"SHI-Labs/NATTEN" ["l"="31.956,35.018"]
"dingmyu/davit" ["l"="31.928,35.028"]
"Sense-X/UniFormer" ["l"="32.407,35.056"]
"NVlabs/FAN" ["l"="31.896,35.015"]
"microsoft/Focal-Transformer" ["l"="34.767,35.893"]
"google-research/maxim" ["l"="35.98,32.417"]
"whai362/PVT" ["l"="34.65,35.866"]
"VainF/Awesome-Anything" ["l"="27.264,31.205"]
"NVIDIA/aistore" ["l"="31.604,34.871"]
"tmbdev/webdataset" ["l"="31.534,34.89"]
"NVIDIA/ais-k8s" ["l"="31.56,34.885"]
"NVlabs/tensorcom" ["l"="31.544,34.872"]
"pytorch/elastic" ["l"="25.238,34.173"]
"petuum/adaptdl" ["l"="-9.374,2.98"]
"pytorch/torchx" ["l"="27.923,35.596"]
"NVIDIA/deepops" ["l"="-9.489,2.835"]
"facebookresearch/fairscale" ["l"="27.651,31.057"]
"pytorch/data" ["l"="23.396,33.65"]
"BaguaSys/bagua" ["l"="22.498,37.01"]
"uber/petastorm" ["l"="25.306,34.101"]
"pytorch/benchmark" ["l"="27.868,35.52"]
"tkestack/gpu-manager" ["l"="-9.435,2.795"]
"NVIDIA/DALI" ["l"="34.321,35.609"]
"billjie1/Chinese-CLIP" ["l"="31.914,34.701"]
"alibaba/EasyNLP" ["l"="32.19,30.346"]
"yuxie11/R2D2" ["l"="31.889,34.708"]
"LianjiaTech/BELLE" ["l"="27.324,31.171"]
"lonePatient/awesome-pretrained-chinese-nlp-models" ["l"="32.188,30.325"]
"mymusise/ChatGLM-Tuning" ["l"="27.336,31.249"]
"TencentARC/T2I-Adapter" ["l"="34.487,29.188"]
"cloneofsimo/lora" ["l"="34.405,29.181"]
"kohya-ss/sd-scripts" ["l"="34.416,29.325"]
"justinpinkney/stable-diffusion" ["l"="34.38,29.122"]
"lkwq007/stablediffusion-infinity" ["l"="34.324,29.15"]
"poloclub/diffusiondb" ["l"="34.452,29.126"]
"ShivamShrirao/diffusers" ["l"="34.362,29.187"]
"currentslab/awesome-vector-search" ["l"="31.942,34.81"]
"MendelXu/zsseg.baseline" ["l"="31.744,34.901"]
"MarkMoHR/Awesome-Referring-Image-Segmentation" ["l"="31.235,33.939"]
"lukemelas/deep-spectral-segmentation" ["l"="31.713,34.907"]
"DerrickWang005/CRIS.pytorch" ["l"="31.216,33.937"]
"facebookresearch/ov-seg" ["l"="31.753,34.92"]
"ArrowLuo/CLIP4Clip" ["l"="31.728,33.779"]
"gabeur/mmt" ["l"="31.75,33.803"]
"danieljf24/awesome-video-text-retrieval" ["l"="31.742,33.792"]
"microsoft/UniVL" ["l"="31.715,33.812"]
"linjieli222/HERO" ["l"="31.712,33.835"]
"m-bain/frozen-in-time" ["l"="31.712,33.78"]
"CryhanFang/CLIP2Video" ["l"="31.739,33.766"]
"jayleicn/moment_detr" ["l"="31.674,33.821"]
"rowanz/merlot" ["l"="31.678,33.79"]
"antoine77340/MIL-NCE_HowTo100M" ["l"="31.734,33.821"]
"bknyaz/sgg" ["l"="31.344,34.548"]
"mert-kurttutan/torchview" ["l"="32.059,34.808"]
"iejMac/video2numpy" ["l"="31.982,34.805"]
"pytorch-labs/tensordict" ["l"="25.511,35.705"]
"NVlabs/SegFormer" ["l"="34.674,35.864"]
"nmhkahn/deep_learning_tutorial" ["l"="44.683,-14.9"]
"dnouri/skorch" ["l"="25.512,33.671"]
"GunhoChoi/PyTorch-FastCampus" ["l"="44.612,-14.936"]
"uber/horovod" ["l"="34.123,35.271"]
"MrNothing/AI-Blocks" ["l"="0.755,40.138"]
"joeddav/devol" ["l"="25.627,33.732"]
"reiinakano/xcessiv" ["l"="25.602,33.772"]
"facebookresearch/loop" ["l"="0.532,40.164"]
"Prodicode/ann-visualizer" ["l"="25.713,33.811"]
"lucidrains/CoCa-pytorch" ["l"="31.746,34.796"]
"google-research/scenic" ["l"="32.503,35.062"]
"google/ml_collections" ["l"="23.511,33.709"]
"google/fiddle" ["l"="23.543,33.657"]
"EPFL-VILAB/MultiMAE" ["l"="34.705,36.031"]
"haltakov/natural-language-image-search" ["l"="31.892,34.75"]
"haltakov/natural-language-youtube-search" ["l"="31.992,34.732"]
"haofanwang/natural-language-joint-query-search" ["l"="31.864,34.734"]
"Zasder3/train-CLIP" ["l"="31.785,34.771"]
"rkouye/es-clip-image-search" ["l"="31.927,34.737"]
"nerdyrodent/CLIP-Guided-Diffusion" ["l"="34.218,28.966"]
"lucidrains/DALLE-pytorch" ["l"="34.342,28.843"]
"eps696/stylegan2" ["l"="33.099,32.894"]
"kingyiusuen/clip-image-search" ["l"="31.948,34.735"]
"PaddlePaddle/VIMER" ["l"="29.802,34.14"]
"AndreyGuzhov/AudioCLIP" ["l"="0.148,39.888"]
"orpatashnik/StyleCLIP" ["l"="33.336,32.996"]
"Vision-CAIR/VisualGPT" ["l"="31.512,34.677"]
"omar-mohamed/GPT2-Chest-X-Ray-Report-Generation" ["l"="28.909,36.626"]
"j-min/VL-T5" ["l"="31.643,34.758"]
"microsoft/PICa" ["l"="31.603,34.78"]
"salesforce/ALPRO" ["l"="31.699,33.791"]
"clip-vil/CLIP-ViL" ["l"="31.674,34.741"]
"microsoft/FIBER" ["l"="31.663,34.765"]
"MikeWangWZHL/VidIL" ["l"="32.339,35.11"]
"sgrvinod/a-PyTorch-Tutorial-to-Sequence-Labeling" ["l"="32.42,29.93"]
"sgrvinod/a-PyTorch-Tutorial-to-Super-Resolution" ["l"="35.724,32.171"]
"bentrevett/pytorch-seq2seq" ["l"="30.012,32.542"]
"dsgiitr/d2l-pytorch" ["l"="23.949,30.972"]
"sgrvinod/a-PyTorch-Tutorial-to-Text-Classification" ["l"="31.612,31.52"]
"ritchieng/the-incredible-pytorch" ["l"="23.763,30.707"]
"yhenon/pytorch-retinanet" ["l"="34.509,35.38"]
"ahkarami/Deep-Learning-in-Production" ["l"="23.85,30.554"]
"bentrevett/pytorch-sentiment-analysis" ["l"="29.96,32.625"]
"victoresque/pytorch-template" ["l"="34.143,35.775"]
"foamliu/Image-Captioning-PyTorch" ["l"="31.778,34.598"]
"Dong-JinKim/DenseRelationalCaptioning" ["l"="31.479,34.641"]
"lizekang/DSTC10-MOD" ["l"="31.972,34.308"]
"gsh199449/stickerchat" ["l"="31.951,34.316"]
"sail-sg/Adan" ["l"="31.861,35.018"]
"lucidrains/Adan-pytorch" ["l"="31.848,35.063"]
"megvii-research/RevCol" ["l"="31.901,35.059"]
"google-research/sam" ["l"="34.406,35.853"]
"Visual-Attention-Network/VAN-Classification" ["l"="22.094,27.695"]
"LayneH/GreenMIM" ["l"="34.803,36.122"]
"nnaisense/evotorch" ["l"="25.45,35.739"]
"zengyan-97/CCLM" ["l"="31.857,34.672"]
"Paranioar/SGRAF" ["l"="31.842,33.708"]
"sail-sg/ptp" ["l"="31.785,34.684"]
"MichaelZhouwang/VLUE" ["l"="31.766,34.697"]
"microsoft/XPretrain" ["l"="31.712,33.756"]
"zengyan-97/X2-VLM" ["l"="31.785,34.701"]
"TencentARC/MCQ" ["l"="31.699,33.763"]
"Paranioar/Cross-modal_Retrieval_Tutorial" ["l"="31.829,33.742"]
"yanxinzju/CSS-VQA" ["l"="31.699,34.508"]
"gaopengcuhk/SMCA-DETR" ["l"="31.67,35.062"]
"kakaobrain/sparse-detr" ["l"="31.69,35.056"]
"zhechen/Deformable-DETR-REGO" ["l"="31.665,35.088"]
"megvii-research/AnchorDETR" ["l"="31.687,35.042"]
"gaopengcuhk/Container" ["l"="31.641,35.109"]
"abc403/SMCA-replication" ["l"="31.645,35.083"]
"twangnh/pnp-detr" ["l"="31.683,35.085"]
"megvii-research/SOLQ" ["l"="34.923,35.783"]
"GT-Vision-Lab/VQA_LSTM_CNN" ["l"="31.639,34.452"]
"anantzoid/VQA-Keras-Visual-Question-Answering" ["l"="31.657,34.421"]
"232525/PureT" ["l"="31.518,34.704"]
"terry-r123/Awesome-Captioning" ["l"="31.552,34.67"]
"luo3300612/Transformer-Captioning" ["l"="31.531,34.681"]
"davidnvq/grit" ["l"="31.543,34.699"]
"Gitsamshi/WeakVRD-Captioning" ["l"="31.491,34.704"]
"xinke-wang/Awesome-Text-VQA" ["l"="31.73,34.579"]
"yashkant/sam-textvqa" ["l"="31.754,34.582"]
"microsoft/TAP" ["l"="31.729,34.606"]
"ZephyrZhuQi/ssbaseline" ["l"="31.77,34.575"]
"uakarsh/latr" ["l"="31.753,34.594"]
"ronghanghu/pythia" ["l"="31.762,34.562"]
"ZeningLin/ViBERTgrid-PyTorch" ["l"="31.774,34.552"]
"jnhwkim/cbp" ["l"="31.726,34.462"]
"jnhwkim/MulLowBiVQA" ["l"="31.705,34.465"]
"peteanderson80/SPICE" ["l"="31.448,34.571"]
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" ["l"="31.552,34.604"]
"sibeiyang/sgmn" ["l"="31.316,33.923"]
"sangminwoo/awesome-vision-and-language" ["l"="31.678,34.706"]
"TheShadow29/awesome-grounding" ["l"="31.265,33.915"]
"naver-ai/vidt" ["l"="31.693,35.071"]
"kaist-dmlab/RecencyBias" ["l"="34.084,36.772"]
"kaist-dmlab/TRAP" ["l"="34.092,36.783"]
"naver-ai/eccv-caption" ["l"="44.183,-15.128"]
"kaist-dmlab/Hi-COVIDNet" ["l"="34.045,36.755"]
"kaist-dmlab/Ada-Boundary" ["l"="34.057,36.763"]
"kaist-dmlab/NETS" ["l"="34.101,36.776"]
"kaist-dmlab/STARE" ["l"="34.057,36.771"]
"ronghanghu/mmf" ["l"="31.797,34.566"]
"guanghuixu/CRN_tvqa" ["l"="31.784,34.569"]
"gaopengcuhk/Stable-Pix2Seq" ["l"="31.84,34.884"]
"gaopengcuhk/Pretrained-Pix2Seq" ["l"="31.853,34.893"]
"gaopengcuhk/Unofficial-Pix2Seq" ["l"="31.857,34.875"]
"google/neural-logic-machines" ["l"="31.524,34.285"]
"ccclyu/awesome-deeplogic" ["l"="31.548,34.287"]
"Glaciohound/VCML" ["l"="31.576,34.337"]
"vacancy/Jacinle" ["l"="31.554,34.312"]
"crazydonkey200/neural-symbolic-machines" ["l"="-10.855,-4.657"]
"ExplorerFreda/VGNSL" ["l"="31.528,34.309"]
"WellyZhang/RAVEN" ["l"="31.468,34.233"]
"asdf0982/vqa-mfb.pytorch" ["l"="31.681,34.487"]
"ronghanghu/lcgn" ["l"="31.554,34.467"]
"doubledaibo/drnet_cvpr2017" ["l"="31.39,34.547"]
"StanfordVL/ReferringRelationships" ["l"="31.372,34.548"]
"chingyaoc/awesome-vqa" ["l"="31.694,34.568"]
"li-xirong/cross-lingual-cap" ["l"="31.789,34.618"]
"weiyuk/fluent-cap" ["l"="31.793,34.606"]
"Zasder3/train-CLIP-FT" ["l"="31.798,34.73"]
"revantteotia/clip-training" ["l"="31.816,34.726"]
"yangjianxin1/ClipCap-Chinese" ["l"="31.968,34.68"]
"DtYXs/Chinese-CLIP" ["l"="31.946,34.69"]
"foamliu/Image-Captioning-v2" ["l"="31.793,34.584"]
"TimoYoung/im2txt_Chinese" ["l"="31.77,34.587"]
"microsoft/GenerativeImage2Text" ["l"="31.717,34.658"]
"microsoft/SwinBERT" ["l"="31.731,33.854"]
"JialianW/GRiT" ["l"="27.426,31.281"]
"microsoft/UniTAB" ["l"="31.764,34.647"]
"microsoft/azfuse" ["l"="31.744,34.647"]
"KunpengLi1994/VSRN" ["l"="31.847,33.738"]
"BryanPlummer/flickr30k_entities" ["l"="31.256,33.898"]
"megvii-research/RepLKNet" ["l"="31.892,35.073"]
"MegEngine/RepLKNet" ["l"="31.857,35.092"]
"VITA-Group/SLaK" ["l"="31.875,35.073"]
"DingXiaoH/RepVGG" ["l"="34.533,35.684"]
"swz30/Restormer" ["l"="35.953,32.38"]
"OliverRensu/Shunted-Transformer" ["l"="34.801,35.972"]
"BAAI-WuDao/BriVL" ["l"="31.875,34.693"]
"google-research-datasets/wit" ["l"="31.705,34.706"]
"moein-shariatnia/OpenAI-CLIP" ["l"="31.852,34.756"]
"fundamentalvision/Uni-Perceiver" ["l"="31.885,34.889"]
"facebookresearch/omnivore" ["l"="32.404,35.078"]
"MCG-NJU/AdaMixer" ["l"="31.728,35.031"]
"yuhuan-wu/P2T" ["l"="31.926,35.099"]
"ziplab/LITv2" ["l"="30.386,35.873"]
"Atten4Vis/DemystifyLocalViT" ["l"="31.926,35.077"]
"okojoalg/sequencer" ["l"="31.9,35.095"]
"fcjian/TOOD" ["l"="34.839,35.624"]
"hkzhang91/ParC-Net" ["l"="34.806,35.881"]
"lichengunc/refer" ["l"="31.252,33.928"]
"facebookresearch/moco-v3" ["l"="34.561,35.993"]
"bytedance/ibot" ["l"="34.653,36.023"]
"microsoft/UniCL" ["l"="31.697,34.825"]
"Eurus-Holmes/Awesome-Multimodal-Research" ["l"="31.619,34.686"]
"jason718/awesome-self-supervised-learning" ["l"="34.428,36.001"]
"A2Zadeh/CMU-MultimodalSDK" ["l"="-1.084,40.029"]
"thunlp/PromptPapers" ["l"="27.603,31.288"]
"dk-liang/Awesome-Visual-Transformer" ["l"="34.543,35.858"]
"yaohungt/Multimodal-Transformer" ["l"="-1.102,40.029"]
"facebookresearch/SlowFast" ["l"="32.54,34.973"]
"facebookresearch/moco" ["l"="34.484,35.975"]
"AishwaryaAgrawal/GVQA" ["l"="31.615,34.458"]
"VT-vision-lab/VQA" ["l"="31.678,34.414"]
"HyeonwooNoh/DPPnet" ["l"="31.705,34.435"]
"yukezhu/visual7w-qa-models" ["l"="31.716,34.409"]
"SeanNaren/minGPT" ["l"="34.173,36.079"]
"facebookresearch/contriever" ["l"="25.855,29.228"]
"lucidrains/RETRO-pytorch" ["l"="25.886,29.303"]
"Shivanshu-Gupta/Visual-Question-Answering" ["l"="31.584,34.462"]
"tbmoon/basic_vqa" ["l"="35.348,33.5"]
"DenisDsh/VizWiz-VQA-PyTorch" ["l"="31.563,34.444"]
"YicongHong/Recurrent-VLN-BERT" ["l"="24.735,36.605"]
"airsplay/R2R-EnvDrop" ["l"="24.704,36.607"]
"cshizhe/VLN-DUET" ["l"="24.749,36.621"]
"cshizhe/VLN-HAMT" ["l"="24.744,36.61"]
"weituo12321/PREVALENT" ["l"="24.72,36.625"]
"jialuli-luka/EnvEdit" ["l"="24.728,36.628"]
"ruotianluo/DiscCaptioning" ["l"="31.46,34.612"]
"richardaecn/cvpr18-caption-eval" ["l"="31.44,34.611"]
"violetteshev/bottom-up-features" ["l"="31.564,34.618"]
"evelinehong/slot-attention-pytorch" ["l"="31.572,34.148"]
"untitled-ai/slot_attention" ["l"="31.587,34.16"]
"tkipf/c-swm" ["l"="31.535,34.182"]
"addtt/object-centric-library" ["l"="31.613,34.189"]
"google-research/slot-attention-video" ["l"="31.568,34.127"]
"charigyang/motiongrouping" ["l"="31.608,34.167"]
"wbw520/scouter" ["l"="31.611,34.132"]
"ajabri/videowalk" ["l"="31.932,33.177"]
"zhixuan-lin/G-SWM" ["l"="31.564,34.181"]
"YoadTew/zero-shot-image-to-text" ["l"="27.88,31.44"]
"j-min/CLIP-Caption-Reward" ["l"="31.82,34.695"]
"galatolofederico/clip-glass" ["l"="31.84,34.708"]
"wl-zhao/VPD" ["l"="31.672,43.33"]
"sail-sg/iFormer" ["l"="31.947,35.056"]
"MILVLG/rosita" ["l"="31.614,34.596"]
"YIKUAN8/Transformers-VQA" ["l"="31.669,34.595"]
"abachaa/VQA-Med-2019" ["l"="31.722,34.563"]
"UCSD-AI4H/PathVQA" ["l"="31.746,34.557"]
"saahiluppal/catr" ["l"="31.499,34.663"]
"RoyalSkye/Image-Caption" ["l"="31.461,34.683"]
"zarzouram/image_captioning_with_transformers" ["l"="31.464,34.699"]
"tgc1997/Awesome-Video-Captioning" ["l"="31.771,33.926"]
"jayleicn/recurrent-transformer" ["l"="31.758,33.893"]
"tgc1997/RMN" ["l"="31.774,33.938"]
"yangbang18/Non-Autoregressive-Video-Captioning" ["l"="31.75,33.942"]
"facebookresearch/grounded-video-description" ["l"="31.783,33.908"]
"SydCaption/SAAT" ["l"="31.758,33.929"]
"xiadingZ/video-caption.pytorch" ["l"="31.809,33.932"]
"wtliao/ImageTransformer" ["l"="31.461,34.669"]
"ShannonAI/OpenViDial" ["l"="31.907,34.33"]
"jokieleung/Maria" ["l"="31.927,34.319"]
"henryhungle/MTN" ["l"="31.932,34.298"]
"YuLiu-LY/BO-QSA" ["l"="31.576,34.1"]
"junkeun-yi/SAVi-pytorch" ["l"="31.551,34.111"]
"singhgautam/steve" ["l"="31.559,34.101"]
"amazon-science/prompt-pretraining" ["l"="31.879,34.943"]
"xmed-lab/CLIP_Surgery" ["l"="31.935,34.95"]
"fudan-zvg/GSS" ["l"="31.896,34.926"]
"Vision-CAIR/ChatCaptioner" ["l"="27.358,31.278"]
"snap-research/R2L" ["l"="31.97,34.96"]
"IDEA-Research/OpenSeeD" ["l"="31.756,34.952"]
"ziplab/EcoFormer" ["l"="30.412,35.852"]
"muzairkhattak/ViFi-CLIP" ["l"="31.627,34.891"]
"CHENGY12/PLOT" ["l"="31.588,34.858"]
"hanoonaR/object-centric-ovd" ["l"="31.647,34.912"]
"abdohelmy/D-3Former" ["l"="31.606,34.914"]
"azshue/TPT" ["l"="31.588,34.884"]
"Amshaker/unetr_plus_plus" ["l"="31.53,34.971"]
"Amshaker/SwiftFormer" ["l"="31.57,34.942"]
"mindflow-institue/DAEFormer" ["l"="29.598,36.406"]
"zhegan27/Semantic_Compositional_Nets" ["l"="31.421,34.604"]
"chenxinpeng/im2p" ["l"="31.342,34.664"]
"doubledaibo/gancaption_iccv2017" ["l"="31.428,34.627"]
"yuleiniu/cfvqa" ["l"="31.756,34.492"]
"yuleiniu/introd" ["l"="31.788,34.486"]
"cdancette/rubi.bootstrap.pytorch" ["l"="31.721,34.494"]
"yangxuntu/lxmertcatt" ["l"="-26.229,18.984"]
"cdancette/vqa-cp-leaderboard" ["l"="31.776,34.495"]
"chrisc36/debias" ["l"="31.769,34.479"]
"soujanyaporia/multimodal-sentiment-analysis" ["l"="-1.083,40.017"]
"megvii-research/PETR" ["l"="30.629,44.181"]
"chaytonmin/Awesome-BEV-Perception-Multi-Cameras" ["l"="30.621,44.174"]
"mmaaz60/EdgeNeXt" ["l"="34.834,35.916"]
"djiajunustc/TransVG" ["l"="31.259,33.867"]
"microsoft/BridgeTower" ["l"="31.744,34.697"]
"Jittor/JSeg" ["l"="31.956,35"]
"Gsunshine/Enjoy-Hamburger" ["l"="31.93,34.991"]
"Visual-Attention-Network/VAN-Segmentation" ["l"="31.931,35.008"]
"hustvl/TopFormer" ["l"="34.867,35.902"]
"tfzhou/ProtoSeg" ["l"="31.77,34.923"]
"hustvl/SparseInst" ["l"="34.863,35.764"]
"rowanz/swagaf" ["l"="31.417,34.571"]
"jonathanherzig/commonsenseqa" ["l"="29.952,31.565"]
"xiaofeng94/VL-PLM" ["l"="31.655,34.928"]
"dyabel/detpro" ["l"="31.662,34.907"]
"yuhangzang/OV-DETR" ["l"="31.682,34.922"]
"zhenyuw16/UniDetector" ["l"="31.697,34.952"]
"fcjian/PromptDet" ["l"="31.651,34.895"]
"yiyang92/vae_captioning" ["l"="31.413,34.615"]
"NoelShin/reco" ["l"="31.721,34.949"]
"ZiqinZhou66/ZegCLIP" ["l"="31.714,34.936"]
"facebookresearch/Generic-Grouping" ["l"="31.666,34.953"]
"iamaaditya/VQA_Keras" ["l"="31.643,34.425"]
"facebookresearch/MIXER" ["l"="26.965,34.382"]
"ilyasu123/rlntm" ["l"="27.01,34.369"]
"facebook/MemNN" ["l"="27.113,34.332"]
"twitter/torch-twrl" ["l"="26.915,34.336"]
"jacobandreas/psketch" ["l"="31.693,34.356"]
"kaishengtai/torch-ntm" ["l"="26.987,34.313"]
"donglixp/lang2logic" ["l"="-10.906,-4.658"]
"YuJiang01/n2nmn_pytorch" ["l"="31.607,34.435"]
"ronghanghu/snmn" ["l"="31.571,34.403"]
"ronghanghu/speaker_follower" ["l"="24.709,36.618"]
"JamesChuanggg/san-torch" ["l"="31.714,34.446"]
"jazzsaxmafia/video_to_sequence" ["l"="31.876,33.97"]
"ruotianluo/neuraltalk2.pytorch" ["l"="31.357,34.444"]
"ry/tensorflow-vgg16" ["l"="34.257,34.994"]
"emansim/text2image" ["l"="33.732,32.492"]
"carpedm20/pixel-rnn-tensorflow" ["l"="33.811,32.509"]
"kimiyoung/review_net" ["l"="31.405,34.585"]
"carpedm20/NTM-tensorflow" ["l"="27.188,34.333"]
"facebook/fb.resnet.torch" ["l"="34.268,35.084"]
"yueatsprograms/Stochastic_Depth" ["l"="26.918,34.293"]
"torchnet/torchnet" ["l"="26.93,34.311"]
"harvardnlp/seq2seq-attn" ["l"="27.07,34.332"]
"jcjohnson/torch-rnn" ["l"="27.038,34.304"]
"facebookresearch/deepmask" ["l"="34.225,35.033"]
"aioz-ai/MICCAI21_MMQ" ["l"="31.821,34.538"]
"chrisc36/bottom-up-attention-vqa" ["l"="31.725,34.507"]
"fawazsammani/knowing-when-to-look-adaptive-attention" ["l"="31.483,34.623"]
"zjuchenlong/sca-cnn.cvpr17" ["l"="31.467,34.633"]
"LuoweiZhou/e2e-gLSTM-sc" ["l"="31.428,34.591"]
"cesc-park/attend2u" ["l"="31.461,34.601"]
"njchoma/transformer_image_caption" ["l"="31.481,34.66"]
"ruotianluo/coco-caption" ["l"="31.518,34.659"]
"JonghwanMun/TextguidedATT" ["l"="31.39,34.606"]
"fkxssaa/Deliberate-Attention-Networks-for-Image-Captioning" ["l"="31.42,34.637"]
"salaniz/pycocoevalcap" ["l"="31.497,34.613"]
"wangleihitcs/CaptionMetrics" ["l"="31.404,34.648"]
"salesforce/densecap" ["l"="31.778,33.892"]
"jialinwu17/self_critical_vqa" ["l"="31.748,34.507"]
"IDEA-Research/Lite-DETR" ["l"="31.782,35.108"]
"IDEA-Research/DQ-DETR" ["l"="31.786,35.143"]
"IDEA-Research/Stable-DINO" ["l"="31.776,35.04"]
"YuanEZhou/Grounded-Image-Captioning" ["l"="31.562,34.645"]
"kdexd/probnmn-clevr" ["l"="31.595,34.312"]
"nerdimite/neuro-symbolic-ai-soc" ["l"="31.592,34.328"]
"ceyzaguirre4/NSM" ["l"="31.566,34.39"]
"OpenGVLab/gv-benchmark" ["l"="34.886,35.809"]
"Sense-X/Co-DETR" ["l"="31.74,35.064"]
"yzhuoning/Awesome-CLIP" ["l"="31.729,34.83"]
"subho406/OmniNet" ["l"="31.584,34.695"]
"chihyaoma/cyclical-visual-captioning" ["l"="31.555,34.736"]
"amanchadha/iPerceive" ["l"="31.647,33.94"]
"KaihuaTang/VCTree-Visual-Question-Answering" ["l"="31.388,34.494"]
"HCPLab-SYSU/KERN" ["l"="31.354,34.564"]
"alirezazareian/gbnet" ["l"="31.338,34.576"]
"taksau/GPS-Net" ["l"="31.314,34.57"]
"alirezazareian/vspnet" ["l"="31.338,34.561"]
"tejas-gokhale/vqa_mutant" ["l"="31.809,34.495"]
"wyzjack/MRMGA4VAD" ["l"="32.046,34.981"]
"wyzjack/SLA2P" ["l"="32.023,34.974"]
"oarriaga/neural_image_captioning" ["l"="31.427,34.467"]
"danieljl/keras-image-captioning" ["l"="31.417,34.48"]
"allenai/container" ["l"="31.629,35.129"]
"chuhaojin/BriVL-BUA-applications" ["l"="31.895,34.677"]
"chuhaojin/WenLan-api-document" ["l"="31.901,34.666"]
"neilfei/brivl-nmi" ["l"="31.919,34.673"]
"li-xirong/coco-cn" ["l"="31.836,34.638"]
"snap-research/MobileR2L" ["l"="32.002,34.964"]
"yandex-research/ddpm-segmentation" ["l"="34.571,28.825"]
"jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning" ["l"="31.591,34.828"]
"OPTML-Group/ILM-VP" ["l"="31.558,34.834"]
"libffcv/ffcv" ["l"="34.343,35.921"]
"yaohungt/Gated-Spatio-Temporal-Energy-Graph" ["l"="31.38,34.424"]
"JingweiJ/ActionGenome" ["l"="31.341,34.476"]
"fabienbaradel/object_level_visual_reasoning" ["l"="31.357,34.397"]
"ylsung/Ladder-Side-Tuning" ["l"="27.739,31.286"]
"taesunwhang/MVAN-VisDial" ["l"="31.836,34.383"]
"gicheonkang/dan-visdial" ["l"="31.843,34.399"]
"choasup/SIN" ["l"="31.67,34.312"]
"msracver/Relation-Networks-for-Object-Detection" ["l"="34.608,35.27"]
"endernewton/iter-reason" ["l"="31.647,34.368"]
"Hwang64/MLKP" ["l"="31.687,34.271"]
"yikang-li/vg_cleansing" ["l"="31.415,34.558"]
"hlamba28/Automatic-Image-Captioning" ["l"="31.472,34.467"]
"Div99/Image-Captioning" ["l"="31.436,34.486"]
"ZhengyaoJiang/NLRL" ["l"="31.491,34.257"]
"ai-systems/DILP-Core" ["l"="31.501,34.268"]
"hyenal/relate" ["l"="31.52,34.171"]
"fanyangxyz/Neural-LP" ["l"="22.959,38.348"]
"logictensornetworks/logictensornetworks" ["l"="31.527,34.257"]
"locuslab/SATNet" ["l"="27.77,42.129"]
"InnerPeace-Wu/densecap-tensorflow" ["l"="31.231,34.7"]
"linjieyangsc/densecap" ["l"="31.207,34.707"]
"InnerPeace-Wu/im2p-tensorflow" ["l"="31.281,34.684"]
"LemonATsu/Keras-Image-Caption" ["l"="31.436,34.428"]
"amaiasalvador/imcap_keras" ["l"="31.4,34.444"]
"zsdonghao/Image-Captioning" ["l"="31.464,34.496"]
"boluoyu/ImageCaption" ["l"="31.405,34.457"]
"apple2373/chainer-caption" ["l"="31.416,34.431"]
"facebookresearch/torchdim" ["l"="23.358,33.627"]
"neural-nuts/Cam2Caption" ["l"="31.447,34.413"]
"albanie/slurm_gpustat" ["l"="31.476,34.901"]
"logictensornetworks/tutorials" ["l"="31.503,34.24"]
"ML-KULeuven/deepproblog" ["l"="-8.798,14.352"]
"tommasocarraro/LTNtorch" ["l"="31.531,34.238"]
"jyhong0304/LTN_pytorch" ["l"="31.51,34.247"]
"logictensornetworks/LTNtorch" ["l"="31.519,34.24"]
"ml-research/nsfr" ["l"="31.512,34.231"]
"NVlabs/FreeSOLO" ["l"="34.662,36.124"]
"zhoudaquan/Refiner_ViT" ["l"="34.717,35.997"]
"ZhangYuanhan-AI/visual_prompt_retrieval" ["l"="31.618,34.903"]
"facebookresearch/CiT" ["l"="31.968,34.867"]
"shijx12/XNM-Net" ["l"="31.542,34.434"]
"zmykevin/UC2" ["l"="31.948,34.642"]
"coderSkyChen/Iterative-Visual-Reasoning.pytorch" ["l"="31.663,34.337"]
"shubhtuls/factored3d" ["l"="32.935,43.266"]
"chapternewscu/image-captioning-with-semantic-attention" ["l"="31.456,34.56"]
"gujiuxiang/MIL.pytorch" ["l"="16.224,37.075"]
"mjhucla/mRNN-CR" ["l"="31.484,34.489"]
"yrcong/STTran" ["l"="31.285,34.487"]
"joaanna/something_else" ["l"="32.367,34.981"]
"MCG-NJU/TRACE" ["l"="31.298,34.468"]
"google-research-datasets/conceptual-12m" ["l"="31.551,34.713"]
"google-research-datasets/conceptual-captions" ["l"="31.579,34.662"]
"igorbrigadir/DownloadConceptualCaptions" ["l"="31.532,34.728"]
"fartashf/vsepp" ["l"="31.854,33.755"]
"ruotianluo/GoogleConceptualCaptioning" ["l"="31.558,34.691"]
"rsokl/MyGrad" ["l"="31.607,34.356"]
"rsokl/noggin" ["l"="31.623,34.319"]
"davidmascharka/MyNN" ["l"="31.613,34.326"]
"jjyang12/upgraded-octo-discocito-3" ["l"="31.607,34.336"]
"avisingh599/visual-qa" ["l"="31.689,34.407"]
"abhshkdz/neural-vqa" ["l"="31.691,34.425"]
"nivwusquorum/tf-adversarial" ["l"="31.717,34.339"]
"renmengye/imageqa-public" ["l"="31.716,34.389"]
"dblN/stochastic_depth_keras" ["l"="31.728,34.36"]
"codekansas/keras-language-modeling" ["l"="31.25,31.575"]
"vinhkhuc/MemN2N-babi-python" ["l"="27.175,34.358"]
"allenai/deep_qa" ["l"="25.83,30.195"]
"tfzhou/ContrastiveSeg" ["l"="34.693,35.968"]
"leonnnop/GMMSeg" ["l"="31.918,33.415"]
"dvlab-research/GFS-Seg" ["l"="33.268,34.066"]
"YiF-Zhang/RegionProxy" ["l"="31.819,34.901"]
"CVI-SZU/CLIMS" ["l"="30.797,37.671"]
"LiheYoung/ST-PlusPlus" ["l"="31.007,37.56"]
"chunbolang/BAM" ["l"="33.295,34.1"]
"LiheYoung/MiningFSS" ["l"="33.273,34.109"]
"shikorab/SceneGraph" ["l"="31.361,34.587"]
"umich-vl/px2graph" ["l"="31.368,34.573"]
"FocalNet/Networks-Beyond-Attention" ["l"="31.907,35.005"]
"FocalNet/FocalNet-DINO" ["l"="31.876,34.979"]
"floodsung/Deep-Reasoning-Papers" ["l"="31.492,34.182"]
"yue-zhongqi/ifsl" ["l"="23.694,35.314"]
"rosinality/mac-network-pytorch" ["l"="31.542,34.396"]
"YuanEZhou/CBTrans" ["l"="31.487,34.728"]
"Cadene/bootstrap.pytorch" ["l"="31.685,34.512"]
"visinf/cos-cvae" ["l"="31.453,34.735"]
"fawazsammani/look-and-modify" ["l"="31.465,34.727"]
"GT-RIPL/Xmodal-Ctx" ["l"="31.501,34.737"]
"jchenghu/ExpansionNet_v2" ["l"="31.519,34.738"]
"SjokerLily/awesome-image-captioning" ["l"="31.514,34.722"]
"mrwu-mac/DIFNet" ["l"="31.508,34.694"]
"jcoreyes/OP3" ["l"="31.542,34.16"]
"mila-iqia/atari-representation-learning" ["l"="25.266,35.712"]
"danijar/dreamer" ["l"="25.252,35.746"]
"MishaLaskin/curl" ["l"="25.278,35.757"]
"maximecb/gym-miniworld" ["l"="25.23,35.783"]
"ramanans1/plan2explore" ["l"="25.277,35.718"]
"jlko/STOVE" ["l"="31.516,34.152"]
"anujshah1003/VQA-Demo-GUI" ["l"="31.697,34.389"]
"VedantYadav/VQA" ["l"="31.712,34.422"]
"rayleizhu/BiFormer" ["l"="31.978,35.118"]
"OpenGVLab/STM-Evaluation" ["l"="31.944,35.083"]
"vmurahari3/visdial-diversity" ["l"="31.789,34.358"]
"shubhamagarwal92/visdial_conv" ["l"="31.837,34.361"]
"samschulter/omnilabeltools" ["l"="31.645,34.946"]
"limbo0000/InstanceLoc" ["l"="31.614,35.045"]
"PerdonLiu/CSE-Autoloss" ["l"="31.554,35.073"]
"WXinlong/DenseCL" ["l"="34.603,36.03"]
"shuuchen/DetCo.pytorch" ["l"="31.571,35.064"]
"xieenze/Trans2Seg" ["l"="31.673,42.212"]
"Jia-Research-Lab/SA-AutoAug" ["l"="31.575,35.084"]
"kkhoot/PAA" ["l"="34.82,35.575"]
"YangtaoWANG95/TokenCut" ["l"="34.696,36.18"]
"LiWentomng/BoxInstSeg" ["l"="21.983,27.707"]
"HVision-NKU/Conv2Former" ["l"="31.968,35.073"]
"thegrims/UsTaxes" ["l"="32.063,34.722"]
"dennis-tra/pcp" ["l"="46.647,-23.473"]
"baxtree/subaligner" ["l"="-30.504,-16.185"]
"AIChallenger/AI_Challenger_2017" ["l"="31.84,34.583"]
"mjhucla/TF-mRNN" ["l"="31.5,34.465"]
"ruotianluo/neuraltalk2-tensorflow" ["l"="31.51,34.44"]
"mlfoundations/model-soups" ["l"="31.919,34.835"]
"LAION-AI/CLIP_benchmark" ["l"="31.926,34.852"]
"orrzohar/PROB" ["l"="31.683,34.98"]
"wusize/ovdet" ["l"="31.651,34.957"]
"tgxs002/CORA" ["l"="31.666,34.973"]
"GuessWhatGame/guesswhat" ["l"="31.77,34.326"]
"batra-mlp-lab/visdial-amt-chat" ["l"="31.806,34.334"]
"HDETR/H-Deformable-DETR-mmdet" ["l"="31.765,35.065"]
"lvis-dataset/lvis-api" ["l"="31.435,34.259"]
"tztztztztz/eql.detectron2" ["l"="33.666,36.661"]
"GothicAi/Instaboost" ["l"="34.784,35.439"]
"thangvubk/Cascade-RPN" ["l"="34.764,35.444"]
"gchhablani/multilingual-image-captioning" ["l"="31.504,34.76"]
"Cloud-CV/VQA" ["l"="31.891,34.21"]
"facebookresearch/neural-light-fields" ["l"="33.489,43.293"]
"SlongLiu/DAB-DETR" ["l"="31.718,35.082"]
"weiyx16/CLIP-pytorch" ["l"="31.893,34.73"]
"sail-sg/inceptionnext" ["l"="31.881,35.121"]
"ChenyuGAO-CS/SMA" ["l"="31.757,34.616"]
"computationalmedia/semstyle" ["l"="31.333,34.625"]
"kacky24/stylenet" ["l"="31.372,34.619"]
"yiyang92/caption-stylenet_tensorflow" ["l"="31.344,34.633"]
"ruotianluo/cider" ["l"="31.489,34.692"]
"Cheems-Seminar/segment-anything-and-name-it" ["l"="31.74,35.1"]
"carpedm20/visual-analogy-tensorflow" ["l"="30.661,33.927"]
"ericjang/tdb" ["l"="27.113,34.206"]
"cheng6076/SNLI-attention" ["l"="26.99,34.333"]
"yrcong/RelTR" ["l"="31.263,34.506"]
"Scarecrow0/SGTR" ["l"="31.282,34.511"]
"liuhengyue/fcsgg" ["l"="35.415,34.877"]
"layer6ai-labs/SGG-Seq2Seq" ["l"="31.295,34.521"]
"waxnkw/IETrans-SGG.pytorch" ["l"="31.271,34.523"]
"anoopsanka/retinal_oct" ["l"="31.23,34.494"]
"google-research/l2p" ["l"="30.141,35.866"]
"YuejiangLIU/awesome-source-free-test-time-adaptation" ["l"="30.249,34.94"]
"ZrrSkywalker/PointCLIP" ["l"="30.752,44.349"]
"ZrrSkywalker/MonoDETR" ["l"="30.585,44.209"]
"HashmatShadab/APR" ["l"="31.602,34.928"]
"lucidrains/nuwa-pytorch" ["l"="34.46,28.994"]
"lucidrains/vector-quantize-pytorch" ["l"="0.243,39.958"]
"crowsonkb/v-diffusion-pytorch" ["l"="34.282,28.973"]
"lxtGH/PFSegNets" ["l"="19.75,26.377"]
"rasmusbergpalm/recurrent-relational-networks" ["l"="27.643,42.151"]
"siddk/relation-network" ["l"="31.845,34.423"]
"arjung128/image-paragraph-captioning" ["l"="31.366,34.686"]
"bupt-mmai/CNN-Caption" ["l"="31.357,34.68"]
"Visual-Attention-Network/VAN-Jittor" ["l"="31.978,35.021"]
"phellonchen/awesome-Vision-and-Language-Pre-training" ["l"="31.72,34.684"]
"jack1yang/image-paragraph-captioning" ["l"="31.321,34.68"]
"Wentong-DST/im2p" ["l"="31.31,34.671"]
"stevehuanghe/image_captioning" ["l"="31.428,34.668"]
"HaleyPei/Implement-of-SCA-CNN" ["l"="31.411,34.673"]
"Awenbocc/med-vqa" ["l"="31.853,34.522"]
"haifangong/CMSA-MTPT-4-MedicalVQA" ["l"="28.779,36.585"]
"Dawn-LX/VidSGG-BIG" ["l"="31.238,34.466"]
"Dawn-LX/VidVRD-tracklets" ["l"="31.218,34.457"]
"kdexd/virtex" ["l"="31.679,34.685"]
"hzxie/RMNet" ["l"="31.874,33.262"]
"gengshan-y/rigidmask" ["l"="31.653,43.152"]
"tfzhou/MATNet" ["l"="31.91,33.241"]
"GewelsJI/FSNet" ["l"="31.977,33.299"]
"zlai0/MAST" ["l"="31.906,33.206"]
"zhegan27/SCN_for_video_captioning" ["l"="31.809,33.948"]
"wbw520/NoisyLSTM" ["l"="31.63,34.11"]
"wbw520/AutoNoduleDetect" ["l"="31.621,34.119"]
"wbw520/MTUNet" ["l"="31.614,34.108"]
"ictnlp/DSTC8-AVSD" ["l"="31.916,34.355"]
"hudaAlamri/DSTC7-Audio-Visual-Scene-Aware-Dialog-AVSD-Challenge" ["l"="31.892,34.357"]
"SiyuanQi/gpnn" ["l"="32.648,35.16"]
"VODKA312/IntroToSelf-control" ["l"="31.674,35.131"]
"MkuuWaUjinga/leopart" ["l"="34.653,36.155"]
"ShoufaChen/CycleMLP" ["l"="34.789,35.985"]
"YehLi/ImageNetModel" ["l"="31.986,35.049"]
"ChristophReich1996/MaxViT" ["l"="34.979,36.035"]
"youngwanLEE/MPViT" ["l"="34.811,35.934"]
"Hi-FT/ERD" ["l"="31.5,35.019"]
"CanPeng123/Faster-ILOD" ["l"="31.504,35.034"]
"JosephKJ/ELI" ["l"="31.522,35.034"]
"dhansmair/flamingo-mini" ["l"="31.772,34.722"]
"mlfoundations/patching" ["l"="25.729,35.506"]
"LightDXY/FT-CLIP" ["l"="31.896,34.844"]
"LAION-AI/scaling-laws-openclip" ["l"="31.979,34.852"]
"arampacha/CLIP-rsicd" ["l"="-33.688,42.348"]
"hologerry/SoCo" ["l"="22.207,27.921"]
"Megvii-BaseDetection/DeFCN" ["l"="34.804,35.577"]
"ajamjoom/Image-Captions" ["l"="31.44,34.688"]
"ruizhaogit/GuessWhat-TemperedPolicyGradient" ["l"="31.777,34.304"]
"ttengwang/Caption-Anything" ["l"="27.278,31.256"]
"rowanz/r2c" ["l"="31.567,34.544"]
"yuweijiang/HGL-pytorch" ["l"="31.566,34.497"]
"shmsw25/mcb-model-for-vqa" ["l"="31.646,34.412"]
"caffeinism/FiLM-pytorch" ["l"="31.54,34.409"]
"devendrachaplot/DeepRL-Grounding" ["l"="24.574,36.535"]
"faresmalik/SEViT" ["l"="31.58,34.926"]
"iejMac/clip-video-encode" ["l"="32.014,34.806"]
"eladhoffer/captionGen" ["l"="31.297,34.42"]
"gujiuxiang/chinese_im2text.pytorch" ["l"="31.318,34.434"]
"codexxxl/GraphVQA" ["l"="31.603,34.53"]
"astro-zihao/mucko" ["l"="31.679,34.541"]
"CrossmodalGroup/GSMN" ["l"="31.835,33.729"]
"tangjiuqi097/ATCAIS" ["l"="31.653,35.055"]
"Fen9/WReN" ["l"="31.432,34.212"]
"WellyZhang/CoPINet" ["l"="31.445,34.215"]
"zkcys001/distracting_feature" ["l"="31.44,34.225"]
"deepmind/abstract-reasoning-matrices" ["l"="31.448,34.202"]
"husheng12345/SRAN" ["l"="31.459,34.215"]
"HaozhiQi/RPIN" ["l"="31.411,34.272"]
"k-r-allen/tool-games" ["l"="31.436,34.283"]
"stepjam/RLBench" ["l"="25.39,35.841"]
"wangpengnorman/FVQA" ["l"="31.707,34.54"]
"paarthneekhara/convolutional-vqa" ["l"="31.647,34.384"]
"paarthneekhara/Weather-From-Map" ["l"="31.627,34.388"]
"adiitya/p2pstream" ["l"="31.641,34.35"]
"Kapilks/Basic-Kernel" ["l"="31.634,34.378"]
"Kapilks/Download-youtube-video" ["l"="31.63,34.368"]
"DeepRNN/visual_question_answering" ["l"="31.67,34.362"]
"mengqiDyangge/HierKD" ["l"="31.666,34.936"]
"facebookresearch/mmbt" ["l"="31.58,34.737"]
"WasifurRahman/BERT_multimodal_transformer" ["l"="-1.12,40.026"]
"yrcong/NODIS" ["l"="31.262,34.475"]
"aimagelab/speaksee" ["l"="31.468,34.653"]
"TengdaHan/slurm_web" ["l"="31.447,34.907"]
"salesforce/BiST" ["l"="31.948,34.284"]
"erobic/negative_analysis_of_grounding" ["l"="31.782,34.51"]
"zjuchenlong/sca-cnn" ["l"="31.622,34.341"]
"davidnvq/visdial" ["l"="31.857,34.402"]
"salesforce/VD-BERT" ["l"="31.847,34.389"]
"idansc/mrr-ndcg" ["l"="31.843,34.369"]
"liqing-ustc/VizWiz_LSTM_CNN_Attention" ["l"="31.531,34.422"]
"doubledaibo/clcaption_nips2017" ["l"="31.37,34.647"]
"ThomasRobertFr/gpu-monitor" ["l"="31.728,34.52"]
"emited/gantk2" ["l"="31.713,34.52"]
"adiitya/frozen" ["l"="31.646,34.326"]
"shtechair/vqa-sva" ["l"="31.706,34.488"]
"gicheonkang/DAN-VisDial" ["l"="31.768,34.426"]
"bhpfelix/Compositional-Attention-Networks-for-Machine-Reasoning-PyTorch" ["l"="31.514,34.379"]
"simpleshinobu/visdial-principles" ["l"="31.827,34.373"]
"yangxuntu/vtranse" ["l"="31.329,34.526"]
"idansc/simple-avsd" ["l"="31.98,34.341"]
"dialogtekgeek/AudioVisualSceneAwareDialog" ["l"="31.944,34.348"]
"idansc/fga" ["l"="32.007,34.336"]
"aravindvarier/Image-Captioning-Pytorch" ["l"="31.424,34.708"]
"SlongLiu/UnsupKeypointsParts" ["l"="31.788,35.16"]
"aioz-ai/MICCAI19-MedVQA" ["l"="31.859,34.537"]
"jshilong/GroupRCNN" ["l"="34.818,35.789"]
"zfchenUnique/DCL-Release" ["l"="31.583,34.291"]
"facebookresearch/long_seq_mae" ["l"="32.37,35.117"]
"yukezhu/visual7w-toolkit" ["l"="31.741,34.39"]
"JXZe/DualVD" ["l"="31.861,34.384"]
"wh0330/CAG_VisDial" ["l"="31.853,34.373"]
"ChangyaoTian/VL-LTR" ["l"="31.867,34.86"]
"ksaito-ut/openworld_ldet" ["l"="31.626,34.966"]
"e-bug/iglue" ["l"="32.022,34.623"]
"e-bug/volta" ["l"="32.081,34.607"]
"lil-lab/nlvr" ["l"="32.114,34.593"]
"marvl-challenge/marvl-code" ["l"="32.106,34.612"]
"XinyuLyu/FGPL" ["l"="31.262,34.533"]
"btgraham/Batchwise-Dropout" ["l"="28.173,32.667"]
"Vision-CAIR/RelTransformer" ["l"="31.278,34.579"]
"rosinality/relation-networks-pytorch" ["l"="31.876,34.436"]
"aelnouby/Relational-Networks" ["l"="31.9,34.434"]
"JXZe/Learning_DualVD" ["l"="31.887,34.382"]
"UKPLab/acl2020-confidence-regularization" ["l"="31.798,34.466"]
"UKPLab/emnlp2020-debiasing-unknown" ["l"="31.808,34.477"]
"Scarecrow0/BGNN-SGG" ["l"="31.233,34.544"]
"muktilin/NICE" ["l"="31.277,34.543"]
"dongxingning/SHA-GCL-for-SGG" ["l"="31.263,34.548"]
"liuzhi136/Visual-Question-Answering" ["l"="31.671,34.384"]
"YiwuZhong/SGG_from_NLS" ["l"="31.183,34.552"]
"jshi31/WS-SGG" ["l"="31.156,34.555"]
"Computer-Vision-in-the-Wild/DataDownload" ["l"="31.566,34.912"]
"guotong1988/Relation-Network-babi" ["l"="31.872,34.42"]
"bhiziroglu/Language-as-an-Abstraction-for-Hierarchical-Deep-Reinforcement-Learning" ["l"="31.501,34.347"]
}