digraph G {
"foxmask/django-th" -> "bipio-server/bipio"
"foxmask/django-th" -> "pirate/pocket-archive-stream"
"foxmask/django-th" -> "huydx/hget" ["e"=1]
"pedro-morgado/spatialaudiogen" -> "facebookresearch/2.5D-Visual-Sound"
"pedro-morgado/spatialaudiogen" -> "SheldonTsui/SepStereo_ECCV2020"
"pedro-morgado/spatialaudiogen" -> "facebookresearch/FAIR-Play"
"cvondrick/soundnet" -> "eborboihuc/SoundNet-tensorflow"
"cvondrick/soundnet" -> "reedscot/nips2016" ["e"=1]
"cvondrick/soundnet" -> "karoldvl/ESC-50" ["e"=1]
"cvondrick/soundnet" -> "hangzhaomit/Sound-of-Pixels"
"cvondrick/soundnet" -> "Guim3/IcGAN" ["e"=1]
"cvondrick/soundnet" -> "qiuqiangkong/audioset_classification" ["e"=1]
"cvondrick/soundnet" -> "andrewowens/multisensory"
"cvondrick/soundnet" -> "afourast/avobjects"
"cvondrick/soundnet" -> "metalbubble/moments_models" ["e"=1]
"cvondrick/soundnet" -> "facebook/eyescream" ["e"=1]
"cvondrick/soundnet" -> "marl/audiosetdl" ["e"=1]
"cvondrick/soundnet" -> "kimiyoung/review_net" ["e"=1]
"facebookresearch/av_hubert" -> "mpc001/Visual_Speech_Recognition_for_Multiple_Languages"
"facebookresearch/av_hubert" -> "lordmartian/deep_avsr"
"facebookresearch/av_hubert" -> "mpc001/Lipreading_using_Temporal_Convolutional_Networks"
"facebookresearch/av_hubert" -> "facebookresearch/AudioMAE" ["e"=1]
"facebookresearch/av_hubert" -> "microsoft/UniSpeech" ["e"=1]
"facebookresearch/av_hubert" -> "TencentGameMate/chinese_speech_pretrain" ["e"=1]
"facebookresearch/av_hubert" -> "LUMIA-Group/Leveraging-Self-Supervised-Learning-for-AVSR"
"facebookresearch/av_hubert" -> "YuanGongND/ssast" ["e"=1]
"facebookresearch/av_hubert" -> "facebookresearch/textlesslib" ["e"=1]
"facebookresearch/av_hubert" -> "krantiparida/awesome-audio-visual"
"facebookresearch/av_hubert" -> "danmic/av-se"
"facebookresearch/av_hubert" -> "s3prl/s3prl" ["e"=1]
"facebookresearch/av_hubert" -> "LAION-AI/audio-dataset" ["e"=1]
"facebookresearch/av_hubert" -> "GeWu-Lab/awesome-audiovisual-learning"
"facebookresearch/av_hubert" -> "joonson/syncnet_python" ["e"=1]
"rizkiarm/LipNet" -> "deepconvolution/LipNet"
"rizkiarm/LipNet" -> "astorfi/lip-reading-deeplearning"
"rizkiarm/LipNet" -> "mpc001/end-to-end-lipreading"
"rizkiarm/LipNet" -> "afourast/deep_lip_reading"
"rizkiarm/LipNet" -> "Fengdalu/LipNet-PyTorch"
"rizkiarm/LipNet" -> "bshillingford/LipNet"
"rizkiarm/LipNet" -> "sailordiary/LipNet-PyTorch"
"rizkiarm/LipNet" -> "tstafylakis/Lipreading-ResNet"
"rizkiarm/LipNet" -> "mpc001/Lipreading_using_Temporal_Convolutional_Networks"
"rizkiarm/LipNet" -> "joseph-zhong/LipReading"
"rizkiarm/LipNet" -> "hassanhub/LipReading"
"rizkiarm/LipNet" -> "Fengdalu/Lipreading-DenseNet3D"
"rizkiarm/LipNet" -> "ajinkyaT/Lip_Reading_in_the_Wild_AVSR"
"rizkiarm/LipNet" -> "voletiv/lipreading-in-the-wild-experiments"
"rizkiarm/LipNet" -> "adwin5/CNN-for-visual-speech-recognition"
"bipio-server/bipio" -> "alpinegizmo/geekier"
"bipio-server/bipio" -> "foxmask/django-th"
"bipio-server/bipio" -> "skynetim/skynet" ["e"=1]
"bipio-server/bipio" -> "ottawaruby/whenbot"
"bipio-server/bipio" -> "cantino/huginn" ["e"=1]
"bipio-server/bipio" -> "MLstate/PEPS" ["e"=1]
"geekplux/markvis" -> "moinism/botui"
"geekplux/markvis" -> "iogf/sukhoi"
"geekplux/markvis" -> "icons8/titanic" ["e"=1]
"geekplux/markvis" -> "tj/gh-polls"
"geekplux/markvis" -> "azat-co/practicalnode" ["e"=1]
"geekplux/markvis" -> "jaredreich/pell" ["e"=1]
"geekplux/markvis" -> "hilongjw/vue-recyclerview" ["e"=1]
"geekplux/markvis" -> "luosijie/vue-manager" ["e"=1]
"geekplux/markvis" -> "gitpoint/git-point" ["e"=1]
"geekplux/markvis" -> "rikschennink/fitty" ["e"=1]
"geekplux/markvis" -> "notifme/notifme-sdk" ["e"=1]
"geekplux/markvis" -> "geekplux/hexo-theme-typing" ["e"=1]
"geekplux/markvis" -> "KingPixil/moon" ["e"=1]
"geekplux/markvis" -> "zhedongzheng/finch" ["e"=1]
"geekplux/markvis" -> "Ma63d/kov-blog" ["e"=1]
"moinism/botui" -> "geekplux/markvis"
"moinism/botui" -> "iogf/sukhoi"
"moinism/botui" -> "moinism/botui-examples"
"moinism/botui" -> "Rich-Harris/butternut" ["e"=1]
"moinism/botui" -> "Bottr-js/Bottr" ["e"=1]
"moinism/botui" -> "dthree/mailit" ["e"=1]
"moinism/botui" -> "charlestati/amplify" ["e"=1]
"moinism/botui" -> "jeromedalbert/real-world-react" ["e"=1]
"moinism/botui" -> "sruda/steroidesign" ["e"=1]
"moinism/botui" -> "pinterest/bonsai" ["e"=1]
"moinism/botui" -> "mplewis/src2png" ["e"=1]
"moinism/botui" -> "toish/chromatism" ["e"=1]
"moinism/botui" -> "KingPixil/wade" ["e"=1]
"stackimpact/stackimpact-python" -> "iogf/crocs"
"stackimpact/stackimpact-python" -> "paxos-bankchain/subconscious"
"stackimpact/stackimpact-python" -> "mikeywaites/kim"
"stackimpact/stackimpact-python" -> "csurfer/pyheatmagic" ["e"=1]
"iogf/sukhoi" -> "astorfi/lip-reading-deeplearning"
"iogf/sukhoi" -> "iogf/crocs"
"iogf/sukhoi" -> "moinism/botui"
"iogf/sukhoi" -> "geekplux/markvis"
"iogf/sukhoi" -> "kristian-lange/net-glimpse"
"iogf/sukhoi" -> "DutchGraa/crackcoin" ["e"=1]
"iogf/sukhoi" -> "gaojiuli/gain" ["e"=1]
"iogf/sukhoi" -> "ergonomica/ergonomica" ["e"=1]
"iogf/sukhoi" -> "stackimpact/stackimpact-python"
"iogf/sukhoi" -> "dmulholland/ivy" ["e"=1]
"iogf/sukhoi" -> "vividvilla/csvtotable"
"iogf/sukhoi" -> "kovidgoyal/html5-parser" ["e"=1]
"iogf/sukhoi" -> "overshard/timestrap"
"iogf/sukhoi" -> "dthree/mailit" ["e"=1]
"iogf/sukhoi" -> "nerevu/riko" ["e"=1]
"alexprengere/FormalSystems" -> "tjitze/RankPL"
"alexprengere/FormalSystems" -> "khinsen/leibniz"
"alexprengere/FormalSystems" -> "Quuxplusone/TNT"
"alexprengere/FormalSystems" -> "pirate/pocket-archive-stream"
"alexprengere/FormalSystems" -> "AthenaModel/athena"
"alexprengere/FormalSystems" -> "cameronfabbri/Colorful-Image-Colorization" ["e"=1]
"vividvilla/csvtotable" -> "iogf/crocs"
"vividvilla/csvtotable" -> "derekeder/csv-to-html-table"
"vividvilla/csvtotable" -> "knadh/otpgateway" ["e"=1]
"vividvilla/csvtotable" -> "iogf/sukhoi"
"vividvilla/csvtotable" -> "secretGeek/AwesomeCSV" ["e"=1]
"vividvilla/csvtotable" -> "SimonBiggs/scriptedforms" ["e"=1]
"vividvilla/csvtotable" -> "metachris/logzero" ["e"=1]
"vividvilla/csvtotable" -> "anishathalye/seashells"
"vividvilla/csvtotable" -> "tomchristie/apistar" ["e"=1]
"vividvilla/csvtotable" -> "stackimpact/stackimpact-python"
"vividvilla/csvtotable" -> "shridarpatil/frappe-go" ["e"=1]
"vividvilla/csvtotable" -> "knadh/dont.build" ["e"=1]
"vividvilla/csvtotable" -> "knadh/profiler"
"vividvilla/csvtotable" -> "csurfer/pypette" ["e"=1]
"vividvilla/csvtotable" -> "kennethreitz/maya" ["e"=1]
"anishathalye/seashells" -> "iogf/crocs"
"anishathalye/seashells" -> "kennethreitz/background" ["e"=1]
"astorfi/lip-reading-deeplearning" -> "iogf/sukhoi"
"astorfi/lip-reading-deeplearning" -> "rizkiarm/LipNet"
"astorfi/lip-reading-deeplearning" -> "afourast/deep_lip_reading"
"astorfi/lip-reading-deeplearning" -> "mpc001/end-to-end-lipreading"
"astorfi/lip-reading-deeplearning" -> "mpc001/Lipreading_using_Temporal_Convolutional_Networks"
"astorfi/lip-reading-deeplearning" -> "joseph-zhong/LipReading"
"astorfi/lip-reading-deeplearning" -> "Fengdalu/Lipreading-DenseNet3D"
"astorfi/lip-reading-deeplearning" -> "astorfi/3D-convolutional-speaker-recognition" ["e"=1]
"astorfi/lip-reading-deeplearning" -> "Fengdalu/LipNet-PyTorch"
"astorfi/lip-reading-deeplearning" -> "deepconvolution/LipNet"
"astorfi/lip-reading-deeplearning" -> "tstafylakis/Lipreading-ResNet"
"astorfi/lip-reading-deeplearning" -> "TimeChi/Lip_Reading_Competition"
"astorfi/lip-reading-deeplearning" -> "lordmartian/deep_avsr"
"astorfi/lip-reading-deeplearning" -> "Rudrabha/Lip2Wav" ["e"=1]
"astorfi/lip-reading-deeplearning" -> "astorfi/TensorFlow-World-Resources" ["e"=1]
"hangzhaomit/Sound-of-Pixels" -> "roudimit/MUSIC_dataset"
"hangzhaomit/Sound-of-Pixels" -> "rhgao/co-separation"
"hangzhaomit/Sound-of-Pixels" -> "andrewowens/multisensory"
"hangzhaomit/Sound-of-Pixels" -> "YapengTian/AVE-ECCV18"
"hangzhaomit/Sound-of-Pixels" -> "rohitrango/objects-that-sound"
"hangzhaomit/Sound-of-Pixels" -> "afourast/avobjects"
"hangzhaomit/Sound-of-Pixels" -> "krantiparida/awesome-audio-visual"
"hangzhaomit/Sound-of-Pixels" -> "facebookresearch/2.5D-Visual-Sound"
"hangzhaomit/Sound-of-Pixels" -> "pedro-morgado/spatialaudiogen"
"hangzhaomit/Sound-of-Pixels" -> "SheldonTsui/SepStereo_ECCV2020"
"hangzhaomit/Sound-of-Pixels" -> "rhgao/Deep-MIML-Network"
"hangzhaomit/Sound-of-Pixels" -> "DTaoo/Discriminative-Sounding-Objects-Localization"
"hangzhaomit/Sound-of-Pixels" -> "facebookresearch/sound-spaces"
"hangzhaomit/Sound-of-Pixels" -> "ardasnck/learning_to_localize_sound_source"
"hangzhaomit/Sound-of-Pixels" -> "hche11/Localizing-Visual-Sounds-the-Hard-Way"
"overshard/timestrap" -> "atech/postal" ["e"=1]
"overshard/timestrap" -> "kshvmdn/fsql" ["e"=1]
"overshard/timestrap" -> "iogf/sukhoi"
"overshard/timestrap" -> "pirate/pocket-archive-stream"
"overshard/timestrap" -> "tannerlinsley/react-move" ["e"=1]
"overshard/timestrap" -> "google/tamperchrome" ["e"=1]
"overshard/timestrap" -> "Staffjoy/suite" ["e"=1]
"overshard/timestrap" -> "kanishka-linux/reminiscence" ["e"=1]
"overshard/timestrap" -> "karllhughes/side-project-marketing" ["e"=1]
"overshard/timestrap" -> "django-helpdesk/django-helpdesk" ["e"=1]
"overshard/timestrap" -> "groveco/django-sql-explorer" ["e"=1]
"overshard/timestrap" -> "foxmask/django-th"
"overshard/timestrap" -> "adtac/commento" ["e"=1]
"overshard/timestrap" -> "hashedin/squealy" ["e"=1]
"overshard/timestrap" -> "tebelorg/TA.Gui" ["e"=1]
"krantiparida/awesome-audio-visual" -> "YapengTian/AVE-ECCV18"
"krantiparida/awesome-audio-visual" -> "afourast/avobjects"
"krantiparida/awesome-audio-visual" -> "danmic/av-se"
"krantiparida/awesome-audio-visual" -> "hche11/VGGSound"
"krantiparida/awesome-audio-visual" -> "GeWu-Lab/awesome-audiovisual-learning"
"krantiparida/awesome-audio-visual" -> "YapengTian/AVVP-ECCV20"
"krantiparida/awesome-audio-visual" -> "facebookresearch/2.5D-Visual-Sound"
"krantiparida/awesome-audio-visual" -> "hangzhaomit/Sound-of-Pixels"
"krantiparida/awesome-audio-visual" -> "pedro-morgado/spatialaudiogen"
"krantiparida/awesome-audio-visual" -> "rhgao/co-separation"
"krantiparida/awesome-audio-visual" -> "facebookresearch/av_hubert"
"krantiparida/awesome-audio-visual" -> "gemengtju/Tutorial_Separation" ["e"=1]
"krantiparida/awesome-audio-visual" -> "jasongief/PSP_CVPR_2021"
"krantiparida/awesome-audio-visual" -> "facebookresearch/VisualVoice"
"krantiparida/awesome-audio-visual" -> "TaoRuijie/TalkNet_ASD"
"facebookresearch/Listen-to-Look" -> "facebookresearch/2.5D-Visual-Sound"
"rohitrango/objects-that-sound" -> "kyuyeonpooh/objects-that-sound"
"rohitrango/objects-that-sound" -> "Kajiyu/LLLNet"
"Fengdalu/LipNet-PyTorch" -> "Fengdalu/Lipreading-DenseNet3D"
"Fengdalu/LipNet-PyTorch" -> "sailordiary/LipNet-PyTorch"
"Fengdalu/LipNet-PyTorch" -> "Fengdalu/learn-an-effective-lip-reading-model-without-pains"
"Fengdalu/LipNet-PyTorch" -> "mpc001/end-to-end-lipreading"
"Fengdalu/LipNet-PyTorch" -> "TimeChi/Lip_Reading_Competition"
"Fengdalu/LipNet-PyTorch" -> "joseph-zhong/LipReading"
"Fengdalu/LipNet-PyTorch" -> "tstafylakis/Lipreading-ResNet"
"Fengdalu/LipNet-PyTorch" -> "afourast/deep_lip_reading"
"Fengdalu/LipNet-PyTorch" -> "mpc001/Lipreading_using_Temporal_Convolutional_Networks"
"Fengdalu/LipNet-PyTorch" -> "NirHeaven/D3D"
"Fengdalu/LipNet-PyTorch" -> "lordmartian/deep_avsr"
"TimeChi/Lip_Reading_Competition" -> "liuzhejun/XWbank_LipReading"
"TimeChi/Lip_Reading_Competition" -> "FesianXu/LipNet_ChineseWordsClassification"
"TimeChi/Lip_Reading_Competition" -> "Fengdalu/Lipreading-DenseNet3D"
"TimeChi/Lip_Reading_Competition" -> "sailordiary/LipNet-PyTorch"
"georgesterpu/avsr-tf1" -> "georgesterpu/Taris"
"lordmartian/deep_avsr" -> "georgesterpu/avsr-tf1"
"lordmartian/deep_avsr" -> "afourast/deep_lip_reading"
"lordmartian/deep_avsr" -> "mpc001/end-to-end-lipreading"
"lordmartian/deep_avsr" -> "mpc001/Lipreading_using_Temporal_Convolutional_Networks"
"lordmartian/deep_avsr" -> "Fengdalu/learn-an-effective-lip-reading-model-without-pains"
"lordmartian/deep_avsr" -> "mpc001/Visual_Speech_Recognition_for_Multiple_Languages"
"lordmartian/deep_avsr" -> "LUMIA-Group/Leveraging-Self-Supervised-Learning-for-AVSR"
"lordmartian/deep_avsr" -> "facebookresearch/av_hubert"
"lordmartian/deep_avsr" -> "tstafylakis/Lipreading-ResNet"
"lordmartian/deep_avsr" -> "VIPL-Audio-Visual-Speech-Understanding/learn-an-effective-lip-reading-model-without-pains"
"lordmartian/deep_avsr" -> "ajinkyaT/Lip_Reading_in_the_Wild_AVSR"
"mpc001/Lipreading_using_Temporal_Convolutional_Networks" -> "mpc001/Visual_Speech_Recognition_for_Multiple_Languages"
"mpc001/Lipreading_using_Temporal_Convolutional_Networks" -> "mpc001/end-to-end-lipreading"
"mpc001/Lipreading_using_Temporal_Convolutional_Networks" -> "lordmartian/deep_avsr"
"mpc001/Lipreading_using_Temporal_Convolutional_Networks" -> "Fengdalu/learn-an-effective-lip-reading-model-without-pains"
"mpc001/Lipreading_using_Temporal_Convolutional_Networks" -> "afourast/deep_lip_reading"
"mpc001/Lipreading_using_Temporal_Convolutional_Networks" -> "facebookresearch/av_hubert"
"mpc001/Lipreading_using_Temporal_Convolutional_Networks" -> "facebookresearch/VisualVoice"
"mpc001/Lipreading_using_Temporal_Convolutional_Networks" -> "tstafylakis/Lipreading-ResNet"
"mpc001/Lipreading_using_Temporal_Convolutional_Networks" -> "joonson/syncnet_python" ["e"=1]
"mpc001/Lipreading_using_Temporal_Convolutional_Networks" -> "ahaliassos/LipForensics" ["e"=1]
"mpc001/Lipreading_using_Temporal_Convolutional_Networks" -> "Fengdalu/LipNet-PyTorch"
"mpc001/Lipreading_using_Temporal_Convolutional_Networks" -> "Fengdalu/Lipreading-DenseNet3D"
"mpc001/Lipreading_using_Temporal_Convolutional_Networks" -> "Rudrabha/Lip2Wav" ["e"=1]
"mpc001/Lipreading_using_Temporal_Convolutional_Networks" -> "danmic/av-se"
"mpc001/Lipreading_using_Temporal_Convolutional_Networks" -> "TimeChi/Lip_Reading_Competition"
"evanplaice/jquery-csv" -> "MounirMesselmeni/html-fileapi"
"evanplaice/jquery-csv" -> "derekeder/csv-to-html-table"
"tstafylakis/Lipreading-ResNet" -> "mpc001/end-to-end-lipreading"
"tstafylakis/Lipreading-ResNet" -> "psyec1/Lipreading-PyTorch"
"tstafylakis/Lipreading-ResNet" -> "Fengdalu/Lipreading-DenseNet3D"
"tstafylakis/Lipreading-ResNet" -> "ajinkyaT/Lip_Reading_in_the_Wild_AVSR"
"tstafylakis/Lipreading-ResNet" -> "lordmartian/deep_avsr"
"TaoRuijie/TalkNet-ASD" -> "SRA2/SPELL"
"TaoRuijie/TalkNet-ASD" -> "okankop/ASDNet"
"TaoRuijie/TalkNet-ASD" -> "X-LANCE/MSDWILD"
"andrewowens/multisensory" -> "avivga/audio-visual-speech-enhancement"
"andrewowens/multisensory" -> "afourast/avobjects"
"andrewowens/multisensory" -> "hangzhaomit/Sound-of-Pixels"
"andrewowens/multisensory" -> "rohitrango/objects-that-sound"
"andrewowens/multisensory" -> "YapengTian/AVE-ECCV18"
"andrewowens/multisensory" -> "hche11/Localizing-Visual-Sounds-the-Hard-Way"
"andrewowens/multisensory" -> "bill9800/speech_separation"
"andrewowens/multisensory" -> "ajinkyaT/Lip_Reading_in_the_Wild_AVSR"
"andrewowens/multisensory" -> "facebookresearch/FAIR-Play"
"andrewowens/multisensory" -> "mpc001/end-to-end-lipreading"
"andrewowens/multisensory" -> "mayurnewase/looking-to-listen-at-cocktail-party"
"rhgao/co-separation" -> "facebookresearch/2.5D-Visual-Sound"
"rhgao/co-separation" -> "ubc-vision/TriBERT"
"rhgao/co-separation" -> "facebookresearch/FAIR-Play"
"rhgao/co-separation" -> "DTaoo/Discriminative-Sounding-Objects-Localization"
"rhgao/co-separation" -> "ardasnck/learning_to_localize_sound_source"
"rhgao/co-separation" -> "SheldonTsui/SepStereo_ECCV2020"
"derekeder/csv-to-html-table" -> "vividvilla/csvtotable"
"derekeder/csv-to-html-table" -> "derekeder/FusionTable-Map-Template" ["e"=1]
"derekeder/csv-to-html-table" -> "openaddresses/pyesridump" ["e"=1]
"derekeder/csv-to-html-table" -> "evanplaice/jquery-csv"
"derekeder/csv-to-html-table" -> "datamade/data-making-guidelines" ["e"=1]
"derekeder/csv-to-html-table" -> "themarshallproject/klaxon" ["e"=1]
"mpc001/Visual_Speech_Recognition_for_Multiple_Languages" -> "mpc001/Lipreading_using_Temporal_Convolutional_Networks"
"mpc001/Visual_Speech_Recognition_for_Multiple_Languages" -> "LUMIA-Group/Leveraging-Self-Supervised-Learning-for-AVSR"
"mpc001/Visual_Speech_Recognition_for_Multiple_Languages" -> "prajwalkr/vtp"
"mpc001/Visual_Speech_Recognition_for_Multiple_Languages" -> "lordmartian/deep_avsr"
"mpc001/Visual_Speech_Recognition_for_Multiple_Languages" -> "facebookresearch/av_hubert"
"mpc001/Visual_Speech_Recognition_for_Multiple_Languages" -> "VIPL-Audio-Visual-Speech-Understanding/learn-an-effective-lip-reading-model-without-pains"
"mpc001/Visual_Speech_Recognition_for_Multiple_Languages" -> "mpc001/end-to-end-lipreading"
"mpc001/Visual_Speech_Recognition_for_Multiple_Languages" -> "ahaliassos/raven"
"mpc001/Visual_Speech_Recognition_for_Multiple_Languages" -> "Fengdalu/learn-an-effective-lip-reading-model-without-pains"
"hche11/Localizing-Visual-Sounds-the-Hard-Way" -> "yiminglin-ai/imdb-clean"
"hche11/Localizing-Visual-Sounds-the-Hard-Way" -> "ardasnck/learning_to_localize_sound_source"
"hche11/Localizing-Visual-Sounds-the-Hard-Way" -> "hche11/VGGSound"
"facebookresearch/sound-spaces" -> "facebookresearch/VisualEchoes"
"facebookresearch/sound-spaces" -> "aluo-x/Learning_Neural_Acoustic_Fields"
"facebookresearch/sound-spaces" -> "pedro-morgado/spatialaudiogen"
"facebookresearch/sound-spaces" -> "SheldonTsui/SepStereo_ECCV2020"
"facebookresearch/sound-spaces" -> "facebookresearch/learning-audio-visual-dereverberation"
"facebookresearch/sound-spaces" -> "anton-jeran/FAST-RIR"
"avivga/audio-visual-speech-enhancement" -> "yakovmon/Real-Time-Audio-Visual-Speech-Enhancement"
"anton-jeran/FAST-RIR" -> "anton-jeran/MESH2IR"
"anton-jeran/FAST-RIR" -> "GAMMA-UMD/pygsound"
"FesianXu/LipNet_ChineseWordsClassification" -> "liuzhejun/XWbank_LipReading"
"FesianXu/LipNet_ChineseWordsClassification" -> "TimeChi/Lip_Reading_Competition"
"liuzhejun/XWbank_LipReading" -> "FesianXu/LipNet_ChineseWordsClassification"
"facebookresearch/VisualVoice" -> "danmic/av-se"
"facebookresearch/VisualVoice" -> "afourast/avobjects"
"facebookresearch/VisualVoice" -> "facebookresearch/2.5D-Visual-Sound"
"facebookresearch/VisualVoice" -> "aispeech-lab/advr-avss"
"facebookresearch/VisualVoice" -> "zexupan/MuSE"
"facebookresearch/VisualVoice" -> "JusperLee/Looking-to-Listen-at-the-Cocktail-Party"
"JusperLee/Looking-to-Listen-at-the-Cocktail-Party" -> "bill9800/speech_separation"
"JusperLee/Looking-to-Listen-at-the-Cocktail-Party" -> "danmic/av-se"
"JusperLee/Looking-to-Listen-at-the-Cocktail-Party" -> "JusperLee/ExamOnline"
"JusperLee/Looking-to-Listen-at-the-Cocktail-Party" -> "JusperLee/Dual-Path-RNN-Pytorch" ["e"=1]
"JusperLee/Looking-to-Listen-at-the-Cocktail-Party" -> "facebookresearch/VisualVoice"
"JusperLee/Looking-to-Listen-at-the-Cocktail-Party" -> "JusperLee/Conv-TasNet" ["e"=1]
"JusperLee/Looking-to-Listen-at-the-Cocktail-Party" -> "JusperLee/Calculate-SNR-SDR" ["e"=1]
"JusperLee/Looking-to-Listen-at-the-Cocktail-Party" -> "JusperLee/Accelerator"
"JusperLee/Looking-to-Listen-at-the-Cocktail-Party" -> "JusperLee/Deep-Learning"
"JusperLee/Looking-to-Listen-at-the-Cocktail-Party" -> "JusperLee/ELF-SR"
"JusperLee/Looking-to-Listen-at-the-Cocktail-Party" -> "JusperLee/GrabCut"
"JusperLee/Looking-to-Listen-at-the-Cocktail-Party" -> "JusperLee/Grass"
"JusperLee/Looking-to-Listen-at-the-Cocktail-Party" -> "JusperLee/Time"
"JusperLee/Looking-to-Listen-at-the-Cocktail-Party" -> "JusperLee/player"
"JusperLee/Looking-to-Listen-at-the-Cocktail-Party" -> "JusperLee/WeChatApp"
"Fengdalu/Lipreading-DenseNet3D" -> "NirHeaven/D3D"
"Fengdalu/Lipreading-DenseNet3D" -> "mpc001/end-to-end-lipreading"
"Fengdalu/Lipreading-DenseNet3D" -> "Fengdalu/LipNet-PyTorch"
"Fengdalu/Lipreading-DenseNet3D" -> "Fengdalu/learn-an-effective-lip-reading-model-without-pains"
"Fengdalu/Lipreading-DenseNet3D" -> "tstafylakis/Lipreading-ResNet"
"Fengdalu/Lipreading-DenseNet3D" -> "ajinkyaT/Lip_Reading_in_the_Wild_AVSR"
"Fengdalu/Lipreading-DenseNet3D" -> "TimeChi/Lip_Reading_Competition"
"Fengdalu/Lipreading-DenseNet3D" -> "sailordiary/LipNet-PyTorch"
"afourast/avobjects" -> "ardasnck/learning_to_localize_sound_source"
"afourast/avobjects" -> "facebookresearch/VisualVoice"
"afourast/avobjects" -> "rhgao/co-separation"
"afourast/avobjects" -> "DTaoo/Discriminative-Sounding-Objects-Localization"
"afourast/avobjects" -> "facebookresearch/2.5D-Visual-Sound"
"afourast/avobjects" -> "danmic/av-se"
"DTaoo/Discriminative-Sounding-Objects-Localization" -> "shvdiwnkozbw/Multi-Source-Sound-Localization"
"adwin5/CNN-for-visual-speech-recognition" -> "adwin5/lipreading-by-convolutional-neural-network-keras"
"voletiv/lipreading-in-the-wild-experiments" -> "DungLe13/lips-reading"
"GeWu-Lab/OGM-GE_CVPR2022" -> "GeWu-Lab/awesome-audiovisual-learning"
"deepconvolution/LipNet" -> "hassanhub/LipReading"
"deepconvolution/LipNet" -> "rizkiarm/LipNet"
"deepconvolution/LipNet" -> "khazit/Lip2Word"
"deepconvolution/LipNet" -> "joseph-zhong/LipReading"
"deepconvolution/LipNet" -> "adwin5/lipreading-by-convolutional-neural-network-keras"
"deepconvolution/LipNet" -> "afourast/deep_lip_reading"
"deepconvolution/LipNet" -> "psyec1/Lipreading-PyTorch"
"deepconvolution/LipNet" -> "voletiv/lipreading-in-the-wild-experiments"
"deepconvolution/LipNet" -> "osalinasv/lipnet"
"danmic/av-se" -> "facebookresearch/VisualVoice"
"danmic/av-se" -> "kagaminccino/LAVSE"
"danmic/av-se" -> "JusperLee/Looking-to-Listen-at-the-Cocktail-Party"
"danmic/av-se" -> "zexupan/MuSE"
"danmic/av-se" -> "dr-pato/audio_visual_speech_enhancement"
"danmic/av-se" -> "afourast/avobjects"
"mpc001/end-to-end-lipreading" -> "tstafylakis/Lipreading-ResNet"
"mpc001/end-to-end-lipreading" -> "mpc001/Lipreading_using_Temporal_Convolutional_Networks"
"mpc001/end-to-end-lipreading" -> "Fengdalu/Lipreading-DenseNet3D"
"mpc001/end-to-end-lipreading" -> "lordmartian/deep_avsr"
"mpc001/end-to-end-lipreading" -> "psyec1/Lipreading-PyTorch"
"mpc001/end-to-end-lipreading" -> "sailordiary/LipNet-PyTorch"
"mpc001/end-to-end-lipreading" -> "afourast/deep_lip_reading"
"mpc001/end-to-end-lipreading" -> "mpc001/Visual_Speech_Recognition_for_Multiple_Languages"
"mpc001/end-to-end-lipreading" -> "NirHeaven/D3D"
"mpc001/end-to-end-lipreading" -> "Fengdalu/learn-an-effective-lip-reading-model-without-pains"
"mpc001/end-to-end-lipreading" -> "Fengdalu/LipNet-PyTorch"
"mpc001/end-to-end-lipreading" -> "hassanhub/LipReading"
"mpc001/end-to-end-lipreading" -> "ajinkyaT/Lip_Reading_in_the_Wild_AVSR"
"mpc001/end-to-end-lipreading" -> "joseph-zhong/LipReading"
"mpc001/end-to-end-lipreading" -> "TimeChi/Lip_Reading_Competition"
"GAMMA-UMD/pygsound" -> "GAMMA-UMD/GWA"
"hche11/VGGSound" -> "hche11/Localizing-Visual-Sounds-the-Hard-Way"
"hche11/VGGSound" -> "YapengTian/AVE-ECCV18"
"hche11/VGGSound" -> "YapengTian/AVVP-ECCV20"
"hche11/VGGSound" -> "yiminglin-ai/imdb-clean"
"hche11/VGGSound" -> "liuxubo717/LASS" ["e"=1]
"hche11/VGGSound" -> "krantiparida/awesome-audio-visual"
"hche11/VGGSound" -> "audio-captioning/audio-captioning-papers" ["e"=1]
"hche11/VGGSound" -> "yinkalario/General-Purpose-Sound-Recognition-Demo" ["e"=1]
"hche11/VGGSound" -> "ardasnck/learning_to_localize_sound_source"
"hche11/VGGSound" -> "microsoft/WavText5K" ["e"=1]
"hche11/VGGSound" -> "etzinis/heterogeneous_separation"
"yiminglin-ai/imdb-clean" -> "ibug-group/fpage"
"pirate/pocket-archive-stream" -> "alexprengere/FormalSystems"
"adwin5/lipreading-by-convolutional-neural-network-keras" -> "adwin5/CNN-for-visual-speech-recognition"
"eborboihuc/SoundNet-tensorflow" -> "cvondrick/soundnet"
"eborboihuc/SoundNet-tensorflow" -> "pseeth/soundnet_keras"
"eborboihuc/SoundNet-tensorflow" -> "Yuliang-Zou/tf_videogan"
"kagaminccino/LAVSE" -> "danmic/av-se"
"kagaminccino/LAVSE" -> "yuwchen/CITISEN"
"kagaminccino/LAVSE" -> "WilliamYu1993/ICSE" ["e"=1]
"psyec1/Lipreading-PyTorch" -> "tstafylakis/Lipreading-ResNet"
"GeWu-Lab/awesome-audiovisual-learning" -> "GeWu-Lab/CSOL_TPAMI2021"
"GeWu-Lab/awesome-audiovisual-learning" -> "GeWu-Lab/MUSIC-AVQA"
"GeWu-Lab/awesome-audiovisual-learning" -> "GenjiB/LAVISH"
"GenjiB/LAVISH" -> "WikiChao/Ego-AV-Loc"
"LUMIA-Group/Leveraging-Self-Supervised-Learning-for-AVSR" -> "ms-dot-k/Multi-head-Visual-Audio-Memory"
"afourast/deep_lip_reading" -> "lordmartian/deep_avsr"
"afourast/deep_lip_reading" -> "mpc001/Lipreading_using_Temporal_Convolutional_Networks"
"afourast/deep_lip_reading" -> "joseph-zhong/LipReading"
"afourast/deep_lip_reading" -> "mpc001/end-to-end-lipreading"
"afourast/deep_lip_reading" -> "Fengdalu/learn-an-effective-lip-reading-model-without-pains"
"afourast/deep_lip_reading" -> "tstafylakis/Lipreading-ResNet"
"afourast/deep_lip_reading" -> "ajinkyaT/Lip_Reading_in_the_Wild_AVSR"
"afourast/deep_lip_reading" -> "afourast/avobjects"
"afourast/deep_lip_reading" -> "Fengdalu/LipNet-PyTorch"
"afourast/deep_lip_reading" -> "khazit/Lip2Word"
"afourast/deep_lip_reading" -> "voletiv/lipreading-in-the-wild-experiments"
"afourast/deep_lip_reading" -> "deepconvolution/LipNet"
"afourast/deep_lip_reading" -> "georgesterpu/avsr-tf1"
"afourast/deep_lip_reading" -> "rizkiarm/LipNet"
"joseph-zhong/LipReading" -> "adwin5/lipreading-by-convolutional-neural-network-keras"
"joseph-zhong/LipReading" -> "afourast/deep_lip_reading"
"joseph-zhong/LipReading" -> "hassanhub/LipReading"
"joseph-zhong/LipReading" -> "TimeChi/Lip_Reading_Competition"
"joseph-zhong/LipReading" -> "psyec1/Lipreading-PyTorch"
"joseph-zhong/LipReading" -> "Fengdalu/Lipreading-DenseNet3D"
"joseph-zhong/LipReading" -> "Fengdalu/LipNet-PyTorch"
"joseph-zhong/LipReading" -> "ajinkyaT/Lip_Reading_in_the_Wild_AVSR"
"joseph-zhong/LipReading" -> "mpc001/end-to-end-lipreading"
"anton-jeran/MESH2IR" -> "anton-jeran/FAST-RIR"
"SheldonTsui/PseudoBinaural_CVPR2021" -> "SheldonTsui/SepStereo_ECCV2020"
"fuankarion/active-speakers-context" -> "okankop/ASDNet"
"fuankarion/active-speakers-context" -> "tuanchien/asd"
"fuankarion/active-speakers-context" -> "TaoRuijie/TalkNet_ASD"
"VIPL-Audio-Visual-Speech-Understanding/learn-an-effective-lip-reading-model-without-pains" -> "ms-dot-k/Multi-head-Visual-Audio-Memory"
"facebookresearch/visual-acoustic-matching" -> "facebookresearch/learning-audio-visual-dereverberation"
"ajinkyaT/Lip_Reading_in_the_Wild_AVSR" -> "georgesterpu/Sigmedia-AVSR"
"facebookresearch/FAIR-Play" -> "facebookresearch/2.5D-Visual-Sound"
"facebookresearch/FAIR-Play" -> "pedro-morgado/spatialaudiogen"
"okankop/ASDNet" -> "fuankarion/active-speakers-context"
"okankop/ASDNet" -> "TaoRuijie/TalkNet_ASD"
"FloretCat/CMRAN" -> "jasongief/PSP_CVPR_2021"
"YapengTian/AVVP-ECCV20" -> "Yu-Wu/Modaily-Aware-Audio-Visual-Video-Parsing"
"YapengTian/AVVP-ECCV20" -> "jasongief/PSP_CVPR_2021"
"YapengTian/AVVP-ECCV20" -> "marmot-xy/CMBS"
"YapengTian/AVVP-ECCV20" -> "MCG-NJU/JoMoLD"
"YapengTian/AVVP-ECCV20" -> "YapengTian/AVE-ECCV18"
"shvdiwnkozbw/Multi-Source-Sound-Localization" -> "DTaoo/Discriminative-Sounding-Objects-Localization"
"shvdiwnkozbw/Multi-Source-Sound-Localization" -> "ardasnck/learning_to_localize_sound_source"
"YapengTian/AVE-ECCV18" -> "YapengTian/AVVP-ECCV20"
"YapengTian/AVE-ECCV18" -> "jasongief/PSP_CVPR_2021"
"YapengTian/AVE-ECCV18" -> "FloretCat/CMRAN"
"YapengTian/AVE-ECCV18" -> "krantiparida/awesome-audio-visual"
"YapengTian/AVE-ECCV18" -> "ardasnck/learning_to_localize_sound_source"
"YapengTian/AVE-ECCV18" -> "hche11/VGGSound"
"YapengTian/AVE-ECCV18" -> "hche11/Localizing-Visual-Sounds-the-Hard-Way"
"YapengTian/AVE-ECCV18" -> "DTaoo/Discriminative-Sounding-Objects-Localization"
"YapengTian/AVE-ECCV18" -> "hangzhaomit/Sound-of-Pixels"
"YapengTian/AVE-ECCV18" -> "afourast/avobjects"
"jasongief/PSP_CVPR_2021" -> "FloretCat/CMRAN"
"jasongief/PSP_CVPR_2021" -> "Yu-Wu/Modaily-Aware-Audio-Visual-Video-Parsing"
"bill9800/speech_separation" -> "JusperLee/Looking-to-Listen-at-the-Cocktail-Party"
"bill9800/speech_separation" -> "mayurnewase/looking-to-listen-at-cocktail-party"
"bill9800/speech_separation" -> "pchao6/LSTM_PIT_Speech_Separation" ["e"=1]
"bill9800/speech_separation" -> "dr-pato/audio_visual_speech_enhancement"
"bill9800/speech_separation" -> "funcwj/conv-tasnet" ["e"=1]
"bill9800/speech_separation" -> "danmic/av-se"
"bill9800/speech_separation" -> "snsun/pit-speech-separation" ["e"=1]
"ardasnck/learning_to_localize_sound_source" -> "shvdiwnkozbw/Multi-Source-Sound-Localization"
"iogf/crocs" -> "stackimpact/stackimpact-python"
"iogf/crocs" -> "iogf/sukhoi"
"iogf/crocs" -> "DutchGraa/crackcoin" ["e"=1]
"iogf/crocs" -> "anishathalye/seashells"
"iogf/crocs" -> "vividvilla/csvtotable"
"iogf/crocs" -> "iogf/lax"
"SheldonTsui/SepStereo_ECCV2020" -> "SheldonTsui/PseudoBinaural_CVPR2021"
"SheldonTsui/SepStereo_ECCV2020" -> "facebookresearch/2.5D-Visual-Sound"
"SheldonTsui/SepStereo_ECCV2020" -> "pedro-morgado/spatialaudiogen"
"SheldonTsui/SepStereo_ECCV2020" -> "facebookresearch/FAIR-Play"
"Fengdalu/learn-an-effective-lip-reading-model-without-pains" -> "mpc001/Lipreading_using_Temporal_Convolutional_Networks"
"Fengdalu/learn-an-effective-lip-reading-model-without-pains" -> "lordmartian/deep_avsr"
"Fengdalu/learn-an-effective-lip-reading-model-without-pains" -> "Fengdalu/Lipreading-DenseNet3D"
"Fengdalu/learn-an-effective-lip-reading-model-without-pains" -> "Fengdalu/LipNet-PyTorch"
"Fengdalu/learn-an-effective-lip-reading-model-without-pains" -> "afourast/deep_lip_reading"
"Fengdalu/learn-an-effective-lip-reading-model-without-pains" -> "mpc001/end-to-end-lipreading"
"TaoRuijie/TalkNet_ASD" -> "okankop/ASDNet"
"TaoRuijie/TalkNet_ASD" -> "fuankarion/active-speakers-context"
"ottawaruby/whenbot" -> "googollee/yaif"
"ibug-group/fpage" -> "yiminglin-ai/imdb-clean"
"ibug-group/fpage" -> "neeek2303/Lenta-Hackathon" ["e"=1]
"facebookresearch/2.5D-Visual-Sound" -> "facebookresearch/FAIR-Play"
"facebookresearch/2.5D-Visual-Sound" -> "pedro-morgado/spatialaudiogen"
"facebookresearch/2.5D-Visual-Sound" -> "SheldonTsui/SepStereo_ECCV2020"
"facebookresearch/2.5D-Visual-Sound" -> "rhgao/co-separation"
"facebookresearch/2.5D-Visual-Sound" -> "SheldonTsui/PseudoBinaural_CVPR2021"
"tjitze/RankPL" -> "strmpnk/hansei" ["e"=1]
"tjitze/RankPL" -> "zkat/conserv"
"aispeech-lab/advr-avss" -> "aispeech-lab/oavss"
"foxmask/django-th" ["l"="26.127,-20.361"]
"bipio-server/bipio" ["l"="26.134,-20.312"]
"pirate/pocket-archive-stream" ["l"="26.078,-20.382"]
"huydx/hget" ["l"="-13.394,1.925"]
"pedro-morgado/spatialaudiogen" ["l"="26.519,-20.717"]
"facebookresearch/2.5D-Visual-Sound" ["l"="26.498,-20.702"]
"SheldonTsui/SepStereo_ECCV2020" ["l"="26.504,-20.723"]
"facebookresearch/FAIR-Play" ["l"="26.51,-20.703"]
"cvondrick/soundnet" ["l"="26.484,-20.625"]
"eborboihuc/SoundNet-tensorflow" ["l"="26.476,-20.581"]
"reedscot/nips2016" ["l"="33.662,32.527"]
"karoldvl/ESC-50" ["l"="1.401,39.195"]
"hangzhaomit/Sound-of-Pixels" ["l"="26.519,-20.687"]
"Guim3/IcGAN" ["l"="33.594,32.622"]
"qiuqiangkong/audioset_classification" ["l"="1.454,39.185"]
"andrewowens/multisensory" ["l"="26.506,-20.648"]
"afourast/avobjects" ["l"="26.487,-20.653"]
"metalbubble/moments_models" ["l"="32.467,34.835"]
"facebook/eyescream" ["l"="33.666,32.473"]
"marl/audiosetdl" ["l"="1.436,39.162"]
"kimiyoung/review_net" ["l"="31.405,34.585"]
"facebookresearch/av_hubert" ["l"="26.455,-20.62"]
"mpc001/Visual_Speech_Recognition_for_Multiple_Languages" ["l"="26.428,-20.611"]
"lordmartian/deep_avsr" ["l"="26.407,-20.618"]
"mpc001/Lipreading_using_Temporal_Convolutional_Networks" ["l"="26.416,-20.6"]
"facebookresearch/AudioMAE" ["l"="0.19,39.907"]
"microsoft/UniSpeech" ["l"="0.374,39.855"]
"TencentGameMate/chinese_speech_pretrain" ["l"="0.399,39.928"]
"LUMIA-Group/Leveraging-Self-Supervised-Learning-for-AVSR" ["l"="26.432,-20.636"]
"YuanGongND/ssast" ["l"="0.186,39.854"]
"facebookresearch/textlesslib" ["l"="0.281,39.992"]
"krantiparida/awesome-audio-visual" ["l"="26.507,-20.667"]
"danmic/av-se" ["l"="26.504,-20.61"]
"s3prl/s3prl" ["l"="0.402,39.901"]
"LAION-AI/audio-dataset" ["l"="0.161,39.942"]
"GeWu-Lab/awesome-audiovisual-learning" ["l"="26.502,-20.575"]
"joonson/syncnet_python" ["l"="35.033,31.652"]
"rizkiarm/LipNet" ["l"="26.357,-20.589"]
"deepconvolution/LipNet" ["l"="26.347,-20.609"]
"astorfi/lip-reading-deeplearning" ["l"="26.327,-20.584"]
"mpc001/end-to-end-lipreading" ["l"="26.397,-20.596"]
"afourast/deep_lip_reading" ["l"="26.381,-20.618"]
"Fengdalu/LipNet-PyTorch" ["l"="26.383,-20.594"]
"bshillingford/LipNet" ["l"="26.336,-20.539"]
"sailordiary/LipNet-PyTorch" ["l"="26.377,-20.565"]
"tstafylakis/Lipreading-ResNet" ["l"="26.381,-20.605"]
"joseph-zhong/LipReading" ["l"="26.368,-20.599"]
"hassanhub/LipReading" ["l"="26.363,-20.571"]
"Fengdalu/Lipreading-DenseNet3D" ["l"="26.388,-20.584"]
"ajinkyaT/Lip_Reading_in_the_Wild_AVSR" ["l"="26.399,-20.626"]
"voletiv/lipreading-in-the-wild-experiments" ["l"="26.34,-20.628"]
"adwin5/CNN-for-visual-speech-recognition" ["l"="26.344,-20.558"]
"alpinegizmo/geekier" ["l"="26.112,-20.284"]
"skynetim/skynet" ["l"="10.584,-28.45"]
"ottawaruby/whenbot" ["l"="26.151,-20.268"]
"cantino/huginn" ["l"="10.308,-34.21"]
"MLstate/PEPS" ["l"="-33.378,-25.487"]
"geekplux/markvis" ["l"="26.155,-20.502"]
"moinism/botui" ["l"="26.197,-20.504"]
"iogf/sukhoi" ["l"="26.182,-20.543"]
"icons8/titanic" ["l"="13.468,-34.207"]
"tj/gh-polls" ["l"="26.086,-20.487"]
"azat-co/practicalnode" ["l"="28.249,-19.51"]
"jaredreich/pell" ["l"="26.232,-26.328"]
"hilongjw/vue-recyclerview" ["l"="14.658,-8.625"]
"luosijie/vue-manager" ["l"="10.15,-12.416"]
"gitpoint/git-point" ["l"="24.905,-29.855"]
"rikschennink/fitty" ["l"="13.381,-34.196"]
"notifme/notifme-sdk" ["l"="28.597,-19.188"]
"geekplux/hexo-theme-typing" ["l"="-36.332,-14.925"]
"KingPixil/moon" ["l"="13.711,-34.14"]
"zhedongzheng/finch" ["l"="31.972,30.014"]
"Ma63d/kov-blog" ["l"="10.431,-9.147"]
"moinism/botui-examples" ["l"="26.22,-20.48"]
"Rich-Harris/butternut" ["l"="25.33,-26.506"]
"Bottr-js/Bottr" ["l"="31.33,27.62"]
"dthree/mailit" ["l"="28.591,-19.112"]
"charlestati/amplify" ["l"="28.796,-19.079"]
"jeromedalbert/real-world-react" ["l"="25.438,-26.42"]
"sruda/steroidesign" ["l"="28.615,-19.13"]
"pinterest/bonsai" ["l"="25.004,-26.547"]
"mplewis/src2png" ["l"="28.569,-19.187"]
"toish/chromatism" ["l"="28.483,-19.086"]
"KingPixil/wade" ["l"="28.677,-19.032"]
"stackimpact/stackimpact-python" ["l"="26.145,-20.619"]
"iogf/crocs" ["l"="26.154,-20.585"]
"paxos-bankchain/subconscious" ["l"="26.155,-20.663"]
"mikeywaites/kim" ["l"="26.111,-20.665"]
"csurfer/pyheatmagic" ["l"="21.56,29.005"]
"kristian-lange/net-glimpse" ["l"="26.135,-20.538"]
"DutchGraa/crackcoin" ["l"="43.842,-28.319"]
"gaojiuli/gain" ["l"="22.86,4.16"]
"ergonomica/ergonomica" ["l"="28.617,-18.623"]
"dmulholland/ivy" ["l"="22.766,4.397"]
"vividvilla/csvtotable" ["l"="26.122,-20.58"]
"kovidgoyal/html5-parser" ["l"="-12.982,25.409"]
"overshard/timestrap" ["l"="26.142,-20.444"]
"nerevu/riko" ["l"="13.526,24.134"]
"alexprengere/FormalSystems" ["l"="26.03,-20.358"]
"tjitze/RankPL" ["l"="26.02,-20.319"]
"khinsen/leibniz" ["l"="25.981,-20.363"]
"Quuxplusone/TNT" ["l"="26.013,-20.376"]
"AthenaModel/athena" ["l"="26,-20.338"]
"cameronfabbri/Colorful-Image-Colorization" ["l"="33.813,32.239"]
"derekeder/csv-to-html-table" ["l"="26.059,-20.596"]
"knadh/otpgateway" ["l"="-38.062,-18.776"]
"secretGeek/AwesomeCSV" ["l"="27.311,-19.395"]
"SimonBiggs/scriptedforms" ["l"="21.199,28.964"]
"metachris/logzero" ["l"="22.851,4.06"]
"anishathalye/seashells" ["l"="26.178,-20.603"]
"tomchristie/apistar" ["l"="22.785,3.997"]
"shridarpatil/frappe-go" ["l"="-38.052,-18.804"]
"knadh/dont.build" ["l"="-38.056,-18.757"]
"knadh/profiler" ["l"="26.092,-20.571"]
"csurfer/pypette" ["l"="22.922,4.34"]
"kennethreitz/maya" ["l"="22.828,4.072"]
"kennethreitz/background" ["l"="22.629,3.705"]
"astorfi/3D-convolutional-speaker-recognition" ["l"="0.415,39.718"]
"TimeChi/Lip_Reading_Competition" ["l"="26.391,-20.568"]
"Rudrabha/Lip2Wav" ["l"="34.977,31.61"]
"astorfi/TensorFlow-World-Resources" ["l"="23.385,31.205"]
"roudimit/MUSIC_dataset" ["l"="26.575,-20.705"]
"rhgao/co-separation" ["l"="26.488,-20.693"]
"YapengTian/AVE-ECCV18" ["l"="26.53,-20.67"]
"rohitrango/objects-that-sound" ["l"="26.561,-20.655"]
"rhgao/Deep-MIML-Network" ["l"="26.538,-20.724"]
"DTaoo/Discriminative-Sounding-Objects-Localization" ["l"="26.488,-20.677"]
"facebookresearch/sound-spaces" ["l"="26.538,-20.759"]
"ardasnck/learning_to_localize_sound_source" ["l"="26.503,-20.684"]
"hche11/Localizing-Visual-Sounds-the-Hard-Way" ["l"="26.538,-20.683"]
"atech/postal" ["l"="28.439,-19.169"]
"kshvmdn/fsql" ["l"="28.467,-19.144"]
"tannerlinsley/react-move" ["l"="25.416,-26.487"]
"google/tamperchrome" ["l"="28.431,-19.211"]
"Staffjoy/suite" ["l"="8.625,18.181"]
"kanishka-linux/reminiscence" ["l"="-32.854,-22.797"]
"karllhughes/side-project-marketing" ["l"="28.347,-19.309"]
"django-helpdesk/django-helpdesk" ["l"="25.894,1.208"]
"groveco/django-sql-explorer" ["l"="25.897,1.301"]
"adtac/commento" ["l"="26.425,-22.424"]
"hashedin/squealy" ["l"="13.917,24.047"]
"tebelorg/TA.Gui" ["l"="28.425,-18.907"]
"hche11/VGGSound" ["l"="26.541,-20.697"]
"YapengTian/AVVP-ECCV20" ["l"="26.561,-20.683"]
"gemengtju/Tutorial_Separation" ["l"="2.503,39.15"]
"jasongief/PSP_CVPR_2021" ["l"="26.55,-20.669"]
"facebookresearch/VisualVoice" ["l"="26.508,-20.63"]
"TaoRuijie/TalkNet_ASD" ["l"="26.458,-20.747"]
"facebookresearch/Listen-to-Look" ["l"="26.492,-20.75"]
"kyuyeonpooh/objects-that-sound" ["l"="26.598,-20.646"]
"Kajiyu/LLLNet" ["l"="26.593,-20.659"]
"Fengdalu/learn-an-effective-lip-reading-model-without-pains" ["l"="26.399,-20.608"]
"NirHeaven/D3D" ["l"="26.368,-20.614"]
"liuzhejun/XWbank_LipReading" ["l"="26.389,-20.533"]
"FesianXu/LipNet_ChineseWordsClassification" ["l"="26.4,-20.543"]
"georgesterpu/avsr-tf1" ["l"="26.381,-20.648"]
"georgesterpu/Taris" ["l"="26.368,-20.674"]
"VIPL-Audio-Visual-Speech-Understanding/learn-an-effective-lip-reading-model-without-pains" ["l"="26.416,-20.641"]
"ahaliassos/LipForensics" ["l"="34.874,31.215"]
"evanplaice/jquery-csv" ["l"="26.016,-20.608"]
"MounirMesselmeni/html-fileapi" ["l"="25.984,-20.618"]
"psyec1/Lipreading-PyTorch" ["l"="26.373,-20.583"]
"TaoRuijie/TalkNet-ASD" ["l"="26.422,-20.81"]
"SRA2/SPELL" ["l"="26.419,-20.837"]
"okankop/ASDNet" ["l"="26.439,-20.774"]
"X-LANCE/MSDWILD" ["l"="26.403,-20.823"]
"avivga/audio-visual-speech-enhancement" ["l"="26.57,-20.635"]
"bill9800/speech_separation" ["l"="26.535,-20.615"]
"mayurnewase/looking-to-listen-at-cocktail-party" ["l"="26.535,-20.636"]
"ubc-vision/TriBERT" ["l"="26.466,-20.709"]
"derekeder/FusionTable-Map-Template" ["l"="15.62,-30.417"]
"openaddresses/pyesridump" ["l"="25.411,29.758"]
"datamade/data-making-guidelines" ["l"="25.348,29.792"]
"themarshallproject/klaxon" ["l"="15.427,-30.502"]
"prajwalkr/vtp" ["l"="26.438,-20.58"]
"ahaliassos/raven" ["l"="26.446,-20.593"]
"yiminglin-ai/imdb-clean" ["l"="26.563,-20.713"]
"facebookresearch/VisualEchoes" ["l"="26.565,-20.77"]
"aluo-x/Learning_Neural_Acoustic_Fields" ["l"="26.551,-20.789"]
"facebookresearch/learning-audio-visual-dereverberation" ["l"="26.53,-20.793"]
"anton-jeran/FAST-RIR" ["l"="26.578,-20.805"]
"yakovmon/Real-Time-Audio-Visual-Speech-Enhancement" ["l"="26.597,-20.626"]
"anton-jeran/MESH2IR" ["l"="26.598,-20.813"]
"GAMMA-UMD/pygsound" ["l"="26.594,-20.838"]
"aispeech-lab/advr-avss" ["l"="26.551,-20.624"]
"zexupan/MuSE" ["l"="26.519,-20.608"]
"JusperLee/Looking-to-Listen-at-the-Cocktail-Party" ["l"="26.549,-20.599"]
"JusperLee/ExamOnline" ["l"="26.582,-20.585"]
"JusperLee/Dual-Path-RNN-Pytorch" ["l"="2.516,39.148"]
"JusperLee/Conv-TasNet" ["l"="2.527,39.132"]
"JusperLee/Calculate-SNR-SDR" ["l"="2.567,39.077"]
"JusperLee/Accelerator" ["l"="26.57,-20.595"]
"JusperLee/Deep-Learning" ["l"="26.581,-20.596"]
"JusperLee/ELF-SR" ["l"="26.573,-20.578"]
"JusperLee/GrabCut" ["l"="26.573,-20.604"]
"JusperLee/Grass" ["l"="26.571,-20.586"]
"JusperLee/Time" ["l"="26.562,-20.583"]
"JusperLee/player" ["l"="26.553,-20.576"]
"JusperLee/WeChatApp" ["l"="26.563,-20.573"]
"shvdiwnkozbw/Multi-Source-Sound-Localization" ["l"="26.472,-20.688"]
"adwin5/lipreading-by-convolutional-neural-network-keras" ["l"="26.347,-20.574"]
"DungLe13/lips-reading" ["l"="26.32,-20.651"]
"GeWu-Lab/OGM-GE_CVPR2022" ["l"="26.515,-20.521"]
"khazit/Lip2Word" ["l"="26.356,-20.635"]
"osalinasv/lipnet" ["l"="26.316,-20.624"]
"kagaminccino/LAVSE" ["l"="26.526,-20.575"]
"dr-pato/audio_visual_speech_enhancement" ["l"="26.529,-20.591"]
"GAMMA-UMD/GWA" ["l"="26.606,-20.858"]
"liuxubo717/LASS" ["l"="0.062,39.798"]
"audio-captioning/audio-captioning-papers" ["l"="-0.104,39.869"]
"yinkalario/General-Purpose-Sound-Recognition-Demo" ["l"="0.116,39.796"]
"microsoft/WavText5K" ["l"="0.087,39.768"]
"etzinis/heterogeneous_separation" ["l"="26.559,-20.728"]
"ibug-group/fpage" ["l"="26.585,-20.728"]
"pseeth/soundnet_keras" ["l"="26.466,-20.556"]
"Yuliang-Zou/tf_videogan" ["l"="26.474,-20.541"]
"yuwchen/CITISEN" ["l"="26.551,-20.551"]
"WilliamYu1993/ICSE" ["l"="2.602,39.255"]
"GeWu-Lab/CSOL_TPAMI2021" ["l"="26.496,-20.55"]
"GeWu-Lab/MUSIC-AVQA" ["l"="26.511,-20.544"]
"GenjiB/LAVISH" ["l"="26.53,-20.546"]
"WikiChao/Ego-AV-Loc" ["l"="26.543,-20.53"]
"ms-dot-k/Multi-head-Visual-Audio-Memory" ["l"="26.424,-20.658"]
"SheldonTsui/PseudoBinaural_CVPR2021" ["l"="26.486,-20.728"]
"fuankarion/active-speakers-context" ["l"="26.455,-20.772"]
"tuanchien/asd" ["l"="26.455,-20.794"]
"facebookresearch/visual-acoustic-matching" ["l"="26.527,-20.819"]
"georgesterpu/Sigmedia-AVSR" ["l"="26.393,-20.663"]
"FloretCat/CMRAN" ["l"="26.566,-20.669"]
"Yu-Wu/Modaily-Aware-Audio-Visual-Video-Parsing" ["l"="26.579,-20.677"]
"marmot-xy/CMBS" ["l"="26.594,-20.695"]
"MCG-NJU/JoMoLD" ["l"="26.597,-20.682"]
"pchao6/LSTM_PIT_Speech_Separation" ["l"="2.576,39.202"]
"funcwj/conv-tasnet" ["l"="2.519,39.165"]
"snsun/pit-speech-separation" ["l"="2.623,39.211"]
"iogf/lax" ["l"="26.108,-20.618"]
"googollee/yaif" ["l"="26.155,-20.242"]
"neeek2303/Lenta-Hackathon" ["l"="35.123,31.471"]
"strmpnk/hansei" ["l"="-7.025,12.542"]
"zkat/conserv" ["l"="26.008,-20.299"]
"aispeech-lab/oavss" ["l"="26.577,-20.618"]
}